{"text": "AR = ['/usr/bin/ar']\nARFLAGS = ['rcs']\nBINDIR = '/usr/local/bin'\nCC = ['/usr/bin/gcc']\nCCLNK_SRC_F = []\nCCLNK_TGT_F = ['-o']\nCC_NAME = 'gcc'\nCC_SRC_F = []\nCC_TGT_F = ['-c', '-o']\nCC_VERSION = ('4', '8', '4')\nCFLAGS = ['-I/home/cypher-thor/nicks/audio-programming/Minaton', '-DNDEBUG', '-fshow-column', '-std=c99']\nCFLAGS_MACBUNDLE = ['-fPIC']\nCFLAGS_cshlib = ['-fPIC']\nCHECKED_LV2_1_4_1 = 2\nCOMPILER_CC = 'gcc'\nCPPPATH_ST = '-I%s'\nCXXFLAGS = ['-I/home/cypher-thor/nicks/audio-programming/Minaton', '-DNDEBUG', '-fshow-column']\nDATADIR = '/usr/local/share'\nDEBUG = False\nDEFINES_LV2_1_4_1 = ['HAVE_LV2_1_4_1=1']\nDEFINES_ST = '-D%s'\nDEST_BINFMT = 'elf'\nDEST_CPU = 'x86_64'\nDEST_OS = 'linux'\nDOCDIR = '/usr/local/share/doc'\nDOCS = False\nHAVE_LV2_1_4_1 = 1\nINCLUDEDIR = '/usr/local/include'\nLIBDIR = '/usr/local/lib'\nLIBPATH_ST = '-L%s'\nLIB_ST = '-l%s'\nLINKFLAGS_MACBUNDLE = ['-bundle', '-undefined', 'dynamic_lookup']\nLINKFLAGS_cshlib = ['-shared']\nLINKFLAGS_cstlib = ['-Wl,-Bstatic']\nLINK_CC = ['/usr/bin/gcc']\nLV2DIR = '/usr/local/lib/lv2'\nMANDIR = '/usr/local/share/man'\nPARDEBUG = False\nPKGCONFIG = ['/usr/bin/pkg-config']\nPKG_lv2 = 'lv2'\nPREFIX = '/usr/local'\nRPATH_ST = '-Wl,-rpath,%s'\nSHLIB_MARKER = '-Wl,-Bdynamic'\nSONAME_ST = '-Wl,-h,%s'\nSTLIBPATH_ST = '-L%s'\nSTLIB_MARKER = '-Wl,-Bstatic'\nSTLIB_ST = '-l%s'\nSYSCONFDIR = '/usr/local/etc'\nVERSION_lv2 = '1.4.1'\ncprogram_PATTERN = '%s'\ncshlib_PATTERN = 'lib%s.so'\ncstlib_PATTERN = 'lib%s.a'\nmacbundle_PATTERN = '%s.bundle'\n"}
{"text": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Bring in all of the public TensorFlow interface into this module.\"\"\"\n\nfrom __future__ import absolute_import as _absolute_import\nfrom __future__ import division as _division\nfrom __future__ import print_function as _print_function\n\nimport os as _os\n\n# pylint: disable=g-bad-import-order\n\n# API IMPORTS PLACEHOLDER\n\nfrom tensorflow.python.tools import component_api_helper as _component_api_helper\n_component_api_helper.package_hook(\n    parent_package_str=__name__,\n    child_package_str=(\n        'tensorflow_estimator.python.estimator.api._v1.estimator'))\n_component_api_helper.package_hook(\n    parent_package_str=__name__,\n    child_package_str=('tensorflow.python.keras.api._v1.keras'))\nfrom tensorflow.python.platform import flags  # pylint: disable=g-import-not-at-top\napp.flags = flags  # pylint: disable=undefined-variable\n"}
{"text": "import logging\n\nclass TransferFunction(object):\n    '''\n    Base (and abstract) class to provide TFs in python programming language.\n\n    Usage: python TF implementations must extend this class and provide, as a minimum,\n    the implementation of the transfer method.\n\n    It is the python equivalent of the JavaTransferExecutor class\n    '''\n    def __init__(self, asceId, asceRunningId, validityTimeFrame, props, instance):\n        '''\n        Constructor\n\n        :param asceId: The ID of the ASCE that runs the python TF\n        :param asceRunningId: The running ID of the ASCE that runs the python TF\n        :param validityTimeFrame: The validity time frame (long)\n        :param props: a dictionary of properties\n        '''\n        assert asceId is not None and asceId!=\"\", \"Invalid ID of ASCE\"\n        self.asceID=asceId\n        logging.debug(\"Building python TF for ASCE %s\",self.asceID)\n        assert asceRunningId is not None and asceRunningId!=\"\", \"Invalid running ID of ASCE\"\n        self.asceRunningId = asceRunningId\n        assert validityTimeFrame>=0, \"Invalid validity time frame \"+validityTimeFrame\n        self.validityTimeFrame=validityTimeFrame\n        if props is None:\n            self.props = {}\n        else:\n            assert isinstance(props,dict)\n            self.props=props\n        self.instance = None\n\n        if instance is not None:\n            assert isinstance(instance,int), \"The instance must be an integer\"\n        self.instance = instance\n\n        logging.info(\"Python TF of %s successfully built\",self.asceRunningId)\n\n    def setTemplateInstance(self, instance):\n        '''\n        Set the instance of the template, if any.\n\n        :param instance: the instance number or None if there is no template\n        :return:\n        '''\n        self.instance=instance\n        if (self.instance is None):\n            logging.debug(\"Python TF of %s is NOT templated\",self.asceRunningId)\n        else:\n            logging.info(\"Python TF of %s has template %d\",self.asceRunningId,self.instance)\n\n    def isTemplated(self):\n        '''\n\n        :return: the number of the instance or NOne if not template\n        '''\n        return self.instance is not None\n\n    def shutdown(self):\n        '''\n        Last method called when the object life terminates.\n        It is usually called to free acquired resources.\n\n        :return:\n        '''\n        pass\n\n    def initialize(self, inputsInfo, outputInfo):\n        '''\n        Initialize the TF.\n\n        Must be overridden if the user provided implementation needs\n        to know the ID and type of the inputs and the output.\n        It iusuall implemented to increase the robustness for example\n        if the user implemented TF compare the value of the input with a threshold,\n        it can be used to check that the input is a numeric type.\n\n        :param inputsInfo: the list of IasioInfo with the ids and type of inputs\n        :param outputInfo: the type and ID of the output\n        :return: None\n        '''\n        pass\n\n    def eval(self,compInputs, actualOutput):\n        '''\n        The eval method to produce the output based on the value of the inputs\n\n        :param compInputs: computing element inputs (IASIOs)\n        :param actualOutput: the actual value of the output i.e. tha value computed at previous\n                             iteration (IASIO)\n        :return: the new output of the ASCE (IASIO)\n        '''\n        raise NotImplementedError('Python TF implementation missing')\n\n    def getValue(self, inputs, id, instance):\n        '''\n        Get a value from its ID taking into account templates\n\n        It is the same method present in the JavaTransferExecutor and in ScalaTransferExecutor\n\n        :param inputs: the map of inputs as received in the eval method\n        :param id: the id (string) of the input (without template)\n        :param instance: the instance (int) or None if not templated\n        :return: the value or None if not found\n        '''\n        if (inputs is None or id is None):\n            raise ValueError(\"Maps of input and id can't be None\")\n        assert isinstance(input,dict), \"Map of inputs expected\"\n        assert isinstance(id,str), \"The ID must be a string\"\n\n        if instance is not None:\n            assert isinstance(instance,int), \"The instance must be an integer (instead of \"+instance+\")\"\n            postFix= \"[!#\"+str(instance)+\"!]\"\n        else:\n            postFix = \"\"\n\n        idToSearch = id + postFix\n\n        return inputs.get(idToSearch)"}
{"text": "# Copyright (c) 2007-2008 The PyAMF Project.\n# See LICENSE for details.\n\n\"\"\"\nGateway for the Django framework.\n\nThis gateway allows you to expose functions in Django to AMF clients and\nservers.\n\n@see: U{Django homepage (external)<http://djangoproject.com>}\n\n@author: U{Arnar Birgisson<mailto:arnarbi@gmail.com>}\n\n@since: 0.1.0\n\"\"\"\n\ndjango = __import__('django')\nhttp = django.http\n\nimport pyamf\nfrom pyamf import remoting\nfrom pyamf.remoting import gateway\n\n__all__ = ['DjangoGateway']\n\nclass DjangoGateway(gateway.BaseGateway):\n    \"\"\"\n    An instance of this class is suitable as a Django view.\n\n    An example usage would be through C{urlconf}::\n\n        from django.conf.urls.defaults import *\n\n        urlpatterns = patterns('',\n            (r'^gateway/', 'yourproject.yourapp.gateway.gw_instance'),\n        )\n\n    where C{yourproject.yourapp.gateway.gw_instance} refers to an instance of\n    this class.\n\n    @ivar expose_request: The standard Django view always has the request\n        object as the first parameter. To disable this functionality, set this\n        to C{False}.\n    @type expose_request: C{bool}\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        kwargs['expose_request'] = kwargs.get('expose_request', True)\n\n        gateway.BaseGateway.__init__(self, *args, **kwargs)\n\n    def getResponse(self, http_request, request):\n        \"\"\"\n        Processes the AMF request, returning an AMF response.\n\n        @param http_request: The underlying HTTP Request.\n        @type http_request: C{HTTPRequest<django.core.http.HTTPRequest>}\n        @param request: The AMF Request.\n        @type request: L{Envelope<pyamf.remoting.Envelope>}\n        @rtype: L{Envelope<pyamf.remoting.Envelope>}\n        @return: The AMF Response.\n        \"\"\"\n        response = remoting.Envelope(request.amfVersion, request.clientType)\n\n        for name, message in request:\n            processor = self.getProcessor(message)\n            response[name] = processor(message, http_request=http_request)\n\n        return response\n\n    def __call__(self, http_request):\n        \"\"\"\n        Processes and dispatches the request.\n\n        @param http_request: The C{HTTPRequest} object.\n        @type http_request: C{HTTPRequest}\n        @return: The response to the request.\n        @rtype: C{HTTPResponse}\n        \"\"\"\n        if http_request.method != 'POST':\n            return http.HttpResponseNotAllowed(['POST'])\n\n        context = pyamf.get_context(pyamf.AMF0)\n        stream = None\n        http_response = http.HttpResponse()\n\n        # Decode the request\n        try:\n            request = remoting.decode(http_request.raw_post_data, context)\n        except pyamf.DecodeError:\n            self.logger.debug(gateway.format_exception())\n            http_response.status_code = 400\n\n            return http_response\n\n        self.logger.debug(\"AMF Request: %r\" % request)\n\n        # Process the request\n        try:\n            response = self.getResponse(http_request, request)\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.logger.debug(gateway.format_exception())\n\n            return http.HttpResponseServerError()\n\n        self.logger.debug(\"AMF Response: %r\" % response)\n\n        # Encode the response\n        try:\n            stream = remoting.encode(response, context)\n        except pyamf.EncodeError:\n            self.logger.debug(gateway.format_exception())\n\n            return http.HttpResponseServerError('Unable to encode the response')\n\n        buf = stream.getvalue()\n        http_response['Content-Type'] = remoting.CONTENT_TYPE\n        http_response['Content-Length'] = str(len(buf))\n        http_response.write(buf)\n\n        return http_response\n"}
{"text": "#!/usr/bin/env python\n#=========================================================================\n# This is OPEN SOURCE SOFTWARE governed by the Gnu General Public\n# License (GPL) version 3, as described at www.opensource.org.\n# Copyright (C)2017 William H. Majoros (martiandna@gmail.com).\n#=========================================================================\nfrom __future__ import (absolute_import, division, print_function, \n   unicode_literals, generators, nested_scopes, with_statement)\nfrom builtins import (bytes, dict, int, list, object, range, str, ascii,\n   chr, hex, input, next, oct, open, pow, round, super, filter, map, zip)\n# The above imports should allow this program to run in both Python 2 and\n# Python 3.  You might need to update your version of module \"future\".\nfrom SlurmWriter import SlurmWriter\n\nTHOUSAND=\"/home/bmajoros/1000G/assembly\"\nGEUVADIS=THOUSAND+\"/geuvadis.txt\"\nSLURM_DIR=THOUSAND+\"/intron-slurms\"\nJOB_NAME=\"INTRON\"\nMAX_PARALLEL=1000\nNICE=500\nMEMORY=0\nTHREADS=0\n\n#=========================================================================\n# main()\n#=========================================================================\n\ndirs=[]\nwith open(GEUVADIS,\"rt\") as IN:\n    for line in IN:\n        id=line.rstrip()\n        dir=THOUSAND+\"/combined/\"+id\n        dirs.append(dir)\n\nwriter=SlurmWriter()\nfor dir in dirs:\n    writer.addCommand(\"cd \"+dir+\"/RNA3\\n\"+\n                      THOUSAND+\"/src/get-intron-retentions.py \"+\n                      \"../1.gff ../2.gff ../1.lengths ../2.lengths \"+\n                      \"depth.txt.gz > IR.txt\\n\"\n                      )\nwriter.setQueue(\"new,all\")\nwriter.nice(NICE)\nif(MEMORY): writer.mem(MEMORY)\nif(THREADS): writer.threads(THREADS)\nwriter.writeArrayScript(SLURM_DIR,JOB_NAME,MAX_PARALLEL)\n\n\n"}
{"text": "\"\"\"Test parameters description\"\"\"\n\nimport pytest\n\nfrom ocelot import *\n\n\"\"\"lattice elements descripteion\"\"\"\n\nQ1 = Quadrupole(l=0.4, k1=-1.3, eid=\"Q1\")\nQ2 = Quadrupole(l=0.8, k1=1.4, eid=\"Q2\")\nQ3 = Quadrupole(l=0.4, k1=-1.7, eid=\"Q3\")\nQ4 = Quadrupole(l=0.5, k1=1.19250444829, eid=\"Q4\")\n\nB = Bend(l=2.7, k1=-.06, angle=2*pi/16., e1=pi/16., e2=pi/16., eid= \"B\")\n\nSF = Sextupole(l=0.01, k2=150.0, eid=\"SF\") #random value\nSD = Sextupole(l=0.01, k2=-150.0, eid=\"SD\") #random value\n\nD1 = Drift(l=2., eid= \"D1\")\nD2 = Drift(l=0.6, eid= \"D2\")\nD3 = Drift(l=0.3, eid= \"D3\")\nD4 = Drift(l=0.7, eid= \"D4\")\nD5 = Drift(l=0.9, eid= \"D5\")\nD6 = Drift(l=0.2, eid= \"D6\")\n\n\n\"\"\"pytest fixtures description\"\"\"\n\n@pytest.fixture(scope='module')\ndef cell():\n    return (D1, Q1, D2, Q2, D3, Q3, D4, B, D5, SD, D5, SF, D6, Q4, D6, SF, D5, SD,D5, B, D4, Q3, D3, Q2, D2, Q1, D1)\n\n\n@pytest.fixture(scope='module')\ndef method():\n    return MethodTM()\n\n\n@pytest.fixture(scope='module')\ndef lattice(cell, method):\n    return MagneticLattice(cell, method=method)\n"}
{"text": "# coding: utf-8\nfrom __future__ import unicode_literals\n\nfrom .common import InfoExtractor\nfrom ..compat import compat_str\nfrom ..utils import (\n\tExtractorError,\n\tint_or_none,\n\tfloat_or_none,\n\tsmuggle_url,\n)\n\n\nclass NineNowIE(InfoExtractor):\n\tIE_NAME = '9now.com.au'\n\t_VALID_URL = r'https?://(?:www\\.)?9now\\.com\\.au/(?:[^/]+/){2}(?P<id>[^/?#]+)'\n\t_GEO_COUNTRIES = ['AU']\n\t_TESTS = [{\n\t\t# clip\n\t\t'url': 'https://www.9now.com.au/afl-footy-show/2016/clip-ciql02091000g0hp5oktrnytc',\n\t\t'md5': '17cf47d63ec9323e562c9957a968b565',\n\t\t'info_dict': {\n\t\t\t'id': '16801',\n\t\t\t'ext': 'mp4',\n\t\t\t'title': 'St. Kilda\\'s Joey Montagna on the potential for a player\\'s strike',\n\t\t\t'description': 'Is a boycott of the NAB Cup \"on the table\"?',\n\t\t\t'uploader_id': '4460760524001',\n\t\t\t'upload_date': '20160713',\n\t\t\t'timestamp': 1468421266,\n\t\t},\n\t\t'skip': 'Only available in Australia',\n\t}, {\n\t\t# episode\n\t\t'url': 'https://www.9now.com.au/afl-footy-show/2016/episode-19',\n\t\t'only_matching': True,\n\t}, {\n\t\t# DRM protected\n\t\t'url': 'https://www.9now.com.au/andrew-marrs-history-of-the-world/season-1/episode-1',\n\t\t'only_matching': True,\n\t}]\n\tBRIGHTCOVE_URL_TEMPLATE = 'http://players.brightcove.net/4460760524001/default_default/index.html?videoId=%s'\n\n\tdef _real_extract(self, url):\n\t\tdisplay_id = self._match_id(url)\n\t\twebpage = self._download_webpage(url, display_id)\n\t\tpage_data = self._parse_json(self._search_regex(\n\t\t\tr'window\\.__data\\s*=\\s*({.*?});', webpage,\n\t\t\t'page data'), display_id)\n\n\t\tfor kind in ('episode', 'clip'):\n\t\t\tcurrent_key = page_data.get(kind, {}).get(\n\t\t\t\t'current%sKey' % kind.capitalize())\n\t\t\tif not current_key:\n\t\t\t\tcontinue\n\t\t\tcache = page_data.get(kind, {}).get('%sCache' % kind, {})\n\t\t\tif not cache:\n\t\t\t\tcontinue\n\t\t\tcommon_data = (cache.get(current_key) or list(cache.values())[0])[kind]\n\t\t\tbreak\n\t\telse:\n\t\t\traise ExtractorError('Unable to find video data')\n\n\t\tvideo_data = common_data['video']\n\n\t\tif video_data.get('drm'):\n\t\t\traise ExtractorError('This video is DRM protected.', expected=True)\n\n\t\tbrightcove_id = video_data.get('brightcoveId') or 'ref:' + video_data['referenceId']\n\t\tvideo_id = compat_str(video_data.get('id') or brightcove_id)\n\t\ttitle = common_data['name']\n\n\t\tthumbnails = [{\n\t\t\t'id': thumbnail_id,\n\t\t\t'url': thumbnail_url,\n\t\t\t'width': int_or_none(thumbnail_id[1:])\n\t\t} for thumbnail_id, thumbnail_url in common_data.get('image', {}).get('sizes', {}).items()]\n\n\t\treturn {\n\t\t\t'_type': 'url_transparent',\n\t\t\t'url': smuggle_url(\n\t\t\t\tself.BRIGHTCOVE_URL_TEMPLATE % brightcove_id,\n\t\t\t\t{'geo_countries': self._GEO_COUNTRIES}),\n\t\t\t'id': video_id,\n\t\t\t'title': title,\n\t\t\t'description': common_data.get('description'),\n\t\t\t'duration': float_or_none(video_data.get('duration'), 1000),\n\t\t\t'thumbnails': thumbnails,\n\t\t\t'ie_key': 'BrightcoveNew',\n\t\t}\n"}
{"text": "from django.shortcuts import render_to_response\nfrom django.http import HttpResponseRedirect\nfrom django.template import RequestContext\nfrom django.utils.datastructures import SortedDict\nfrom instance.models import Host\nfrom webvirtmgr.server import ConnServer\nfrom dashboard.forms import HostAddTcpForm, HostAddSshForm\n\n\ndef sort_host(hosts):\n    \"\"\"\n\n    Sorts dictionary of hosts by key\n\n    \"\"\"\n    if hosts:\n        sorted_hosts = []\n        for host in sorted(hosts.iterkeys()):\n            sorted_hosts.append((host, hosts[host]))\n        return SortedDict(sorted_hosts)\n\n\ndef index(request):\n    \"\"\"\n\n    Index page.\n\n    \"\"\"\n    if not request.user.is_authenticated():\n        return HttpResponseRedirect('/login')\n    else:\n        return HttpResponseRedirect('/dashboard')\n\n\ndef dashboard(request):\n    \"\"\"\n\n    Dashboard page.\n\n    \"\"\"\n    if not request.user.is_authenticated():\n        return HttpResponseRedirect('/login')\n\n    def get_hosts_status(hosts):\n        \"\"\"\n\n        Function return all hosts all vds on host\n\n        \"\"\"\n        all_hosts = {}\n        for host in hosts:\n            try:\n                import socket\n                socket_host = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                socket_host.settimeout(1)\n                if host.type == 'ssh':\n                    socket_host.connect((host.hostname, host.port))\n                else:\n                    socket_host.connect((host.hostname, 16509))\n                socket_host.close()\n                status = 1\n            except Exception as err:\n                status = err\n            all_hosts[host.id] = (host.name, host.hostname, status)\n        return all_hosts\n\n    hosts = Host.objects.filter()\n    hosts_info = get_hosts_status(hosts)\n    form = None\n\n    if request.method == 'POST':\n        if 'host_del' in request.POST:\n            del_host = Host.objects.get(id=request.POST.get('host_id', ''))\n            del_host.delete()\n            return HttpResponseRedirect(request.get_full_path())\n        if 'host_tcp_add' in request.POST:\n            form = HostAddTcpForm(request.POST)\n            if form.is_valid():\n                data = form.cleaned_data\n                new_host = Host(name=data['name'],\n                                hostname=data['hostname'],\n                                type='tcp',\n                                login=data['login'],\n                                password=data['password1']\n                                )\n                new_host.save()\n                return HttpResponseRedirect(request.get_full_path())\n        if 'host_ssh_add' in request.POST:\n            form = HostAddSshForm(request.POST)\n            if form.is_valid():\n                data = form.cleaned_data\n                new_host = Host(name=data['name'],\n                                hostname=data['hostname'],\n                                type='ssh',\n                                port=data['port'],\n                                login=data['login']\n                                )\n                new_host.save()\n                return HttpResponseRedirect(request.get_full_path())\n\n    hosts_info = sort_host(hosts_info)\n\n    return render_to_response('dashboard.html', {'hosts_info': hosts_info,\n                                                 'form': form,\n                                                 },\n                              context_instance=RequestContext(request))\n\n\ndef infrastructure(request):\n    \"\"\"\n\n    Infrastructure page.\n\n    \"\"\"\n    if not request.user.is_authenticated():\n        return HttpResponseRedirect('/login')\n\n    hosts = Host.objects.filter().order_by('id')\n    hosts_vms = {}\n    host_info = None\n    host_mem = None\n\n    for host in hosts:\n        try:\n            import socket\n            socket_host = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            socket_host.settimeout(1)\n            if host.type == 'ssh':\n                socket_host.connect((host.hostname, host.port))\n            else:\n                socket_host.connect((host.hostname, 16509))\n            socket_host.close()\n            status = 1\n        except:\n            status = 2\n\n        if status == 1:\n            conn = ConnServer(host)\n            host_info = conn.node_get_info()\n            host_mem = conn.memory_get_usage()\n            hosts_vms[host.id, host.name, status, host_info[2], host_mem[0], host_mem[2]] = conn.vds_on_cluster()\n        else:\n            hosts_vms[host.id, host.name, status, None, None, None] = None\n\n    for host in hosts_vms:\n        hosts_vms[host] = sort_host(hosts_vms[host])\n    hosts_vms = sort_host(hosts_vms)\n\n    return render_to_response('infrastructure.html', {'hosts_info': host_info,\n                                                      'host_mem': host_mem,\n                                                      'hosts_vms': hosts_vms,\n                                                      'hosts': hosts\n                                                      },\n                              context_instance=RequestContext(request))\n\n\ndef page_setup(request):\n    return render_to_response('setup.html', {}, context_instance=RequestContext(request))\n"}
{"text": "class Person:                               # Portable: 2.X or 3.X\n    def __init__(self, name):               # On [Person()]\n        self._name = name                   # Triggers __setattr__!\n\n    def __getattribute__(self, attr):                 # On [obj.any]\n        print('get: ' + attr)\n        if attr == 'name':                            # Intercept all names\n            attr = '_name'                            # Map to internal name\n        return object.__getattribute__(self, attr)    # Avoid looping here\n\n    def __setattr__(self, attr, value):     # On [obj.any = value]\n        print('set: ' + attr)\n        if attr == 'name':\n            attr = '_name'                  # Set internal name\n        self.__dict__[attr] = value         # Avoid looping here\n\n    def __delattr__(self, attr):            # On [del obj.any]\n        print('del: ' + attr)\n        if attr == 'name':\n            attr = '_name'                  # Avoid looping here too\n        del self.__dict__[attr]             # but much less common\n\nbob = Person('Bob Smith')           # bob has a managed attribute\nprint(bob.name)                     # Runs __getattr__\nbob.name = 'Robert Smith'           # Runs __setattr__\nprint(bob.name)\ndel bob.name                        # Runs __delattr__\n\nprint('-'*20)\nsue = Person('Sue Jones')           # sue inherits property too\nprint(sue.name)\n#print(Person.name.__doc__)         # No equivalent here\n"}
{"text": "from wfuzz.externals.moduleman.plugin import moduleman_plugin\nfrom wfuzz.plugin_api.payloadtools import ShodanIter\nfrom wfuzz.plugin_api.base import BasePayload\nfrom wfuzz.fuzzobjects import FuzzWordType\n\n\n@moduleman_plugin\nclass shodanp(BasePayload):\n    name = \"shodanp\"\n    author = (\"Xavi Mendez (@xmendez)\",)\n    version = \"0.1\"\n    description = (\"Queries the Shodan API\",)\n\n    summary = \"Returns URLs of a given Shodan API search (needs api key).\"\n    category = [\"default\"]\n    priority = 99\n\n    parameters = (\n        (\"search\", \"\", True, \"Shodan search string.\"),\n        (\"page\", \"0\", False, \"Offset page, starting at zero.\"),\n        (\n            \"limit\",\n            \"0\",\n            False,\n            \"Number of pages (1 query credit = 100 results). Zero for all.\",\n        ),\n    )\n\n    default_parameter = \"search\"\n\n    def __init__(self, params):\n        BasePayload.__init__(self, params)\n\n        search = params[\"search\"]\n        page = int(params[\"page\"])\n        limit = int(params[\"limit\"])\n\n        self._it = ShodanIter(search, page, limit)\n\n    def count(self):\n        return -1\n\n    def close(self):\n        self._it._stop()\n\n    def get_type(self):\n        return FuzzWordType.WORD\n\n    def get_next(self):\n        match = next(self._it)\n\n        port = match[\"port\"]\n        scheme = \"https\" if \"ssl\" in match or port == 443 else \"http\"\n\n        if match[\"hostnames\"]:\n            for hostname in match[\"hostnames\"]:\n                return \"{}://{}:{}\".format(scheme, hostname, port)\n        else:\n            return \"{}://{}:{}\".format(scheme, match[\"ip_str\"], port)\n"}
{"text": "'''\n\t\n    Use Case:  Exploring the Concept of Serializing and De-Serializing in Python\n\n    Shortly called as pickling, the needs is to store the object and later resurrect it \n\n        ----> Use dump for serializing the records into a file\n        ----> Use dumps for serializing the records into a string\n        ----> Use load for serializing the records from a file\n        ----> Use loads for serializing the records from a string\n\n'''\n\n\n'''\n    Module needed for pickling  \n'''\n\nimport pickle\n\n'''\n    Date and time \n'''\nimport datetime\n\n\n\n'''\n    Lets serialize some basic data-types\n'''\n\n\n# Define a Dictionary\n\npickleme = {}\n\n'''\n    To this dictionary let's add several data types such as list, tuple, dictionary,int,str,float,boolean etc..\n'''\n\npickleme['name'] = 'Einstein'\npickleme['type'] = True\npickleme['number']= 10000\npickleme[100] = 10000\npickleme['more'] = [100.12,700.90,800.22,900.20,300.6,1000,789,679,27,\"21N\"]\npickleme['data'] = [100,200,300,400,500,\"Python\",\"Javascript\",\"C\",\"Java\",True,False,None]\npickleme['info'] = (100,200,300,400,500,\"Python\",\"Javascript\",\"C\",\"Java\",True,False,None)\npickleme['currtime'] = datetime.datetime.now()\n\n'''\n    A Simple Class\n'''\n\n# New Style\nclass Me(object):\n\n    hello = 'world'\n\n    def simple_func(self):\n\n        return \"I belong to class Me\"\n    \n\n# Old Style\nclass Python :pass\n\n''' \n    \n    Please remember that none of the class attributes will be restored while unpickling .\n    In this case hello = 'world' will not be available while deserializing\n    There are ways to do it but...\n\n'''\n\npickleme['newstyleclass'] = Me\npickleme['oldstyleclass'] = Python\n\n\n\n'''\n    Lets pickle this information to a file\n'''\n\nif __name__ ==\"__main__\":\n\n    \n    ''' \n         Step 1 :: Serialize it - load into some place\n\n    '''\n    with open('pickleme.pickle','wb') as writer:\n        \n        '''  \n            Use dump for serializing the records into a file\n            Use dumps for serializing the records into a string\n        '''\n        \n        pickle.dump(pickleme,writer)\n\n\n    ''' \n         Step 2 :: De-Serialize it - load from some place\n\n    '''\n\n    with open('pickleme.pickle','rb') as reader:\n\n        '''  \n            Use load for serializing the records from a file\n            Use loads for serializing the records from a string\n        '''\n\n\n        ''' \n\n            When we load it actually returns a dictionary, hence we can  iterate on it \n\n            Iterate the dictionary and check for the items, if they are restored properly and do something more useful    \n\n        '''\n\n        getdata = pickle.load(reader)\n\n        for key,val in getdata.items():\n\n            print key,val,'\\n'\n\n\n\n        ''' Now lets pick the class object Me from the returned dict and analyse more '''\n        myclass = getdata['newstyleclass']\n        print myclass.__dict__\nn\n"}
{"text": "#!/usr/bin/env python3\n\n\n# Find videos with duplicates: SELECT Track.track_name, Artist.artist_name,Track.youtube_link FROM Track JOIN Artist WHERE Track.artist_id = Artist.id GROUP BY Track.youtube_link HAVING count(*) >=2 \n\nfrom PlaylistDatabase import PlaylistDatabase\n\ndb = PlaylistDatabase(config_file='PlaylistDatabaseConfig.ini')\n\nvideo = input('Enter the video ID: ')\nif video.startswith('https://youtu.be/'):\n    pass\nelif video.startswith('https://www.youtube.com/watch?v='):\n    video.replace('https://www.youtube.com/watch?v=','https://youtu.be/')\nelse:\n    video = 'https://youtu.be/'+video\ndb._cur.execute('''SELECT Track.id, Track.track_name, Artist.artist_name, Album.album_name, Track.artist_id, Track.album_id, Track.youtube_link from Track JOIN Artist JOIN Album WHERE Track.youtube_link=%s AND Track.album_id=Album.id AND Track.artist_id=Artist.id''',(video,))\n\ntrack = db._cur.fetchall()\n\nif len(track) > 1:\n    print('\\nWARNING: More than one track has the same video.\\n')\n\n    for ii,t in enumerate(track):\n        print\n\n        track_id,track_name,artist_name,album_name,artist_id,album_id,youtube_link = track[ii]\n\n        print('Track '+str(ii)+' is: ',track_id,track_name,artist_name,album_name,artist_id,album_id, youtube_link)\n\n    ii=int(input('\\nWhat track do you want to use? '))\n\nelse:\n    ii=0\n\ntrack_id,track_name,artist_name,album_name,artist_id,album_id,youtube_link = track[ii]\n\nprint('Track '+str(ii)+' is: ',track_id,track_name,artist_name,album_name,artist_id,album_id, youtube_link)\n\n#yesorno = input('Do you want to delete this track and add it to the ignore lists? (yes/no): ')\nyesorno='no'\nif yesorno.lower()=='yes':\n\n    db._cur.execute('''SELECT Playlist.*,Station.* FROM Playlist JOIN Station WHERE Playlist.track_id=%s AND Playlist.station_id=Station.id''',(track_id,))\n    \n    stations = db._cur.fetchall()\n    \n    unique_station = {}\n    \n    for s in stations:\n        playlist_entry_id, track_id, pl_station_id,playtime,station_id,station_name,station_url,ignore_artists,ignore_titles,playlist_url = s\n            \n        unique_station[station_id] = (station_name,station_url,ignore_artists,ignore_titles,playlist_url)\n        \n    print(unique_station)\n    \n    for id in unique_station:\n        exec('ignore_artists = ' + unique_station[id][2])\n        exec('ignore_titles = ' + unique_station[id][3])\n        if artist_name not in ignore_artists:\n            ignore_artists.append(artist_name)\n        if track_name not in ignore_titles:\n            ignore_titles.append(track_name)\n        unique_station[id] = unique_station[id][0],unique_station[id][1],str(ignore_artists),str(ignore_titles),unique_station[id][4]\n        db._cur.execute('''\n        UPDATE Station\n        SET ignore_artists=%s, ignore_titles=%s\n        WHERE Station.id=%s\n        ''',(str(ignore_artists),str(ignore_titles), id))\n        db._conn.commit()\n            \n    print(unique_station)\n    \n    \n    # Get all tracks with the matching artist id and album id\n    all_tracks = []\n    db._cur.execute('''SELECT Track.id FROM Track WHERE Track.album_id=%s AND Track.artist_id=%s''',(album_id,artist_id))\n    for id in db._cur.fetchall():\n        if id not in all_tracks:\n            all_tracks.append(id[0])\n\n    for id in all_tracks:\n        # Remove the station entries\n        db._cur.execute('''DELETE FROM Playlist WHERE Playlist.track_id=%s''',(id,))\n        # Remove the track entries\n        db._cur.execute('''DELETE FROM Track WHERE Track.id=%s''',(id,))\n    \n    # Remove the album entries\n    db._cur.execute('''DELETE FROM Album WHERE Album.id=%s''',(album_id,))\n    \n    # Remove the artist entries\n    db._cur.execute('''DELETE FROM Artist WHERE Artist.id=%s''',(artist_id,))\n    \n    db._conn.commit()\n    #Tracks = db._cur.fetchall()\nelse:\n    #yesorno = input('Do you want to update the youtube URL for this track? (yes/no): ')\n    yesorno='yes'\n    if yesorno.lower() == 'yes':\n        url = input('Enter the new youtube url: ')\n        \n        if url == '':\n            print('No URL Specified... Exiting.')\n        else:\n            if url.startswith('https://youtu.be/'):\n                pass\n            elif url.startswith('https://www.youtube.com/watch?v='):\n                url.replace('https://www.youtube.com/watch?v=','https://youtu.be/')\n            else:\n                url = 'https://youtu.be/'+url                   \n    \n            db._cur.execute('''\n            UPDATE Track\n            SET youtube_link=%s\n            WHERE Track.id=%s\n            ''',(url,track_id))\n            db._conn.commit()\n\n    else:\n        print('Not modifying database.')\n"}
{"text": "\"\"\"\nUnit tests for trust-region optimization routines.\n\nTo run it in its simplest form::\n  nosetests test_optimize.py\n\n\"\"\"\nfrom __future__ import division, print_function, absolute_import\n\nimport numpy as np\nfrom numpy.testing import (TestCase, assert_, assert_equal, assert_allclose,\n                           run_module_suite)\nfrom scipy.optimize import (minimize, rosen, rosen_der, rosen_hess,\n                            rosen_hess_prod)\n\n\nclass Accumulator:\n    \"\"\" This is for testing callbacks.\"\"\"\n\n    def __init__(self):\n        self.count = 0\n        self.accum = None\n\n    def __call__(self, x):\n        self.count += 1\n        if self.accum is None:\n            self.accum = np.array(x)\n        else:\n            self.accum += x\n\n\nclass TestTrustRegionSolvers(TestCase):\n    def setUp(self):\n        self.x_opt = [1.0, 1.0]\n        self.easy_guess = [2.0, 2.0]\n        self.hard_guess = [-1.2, 1.0]\n\n    def test_dogleg_accuracy(self):\n        # test the accuracy and the return_all option\n        x0 = self.hard_guess\n        r = minimize(rosen, x0, jac=rosen_der, hess=rosen_hess, tol=1e-8,\n                     method='dogleg', options={'return_all': True}, )\n        assert_allclose(x0, r['allvecs'][0])\n        assert_allclose(r['x'], r['allvecs'][-1])\n        assert_allclose(r['x'], self.x_opt)\n\n    def test_dogleg_callback(self):\n        # test the callback mechanism and the maxiter and return_all options\n        accumulator = Accumulator()\n        maxiter = 5\n        r = minimize(rosen, self.hard_guess, jac=rosen_der, hess=rosen_hess,\n                     callback=accumulator, method='dogleg',\n                     options={'return_all': True, 'maxiter': maxiter}, )\n        assert_equal(accumulator.count, maxiter)\n        assert_equal(len(r['allvecs']), maxiter + 1)\n        assert_allclose(r['x'], r['allvecs'][-1])\n        assert_allclose(sum(r['allvecs'][1:]), accumulator.accum)\n\n    def test_solver_concordance(self):\n        # Assert that dogleg uses fewer iterations than ncg on the Rosenbrock\n        # test function, although this does not necessarily mean\n        # that dogleg is faster or better than ncg even for this function\n        # and especially not for other test functions.\n        f = rosen\n        g = rosen_der\n        h = rosen_hess\n        for x0 in (self.easy_guess, self.hard_guess):\n            r_dogleg = minimize(f, x0, jac=g, hess=h, tol=1e-8,\n                                method='dogleg', options={'return_all': True})\n            r_trust_ncg = minimize(f, x0, jac=g, hess=h, tol=1e-8,\n                                   method='trust-ncg',\n                                   options={'return_all': True})\n            r_ncg = minimize(f, x0, jac=g, hess=h, tol=1e-8,\n                             method='newton-cg', options={'return_all': True})\n            assert_allclose(self.x_opt, r_dogleg['x'])\n            assert_allclose(self.x_opt, r_trust_ncg['x'])\n            assert_allclose(self.x_opt, r_ncg['x'])\n            assert_(len(r_dogleg['allvecs']) < len(r_ncg['allvecs']))\n\n    def test_trust_ncg_hessp(self):\n        for x0 in (self.easy_guess, self.hard_guess):\n            r = minimize(rosen, x0, jac=rosen_der, hessp=rosen_hess_prod,\n                         tol=1e-8, method='trust-ncg')\n            assert_allclose(self.x_opt, r['x'])\n\n\nif __name__ == '__main__':\n    run_module_suite()\n"}
{"text": "'''\n\nAll data migrate operations for test.\n\n@author: Legion\n'''\n\nimport apibinding.api_actions as api_actions\nimport zstackwoodpecker.test_util as test_util\nimport account_operations\nimport apibinding.inventory as inventory\n\ndef ps_migrage_vm(dst_ps_uuid, vm_uuid, session_uuid=None, withDataVolumes=False, withSnapshots=False):\n    action = api_actions.PrimaryStorageMigrateVmAction()\n    action.dstPrimaryStorageUuid = dst_ps_uuid\n    action.vmInstanceUuid = vm_uuid\n    action.timeout = 7200000\n    action.withDataVolumes = withDataVolumes\n    action.withSnapshots = withSnapshots\n    test_util.action_logger('Migrate [vm uuid: %s] to [Primary Storage: %s]' % (vm_uuid, dst_ps_uuid))\n    evt = account_operations.execute_action_with_session(action, session_uuid) \n    return evt.inventory\n\ndef ps_migrage_volume(dst_ps_uuid, vol_uuid, volume_type=None, session_uuid=None):\n    action = api_actions.PrimaryStorageMigrateVolumeAction()\n    action.dstPrimaryStorageUuid = dst_ps_uuid\n    action.volumeUuid = vol_uuid\n    action.timeout = 7200000\n    test_util.action_logger('Migrate [%s Volume: %s] to [Primary Storage: %s]' % (volume_type, vol_uuid, dst_ps_uuid))\n    evt = account_operations.execute_action_with_session(action, session_uuid) \n    return evt.inventory\n\ndef ps_migrage_root_volume(dst_ps_uuid, vol_uuid, session_uuid=None):\n    evt_inv = ps_migrage_volume(dst_ps_uuid=dst_ps_uuid, vol_uuid=vol_uuid, volume_type='Root', session_uuid=session_uuid)\n    return evt_inv\n\ndef ps_migrage_data_volume(dst_ps_uuid, vol_uuid, session_uuid=None):\n    evt_inv = ps_migrage_volume(dst_ps_uuid=dst_ps_uuid, vol_uuid=vol_uuid, volume_type='Data', session_uuid=session_uuid)\n    return evt_inv\n\ndef bs_migrage_image(dst_bs_uuid, src_bs_uuid, image_uuid, session_uuid=None):\n    action = api_actions.BackupStorageMigrateImageAction()\n    action.dstBackupStorageUuid = dst_bs_uuid\n    action.srcBackupStorageUuid = src_bs_uuid\n    action.imageUuid = image_uuid\n    test_util.action_logger('Migrate [Image: %s] from [Backup Storage: %s ]to [Backup Storage: %s]' % (image_uuid, src_bs_uuid, dst_bs_uuid))\n    evt = account_operations.execute_action_with_session(action, session_uuid) \n    return evt.inventory\n\ndef get_ps_candidate_for_vol_migration(vol_uuid, session_uuid=None):\n    action = api_actions.GetPrimaryStorageCandidatesForVolumeMigrationAction()\n    action.volumeUuid = vol_uuid\n    test_util.action_logger('Get Primary Storage Candidates for Volume Migration')\n    evt = account_operations.execute_action_with_session(action, session_uuid) \n    return evt.inventories\n\ndef get_bs_candidate_for_image_migration(src_bs_uuid, session_uuid=None):\n    action = api_actions.GetBackupStorageCandidatesForImageMigrationAction()\n    action.srcBackupStorageUuid = src_bs_uuid\n    test_util.action_logger('Get Backup Storage Candidates for Volume Migration')\n    evt = account_operations.execute_action_with_session(action, session_uuid) \n    return evt.inventories\n\ndef get_ps_candidate_for_vm_migration(vm_uuid, session_uuid=None):\n    action = api_actions.GetPrimaryStorageCandidatesForVmMigrationAction()\n    action.vmInstanceUuid = vm_uuid\n    test_util.action_logger('Get Primary Storage Candidates for Vm Migration')\n    evt = account_operations.execute_action_with_session(action, session_uuid) \n    return evt.inventories\n"}
{"text": "# encoding=utf8\nimport sys\nfrom nltk.corpus import wordnet as wn\nfrom os import listdir\nfrom os.path import isfile, join,isdir\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport  os\n#reload(sys)\nsys.setdefaultencoding('utf8')\nwordnet_lemmatizer = WordNetLemmatizer()\nprint(wordnet_lemmatizer.lemmatize('dogs'))\n\n\nPATH_good = \"../Helpers/BratDataProcessing/WekaDataSet/Actor/Good\"\nPATH_bad = \"../Helpers/BratDataProcessing/WekaDataSet/Actor/Bad\"\ndirectory1 = \"../Helpers/BratDataProcessing/WekaDataSet/Actor/Bad_preprocessed\"\nif not os.path.exists(directory1):\n    os.makedirs(directory1)\n\ndirectory2 = \"../Helpers/BratDataProcessing/WekaDataSet/Actor/Good_preprocessed\"\nif not os.path.exists(directory2):\n    os.makedirs(directory2)\nonlyfiles = [f for f in listdir(PATH_good) if (f.endswith(\".txt\"))]\nfor file in onlyfiles:\n    print(file)\n    fl = open(PATH_good + '/' + file, 'r')\n    content = fl.read()\n    tokens = nltk.word_tokenize(content)\n    new_content = \"\"\n    for token in tokens:\n        t = wordnet_lemmatizer.lemmatize(unicode(token,errors='ignore'))\n        synsets = wn.synsets(t)\n        for synset in synsets:\n            for lemma in synset.lemmas():\n                new_content = new_content+\" \"+ lemma.name()\n        new_content = new_content+\" \"+token\n    f2 = open(directory2 + '/' + file, 'w')\n    f2.write(new_content)\n    f2.close()\n    fl.close()\n\nonlyfiles = [f for f in listdir(PATH_bad) if (f.endswith(\".txt\"))]\nfor file in onlyfiles:\n    print(file)\n    fl = open(PATH_bad + '/' + file, 'r')\n    content = fl.read()\n    tokens = nltk.word_tokenize(content)\n    new_content = \"\"\n    for token in tokens:\n        t = wordnet_lemmatizer.lemmatize(unicode(token,errors='ignore'))\n        synsets = wn.synsets(t)\n        for synset in synsets:\n            for lemma in synset.lemmas():\n                new_content = new_content+\" \"+ lemma.name()\n        new_content = new_content + \" \" + token\n    f2 = open(directory1 + '/' + file, 'w')\n    f2.write(new_content)\n    f2.close()\n    fl.close()\n\nprint(wn.synsets('innovative'))"}
{"text": "import sys, pygame\nimport numpy as np\nfrom numpy.random import random\n\npygame.init()\n\nsize = width, height = 1000, 800\n\namplitude_max = 4.\namplitude = amplitude_max * random()\nspeed = [amplitude*random(), amplitude*random()]\nblack = 0, 0, 0\n\nscreen = pygame.display.set_mode(size)\ndone = False\nis_blue = True\nx = 30\ny = 30\n\nball = pygame.image.load(\"../../images/owl_books.png\")\nball = pygame.transform.scale(ball, (80, 60))\nballrect = ball.get_rect()\n\nclock = pygame.time.Clock()\n\nwhile not done:\n    for event in pygame.event.get():\n        if event.type == pygame.QUIT:\n            done = True\n        if event.type == pygame.KEYDOWN \\\n        and event.key == pygame.K_SPACE:\n            is_blue = not is_blue\n\n    pressed = pygame.key.get_pressed()\n    if pressed[pygame.K_UP]: y -= 3\n    if pressed[pygame.K_DOWN]: y += 3\n    if pressed[pygame.K_LEFT]: x -= 3\n    if pressed[pygame.K_RIGHT]: x += 3\n\n    screen.fill((0,0,0))\n    if is_blue:\n        color = (0, 128, 255)\n    else:\n        color = (255, 100, 0)\n\n    ballrect = ballrect.move(speed)\n    if ballrect.left < 0 or ballrect.right > width:\n        speed[0] = -speed[0]\n    if ballrect.top < 0 or ballrect.bottom > height:\n        speed[1] = -speed[1]\n\n    if ballrect.left < x and x < ballrect.right and ballrect.top < y and y < ballrect.bottom :\n        # ... collapse : change velocity and color\n        color = (255, 100, 0)\n        amplitude = amplitude_max * random()\n        speed = [amplitude*random(), amplitude*random()]\n    else:\n        color = (0, 128, 255)\n\n    pygame.draw.rect(screen, color, pygame.Rect(x,y,60,60))\n    screen.blit(ball, ballrect)\n\n    pygame.display.flip()\n    clock.tick(60)\n"}
{"text": "#!/usr/bin/env python2\n# vim:fileencoding=UTF-8:ts=4:sw=4:sta:et:sts=4:ai\nfrom __future__ import with_statement\n\n__license__   = 'GPL v3'\n__copyright__ = '2009, Kovid Goyal <kovid@kovidgoyal.net>'\n__docformat__ = 'restructuredtext en'\n\n\nimport __builtin__, sys, os\n\nfrom calibre import config_dir\n\n\nclass PathResolver(object):\n\n    def __init__(self):\n        self.locations = [sys.resources_location]\n        self.cache = {}\n\n        def suitable(path):\n            try:\n                return os.path.exists(path) and os.path.isdir(path) and \\\n                       os.listdir(path)\n            except:\n                pass\n            return False\n\n        self.default_path = sys.resources_location\n\n        dev_path = os.environ.get('CALIBRE_DEVELOP_FROM', None)\n        self.using_develop_from = False\n        if dev_path is not None:\n            dev_path = os.path.join(os.path.abspath(\n                os.path.dirname(dev_path)), 'resources')\n            if suitable(dev_path):\n                self.locations.insert(0, dev_path)\n                self.default_path = dev_path\n                self.using_develop_from = True\n\n        user_path = os.path.join(config_dir, 'resources')\n        self.user_path = None\n        if suitable(user_path):\n            self.locations.insert(0, user_path)\n            self.user_path = user_path\n\n    def __call__(self, path, allow_user_override=True):\n        path = path.replace(os.sep, '/')\n        key = (path, allow_user_override)\n        ans = self.cache.get(key, None)\n        if ans is None:\n            for base in self.locations:\n                if not allow_user_override and base == self.user_path:\n                    continue\n                fpath = os.path.join(base, *path.split('/'))\n                if os.path.exists(fpath):\n                    ans = fpath\n                    break\n\n            if ans is None:\n                ans = os.path.join(self.default_path, *path.split('/'))\n\n            self.cache[key] = ans\n\n        return ans\n\n_resolver = PathResolver()\n\n\ndef get_path(path, data=False, allow_user_override=True):\n    fpath = _resolver(path, allow_user_override=allow_user_override)\n    if data:\n        with open(fpath, 'rb') as f:\n            return f.read()\n    return fpath\n\n\ndef get_image_path(path, data=False, allow_user_override=True):\n    if not path:\n        return get_path('images', allow_user_override=allow_user_override)\n    return get_path('images/'+path, data=data, allow_user_override=allow_user_override)\n\n\ndef js_name_to_path(name, ext='.coffee'):\n    path = (u'/'.join(name.split('.'))) + ext\n    d = os.path.dirname\n    base = d(d(os.path.abspath(__file__)))\n    return os.path.join(base, path)\n\n\ndef _compile_coffeescript(name):\n    from calibre.utils.serve_coffee import compile_coffeescript\n    src = js_name_to_path(name)\n    with open(src, 'rb') as f:\n        cs, errors = compile_coffeescript(f.read(), src)\n        if errors:\n            for line in errors:\n                print (line)\n            raise Exception('Failed to compile coffeescript'\n                    ': %s'%src)\n        return cs\n\n\ndef compiled_coffeescript(name, dynamic=False):\n    import zipfile\n    zipf = get_path('compiled_coffeescript.zip', allow_user_override=False)\n    with zipfile.ZipFile(zipf, 'r') as zf:\n        if dynamic:\n            import json\n            existing_hash = json.loads(zf.comment or '{}').get(name + '.js')\n            if existing_hash is not None:\n                import hashlib\n                with open(js_name_to_path(name), 'rb') as f:\n                    if existing_hash == hashlib.sha1(f.read()).hexdigest():\n                        return zf.read(name + '.js')\n            return _compile_coffeescript(name)\n        else:\n            return zf.read(name+'.js')\n\n__builtin__.__dict__['P'] = get_path\n__builtin__.__dict__['I'] = get_image_path\n"}
{"text": "#!/usr/bin/env python3\n\nMOD = 2 ** 16\n\ndef func0():\n    if j < 0:\n        return x1 << -j\n    else:\n        return x1 >> j\n\ndef func1():\n    if j < 0:\n        return y >> -j\n    else:\n        return y << j\n\nx1 = int(input(), 16)\nx0 = 0\na = 0\ny = 0\nn = 0\nc = 0\n\nt = x1\nwhile t > 0:\n    t >>= 1\n    n += 1\nn += 16\nn += n & 1\n\nprint(hex(x1), '=', x1)\nprint(  '{:2}'.format('i'),\n        '{:1}'.format('c'),\n        '{:6}'.format('a'),\n        '{:8}'.format('y'),\n        '{:6}'.format('x1'),\n        '{:6}'.format('x0'),\n        '{:6}'.format('x21'),\n        '{:6}'.format('x20'),\n        '{:6}'.format('y21'),\n        '{:6}'.format('y20'),\n        )\n\nfor i in range(n, -1, -2):\n    j = i - 16\n    y21 = 0\n    a <<= 1\n    y <<= 1\n    if y >= MOD:\n        y %= MOD  # \u4e0b16\u30d3\u30c3\u30c8\u3092\u3068\u308b\n        y21 += 1\n    y20 = 1 | y\n    c = True\n    f0 = func0() % MOD\n    x20 = x0 >> i\n    x20 += f0\n    x21 = x1 >> i\n    if x21 < y21:\n        c = False\n    if x21 == y21:\n        if x20 < y20:\n            c = False\n    if c:\n        a += 1\n        y += 1\n        x1 -= func1()\n        x0 -= (y << i) % MOD  # \u4e0b16\u30d3\u30c3\u30c8\u3092\u3068\u308b\n        if x0 < 0:\n            x1 -= 1\n            x0 += MOD  # \u4e0b16\u30d3\u30c3\u30c8\u3092\u3068\u308b\n        y += 1\n\n    print(  '{:02}'.format(i),\n            '{:1}'.format(c),\n            '{:#06x}'.format(a),\n            '{:#08x}'.format(y),\n            '{:#06x}'.format(x1),\n            '{:#06x}'.format(x0),\n            '{:#06x}'.format(x21),\n            '{:#06x}'.format(x20),\n            '{:#06x}'.format(y21),\n            '{:#06x}'.format(y20),\n            )\n\nprint(hex(a), '=', a / 256)\n"}
{"text": "#  Copyright 2015-present Scikit Flow Authors. All Rights Reserved.\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n\nfrom sklearn import datasets, cross_validation, metrics\nfrom sklearn import preprocessing\n\nfrom tensorflow.contrib import skflow\n\n# Load dataset\nboston = datasets.load_boston()\nX, y = boston.data, boston.target\n\n# Split dataset into train / test\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y,\n    test_size=0.2, random_state=42)\n\n# scale data (training set) to 0 mean and unit Std. dev\nscaler = preprocessing.StandardScaler()\nX_train = scaler.fit_transform(X_train)\n\n# Build 2 layer fully connected DNN with 10, 10 units respecitvely.\nregressor = skflow.TensorFlowDNNRegressor(hidden_units=[10, 10],\n    steps=5000, learning_rate=0.1, batch_size=1)\n\n# Fit\nregressor.fit(X_train, y_train)\n\n# Predict and score\nscore = metrics.mean_squared_error(regressor.predict(scaler.fit_transform(X_test)), y_test)\n\nprint('MSE: {0:f}'.format(score))\n"}
{"text": "#!/usr/bin/env python\n## category Misc\n## desc Checks a BAM file for corruption\n'''\nChecks a BAM file for corruption\n'''\nimport sys\nimport os\n\nimport ngsutils.bam\n\n\ndef bam_check(fname, quiet=False):\n    if not quiet:\n        sys.stdout.write('%s: ' % fname)\n        sys.stdout.flush()\n    fail = False\n    i = 1\n    try:\n        bamfile = ngsutils.bam.bam_open(fname)\n        for read in ngsutils.bam.bam_iter(bamfile):\n            i += 1\n            pass\n        bamfile.close()\n    except KeyboardInterrupt:\n        if not quiet:\n            sys.stdout.write('\\n')\n        sys.exit(-1)\n    except:\n        fail = True\n        pass\n\n    if fail:\n        if not quiet:\n            sys.stdout.write('ERROR! (line %s)\\n' % (i + 1))\n        return False\n\n    if not quiet:\n        sys.stdout.write('OK\\n')\n    return True\n\n\ndef usage():\n    print __doc__\n    print \"Usage: bamutils check bamfile...\"\n    sys.exit(-1)\n\nif __name__ == \"__main__\":\n    fnames = []\n\n    for arg in sys.argv[1:]:\n        if arg == \"-h\":\n            usage()\n        elif os.path.exists(arg):\n            fnames.append(arg)\n        else:\n            sys.stderr.write(\"File: %s not found!\\n\" % arg)\n            usage()\n\n    if not fnames:\n        usage()\n\n    fail = False\n    for f in fnames:\n        if not bam_check(f):\n            fail = True\n\n    if fail:\n        sys.exit(1)\n"}
{"text": "from django.core.management.base import BaseCommand, CommandError\nfrom django.contrib.auth.models import User\nfrom django.core.mail import send_mail\nfrom django.conf import settings\n\nfrom monitor import models\n\nfrom crontab import CronTab\nimport os\nimport sys\n\ntry:\n\taction = sys.argv[1:][1]\nexcept:\n\taction = ''\n\ndef find_job(tab, comment):\n\tfor job in tab:\n   \t\tif job.comment == comment:\n   \t\t\treturn job\n   \treturn None\n\nclass Command(BaseCommand):\n\n\thelp = 'Installs cron tasks for the monitor.'\n\t\n\n\tdef handle(self, *args, **options):\n\n\t\tmonitor_list = models.Monitor.objects.all()\n\t\tvirtualenv = os.environ.get('VIRTUAL_ENV', None)\n\t\ttab = CronTab()\n\n\t\tfor monitor in monitor_list:\n\t\t\tcurrent_job = find_job(tab, \"fetcher_droid_%s\" % monitor.slug)\n\n\t\t\tif current_job == None:\n\t\t\t\tdjango_command = \"&& python %s/manage.py fetcher_droid %s >> /var/log/mondroid/%s.fetcher.log\" % (settings.BASE_DIR, monitor.slug, monitor.slug)\n\n\t\t\t\tif virtualenv:\n\t\t\t\t\tcommand = 'export PATH=%s/bin:/usr/local/bin:/usr/bin:/bin %s' % (virtualenv, django_command)\n\t\t\t\telse:\n\t\t\t\t\tcommand = '%s' % (django_command)\n\n\t\t\t\tcron_job = tab.new(command, comment=\"fetcher_droid_%s\" % monitor.slug)\n\t\t\t\tcron_job.minute.every(5)\n\n\t\t# Install the parser droid command if it doesn't exist already\n\t\tcurrent_job = find_job(tab, \"parser_droid\")\n\t\tif current_job == None:\n\t\t\tif virtualenv:\n\t\t\t\tcommand = 'export PATH=%s/bin:/usr/local/bin:/usr/bin:/bin && python %s/manage.py parser_droid' % (virtualenv, settings.BASE_DIR)\n\t\t\tcron_job = tab.new(command, comment=\"parser_droid\")\n\t\t\tcron_job.minute.every(5)\n\n\t\tif action == 'test':\n\t\t\tprint tab.render()\n\t\telif action == 'quiet':\n\t\t\tpass\n\t\telse:\n\t\t\ttab.write()"}
{"text": "import inspect\nimport os\nimport platform\n\nimport numpy as np\nimport pandas as pd\nfrom pkg_resources import parse_version\nimport pytest\n\nimport pvlib\n\npvlib_base_version = \\\n    parse_version(parse_version(pvlib.__version__).base_version)\n\n\n# decorator takes one argument: the base version for which it should fail\n# for example @fail_on_pvlib_version('0.7') will cause a test to fail\n# on pvlib versions 0.7a, 0.7b, 0.7rc1, etc.\n# test function may not take args, kwargs, or fixtures.\ndef fail_on_pvlib_version(version):\n    # second level of decorator takes the function under consideration\n    def wrapper(func):\n        # third level defers computation until the test is called\n        # this allows the specific test to fail at test runtime,\n        # rather than at decoration time (when the module is imported)\n        def inner():\n            # fail if the version is too high\n            if pvlib_base_version >= parse_version(version):\n                pytest.fail('the tested function is scheduled to be '\n                            'removed in %s' % version)\n            # otherwise return the function to be executed\n            else:\n                return func()\n        return inner\n    return wrapper\n\n\n# commonly used directories in the tests\ntest_dir = os.path.dirname(\n    os.path.abspath(inspect.getfile(inspect.currentframe())))\ndata_dir = os.path.join(test_dir, os.pardir, 'data')\n\n\nhas_python2 = parse_version(platform.python_version()) < parse_version('3')\n\nplatform_is_windows = platform.system() == 'Windows'\nskip_windows = pytest.mark.skipif(platform_is_windows,\n                                  reason='does not run on windows')\n\ntry:\n    import scipy\n    has_scipy = True\nexcept ImportError:\n    has_scipy = False\n\nrequires_scipy = pytest.mark.skipif(not has_scipy, reason='requires scipy')\n\n\ntry:\n    import tables\n    has_tables = True\nexcept ImportError:\n    has_tables = False\n\nrequires_tables = pytest.mark.skipif(not has_tables, reason='requires tables')\n\n\ntry:\n    import ephem\n    has_ephem = True\nexcept ImportError:\n    has_ephem = False\n\nrequires_ephem = pytest.mark.skipif(not has_ephem, reason='requires ephem')\n\n\ndef pandas_0_17():\n    return parse_version(pd.__version__) >= parse_version('0.17.0')\n\n\nneeds_pandas_0_17 = pytest.mark.skipif(\n    not pandas_0_17(), reason='requires pandas 0.17 or greater')\n\n\ndef numpy_1_10():\n    return parse_version(np.__version__) >= parse_version('1.10.0')\n\n\nneeds_numpy_1_10 = pytest.mark.skipif(\n    not numpy_1_10(), reason='requires numpy 1.10 or greater')\n\n\ndef pandas_0_22():\n    return parse_version(pd.__version__) >= parse_version('0.22.0')\n\n\nneeds_pandas_0_22 = pytest.mark.skipif(\n    not pandas_0_22(), reason='requires pandas 0.22 or greater')\n\n\ndef has_spa_c():\n    try:\n        from pvlib.spa_c_files.spa_py import spa_calc\n    except ImportError:\n        return False\n    else:\n        return True\n\n\nrequires_spa_c = pytest.mark.skipif(not has_spa_c(), reason=\"requires spa_c\")\n\n\ndef has_numba():\n    try:\n        import numba\n    except ImportError:\n        return False\n    else:\n        vers = numba.__version__.split('.')\n        if int(vers[0] + vers[1]) < 17:\n            return False\n        else:\n            return True\n\n\nrequires_numba = pytest.mark.skipif(not has_numba(), reason=\"requires numba\")\n\ntry:\n    import siphon\n    has_siphon = True\nexcept ImportError:\n    has_siphon = False\n\nrequires_siphon = pytest.mark.skipif(not has_siphon,\n                                     reason='requires siphon')\n\ntry:\n    import netCDF4  # noqa: F401\n    has_netCDF4 = True\nexcept ImportError:\n    has_netCDF4 = False\n\nrequires_netCDF4 = pytest.mark.skipif(not has_netCDF4,\n                                      reason='requires netCDF4')\n\ntry:\n    import pvfactors  # noqa: F401\n    has_pvfactors = True\nexcept ImportError:\n    has_pvfactors = False\n\nrequires_pvfactors = pytest.mark.skipif(not has_pvfactors,\n                                        reason='requires pvfactors')\n"}
{"text": "#!/usr/bin/python3\n\nclass TNode:\n\n    def __init__(self, children):\n        self.children = children\n\n    def is_leaf(self):\n        return self.children is None\n\n    def __repr__(self):\n        return repr(self.children)\n\n\nclass PaTrie:\n\n    def __init__(self):\n        self.root = None\n\n    def __contains__(self, word):\n        cur = self.root\n        if cur is None:\n            return False\n\n        i = 0\n        while cur is not None and not cur.is_leaf():\n            for label, child in cur.children.items():\n                if len(label) == 0 and i < len(word):\n                    continue\n\n                if word[i:i + len(label)] == label:\n                    cur = child\n                    i += len(label)\n                    break\n            else:\n                return False\n        return i == len(word)\n\n    def insert(self, word):\n        cur = self.root\n        if cur is None:\n            self.root = TNode({ word: None })\n            return\n\n        i = 0\n        while not cur.is_leaf():\n            for label, child in cur.children.items():\n                cl = self.common_prefix_len(word[i:], label)\n                if cl:\n                    if cl == len(label):\n                        cur = child\n                        i += len(label)\n                        break\n\n                    del cur.children[label]\n\n                    cur.children[label[:cl]] = TNode({\n                        label[cl:]: child,\n                        word[i + cl:]: TNode(None),\n                    })\n                    return\n\n            else:\n                cur.children[word[i:]] = TNode(None)\n                return\n\n        cur.children = {\n            \"\": TNode(None),\n            word[i:]: TNode(None)\n        }\n\n    def __str__(self):\n        s = []\n        def _str(tnode, sofar, label, prepend):\n            if tnode is None:\n                return\n            if tnode.is_leaf():\n                if label:\n                    s.append(prepend + \"+ \" + label)\n                s.append(prepend + \"  {\"+sofar+\"}\")\n            else:\n                s.append(prepend + \"+ \" + label)\n                for label, child in tnode.children.items():\n                    _str(child, sofar + label, label, prepend + \"  \")\n\n        if self.root is not None:\n            _str(self.root, \"\", \"\", \"\")\n\n        return \"\\n\".join(s)\n\n\n    def common_prefix_len(self, a, b):\n        i = 0\n        for x, y in zip(a, b):\n            if x == y:\n                i += 1\n            else:\n                break\n        return i\n\n\nif __name__ == \"__main__\":\n    t = PaTrie()\n    words = \"autobus\", \"auto\", \"abraka\", \"dabra\", \"abrakadabra\", \"honza\", \"honirna\", \"honicka\", \"hony\", \"ho\", \"h\"\n    for w in words:\n        t.insert(w)\n        print(\"AFTER INSERTING\", w)\n        print(t.root)\n        print(t)\n        print()\n"}
{"text": "\"\"\"\nYanker\n\nUsage:\n    yanker [--threads=<tnum>]\n\"\"\"\n__version__ = '1.0.1'\n\nimport Queue\nimport threading\nimport youtube_dl as ydl\nimport pyperclip as clip\nimport time\nfrom docopt import docopt\n\n\nclass ErrLogger(object):\n    def debug(self, msg):\n        pass\n\n    def warning(self, msg):\n        pass\n\n    def error(self, msg):\n        print msg\n\n\nclass Worker(threading.Thread):\n    def __init__(self, tasks):\n        threading.Thread.__init__(self)\n        self.tasks = tasks\n        self.daemon = True\n        self.start()\n\n    def run(self):\n        while True:\n            vid = self.tasks.get()\n            vid.download()\n            self.tasks.task_done()\n\n\nclass Video:\n    def progress(self, s):\n        if s['status'] == 'finished':\n            print 'Finished {}'.format(s['filename'])\n\n    def __init__(self, url, opts={}):\n        self.url = url\n        self.ydl_opts = {\n            'progress_hooks': [self.progress],\n            'logger': ErrLogger()\n        }\n        self.ydl_opts.update(opts)\n\n    def download(self):\n        print 'Downloading: {}'.format(self.url)\n        with ydl.YoutubeDL(self.ydl_opts) as y:\n            try:\n                y.download([self.url])\n            except ydl.DownloadError:\n                print 'Unsupported URL, skipping'\n\n\nclass Watcher:\n    def __init__(self, urls=[], threads=2):\n        self.queue = Queue.Queue(0)\n        self.threads = threads\n        self.stopped = False\n        self.grabbed_urls = set([])\n        for _ in range(threads): Worker(self.queue)\n\n    def run(self):\n        recent = ''\n        while not self.stopped:\n            current = clip.paste()\n            if recent != current:\n                recent = current\n                if current.startswith(('http://', 'https://',)) and current not in self.grabbed_urls:\n                    print 'Added: {}'.format(current)\n                    self.grabbed_urls.add(current)\n                    self.queue.put(Video(current))\n                elif current in self.grabbed_urls:\n                    print 'Already grabbed {}'.format(current)\n            time.sleep(0.25)\n\n\ndef run():\n    args = docopt(__doc__, version='Yanker {}'.format(__version__))\n    threads = args['--threads']\n    if not threads:\n        threads = 2\n    else:\n        threads = int(threads)\n    print 'Starting Yanker with {} threads...'.format(threads)\n    watch = Watcher(threads=threads)\n    try:\n        watch.run()\n    except KeyboardInterrupt:\n        print 'Stopping...'\n        watch.stopped = True\n"}
{"text": "#!/usr/bin/env python\n\n# https://pymotw.com/3/os.path/\n\nimport os\nimport os.path\nimport time\nimport argparse\n\n\nAPPNAME='lister'\n__version__ = '0.0.1'\n\n\ndef config_args():\n    \"\"\"\n    Configure command line arguments\n    \"\"\"\n    parser = argparse.ArgumentParser(description=APPNAME,\n        epilog=(\"Version {}\".format(__version__)))\n    #parser.add_argument('-c', metavar='CONFIGFILE', required=False, help='path to config file',\n    #    default=DESTINY_CONFIG_FILE)\n    #parser.add_argument('--log', metavar='LOGFILE', required=False, help='path to log file',\n    #    default=DESTINY_LOGFILE)\n    parser.add_argument('files', metavar='F', nargs='+',\n                    help='file or directory to evaluate')\n    parser.add_argument('--version', action='version', version=('%(prog)s ' + __version__))\n    parser.add_argument('--debug', required=False, help='Enable debugging of this script', action=\"store_true\")\n    args = parser.parse_args()\n    return args\n\n\ndef ftime(filepath):\n    print('File         : {}'.format(filepath))\n    print('Access time  :', time.ctime(os.path.getatime(filepath)))\n    print('Modified time:', time.ctime(os.path.getmtime(filepath)))\n    print('Change time  :', time.ctime(os.path.getctime(filepath)))\n    print('Size         :', os.path.getsize(filepath))\n\n\ndef finfo(filepath):\n    print('File        : {!r}'.format(filepath))\n    print('Absolute    :', os.path.isabs(filepath))\n    print('Is file?    :', os.path.isfile(filepath))\n    print('Is Dir?     :', os.path.isdir(filepath))\n    print('Is Link?    :', os.path.islink(filepath))\n    print('Mountpoint? :', os.path.ismount(filepath))\n    print('Exists?     :', os.path.exists(filepath))\n    print('Link Exists?:', os.path.lexists(filepath))\n\n\nif __name__ == '__main__':\n    args = config_args()\n    for filepath in args.files:\n        #print(type(filepath))\n        #print(repr(filepath))\n        fp = os.path.abspath(filepath)\n        ftime(fp)\n        finfo(fp)\n"}
{"text": "from corpus import Corpus\nimport processing\n\n__author__ = 'bengt'\nimport argparse\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('train', help='Path to training corpus.')\n    parser.add_argument('corpus', help='Path to corpus.')\n    parser.add_argument('n', help='Tag sentences shorter than this length.')\n    args = parser.parse_args()\n\n    train_corpus = Corpus(args.train)\n    corpus = Corpus(args.corpus)\n    n = int(args.n)\n\n    pos_frequencies = processing.pos_frequencies(train_corpus)\n    poses_for_word_from_train, total_pos_count = processing.calculate_poses_for_word(train_corpus)\n    pos_bigram_probabilities_train = processing.calculate_pos_bigram_probabilities(train_corpus)\n    word_pos_probabilities_train = processing.calculate_word_pos_probabilities(train_corpus)\n\n    sentences = [sentence for sentence in corpus.get_sentences() if len(sentence) < n]\n\n    WORD_GIVEN_POS = 0\n    POS_GIVEN_PREVPOS = 1\n\n    for sentence in sentences:\n        prev_pos = '<s>'\n        columns = {}\n        current_sentence = []\n        for word in sentence:\n            id, form, lemma, plemma, pos, ppos = word\n\n            current_sentence.append([id, form, lemma, plemma, pos])\n\n            columns[id] = {}\n            if form in poses_for_word_from_train:\n                for (pos_for_word, pos_for_word_count) in poses_for_word_from_train[form].items():\n                    p_word_given_pos = word_pos_probabilities_train['{0} {1}'.format(form, pos_for_word)]\n\n                    pos_bigram = '{0} {1}'.format(prev_pos, pos_for_word)\n                    if pos_bigram in pos_bigram_probabilities_train:\n                        p_pos_given_prevpos = pos_bigram_probabilities_train[pos_bigram]\n                    else:\n                        p_pos_given_prevpos = 0.00001 # Low chance that this is what we want\n\n                    columns[id][pos_for_word] = {}\n                    columns[id][pos_for_word][WORD_GIVEN_POS] = p_word_given_pos\n                    columns[id][pos_for_word][POS_GIVEN_PREVPOS] = p_pos_given_prevpos\n            else:\n                most_common_pos = max(pos_frequencies.items(), key=lambda x: x[1])\n\n                if form in word_pos_probabilities_train:\n                    p_word_given_pos = word_pos_probabilities_train['{0} {1}'.format(form, most_common_pos[0])]\n                else:\n                    p_word_given_pos = 0.00001  # Low chance that this is what we want\n\n                p_pos_given_prevpos = pos_bigram_probabilities_train['{0} {1}'.format(prev_pos, most_common_pos[0])]\n\n                columns[id][most_common_pos[0]] = {}\n                columns[id][most_common_pos[0]][WORD_GIVEN_POS] = p_word_given_pos\n                columns[id][most_common_pos[0]][POS_GIVEN_PREVPOS] = p_pos_given_prevpos\n\n            prev_pos = pos\n\n        path = {}\n        trellis = {}\n        for (column_id, poses) in sorted(columns.items(), key=lambda x: int(x[0])):\n            column_id = int(column_id)\n            trellis[column_id] = {}\n            for (current_pos, data) in poses.items():\n                current_word_given_pos = data[WORD_GIVEN_POS]\n                current_pos_given_prevpos = data[POS_GIVEN_PREVPOS]\n                if column_id == 0:\n                    break\n                elif column_id == 1:\n                    trellis[column_id][current_pos] = current_word_given_pos * current_pos_given_prevpos\n                else:\n\n                    max_prev_column = max([(id, data * current_pos_given_prevpos) for id, data in\n                                           trellis[column_id - 1].items()\n                                          ], key=lambda x: x[1])\n                    p = max_prev_column[1] * current_word_given_pos\n                    trellis[column_id][current_pos] = p\n\n            if column_id == 0:\n                continue\n            else:\n                path[column_id] = (max(trellis[column_id].items(), key=lambda x: x[1])[0])\n\n        for (id, predicted) in sorted(path.items(), key=lambda x: x[0]):\n            if id == 1:\n                print()\n            id, form, lemma, plemma, pos = current_sentence[id]\n            print('{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}'.format(id, form, lemma, plemma, pos, predicted))\n\n\nif __name__ == '__main__':\n    main()"}
{"text": "import cyscs as scs\nimport pytest\nimport cyscs.examples as ex\n\nimport numpy as np\n\n\ndef test_cache():\n    data, cone = ex.many_iter_ecp()\n\n    work = scs.Workspace(data, cone)\n\n    sol = work.solve()\n\ndef test_settings():\n    expected_keys = set(['normalize', 'use_indirect', 'scale', 'verbose',\n                        'eps', 'cg_rate', 'max_iters', 'alpha', 'rho_x'])\n\n    data, cone, _ = ex.simple_socp()\n    work = scs.Workspace(data, cone)\n\n    assert 'warm_start' not in work.settings\n    assert set(work.settings.keys()) == expected_keys\n\n    work.solve()\n\n    assert 'warm_start' not in work.settings\n    assert set(work.settings.keys()) == expected_keys\n\n\ndef test_fixed_settings():\n    data, cone, _ = ex.simple_socp()\n    work = scs.Workspace(data, cone)\n\n    expected_fixed = set(['normalize', 'use_indirect', 'scale', 'rho_x'])\n\n    assert set(work.fixed.keys()) == expected_fixed\n\n    with pytest.raises(Exception):\n        work.settings['rho_x'] = 3.14159\n        # should raise an exception because we changed a fixed setting\n        work.solve()\n\n\ndef test_data_keys():\n    data, cone, _ = ex.simple_socp()\n    work = scs.Workspace(data, cone)\n\n    assert 'A' not in work.data\n\n    assert set(work.data.keys()) == set(['b','c'])\n\ndef test_A():\n    data, cone, true_x = ex.simple_socp()\n    work = scs.Workspace(data, cone)\n\n    # corrupt the original data (but SCS should have made an internal copy, so this is ok)\n    data['A'][:] = 3\n\n    sol = work.solve(eps=1e-6)\n\n    assert np.allclose(sol['x'], true_x)\n\n    # now, solving on corrupted data shouldn't work\n    work = scs.Workspace(data, cone)\n\n    sol = work.solve(eps=1e-6)\n\n    assert not np.allclose(sol['x'], true_x)\n    \n\ndef test_settings_change():\n    data, cone, _ = ex.simple_socp()\n    work = scs.Workspace(data, cone)\n\n    assert work.settings['eps'] == 1e-3\n\n    work.solve(eps=1e-6)\n\n    assert work.settings['eps'] == 1e-6\n\ndef test_warm_start():\n    # if warm-starting, the input warm-start vector should not be modified\n    data, cone, true_x = ex.simple_socp()\n    work = scs.Workspace(data, cone)\n\n    sol = work.solve(eps=1e-2)\n\n    assert np.linalg.norm(sol['x'] - true_x) > 1e-3\n\n    sol2 = work.solve(warm_start=sol, eps=1e-9)\n\n    assert np.linalg.norm(sol2['x'] - true_x) <= 1e-9\n\n    assert np.linalg.norm(sol['x'] - sol2['x']) > 0\n    assert sol['x'] is not sol2['x']\n\ndef test_many_iter_ecp():\n    # warm starting with a solution at a lower tolerance should reduce\n    # the number of iterations needed\n    data, cone = ex.many_iter_ecp()\n\n    # intially takes ~920 iters for eps 1e-4\n    work = scs.Workspace(data, cone, eps=1e-4)\n    sol = work.solve()\n    assert sol['info']['iter'] >= 800\n\n    # ~640 for eps 1e-3\n    sol = work.solve(eps=1e-3)\n    assert 500 <= sol['info']['iter'] <= 700\n\n    # use 1e-3 sol as warm start for 1e-4\n    # extra digit only takes ~280 iters more\n    sol = work.solve(warm_start = sol, eps=1e-4)\n    assert sol['info']['iter'] < 300\n\n\n\n\n\n"}
{"text": "#!/usr/bin/python2\n\n#--------------------------------\n#Takes in mbox, spits out csv with email info and basic geolocation, plus other header fields.\n#--------------------------------\n\n#This product includes GeoLite2 data created by MaxMind, available from\n#<a href=\"http://www.maxmind.com\">http://www.maxmind.com</a>.\n\nimport mailbox\nimport sys\nimport csv\nimport re\nfrom os import path\nimport pprint\nimport argparse\nimport geoip2.database\nimport geoip2.errors\nimport pygeoip\nimport email.utils\nfrom email.utils import getaddresses\n\ndef get_iprecord(ip):\n    try:\n        geo = reader.city(ip)\n\torg = reader2.org_by_addr(ip)\n    except (geoip2.errors.AddressNotFoundError, ValueError):\n        return None,None,None\n    if geo.city.name:\n\tcityname=geo.city.name.encode('ascii','ignore')\n    else:\n\tcityname=geo.city.name\n\n    return geo.country.iso_code, cityname, org\n\ndef main():\n\n  # first some sanity tests on the command-line arguments\n  #sys.argv = ['mbox_to_mysql','list1.mbox','mailman','lists',] # !@!@! APS here for testing purposes only - comment out for live run\n\n  parser = argparse.ArgumentParser(description='Parse mbox file')\n  parser.add_argument('mbox', help='mbox file to parse')\n  parser.add_argument('outfile', help='output csv file')\n  args = parser.parse_args()\n  if not path.isfile(args.mbox):\n      parser.error(\"the file %s does not exist\"%args.mbox)\n  mbox = args.mbox\n  outfile = args.outfile\n  ipPattern = re.compile('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}')\n\n  global reader\n  reader = geoip2.database.Reader('geo/GeoLite2-City.mmdb')\n  global reader2\n  reader2 = pygeoip.GeoIP('geo/GeoIPOrg.dat')\n\n  f = open(outfile, 'wt')\n  try:\n    \twriter = csv.writer(f)\n  \twriter.writerow( ('Date','From','From Email','Return-Path Email','To','To Email','Recipients','X-To','Subject','Received-Last','Org','City', 'Country','X-IP','X-Org', 'X-City', 'X-Country','X-Mailer'))\n\n\tfor message in mailbox.mbox(mbox):\n    \t\tFrom = str(message['From'])\n\t\tfname,femail = email.utils.parseaddr(From)\n\t\t#print fname\n\t\tReturn = str(message['Return-Path'])\n\t\trname,remail = email.utils.parseaddr(Return)\n\t\t#print remail\n\t\tTo = str(message['To'])\n\t\ttname,temail = email.utils.parseaddr(To)\n\t\t\n\t\ttos = message.get_all('to', [])\n\t\tccs = message.get_all('cc', [])\n\t\tresent_tos = message.get_all('resent-to', [])\n\t\tresent_ccs = message.get_all('resent-cc', [])\n\t\tall_recipients = getaddresses(tos + ccs + resent_tos + resent_ccs)\n\t\t\n\t\tXTo = str(message['X-Apparently-To'])\n\t\t#findIP = re.findall(ipPattern,s)\n\t\tDate = str(message['Date'])\n\t\tSubject = str(message['Subject'])\n\n\t\tReceived = re.findall(ipPattern,str(message['Received']))\n\t\tif Received:\n\t\t\t#print Received[-1]\n\t\t\tcountry, city, org = get_iprecord(Received[-1])\t\t\t\n\t\t\t#print get_iprecord(Received[-1])\n\t\t\t#print org\n\t\telse:\n\t\t\tReceived = \"None\"\n\n\t\tXIP = message['X-Originating-IP']\n\t\tif XIP:\n\t\t\tXIP = str(XIP).strip('[]')\n\t\t\t#print (\"XIP: %s.\" % XIP)\n\t\t\tXcountry, Xcity, Xorg = get_iprecord(XIP)\n\t\telse:\n\t\t\tXIP = \"None\"\n\t\t\tXcountry = \"None\"\n\t\t\tXcity = \"None\"\n\t\t\tXorg = \"None\"\n\n\t\tXMailer = str(message['X-Mailer'])\n\t\t#Attachment = message.get_filename()\n\t\t#Body = str(message['Body'])\n\n\t\twriter.writerow((Date,fname,femail,remail,tname,temail,all_recipients,XTo,Subject,Received[-1],org,city,country,XIP,Xorg,Xcity,Xcountry,XMailer))\n  finally:\n    \tf.close()\n\n#print open(sys.argv[1], 'rt').read()\n\n\n\nif __name__ == '__main__':\n   main()\n"}
{"text": "from PluginManager import PluginManager\nfrom PluginDispatcher import PluginDispatcher\nfrom Configuration import ConfigFile\nfrom Util import call\nfrom re import match\nfrom sys import path\nfrom os import getcwd\nfrom Util import dictJoin\nfrom Logging import LogFile\n\npath.append(getcwd())\n\nlog = LogFile(\"Core\")\n\n\nclass Core:\n    _PluginManager = None\n    _PluginDispatcher = None\n    _ResponseObject = None\n    _Connector = None\n    _Config = None\n\n    def _LoadConnector(self, ConName):\n        try:\n            con = __import__(\"%s.Connector\" % ConName,\n                    globals(), locals(), \"Connector\")\n            log.debug(\"Got connector:\", con)\n            cls = getattr(con, \"Connector\", None)\n        except :\n            log.exception(\"Exception while loading connector\")\n            cls = None\n        log.debug(\"Connectors class\", cls)\n        if cls:\n            c = cls()\n            log.debug(\"Connector constructed\")\n            return c\n\n        log.critical(\"No connector\")\n        return cls\n\n    def HandleEvent(self, event):\n        log.dict(event,\"HandleEvent\")\n\n        pm = self._PluginManager\n        if not pm:\n            log.warning(\"No plugin manager\")\n            return\n\n        pd = self._PluginDispatcher\n        if not pd:\n            log.warning(\"No plugin dispatcher\")\n            return\n\n        ro = self._ResponseObject\n        if not ro:\n            log.warning(\"no response object\")\n            pass\n\n        matches = pm.GetMatchingFunctions(event)\n        log.debug(\"Matched %i hook(s).\" % len(matches))\n\n        for inst, func, args, servs in matches:\n            newEvent = dictJoin(event, dictJoin(args,\n                {\"self\": inst, \"response\": ro}))\n            log.debug(\"Services found for plugin:\", servs)\n            if servs:\n                log.debug(\"Event before processing:\", newEvent)\n\n            servDict={}\n            servDict[\"event\"]=newEvent\n            servDict[\"pm\"]=self._PluginManager\n            servDict[\"pd\"]=self._PluginDispatcher\n            servDict[\"ro\"]=self._ResponseObject\n            servDict[\"c\"]=self._Connector\n            servDict[\"core\"]=self\n            servDict[\"config\"]=self._Config\n            for servName in servs:\n                serv = pm.GetService(servName)\n                log.debug(\"Processing service\",servName,serv)\n                call(serv.onEvent,servDict)\n\n            if servs:\n                log.dict(newEvent,\"Event after processing:\")\n            #issue 5 fix goes here\n            newEvent.update(servDict)\n            pd.Enqueue((func, newEvent))\n\n    def __init__(self):\n        self._Config = ConfigFile(\"Core\")\n        if not self._Config:\n            log.critical(\"No log file loaded!\")\n            return\n        ConName = self._Config[\"Core\", \"Provider\"]\n        if ConName == None:\n            log.critical(\"No Core:Provider in Core.cfg\")\n            del self._Connector\n            return \n            \n        self._Connector=self._LoadConnector(ConName) \n    \tif self._Connector:\n            self._PluginManager = PluginManager(ConName)\n            self._PluginDispatcher = PluginDispatcher()\n            self._Connector.SetEventHandler(self.HandleEvent)\n            self._ResponseObject = self._Connector.GetResponseObject()\n            self._PluginDispatcher.SetResponseHandler(\n                self._Connector.HandleResponse)\n\n    def Start(self):\n        if not self._Connector:\n            log.warning(\"Could not start, no connector.\")\n            return\n\n        log.debug(\"Starting\")\n        log.debug(\"Auto loading plugins\")\n        self.AutoLoad()\n        log.debug(\"Auto load complete\")\n\n        if self._Connector:\n            log.debug(\"Connector starting\")\n            self._Connector.Start()\n        #else log error?\n\n    def Stop(self):\n        log.debug(\"Stopping\")\n        if self._PluginDispatcher:\n            self._PluginDispatcher.Stop()\n        if self._PluginManager:\n            self._PluginManager.Stop()\n        if self._Connector:\n            self._Connector.Stop()\n\n    def AutoLoad(self):\n        if not self._PluginManager:\n            return\n        pm = self._PluginManager\n        log.note(\"Starting autoload\", \"Root:\" + pm.root)\n        cf = ConfigFile(pm.root, \"Autoload\")\n        lines = [\"Configuration:\"]\n        for i in cf:\n            lines.append(i)\n            for j in cf[i]:\n                lines.append(\"  %s=%s\"%(j,cf[i,j]))\n        log.debug(*lines)\n        if cf:\n            log.debug(\"Autoloading plugins.\")\n\n            names = cf[\"Plugins\", \"Names\"]\n            log.debug(\"Autoloading plugins\", names)\n            if names:\n                for name in names.split():\n                    pm.LoadPlugin(name)\n                log.debug(\"Autoloading finished.\")\n                pd=self._PluginDispatcher\n                handler = pd.GetResponseHandler()\n                log.debug(\"Updating dedicated thread pool\",self._ResponseObject,handler)\n                pd.EnsureDedicated(pm.GetDedicated(),self._ResponseObject,handler)\n                    \n        else:\n            log.note(\"No Autoload configuration file\")\n\n \n\nif __name__ == \"__main__\":\n    try:\n        c = Core()\n        try:\n            c.Start()\n        except:\n            log.exception(\"Exception while starting.\")\n        c.Stop()\n    except:\n        log.exception(\"Exception while stopping.\")\n    log.debug(\"End of core\")\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\nimport blanc_basic_assets.fields\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('assets', '__first__'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='RecurringEvent',\n            fields=[\n                ('id', models.AutoField(serialize=False, verbose_name='ID', primary_key=True, auto_created=True)),\n                ('title', models.CharField(max_length=100, db_index=True)),\n                ('description', models.TextField()),\n                ('day_of_the_week', models.PositiveSmallIntegerField(db_index=True, choices=[(0, 'Sunday'), (1, 'Monday'), (2, 'Tuesday'), (3, 'Wednesday'), (4, 'Thursday'), (5, 'Friday'), (6, 'Saturday'), (7, 'Sunday')])),\n                ('time', models.TimeField(db_index=True)),\n                ('frequency', models.CharField(max_length=200, blank=True)),\n                ('published', models.BooleanField(default=True, db_index=True)),\n            ],\n            options={\n                'ordering': ('day_of_the_week', 'time'),\n            },\n        ),\n        migrations.CreateModel(\n            name='SpecialEvent',\n            fields=[\n                ('id', models.AutoField(serialize=False, verbose_name='ID', primary_key=True, auto_created=True)),\n                ('title', models.CharField(max_length=100, db_index=True)),\n                ('slug', models.SlugField(max_length=100, unique=True)),\n                ('summary', models.CharField(max_length=100, help_text='A short sentence description of the event')),\n                ('description', models.TextField(help_text='All of the event details we have')),\n                ('start', models.DateTimeField(help_text='Start time/date.', db_index=True)),\n                ('start_date', models.DateField(editable=False, db_index=True)),\n                ('end', models.DateTimeField(help_text='End time/date.')),\n                ('end_date', models.DateField(editable=False, db_index=True)),\n                ('published', models.BooleanField(default=True, db_index=True)),\n                ('image', blanc_basic_assets.fields.AssetForeignKey(blank=True, null=True, to='assets.Image')),\n            ],\n            options={\n                'ordering': ('start',),\n            },\n        ),\n    ]\n"}
{"text": "# This file is part of beets.\n# Copyright 2015, Adrian Sampson.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to\n# the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n\n\"\"\"Facilities for automatically determining files' correct metadata.\n\"\"\"\n\nfrom beets import logging\nfrom beets import config\n\n# Parts of external interface.\nfrom .hooks import AlbumInfo, TrackInfo, AlbumMatch, TrackMatch  # noqa\nfrom .match import tag_item, tag_album  # noqa\nfrom .match import Recommendation  # noqa\n\n# Global logger.\nlog = logging.getLogger('beets')\n\n\n# Additional utilities for the main interface.\n\ndef apply_item_metadata(item, track_info):\n    \"\"\"Set an item's metadata from its matched TrackInfo object.\n    \"\"\"\n    item.artist = track_info.artist\n    item.artist_sort = track_info.artist_sort\n    item.artist_credit = track_info.artist_credit\n    item.title = track_info.title\n    item.mb_trackid = track_info.track_id\n    if track_info.artist_id:\n        item.mb_artistid = track_info.artist_id\n    # At the moment, the other metadata is left intact (including album\n    # and track number). Perhaps these should be emptied?\n\n\ndef apply_metadata(album_info, mapping):\n    \"\"\"Set the items' metadata to match an AlbumInfo object using a\n    mapping from Items to TrackInfo objects.\n    \"\"\"\n    for item, track_info in mapping.iteritems():\n        # Album, artist, track count.\n        if track_info.artist:\n            item.artist = track_info.artist\n        else:\n            item.artist = album_info.artist\n        item.albumartist = album_info.artist\n        item.album = album_info.album\n\n        # Artist sort and credit names.\n        item.artist_sort = track_info.artist_sort or album_info.artist_sort\n        item.artist_credit = (track_info.artist_credit or\n                              album_info.artist_credit)\n        item.albumartist_sort = album_info.artist_sort\n        item.albumartist_credit = album_info.artist_credit\n\n        # Release date.\n        for prefix in '', 'original_':\n            if config['original_date'] and not prefix:\n                # Ignore specific release date.\n                continue\n\n            for suffix in 'year', 'month', 'day':\n                key = prefix + suffix\n                value = getattr(album_info, key) or 0\n\n                # If we don't even have a year, apply nothing.\n                if suffix == 'year' and not value:\n                    break\n\n                # Otherwise, set the fetched value (or 0 for the month\n                # and day if not available).\n                item[key] = value\n\n                # If we're using original release date for both fields,\n                # also set item.year = info.original_year, etc.\n                if config['original_date']:\n                    item[suffix] = value\n\n        # Title.\n        item.title = track_info.title\n\n        if config['per_disc_numbering']:\n            item.track = track_info.medium_index or track_info.index\n            item.tracktotal = track_info.medium_total or len(album_info.tracks)\n        else:\n            item.track = track_info.index\n            item.tracktotal = len(album_info.tracks)\n\n        # Disc and disc count.\n        item.disc = track_info.medium\n        item.disctotal = album_info.mediums\n\n        # MusicBrainz IDs.\n        item.mb_trackid = track_info.track_id\n        item.mb_albumid = album_info.album_id\n        if track_info.artist_id:\n            item.mb_artistid = track_info.artist_id\n        else:\n            item.mb_artistid = album_info.artist_id\n        item.mb_albumartistid = album_info.artist_id\n        item.mb_releasegroupid = album_info.releasegroup_id\n\n        # Compilation flag.\n        item.comp = album_info.va\n\n        # Miscellaneous metadata.\n        for field in ('albumtype',\n                      'label',\n                      'asin',\n                      'catalognum',\n                      'script',\n                      'language',\n                      'country',\n                      'albumstatus',\n                      'albumdisambig'):\n            value = getattr(album_info, field)\n            if value is not None:\n                item[field] = value\n        if track_info.disctitle is not None:\n            item.disctitle = track_info.disctitle\n\n        if track_info.media is not None:\n            item.media = track_info.media\n"}
{"text": "'''\nuix.textinput tests\n========================\n'''\n\nimport unittest\n\nfrom kivy.tests.common import GraphicUnitTest\nfrom kivy.uix.textinput import TextInput\n\n\nclass TextInputTest(unittest.TestCase):\n\n    def test_focusable_when_disabled(self):\n        ti = TextInput()\n        ti.disabled = True\n        ti.focused = True\n        ti.bind(focus=self.on_focused)\n\n    def on_focused(self, instance, value):\n        self.assertTrue(instance.focused, value)\n\n    def test_wordbreak(self):\n        self.test_txt = \"Firstlongline\\n\\nSecondveryverylongline\"\n\n        ti = TextInput(width='30dp', size_hint_x=None)\n        ti.bind(text=self.on_text)\n        ti.text = self.test_txt\n\n    def on_text(self, instance, value):\n        # Check if text is modified while recreating from lines and lines_flags\n        self.assertEquals(instance.text, self.test_txt)\n\n        # Check if wordbreaking is correctly done\n        # If so Secondvery... should start from the 7th line\n        pos_S = self.test_txt.index('S')\n        self.assertEquals(instance.get_cursor_from_index(pos_S), (0, 6))\n\n\nclass TextInputGraphicTest(GraphicUnitTest):\n    def test_text_validate(self):\n        ti = TextInput(multiline=False)\n        ti.focus = True\n\n        self.render(ti)\n        self.assertFalse(ti.multiline)\n        self.assertTrue(ti.focus)\n        self.assertTrue(ti.text_validate_unfocus)\n\n        ti.validate_test = None\n\n        ti.bind(on_text_validate=lambda *_: setattr(\n            ti, 'validate_test', True\n        ))\n        ti._key_down(\n            (\n                None,     # displayed_str\n                None,     # internal_str\n                'enter',  # internal_action\n                1         # scale\n            ),\n            repeat=False\n        )\n        self.assertTrue(ti.validate_test)\n        self.assertFalse(ti.focus)\n\n        ti.validate_test = None\n        ti.text_validate_unfocus = False\n        ti.focus = True\n        self.assertTrue(ti.focus)\n\n        ti._key_down(\n            (None, None, 'enter', 1),\n            repeat=False\n        )\n        self.assertTrue(ti.validate_test)\n        self.assertTrue(ti.focus)\n\n\nif __name__ == '__main__':\n    import unittest\n    unittest.main()\n"}
{"text": "\"\"\"Use of ``noarch`` and ``skip``\n\nWhen to use ``noarch`` and when to use ``skip`` or pin the interpreter\nis non-intuitive and idiosynractic due to ``conda`` legacy\nbehavior. These checks aim at getting the right settings.\n\n\"\"\"\n\nimport re\n\nfrom . import LintCheck, ERROR, WARNING, INFO\n\n\n# Noarch or not checks:\n#\n# - Python packages that use no compiler should be\n#   a) Marked ``noarch: python``\n#   b) Not use ``skip: True  # [...]`` except for osx/linux,\n#      but use ``- python [<>]3``\n# - Python packages that use a compiler should be\n#   a) NOT marked ``noarch: python``\n#   b) Not use ``- python [<>]3``,\n#      but use ``skip: True  # [py[23]k]``\n\nclass should_be_noarch_python(LintCheck):\n    \"\"\"The recipe should be build as ``noarch``\n\n    Please add::\n\n        build:\n          noarch: python\n\n    Python packages that don't require a compiler to build are\n    normally architecture independent and go into the ``noarch``\n    subset of packages.\n\n    \"\"\"\n    def check_deps(self, deps):\n        if 'python' not in deps:\n            return  # not a python package\n        if all('build' not in loc for loc in deps['python']):\n            return  # only uses python in run/host\n        if any(dep.startswith('compiler_') for dep in deps):\n            return  # not compiled\n        if self.recipe.get('build/noarch', None) == 'python':\n            return  # already marked noarch: python\n        self.message(section='build', data=True)\n\n    def fix(self, _message, _data):\n        self.recipe.set('build/noarch', 'python')\n        return True\n\n\nclass should_be_noarch_generic(LintCheck):\n    \"\"\"The recipe should be build as ``noarch``\n\n    Please add::\n\n        build:\n          noarch: generic\n\n    Packages that don't require a compiler to build are normally\n    architecture independent and go into the ``noarch`` subset of\n    packages.\n\n    \"\"\"\n    requires = ['should_be_noarch_python']\n    def check_deps(self, deps):\n        if any(dep.startswith('compiler_') for dep in deps):\n            return  # not compiled\n        if self.recipe.get('build/noarch', None):\n            return  # already marked noarch\n        self.message(section='build', data=True)\n\n    def fix(self, _message, _data):\n        self.recipe.set('build/noarch', 'generic')\n        return True\n\n\nclass should_not_be_noarch_compiler(LintCheck):\n    \"\"\"The recipe uses a compiler but is marked noarch\n\n    Recipes using a compiler should not be marked noarch.\n\n    Please remove the ``build: noarch:`` section.\n\n    \"\"\"\n    def check_deps(self, deps):\n        if not any(dep.startswith('compiler_') for dep in deps):\n            return  # not compiled\n        if self.recipe.get('build/noarch', False) is False:\n            return  # no noarch, or noarch=False\n        self.message(section='build/noarch')\n\n\nclass should_not_be_noarch_skip(LintCheck):\n    \"\"\"The recipe uses ``skip: True`` but is marked noarch\n\n    Recipes marked as ``noarch`` cannot use skip.\n\n    \"\"\"\n    def check_recipe(self, recipe):\n        if self.recipe.get('build/noarch', False) is False:\n            return  # no noarch, or noarch=False\n        if self.recipe.get('build/skip', False) is False:\n            return  # no skip or skip=False\n        self.message(section='build/noarch')\n\n\nclass should_not_use_skip_python(LintCheck):\n    \"\"\"The recipe should be noarch and not use python based skipping\n\n    Please use::\n\n       requirements:\n          build:\n            - python >3  # or <3\n          run:\n            - python >3  # or <3\n\n    The ``build: skip: True`` feature only works as expected for\n    packages built specifically for each \"platform\" (i.e. Python\n    version and OS). This package should be ``noarch`` and not use\n    skips.\n\n    \"\"\"\n    bad_skip_terms = ('py2k', 'py3k', 'python')\n\n    def check_deps(self, deps):\n        if 'python' not in deps:\n            return  # not a python package\n        if any(dep.startswith('compiler_') for dep in deps):\n            return  # not compiled\n        if self.recipe.get('build/skip', None) is None:\n            return  # no build: skip: section\n        skip_line = self.recipe.get_raw('build/skip')\n        if not any(term in skip_line for term in self.bad_skip_terms):\n            return  # no offending skip terms\n        self.message(section='build/skip')\n\n\nclass should_not_be_noarch_source(LintCheck):\n    \"\"\"The recipe uses per platform sources and cannot be noarch\n\n    You are downloading different upstream sources for each\n    platform. Remove the noarch section or use just one source for all\n    platforms.\n    \"\"\"\n    _pat = re.compile(r'# +\\[.*\\]')\n\n    def check_source(self, source, section):\n        if self.recipe.get('build/noarch', False) is False:\n            return  # no noarch, or noarch=False\n        # just search the entire source entry for a comment\n        if self._pat.search(self.recipe.get_raw(f\"{section}\")):\n            self.message(section)\n"}
{"text": "# Copyright 2011-2019 Camptocamp SA\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).\n\nfrom odoo import fields, models\n\n# make pytest happy\n# in pytest context module dependencies are not loaded\n# thus geo fields are unknown\nfrom odoo.addons import base_geoengine  # noqa\n\n\nclass DummyAbstractModel(models.AbstractModel):\n    _name = 'test.abstract.dummy'\n    _description = 'test.abstract.dummy'\n\n    geo_line = fields.GeoLine(string=\"Line\")\n\n\nclass DummyInheritAbstract(models.Model):\n    _name = 'test.dummy.from_abstract'\n\n    _inherit = 'test.abstract.dummy'\n\n    name = fields.Char()\n\n\nclass DummyModel(models.Model):\n    _name = 'test.dummy'\n    _description = 'test.dummy'\n\n    name = fields.Char()\n    geo_multipolygon = fields.GeoMultiPolygon()\n    geo_point = fields.GeoPoint()\n\n\nclass DummyModelRelated(models.Model):\n    _name = 'test.dummy.related'\n    _description = 'test.dummy.related'\n\n    dummy_test_id = fields.Many2one(\n        comodel_name='test.dummy', string='Dummy test')\n    dummy_geo_multipolygon = fields.GeoMultiPolygon(\n        related='dummy_test_id.geo_multipolygon', string='Related Geom')\n"}
{"text": "import os\nfrom io import open\n\nimport versioneer\n\nfrom setuptools import setup\n\nhere = os.path.abspath(os.path.dirname(__file__))\nwith open(os.path.join(here, 'README.rst'), encoding='utf-8') as f:\n    long_description = ''.join([\n            line for line in f.readlines() if 'travis-ci' not in line\n        ])\n\nsetup(\n    name='hydromet_graph',\n    version=versioneer.get_version(),\n    cmdclass=versioneer.get_cmdclass(),\n    description='Graphing library for hydrological and meteorological variables.',\n    long_description=long_description,\n    author='Andrew MacDonald',\n    author_email='andrew@maccas.net',\n    license='BSD',\n    url='https://github.com/amacd31/hydromet_graph',\n    install_requires=['numpy', 'pandas'],\n    packages = ['hydromet_graph'],\n    test_suite = 'tests',\n    classifiers=[\n        'Development Status :: 3 - Alpha',\n        'Intended Audience :: Developers',\n        'Intended Audience :: Science/Research',\n        'Topic :: Software Development :: Build Tools',\n        'Topic :: Software Development :: Libraries :: Python Modules',\n        'License :: OSI Approved :: BSD License',\n\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.2',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n    ],\n)\n"}
{"text": "# -*- coding: utf-8 -*-\n# \u00a9 2016 Pedro M. Baeza <pedro.baeza@tecnativa.com>\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).\n\nfrom openerp.tests import common\nfrom openerp.exceptions import Warning as UserError\nfrom ..hooks import set_default_map_settings\n\n\nclass TestPartnerExternalMap(common.TransactionCase):\n    def setUp(self):\n        super(TestPartnerExternalMap, self).setUp()\n        self.user = self.env['res.users'].create({\n            'name': 'Test user',\n            'login': 'test_login',\n            'context_map_website_id': self.ref(\n                'partner_external_map.google_maps'),\n            'context_route_map_website_id': self.ref(\n                'partner_external_map.google_maps'),\n        })\n        self.user.partner_id.city = 'Tomelloso'\n        self.partner = self.env['res.partner'].create({\n            'name': 'Test partner',\n            'city': 'Madrid',\n        })\n\n    def test_post_init_hook(self):\n        # Call this again for coverage purposes, but it has been already run\n        set_default_map_settings(self.cr, self.registry)\n        self.assertTrue(self.env.user.context_map_website_id)\n        self.assertTrue(self.env.user.context_route_map_website_id)\n        self.assertEqual(self.env.user.partner_id,\n                         self.env.user.context_route_start_partner_id)\n\n    def test_create_user(self):\n        self.assertEqual(\n            self.user.partner_id, self.user.context_route_start_partner_id)\n\n    def test_open_map(self):\n        action = self.partner.sudo(self.user.id).open_map()\n        self.assertEqual(\n            action['url'], \"https://www.google.com/maps?ie=UTF8&q=Madrid\")\n\n    def test_open_route_map(self):\n        action = self.partner.sudo(self.user.id).open_route_map()\n        self.assertEqual(\n            action['url'], \"https://www.google.com/maps?saddr=Tomelloso&daddr=\"\n                           \"Madrid&directionsmode=driving\")\n\n    def test_open_map_with_coordinates(self):\n        # Simulate that we have the base_geolocalize module installed creating\n        # by hand the variables - This can't be done with routes\n        partner = self.partner.sudo(self.user.id)\n        partner.partner_latitude = 39.15837\n        partner.partner_longitude = -3.02145\n        action = partner.open_map()\n        self.assertEqual(\n            action['url'],\n            \"https://www.google.com/maps?z=15&q=39.15837,-3.02145\")\n\n    def test_exception_no_map_website(self):\n        self.user.context_map_website_id = False\n        with self.assertRaises(UserError):\n            self.partner.sudo(self.user.id).open_map()\n\n    def test_exception_no_map_route_website(self):\n        self.user.context_route_start_partner_id = False\n        with self.assertRaises(UserError):\n            self.partner.sudo(self.user.id).open_route_map()\n\n    def test_exception_no_starting_partner(self):\n        self.user.context_route_map_website_id = False\n        with self.assertRaises(UserError):\n            self.partner.sudo(self.user.id).open_route_map()\n\n    def test_exception_no_address_url(self):\n        self.user.context_map_website_id.address_url = False\n        with self.assertRaises(UserError):\n            self.partner.sudo(self.user.id).open_map()\n\n    def test_exception_no_route_address_url(self):\n        self.user.context_map_website_id.route_address_url = False\n        with self.assertRaises(UserError):\n            self.partner.sudo(self.user.id).open_route_map()\n"}
{"text": "# -*- coding: utf-8 -*-\n# Generated by Django 1.9.9 on 2016-08-23 13:57\nfrom __future__ import unicode_literals\n\nfrom django.conf import settings\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('topics', '0004_auto_20160822_0853'),\n    ]\n\n    operations = [\n        migrations.AlterModelOptions(\n            name='category',\n            options={'verbose_name_plural': 'Kategoriler'},\n        ),\n        migrations.AlterModelOptions(\n            name='entry',\n            options={'verbose_name_plural': 'Yorumlar'},\n        ),\n        migrations.AlterModelOptions(\n            name='favoutire',\n            options={'verbose_name_plural': 'Favoriler'},\n        ),\n        migrations.AlterField(\n            model_name='entry',\n            name='topic',\n            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, to='topics.Topic'),\n        ),\n        migrations.AlterField(\n            model_name='entry',\n            name='user',\n            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL),\n        ),\n        migrations.AlterField(\n            model_name='favoutire',\n            name='entry',\n            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, to='topics.Entry'),\n        ),\n        migrations.AlterField(\n            model_name='favoutire',\n            name='user',\n            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL),\n        ),\n        migrations.AlterField(\n            model_name='topic',\n            name='user',\n            field=models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, to=settings.AUTH_USER_MODEL),\n        ),\n    ]\n"}
{"text": "# coding=utf-8\n# --------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n#\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is\n# regenerated.\n# --------------------------------------------------------------------------\n\nfrom .directory_object import DirectoryObject\n\n\nclass User(DirectoryObject):\n    \"\"\"Active Directory user information.\n\n    Variables are only populated by the server, and will be ignored when\n    sending a request.\n\n    :param additional_properties: Unmatched properties from the message are\n     deserialized this collection\n    :type additional_properties: dict[str, object]\n    :ivar object_id: The object ID.\n    :vartype object_id: str\n    :ivar deletion_timestamp: The time at which the directory object was\n     deleted.\n    :vartype deletion_timestamp: datetime\n    :param object_type: Constant filled by server.\n    :type object_type: str\n    :param immutable_id: This must be specified if you are using a federated\n     domain for the user's userPrincipalName (UPN) property when creating a new\n     user account. It is used to associate an on-premises Active Directory user\n     account with their Azure AD user object.\n    :type immutable_id: str\n    :param usage_location: A two letter country code (ISO standard 3166).\n     Required for users that will be assigned licenses due to legal requirement\n     to check for availability of services in countries. Examples include:\n     \"US\", \"JP\", and \"GB\".\n    :type usage_location: str\n    :param given_name: The given name for the user.\n    :type given_name: str\n    :param surname: The user's surname (family name or last name).\n    :type surname: str\n    :param user_type: A string value that can be used to classify user types\n     in your directory, such as 'Member' and 'Guest'. Possible values include:\n     'Member', 'Guest'\n    :type user_type: str or ~azure.graphrbac.models.UserType\n    :param account_enabled: Whether the account is enabled.\n    :type account_enabled: bool\n    :param display_name: The display name of the user.\n    :type display_name: str\n    :param user_principal_name: The principal name of the user.\n    :type user_principal_name: str\n    :param mail_nickname: The mail alias for the user.\n    :type mail_nickname: str\n    :param mail: The primary email address of the user.\n    :type mail: str\n    :param sign_in_names: The sign-in names of the user.\n    :type sign_in_names: list[~azure.graphrbac.models.SignInName]\n    \"\"\"\n\n    _validation = {\n        'object_id': {'readonly': True},\n        'deletion_timestamp': {'readonly': True},\n        'object_type': {'required': True},\n    }\n\n    _attribute_map = {\n        'additional_properties': {'key': '', 'type': '{object}'},\n        'object_id': {'key': 'objectId', 'type': 'str'},\n        'deletion_timestamp': {'key': 'deletionTimestamp', 'type': 'iso-8601'},\n        'object_type': {'key': 'objectType', 'type': 'str'},\n        'immutable_id': {'key': 'immutableId', 'type': 'str'},\n        'usage_location': {'key': 'usageLocation', 'type': 'str'},\n        'given_name': {'key': 'givenName', 'type': 'str'},\n        'surname': {'key': 'surname', 'type': 'str'},\n        'user_type': {'key': 'userType', 'type': 'str'},\n        'account_enabled': {'key': 'accountEnabled', 'type': 'bool'},\n        'display_name': {'key': 'displayName', 'type': 'str'},\n        'user_principal_name': {'key': 'userPrincipalName', 'type': 'str'},\n        'mail_nickname': {'key': 'mailNickname', 'type': 'str'},\n        'mail': {'key': 'mail', 'type': 'str'},\n        'sign_in_names': {'key': 'signInNames', 'type': '[SignInName]'},\n    }\n\n    def __init__(self, additional_properties=None, immutable_id=None, usage_location=None, given_name=None, surname=None, user_type=None, account_enabled=None, display_name=None, user_principal_name=None, mail_nickname=None, mail=None, sign_in_names=None):\n        super(User, self).__init__(additional_properties=additional_properties)\n        self.immutable_id = immutable_id\n        self.usage_location = usage_location\n        self.given_name = given_name\n        self.surname = surname\n        self.user_type = user_type\n        self.account_enabled = account_enabled\n        self.display_name = display_name\n        self.user_principal_name = user_principal_name\n        self.mail_nickname = mail_nickname\n        self.mail = mail\n        self.sign_in_names = sign_in_names\n        self.object_type = 'User'\n"}
{"text": "\"\"\"empty message\n\nRevision ID: a726fff4ed59\nRevises: 0022_add_pending_status\nCreate Date: 2016-05-25 15:47:32.568097\n\n\"\"\"\n\n# revision identifiers, used by Alembic.\nrevision = '0022_add_pending_status'\ndown_revision = '0021_add_delivered_failed_counts'\n\nfrom alembic import op\nimport sqlalchemy as sa\n\n\ndef upgrade():\n    ### commands auto generated by Alembic - please adjust! ###\n    status_type = sa.Enum('sending', 'delivered', 'pending', 'failed',\n                          'technical-failure', 'temporary-failure', 'permanent-failure',\n                          name='notify_status_types')\n    status_type.create(op.get_bind())\n    op.add_column('notifications', sa.Column('new_status', status_type, nullable=True))\n    op.execute('update notifications set new_status = CAST(CAST(status as text) as notify_status_types)')\n    op.alter_column('notifications', 'status', new_column_name='old_status')\n    op.alter_column('notifications', 'new_status', new_column_name='status')\n    op.drop_column('notifications', 'old_status')\n    op.get_bind()\n    op.execute('DROP TYPE notification_status_type')\n    op.alter_column('notifications', 'status', nullable=False)\n\n    ### end Alembic commands ###\n\n\ndef downgrade():\n    ### commands auto generated by Alembic - please adjust! ###\n    status_type = sa.Enum('sending', 'delivered', 'failed',\n                          'technical-failure', 'temporary-failure', 'permanent-failure',\n                          name='notification_status_type')\n    status_type.create(op.get_bind())\n    op.add_column('notifications', sa.Column('old_status', status_type, nullable=True))\n\n    op.execute(\"update notifications set status = 'sending' where status = 'pending'\")\n    op.execute('update notifications set old_status = CAST(CAST(status as text) as notification_status_type)')\n    op.alter_column('notifications', 'status', new_column_name='new_status')\n    op.alter_column('notifications', 'old_status', new_column_name='status')\n    op.drop_column('notifications', 'new_status')\n    op.get_bind()\n    op.execute('DROP TYPE notify_status_types')\n    op.alter_column('notifications', 'status', nullable=False)\n    ### end Alembic commands ###\n"}
{"text": "import numpy as np\nfrom scipy.stats import sem\nfrom uncertainties import ufloat\nimport uncertainties.unumpy as unp\n\nZeiten = np.genfromtxt(\"Restdaten.txt\", unpack = True)\n\n#print(Zeiten[2:])\n\nZeiten /= 5\n\n#print(Zeiten[2:])\n\ndaten = np.genfromtxt(\"WRGdyn.txt\", unpack = True)\n\nMittelwerte = np.array([np.mean(row) for row in Zeiten])\nFehler = np.array([np.std(row, ddof = 1) for row in Zeiten])\ns = 1/np.sqrt(len(Zeiten[0]))\nFehler = s*Fehler\n\nT = np.array([ufloat(x, Fehler[index]) for index, x in np.ndenumerate(Mittelwerte)])\n\nWrg = ufloat(daten[0], daten[1])\nId = ufloat(daten[2], daten[3])\n\n#Tr\u00e4gheitsmoment der Kugel\n\nMasseK = 0.8124\nRadiusK = 0.13766 / 2\n\nIk_theoretisch = 2/5 * MasseK * RadiusK**2\n\nIk_praktisch = T[0]**2 * Wrg /(4* (np.pi**2)) - Id\n\n#print(\"IK: \")\n#print(T[0])\nprint(Ik_theoretisch, Ik_praktisch)\nprint(Ik_praktisch/Ik_theoretisch)\n\n#tr\u00e4gheitsmoment des Zylinders\n\nMasseZ = 1.0058\nRadiusZ = 0.08024/2\nH\u00f6heZ = 0.13990\n\nIz_theoretisch = 1/2 * MasseZ * RadiusZ**2\n\nIz_praktisch = T[1]**2 * Wrg / (4* (np.pi**2)) - Id\n\n#print(\"IZ: \")\n#print(T[1])\nprint(Iz_theoretisch, Iz_praktisch)\nprint(Iz_praktisch/Iz_theoretisch)\n\n#Tr\u00e4gheitsmoment Position 1\n\nIp1 = T[2]**2 * Wrg / (4 * (np.pi**2)) - Id\n\n#Tr\u00e4gheitsmoment Position 2\n\nIp2 = T[3]**2 * Wrg / (4 * (np.pi**2)) - Id\n\n#print(T[2], T[3])\nprint(Ip1, Ip2)\n"}
{"text": "## {{{ http://code.activestate.com/recipes/81611/ (r2)\r\ndef int_to_roman(input):\r\n\t\"\"\"\r\n\tConvert an integer to Roman numerals.\r\n\r\n\tExamples:\r\n\t>>> int_to_roman(0)\r\n\tTraceback (most recent call last):\r\n\tValueError: Argument must be between 1 and 3999\r\n\r\n\t>>> int_to_roman(-1)\r\n\tTraceback (most recent call last):\r\n\tValueError: Argument must be between 1 and 3999\r\n\r\n\t>>> int_to_roman(1.5)\r\n\tTraceback (most recent call last):\r\n\tTypeError: expected integer, got <type 'float'>\r\n\r\n\t>>> for i in range(1, 21): print int_to_roman(i)\r\n\t...\r\n\tI\r\n\tII\r\n\tIII\r\n\tIV\r\n\tV\r\n\tVI\r\n\tVII\r\n\tVIII\r\n\tIX\r\n\tX\r\n\tXI\r\n\tXII\r\n\tXIII\r\n\tXIV\r\n\tXV\r\n\tXVI\r\n\tXVII\r\n\tXVIII\r\n\tXIX\r\n\tXX\r\n\t>>> print int_to_roman(2000)\r\n\tMM\r\n\t>>> print int_to_roman(1999)\r\n\tMCMXCIX\r\n\t\"\"\"\r\n\tif type(input) != type(1):\r\n\t\traise TypeError, \"expected integer, got %s\" % type(input)\r\n\tif not 0 < input < 4000:\r\n\t\traise ValueError, \"Argument must be between 1 and 3999\"\t\r\n\tints = (1000, 900,  500, 400, 100,  90, 50,  40, 10,  9,\t5,  4,\t1)\r\n\tnums = ('M',  'CM', 'D', 'CD','C', 'XC','L','XL','X','IX','V','IV','I')\r\n\tresult = \"\"\r\n\tfor i in range(len(ints)):\r\n\t\tcount = int(input / ints[i])\r\n\t\tresult += nums[i] * count\r\n\t\tinput -= ints[i] * count\r\n\treturn result\r\n\r\n\r\n\r\ndef roman_to_int(input):\r\n\t\"\"\"\r\n\tConvert a roman numeral to an integer.\r\n\t\r\n\t>>> r = range(1, 4000)\r\n\t>>> nums = [int_to_roman(i) for i in r]\r\n\t>>> ints = [roman_to_int(n) for n in nums]\r\n\t>>> print r == ints\r\n\t1\r\n\r\n\t>>> roman_to_int('VVVIV')\r\n\tTraceback (most recent call last):\r\n\t ...\r\n\tValueError: input is not a valid roman numeral: VVVIV\r\n\t>>> roman_to_int(1)\r\n\tTraceback (most recent call last):\r\n\t ...\r\n\tTypeError: expected string, got <type 'int'>\r\n\t>>> roman_to_int('a')\r\n\tTraceback (most recent call last):\r\n\t ...\r\n\tValueError: input is not a valid roman numeral: A\r\n\t>>> roman_to_int('IL')\r\n\tTraceback (most recent call last):\r\n\t ...\r\n\tValueError: input is not a valid roman numeral: IL\r\n\t\"\"\"\r\n\tif type(input) not in (str, unicode):\r\n\t\traise TypeError, \"expected string, got %s\" % type(input)\r\n\tinput = input.upper()\r\n\tnums = ['M', 'D', 'C', 'L', 'X', 'V', 'I']\r\n\tints = [1000, 500, 100, 50,  10,  5,\t1]\r\n\tplaces = []\r\n\tfor c in input:\r\n\t\tif not c in nums:\r\n\t\t\traise ValueError, \"input is not a valid roman numeral: %s\" % input\r\n\tfor i in range(len(input)):\r\n\t\tc = input[i]\r\n\t\tvalue = ints[nums.index(c)]\r\n\t\t# If the next place holds a larger number, this value is negative.\r\n\t\ttry:\r\n\t\t\tnextvalue = ints[nums.index(input[i +1])]\r\n\t\t\tif nextvalue > value:\r\n\t\t\t\tvalue *= -1\r\n\t\texcept IndexError:\r\n\t\t\t# there is no next place.\r\n\t\t\tpass\r\n\t\tplaces.append(value)\r\n\tsum = 0\r\n\tfor n in places: sum += n\r\n\t# Easiest test for validity...\r\n\tif int_to_roman(sum) == input:\r\n\t\treturn sum\r\n\telse:\r\n\t\traise ValueError, 'input is not a valid roman numeral: %s' % input\r\n## end of http://code.activestate.com/recipes/81611/ }}}\r\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, unicode_literals\nfrom gaebusiness.business import CommandExecutionException\nfrom tekton.gae.middleware.json_middleware import JsonResponse\nfrom amigo_app import facade\n\n\ndef index():\n    cmd = facade.list_amigos_cmd()\n    amigo_list = cmd()\n    short_form=facade.amigo_short_form()\n    amigo_short = [short_form.fill_with_model(m) for m in amigo_list]\n    return JsonResponse(amigo_short)\n\n\ndef save(**amigo_properties):\n    cmd = facade.save_amigo_cmd(**amigo_properties)\n    return _save_or_update_json_response(cmd)\n\n\ndef update(amigo_id, **amigo_properties):\n    cmd = facade.update_amigo_cmd(amigo_id, **amigo_properties)\n    return _save_or_update_json_response(cmd)\n\n\ndef delete(amigo_id):\n    facade.delete_amigo_cmd(amigo_id)()\n\n\ndef _save_or_update_json_response(cmd):\n    try:\n        amigo = cmd()\n    except CommandExecutionException:\n        return JsonResponse({'errors': cmd.errors})\n    short_form=facade.amigo_short_form()\n    return JsonResponse(short_form.fill_with_model(amigo))\n\n"}
{"text": "#!/usr/bin/python\n# coding: UTF-8\n\nimport sys\nfrom socket import *\nimport struct\nimport raid_pb2\nimport wanyaogu_pb2\nimport login_pb2\nimport cast_skill_pb2\nimport move_direct_pb2\nimport team_pb2\nimport datetime\nimport get_one_msg\nimport scene_transfer_pb2\nimport horse_pb2\n\nWATCH_PLAYER = {8589935415}\n\nHOST='127.0.0.1'\nPORT=13697\nPORT=get_one_msg.get_dumpsrv_port()\nADDR=(HOST, PORT)\nclient=socket(AF_INET, SOCK_STREAM)\nclient.connect(ADDR)\n\nlast_data = \"\"\nplayer_list = {}\n\ndef get_buff_data(t1):\n    retdata = \"\"\n    for buffinfo in t1.buff_info:\n        tmp = \"(%d) \" % buffinfo.id\n        retdata = retdata + tmp\n    return retdata\n\nwhile True:\n    ret, last_data, player_id, msg_id, pb_data = get_one_msg.get_one_msg(client, last_data)\n    if ret == -1:\n        break\n    if ret == 0:\n        continue\n\n\n#    data_len = data_len - 8 - 16\n#    msg_format = \"=IHH\" + str(data_len) + 'sQIHH' \n#    msg_len, msg_id, seq, pb_data, player_id, t1, t1, t1 = struct.unpack(msg_format, data)\n\n#    print \"read msg:\", msg_id\n\n#    if not player_id in WATCH_PLAYER:\n#        continue;\n\n#\u573a\u666f\u5207\u6362 10112\n    if msg_id == 10112:\n        req = scene_transfer_pb2.scene_transfer_answer();\n        req.ParseFromString(pb_data)\n        oldtime=datetime.datetime.now()\n        print  oldtime.time(), \": %lu \u8fdb\u5165\u573a\u666f[%s]\" % (player_id, req.new_scene_id)        \n    \n    \n#\u526f\u672c\u5b8c\u6210 10812\n    if msg_id == 10812:\n        req = raid_pb2.raid_finish_notify();\n        req.ParseFromString(pb_data)\n        oldtime=datetime.datetime.now()\n        print  oldtime.time(), \": %lu \u526f\u672c\u7ed3\u7b97[%s]\" % (player_id, req.star)                \n\n#\u4e07\u5996\u5361\u5217\u8868  11401\n    if msg_id == 11401:\n        req = wanyaogu_pb2.list_wanyaoka_answer()\n        req.ParseFromString(pb_data)\n        oldtime=datetime.datetime.now()\n        print  oldtime.time(), \": %lu \u4e07\u5996\u5361\u5217\u8868[%s]\" % (player_id, req.wanyaoka_id)                \n\n#\u4e07\u5996\u8c37\u5173\u5361\u5f00\u59cb\u901a\u77e5  11402\n    if msg_id == 11402:\n        req = wanyaogu_pb2.wanyaogu_start_notify()\n        req.ParseFromString(pb_data)\n        oldtime=datetime.datetime.now()\n        print  oldtime.time(), \": %lu \u4e07\u5996\u8c37\u5f00\u59cb[%lu]\" % (player_id, req.start_time)                \n\n#\u4e07\u5996\u8c37\u5173\u5361\u706b\u7089\u6302\u673a\u901a\u77e5 11403\n    if msg_id == 11403:\n        oldtime=datetime.datetime.now()\n        print  oldtime.time(), \": %lu \u4e07\u5996\u8c37\u706b\u7089\u6302\u673a\" % (player_id)\n\n#\u8fdb\u5165\u6e38\u620f\u5bf9\u65f6\n    if msg_id == 10007:\n        req = login_pb2.EnterGameAnswer()\n        req.ParseFromString(pb_data)        \n        oldtime=datetime.datetime.now()\n        print  oldtime.time(), \": %lu \u8fdb\u5165\u6e38\u620f %u %d [%u %s %s]\" % (player_id, req.curTime, req.direct, req.sceneId, req.posX, req.posZ)        \n\n#11404 //\u83b7\u5f97\u4e07\u5996\u5361\u901a\u77e5 wanyaoka_get_notify\n    if msg_id == 11404:\n        req = wanyaogu_pb2.wanyaoka_get_notify()\n        req.ParseFromString(pb_data)        \n        oldtime=datetime.datetime.now()\n        print  oldtime.time(), \": %lu \u83b7\u5f97\u4e07\u5996\u5361 %s\" % (player_id, req.wanyaoka_id)\n"}
{"text": "\"\"\" run with\n\nnosetests -v --nocapture --nologcapture\nnosetests -v  --nocapture test_inventory.py:Test_Inventory.test_06\nnosetests -v\n\n\"\"\"\nfrom __future__ import print_function\nfrom datetime import datetime\n\nfrom cloudmesh_base.util import HEADING\nfrom cloudmesh.inventory import Inventory\nfrom pprint import pprint\n\n\nclass Test_Inventory:\n\n    def setup(self):\n        self.cluster = \"bravo\"\n        self.name = \"b010\"\n        self.inventory = Inventory()\n        self.inventory.clear()\n        self.inventory.generate()\n        print(\"GENERATION COMPLETE\")\n\n    def tearDown(self):\n        pass\n\n    def test_clear(self):\n        HEADING()\n        self.inventory.clear()\n\n    def test_find(self):\n        HEADING()\n        r = self.inventory.find({})\n        print(r.count())\n        assert r.count > 0\n\n    def test_host(self):\n        HEADING()\n        data = self.inventory.host(self.name)\n        pprint(data)\n\n    def test_list(self):\n        HEADING()\n        data = self.inventory.hostlist(self.cluster)\n        # pprint(data)\n\n    def test_combine(self):\n\n        attribute = \"cm_temp\"\n        value = \"32\"\n\n        print(\"SET ATTRIBUTE\")\n        print(70 * '=')\n        data = self.inventory.set_attribute(self.name, attribute, value)\n        print(70 * '=')\n        print(data)\n\n        print(\"GET ATTRIBUTE\")\n        data = self.inventory.get_attribute(self.name, attribute)\n        print(data)\n\n        data = self.inventory.host(self.name)\n        pprint(data)\n\n    def test_set(self):\n        HEADING()\n\n        \"\"\"\n        data = self.inventory.find({'cm_id': self.name})\n\n        for e in data:\n            pprint (e)\n        \"\"\"\n        print(70 * '=')\n        \"\"\"\n        print \"BEFORE\"\n\n        data = self.inventory.host(self.name)\n        pprint(data)\n        \"\"\"\n\n        attribute = \"cm_temp\"\n        value = \"32\"\n\n        print(\"SET ATTRIBUTE\")\n        print(70 * '=')\n        data = self.inventory.set_attribute(self.name, attribute, value)\n        print(70 * '=')\n        print(data)\n\n        print(\"GET ATTRIBUTE\")\n        data = self.inventory.get_attribute(self.name, attribute)\n        print(data)\n\n    def test_i066(self):\n        HEADING()\n\n        name = \"i066\"\n        attribute = \"cm_doesnotexist\"\n        print(\"GET ATTRIBUTE\")\n        data = self.inventory.get_attribute(name, attribute)\n        print(data)\n\n    \"\"\"\n        data = self.inventory.host(self.name)\n        print \"AFTER\"\n        pprint(data)\n\n\n\n    ef test_ipaddr(self):\n        HEADING()\n\n        print self.inventory.ipadr (self.name, \"public\")\n        print self.inventory.ipadr (self.name, \"internal\")\n    \"\"\"\n"}
{"text": "'''\n ' This file is part of redlite.\n '\n ' redlite is free software: you can redistribute it and/or modify\n ' it under the terms of the GNU General Public License as published by\n ' the Free Software Foundation, either version 3 of the License, or\n ' (at your option) any later version.\n '\n ' redlite is distributed in the hope that it will be useful,\n ' but WITHOUT ANY WARRANTY; without even the implied warranty of\n ' MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n ' GNU General Public License for more details.\n '\n ' You should have received a copy of the GNU General Public License\n ' along with redlite.  If not, see <http://www.gnu.org/licenses/>.\n'''\n\nimport core as redlite\nimport serial\nfrom sys import exit\n\n# Constants\nSERIAL_PORT = \"COM1\"\nLIGHT_ON_SECONDS = 20\nCOMMAND_ON = \"on\"\nCOMMAND_OFF = \"off\"\n\n# Global Variables\ncyclesOn = 0\nlightOn = False\nrefreshCounter = 0\n\n# Setup\nteam = redlite.promptTeam()\n\nif(team == \"\"):\n\tprint(\"Exiting...\")\n\texit()\n\ngame, teamType = redlite.findGame(t)\n\nif game == -1:\n\tprint(\"No Game Today\")\n\texit()\n\ndata = redlite.loadGameData(game, teamType)\n\nrefreshRate = data[0]\nteam = data[1]\nlastEvent = data[2]\n\nser = serial.Serial(SERIAL_PORT, 9600, timeout=1)\n\n# Main Loop\nwhile(True):\n\t\n\tif (lightOn == True):\n\t\tcyclesOn = cyclesOn + 1\n\n\tif (cyclesOn >= LIGHT_ON_SECONDS):\n\t\tser.write(COMMAND_OFF)\n\t\tcyclesOn = 0\n\t\tlightOn = False\n\n\tif(refreshCounter >= refreshRate):\n\n\t\trefreshCounter = 0\n\n\t\tgoal, lastEvent, refreshRate = redlite.goal(game, team, lastEvent)\n\n\t\tif (goal):\n\t\t\tser.write(COMMAND_ON)\n\t\t\tlightOn = True\n\n\telse:\n\t\trefreshCounter = refreshCounter + 1\n\n\tif(refreshRate == 0):\n\t\tbreak\n\n\ttime.sleep(1)\n\n# End of Main Loop\n\nser.close()"}
{"text": "import unittest\n\nfrom malcolm.core import Alarm, AlarmSeverity, Process\nfrom malcolm.modules.builtin.controllers import BasicController\nfrom malcolm.modules.builtin.infos import HealthInfo\n\n\nclass TestBasicController(unittest.TestCase):\n    def setUp(self):\n        self.process = Process(\"proc\")\n        self.o = BasicController(\"MyMRI\")\n        self.process.add_controller(self.o)\n        self.process.start()\n        self.b = self.process.block_view(\"MyMRI\")\n\n    def tearDown(self):\n        self.process.stop(timeout=2)\n\n    def update_health(self, num, alarm=Alarm.ok):\n        self.o.update_health(num, HealthInfo(alarm))\n\n    def test_set_health(self):\n        self.update_health(1, Alarm(severity=AlarmSeverity.MINOR_ALARM))\n        self.update_health(2, Alarm(severity=AlarmSeverity.MAJOR_ALARM))\n        assert self.b.health.alarm.severity == AlarmSeverity.MAJOR_ALARM\n\n        self.update_health(1, Alarm(severity=AlarmSeverity.UNDEFINED_ALARM))\n        self.update_health(2, Alarm(severity=AlarmSeverity.INVALID_ALARM))\n        assert self.b.health.alarm.severity == AlarmSeverity.UNDEFINED_ALARM\n\n        self.update_health(1)\n        self.update_health(2)\n        assert self.o.health.value == \"OK\"\n"}
{"text": "# -*- coding: utf-8 -*-\n#\n# Copyright 2015-2020 BigML\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n\n\"\"\"Options for BigMLer report option\n\n\"\"\"\nDEFAULT_PORT = 8085\n\ndef get_report_options(defaults=None):\n    \"\"\"Report-related options\n\n    \"\"\"\n\n    if defaults is None:\n        defaults = {}\n    options = {\n\n        # Retrieves the information from the resources in the directory.\n        '--from-dir': {\n            'dest': 'from_dir',\n            'default': defaults.get('from_dir', None),\n            'help': (\"Retrieves the information from the\"\n                     \" resources in the directory.\")},\n\n        # Uses the user-given port to start a local HTTP server\n        '--port': {\n            'dest': 'port',\n            'type': int,\n            'default': defaults.get('port', DEFAULT_PORT),\n            'help': (\"Port where the local HTTP server is bound.\")},\n\n        # Not starting the local HTTP server. Only generates the report files.\n        '--no-server': {\n            'dest': 'no_server',\n            'action': 'store_true',\n            'default': defaults.get('no_server', False),\n            'help': (\"Not starting the HTTP server. Only generates\"\n                     \" the report files.\")},\n\n        # Starting the local HTTP server, as opposed to --no-server\n        '--server': {\n            'dest': 'no_server',\n            'action': 'store_false',\n            'default': defaults.get('no_server', False),\n            'help': (\"Sarting the HTTP server to view the report files.\")}\n        }\n\n    return options\n"}
{"text": "###############################################################################\n#\n#    Copyright (C) 2001-2014 Micronaet SRL (<http://www.micronaet.it>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as published\n#    by the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n###############################################################################\n\n{\n    'name': 'Show photo switch',\n    'version': '0.1',\n    'category': 'Product',\n    'description': '''  \n        Parameter for user to show photo in product form      \n        ''',\n    'author': 'Micronaet S.r.l. - Nicola Riolini',\n    'website': 'http://www.micronaet.it',\n    'license': 'AGPL-3',\n    'depends': [\n        'base',\n        'product',\n        'quotation_photo',\n        ],\n    'init_xml': [],\n    'demo': [],\n    'data': [\n        'photo_view.xml',\n        ],\n    'active': False,\n    'installable': True,\n    'auto_install': False,\n    }\n"}
{"text": "# -*- coding: utf-8 -*-\n\n###############################################################################\n#\n# GetActivityWeeklyGoals\n# Get a user's current weekly activity goals.\n#\n# Python versions 2.6, 2.7, 3.x\n#\n# Copyright 2014, Temboo Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,\n# either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n#\n#\n###############################################################################\n\nfrom temboo.core.choreography import Choreography\nfrom temboo.core.choreography import InputSet\nfrom temboo.core.choreography import ResultSet\nfrom temboo.core.choreography import ChoreographyExecution\n\nimport json\n\nclass GetActivityWeeklyGoals(Choreography):\n\n    def __init__(self, temboo_session):\n        \"\"\"\n        Create a new instance of the GetActivityWeeklyGoals Choreo. A TembooSession object, containing a valid\n        set of Temboo credentials, must be supplied.\n        \"\"\"\n        super(GetActivityWeeklyGoals, self).__init__(temboo_session, '/Library/Fitbit/Activities/GetActivityWeeklyGoals')\n\n\n    def new_input_set(self):\n        return GetActivityWeeklyGoalsInputSet()\n\n    def _make_result_set(self, result, path):\n        return GetActivityWeeklyGoalsResultSet(result, path)\n\n    def _make_execution(self, session, exec_id, path):\n        return GetActivityWeeklyGoalsChoreographyExecution(session, exec_id, path)\n\nclass GetActivityWeeklyGoalsInputSet(InputSet):\n    \"\"\"\n    An InputSet with methods appropriate for specifying the inputs to the GetActivityWeeklyGoals\n    Choreo. The InputSet object is used to specify input parameters when executing this Choreo.\n    \"\"\"\n    def set_AccessTokenSecret(self, value):\n        \"\"\"\n        Set the value of the AccessTokenSecret input for this Choreo. ((required, string) The Access Token Secret retrieved during the OAuth process.)\n        \"\"\"\n        super(GetActivityWeeklyGoalsInputSet, self)._set_input('AccessTokenSecret', value)\n    def set_AccessToken(self, value):\n        \"\"\"\n        Set the value of the AccessToken input for this Choreo. ((required, string) The Access Token retrieved during the OAuth process.)\n        \"\"\"\n        super(GetActivityWeeklyGoalsInputSet, self)._set_input('AccessToken', value)\n    def set_ConsumerKey(self, value):\n        \"\"\"\n        Set the value of the ConsumerKey input for this Choreo. ((required, string) The Consumer Key provided by Fitbit.)\n        \"\"\"\n        super(GetActivityWeeklyGoalsInputSet, self)._set_input('ConsumerKey', value)\n    def set_ConsumerSecret(self, value):\n        \"\"\"\n        Set the value of the ConsumerSecret input for this Choreo. ((required, string) The Consumer Secret provided by Fitbit.)\n        \"\"\"\n        super(GetActivityWeeklyGoalsInputSet, self)._set_input('ConsumerSecret', value)\n    def set_ResponseFormat(self, value):\n        \"\"\"\n        Set the value of the ResponseFormat input for this Choreo. ((optional, string) The format that you want the response to be in: xml or json. Defaults to json.)\n        \"\"\"\n        super(GetActivityWeeklyGoalsInputSet, self)._set_input('ResponseFormat', value)\n    def set_UserID(self, value):\n        \"\"\"\n        Set the value of the UserID input for this Choreo. ((optional, string) The user's encoded id. Defaults to \"-\" (dash) which will return data for the user associated with the token credentials provided.)\n        \"\"\"\n        super(GetActivityWeeklyGoalsInputSet, self)._set_input('UserID', value)\n\nclass GetActivityWeeklyGoalsResultSet(ResultSet):\n    \"\"\"\n    A ResultSet with methods tailored to the values returned by the GetActivityWeeklyGoals Choreo.\n    The ResultSet object is used to retrieve the results of a Choreo execution.\n    \"\"\"\n\n    def getJSONFromString(self, str):\n        return json.loads(str)\n\n    def get_Response(self):\n        \"\"\"\n        Retrieve the value for the \"Response\" output from this Choreo execution. (The response from Fitbit.)\n        \"\"\"\n        return self._output.get('Response', None)\n\nclass GetActivityWeeklyGoalsChoreographyExecution(ChoreographyExecution):\n\n    def _make_result_set(self, response, path):\n        return GetActivityWeeklyGoalsResultSet(response, path)\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8\n# ----------------------------------------------------------------------\n# Splits wavs into splits\n# ----------------------------------------------------------------------\n# Ivan Vladimir Meza-Ruiz/ ivanvladimir at turing.iimas.unam.mx\n# 2013/IIMAS/UNAM\n# ----------------------------------------------------------------------\n\n#!/usr/bin/env python\n# -*- coding: utf-8\n# ----------------------------------------------------------------------\n# Author ID main using a sparse representation\n# ----------------------------------------------------------------------\n# Ivan V. Meza\n# 2014/IIMAS, M\u00e9xico\n# ----------------------------------------------------------------------\n# authorid.py is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU General Public License as published by\n#    the Free Software Foundation, either version 3 of the License, or\n#    (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU General Public License for more details.\n#\n#    You should have received a copy of the GNU General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n# -------------------------------------------------------------------------\nfrom __future__ import print_function\n\n# System libraries\nimport argparse\nimport sys\nimport os\nfrom bs4 import BeautifulSoup\nfrom multiprocessing import Pool\nfrom numpy import vstack, save\nfrom essentia.standard import *\nimport audiolab\n\n\n# Local import\nparent=os.path.abspath('../../')\nsys.path.append(parent)\nfrom sonidero.stats import StatsCalculator\n\n\nw = Windowing(type = 'hann')\nspectrum  = Spectrum()\nmelbands  = MelBands(numberBands=40,inputSize=513)\nwithening = SpectralWhitening()\npeaks     = SpectralPeaks(maxFrequency=22100)\nstatcalculator = StatsCalculator()\nstatcalculator.set('all')\n\nverbose = lambda *a: None\n\ndef extract_feats(xmlname,dirwave):\n    verbose('Openning ',xmlname)\n    with open(xmlname) as xml_:\n        birdinfo=BeautifulSoup(xml_.read())\n    wavname=os.path.join(dirwave,birdinfo.filename.string)\n    verbose('Openning ',wavname)\n    sound=audiolab.sndfile(wavname,'read')\n    audio=sound.read_frames(sound.get_nframes())\n    energies=[]\n    for frame in FrameGenerator(essentia.array(audio), frameSize = 1024, hopSize = 210):\n        fspectrum=spectrum(w(frame))\n        fpeaksF,fpeaksM=peaks(fspectrum)\n        withening(fspectrum,fpeaksF,fpeaksM)\n        band=melbands(fspectrum)\n        energies.append(band)\n    if birdinfo.classid:\n        idd=birdinfo.classid.string\n    else:\n        idd='u'\n    return idd, vstack(energies)\n            \n    \ndef process_file(filename,wavdir,outdir):\n    verbose('Extracting features from',filename)\n    idd,feats=extract_feats(filename,wavdir)\n    name=os.path.basename(filename)\n    name=os.path.splitext(name)[0]\n    save(os.path.join(outdir,name),feats)\n    return (name,idd)\n\ndef process_file_(args):\n    return process_file(*args)\n\n\n# MAIN program\nif __name__ == \"__main__\":\n    # Command line options\n    p = argparse.ArgumentParser(\"Feature extaction from wav files\")\n    p.add_argument(\"XMLDIR\",default=None,\n            action=\"store\", help=\"Directory with xml descriptions or xml\")\n    p.add_argument(\"WAVDIR\",default=\"wav\",\n            action=\"store\", help=\"Directory with wav files\")\n    p.add_argument(\"OUTDIR\",default=\"wav\",\n            action=\"store\", help=\"Output directory files\")\n    p.add_argument(\"--processors\",default=1,type=int,\n            action=\"store\", dest=\"nprocessors\",\n            help=\"Number of processors [1]\")\n    p.add_argument(\"-v\", \"--verbose\",\n            action=\"store_true\", dest=\"verbose\",\n            help=\"Verbose mode [Off]\")\n    opts = p.parse_args()\n\n    # Managing configuration ----------------------------------------------\n    # prepara funci\u00f3n de verbose\n    if opts.verbose:\n        def verbose(*args,**kargs):\n            print(*args,**kargs)\n\n    # Process if only one ------------------------------------------------\n    if not os.path.isdir(opts.XMLDIR):\n        idd=process_file(opts.XMLDIR,opts.WAVDIR,opts.OUTDIR)\n        save(os.path.join(opts.OUTDIR,'ids'),[idd])\n        sys.exit(0)\n        \n    # Preparing processors -----------------------------------------------\n    pool =  Pool(processes=opts.nprocessors)\n    # Traverse xml files -------------------------------------------------\n    for root, dirs, files in os.walk(opts.XMLDIR):\n        args=[ (os.path.join(root,file),opts.WAVDIR,opts.OUTDIR) \n                for file in files\n                    if file.endswith('.xml')]\n        verbose('Processing',len(args),'files from',root)\n        if opts.nprocessors > 1:\n            idds=pool.map(process_file_,args)\n        else:\n            idds=map(process_file_,args)\n    save(os.path.join(opts.OUTDIR,'ids'),idds)\n\n\n            \n\n"}
{"text": "#!/usr/bin/env python3\n\nimport json\nfrom subprocess import PIPE, Popen\n\nfrom requests import get\n\n\nrepo_master_raw = 'https://raw.githubusercontent.com/g0v/awesome-g0v/master/'\nreadme = 'readme.md'\nparser = 'parse.ls'\nawesome_g0v = 'awesome-g0v.json'\noutfile = 'url_list.json'\n\n\ndef get_source():\n    readme_url = repo_master_raw + readme\n    parser_url = repo_master_raw + parser\n\n    with open('./data/{}'.format(readme), 'wb+') as f:\n        response = get(readme_url)\n        f.write(response.content)\n\n    with open('./data/{}'.format(parser), 'wb+') as f:\n        response = get(parser_url)\n        f.write(response.content)\n\n\ndef run_parser():\n    try:\n        with Popen(['lsc', parser], cwd='./data/', stdout=PIPE) as p:\n            print(p.stdout.read().decode('utf-8'))\n    except Exception as e:\n        print(e)\n\n\ndef output_url_list():\n    with open('./data/{}'.format(awesome_g0v), 'r') as f:\n        js = json.load(f)\n        rs = [j['repository'] for j in js if 'github.com' in j['repository']]\n\n    with open('./data/{}'.format(outfile), 'w+') as f:\n        f.write(json.dumps(rs))\n\nif __name__ == \"__main__\":\n    get_source()\n    run_parser()\n    output_url_list()\n"}
{"text": "# -*- coding: utf-8 -*-\n'''\n Copyright (C) 2013 onwards University of Deusto\n  \n All rights reserved.\n \n This software is licensed as described in the file COPYING, which\n you should have received as part of this distribution.\n \n This software consists of contributions made by many individuals, \n listed below:\n \n @author: Aitor G\u00f3mez Goiri <aitor.gomez@deusto.es>\n'''\n\nfrom actuation.proofs.reason import EyeReasoner\nfrom actuation.scenarios.abstract import AbstractSimulation, main\nfrom actuation.impl.space import CoordinationSpace\nfrom actuation.impl.rest.lamp.provider import LampProviderRESTMock\nfrom actuation.impl.space.lamp.mock.consumer import LampConsumerSpaceMock\nfrom actuation.impl.mix import IntermediaryAgent\nfrom actuation.impl.rest.mock.discovery import MockDiscovery\n\n\nclass OnlySpaceBasedDevicesSimulator(AbstractSimulation):\n    \n    def __init__(self, input_folder, output_folder, path_to_reasoner, num_providers, debug = False):\n        super(OnlySpaceBasedDevicesSimulator, self).__init__( output_folder )\n        self.input_folder = input_folder\n        self.reasoner = EyeReasoner( path_to_reasoner )\n    \n    @property    \n    def lc(self):\n        return self.nodes[\"consumer\"]\n    \n    @lc.setter\n    def lc(self, value):\n        self.nodes[\"consumer\"] = value\n    \n    @property    \n    def lp(self):\n        return self.nodes[\"provider\"]\n    \n    @lp.setter\n    def lp(self, value):\n        self.nodes[\"provider\"] = value\n    \n    def configure(self):\n        debug = True\n        discovery = MockDiscovery()\n        \n        self.space = CoordinationSpace(\"mixedSpace\")\n        self.nodes[\"agent\"] = IntermediaryAgent( self.space,\n                                                 self.input_folder+\"mix/\",\n                                                 self.output_folder,\n                                                 self.reasoner,\n                                                 discovery)\n        \n        self.lp = LampProviderRESTMock( self.input_folder + \"rest/\", self.output_folder)\n        self.lc = LampConsumerSpaceMock( self.space,\n                                         self.input_folder + \"space/\",\n                                         self.output_folder,\n                                         debug = debug )\n        discovery.add_discovered( self.lp, \"example.org\")\n    \n    def execute(self):\n        \"\"\"\n        Executes the scenario with a REST provider and a consumer using a space.\n        \"\"\"\n        light_value = 30\n        self.lc.subscribe_to_result(light_value)\n        self.lc.write_task(light_value)\n    \n    def check(self):\n        rsc = self.lp.get_resource(\"/lamp/actuators/light/2/\")\n        # TODO check that the value of this resource is the desired one (manually checked) \n        return rsc is not None\n\n\nif __name__ == '__main__':\n    main( OnlySpaceBasedDevicesSimulator )"}
{"text": "# vim: tabstop=4 shiftwidth=4 softtabstop=4\n\n# Copyright 2013 OpenStack LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\n\nfrom keystone.common import dependency\nfrom keystone.common import logging\nfrom keystone import auth\nfrom keystone import exception\nfrom keystone import token\n\n\nMETHOD_NAME = 'token'\n\nLOG = logging.getLogger(__name__)\n\n\nclass Token(auth.AuthMethodHandler):\n    def __init__(self):\n        self.token_api = token.Manager()\n\n    def authenticate(self, context, auth_payload, user_context):\n        try:\n            if 'id' not in auth_payload:\n                raise exception.ValidationError(attribute='id',\n                                                target=METHOD_NAME)\n            token_id = auth_payload['id']\n            token_ref = self.token_api.get_token(context, token_id)\n            user_context.setdefault(\n                'user_id', token_ref['token_data']['token']['user']['id'])\n            # to support Grizzly-3 to Grizzly-RC1 transition\n            expires_at = token_ref['token_data']['token'].get(\n                'expires_at', token_ref['token_data']['token'].get('expires'))\n            user_context.setdefault('expires_at', expires_at)\n            user_context['extras'].update(\n                token_ref['token_data']['token']['extras'])\n            user_context['method_names'].extend(\n                token_ref['token_data']['token']['methods'])\n            if 'trust' in token_ref['token_data']:\n                raise exception.Forbidden(e)\n        except AssertionError as e:\n            LOG.error(e)\n            raise exception.Unauthorized(e)\n"}
{"text": "# (C) Copyright 2016 Hewlett Packard Enterprise Development LP\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n# implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\nimport urlparse\n\nimport requests\n\nfrom monasca_notification.monitoring import client\nfrom monasca_notification.monitoring.metrics import NOTIFICATION_SEND_TIMER\nfrom monasca_notification.plugins import abstract_notifier\n\n\"\"\"\n   notification.address = https://hipchat.hpcloud.net/v2/room/<room_id>/notification?auth_token=432432\n\n   How to get access token?\n       1) Login to Hipchat with the user account which is used for notification\n       2) Go to this page. https://hipchat.hpcloud.net/account/api (Replace your hipchat server name)\n       3) You can see option to \"Create token\". Use the capability \"SendNotification\"\n\n   How to get the Room ID?\n       1) Login to Hipchat with the user account which is used for notification\n       2) Go to this page. https://hipchat.hpcloud.net/account/api (Replace your hipchat server name)\n       3) Click on the Rooms tab\n       4) Click on any Room of your choice.\n       5) Room ID is the API ID field\n\n\"\"\"\n\nSEVERITY_COLORS = {\"low\": 'green',\n                   'medium': 'gray',\n                   'high': 'yellow',\n                   'critical': 'red'}\nSTATSD_CLIENT = client.get_client()\nSTATSD_TIMER = STATSD_CLIENT.get_timer()\n\n\nclass HipChatNotifier(abstract_notifier.AbstractNotifier):\n    def __init__(self, log):\n        super(HipChatNotifier, self).__init__(\"hipchat\")\n        self._log = log\n\n    def _build_hipchat_message(self, notification):\n        \"\"\"Builds hipchat message body\n        \"\"\"\n        body = {'alarm_id': notification.alarm_id,\n                'alarm_definition_id': notification.raw_alarm['alarmDefinitionId'],\n                'alarm_name': notification.alarm_name,\n                'alarm_description': notification.raw_alarm['alarmDescription'],\n                'alarm_timestamp': notification.alarm_timestamp,\n                'state': notification.state,\n                'old_state': notification.raw_alarm['oldState'],\n                'message': notification.message,\n                'tenant_id': notification.tenant_id,\n                'metrics': notification.metrics}\n\n        hipchat_request = {}\n        hipchat_request['color'] = self._get_color(notification.severity.lower())\n        hipchat_request['message_format'] = 'text'\n        hipchat_request['message'] = json.dumps(body, indent=3)\n\n        return hipchat_request\n\n    def _get_color(self, severity):\n        return SEVERITY_COLORS.get(severity, 'purple')\n\n    @STATSD_TIMER.timed(NOTIFICATION_SEND_TIMER, dimensions={'notification_type': 'hipchat'})\n    def send_notification(self, notification):\n        \"\"\"Send the notification via hipchat\n            Posts on the given url\n        \"\"\"\n\n        hipchat_message = self._build_hipchat_message(notification)\n        parsed_url = urlparse.urlsplit(notification.address)\n\n        query_params = urlparse.parse_qs(parsed_url.query)\n        # URL without query params\n        url = urlparse.urljoin(notification.address, urlparse.urlparse(notification.address).path)\n\n        # Default option is to do cert verification\n        verify = self._config.get('insecure', False)\n        # If ca_certs is specified, do cert validation and ignore insecure flag\n        if (self._config.get(\"ca_certs\")):\n            verify = self._config.get(\"ca_certs\")\n\n        proxyDict = None\n        if (self._config.get(\"proxy\")):\n            proxyDict = {\"https\": self._config.get(\"proxy\")}\n\n        try:\n            # Posting on the given URL\n            result = requests.post(url=url,\n                                   data=hipchat_message,\n                                   verify=verify,\n                                   params=query_params,\n                                   proxies=proxyDict,\n                                   timeout=self._config['timeout'])\n\n            if result.status_code in range(200, 300):\n                self._log.info(\"Notification successfully posted.\")\n                return True\n            else:\n                msg = \"Received an HTTP code {} when trying to send to hipchat on URL {} with response {}.\"\n                self._log.error(msg.format(result.status_code, url, result.text))\n                return False\n        except Exception:\n            self._log.exception(\"Error trying to send to hipchat on URL {}\".format(url))\n            return False\n"}
{"text": "import copy\r\n\r\ndef pprint(A):\r\n    n = len(A)\r\n    for i in range(0, n):\r\n        line = \"\"\r\n        for j in range(0, n+1):\r\n            A[i][j] = float(A[i][j])\r\n            line += str(A[i][j]) + \"\\t\"\r\n            if j == n-1:\r\n                line += \"| \"\r\n        print(line)\r\n    print(\"\")\r\n\r\n\r\ndef gauss(A):\r\n    n = len(A)\r\n\r\n    for i in range(0, n):\r\n        # Search for maximum in this column\r\n        maxEl = abs(A[i][i])\r\n        maxRow = i\r\n        for k in range(i+1, n):\r\n            if abs(A[k][i]) > maxEl:\r\n                maxEl = abs(A[k][i])\r\n                maxRow = k\r\n\r\n        # Swap maximum row with current row (column by column)\r\n        forcheck = copy.deepcopy(A)\r\n        for k in range(i, n+1):\r\n            tmp = A[maxRow][k]\r\n            A[maxRow][k] = A[i][k]\r\n            A[i][k] = tmp\r\n        if forcheck!=A:\r\n            #print(\"row interchange\")\r\n            pprint(A)\r\n\r\n        # Make all rows below this one 0 in current column\r\n        forcheck = copy.deepcopy(A)\r\n        for k in range(i+1, n):\r\n            c = -A[k][i]/A[i][i]\r\n            for j in range(i, n+1):\r\n                if i == j:\r\n                    A[k][j] = 0\r\n                else:\r\n                    A[k][j] += c * A[i][j]\r\n        if forcheck != A: pprint(A)\r\n        \r\n\r\n    # Solve equation Ax=b for an upper triangular matrix A\r\n    x = [0 for i in range(n)]\r\n    for i in range(n-1, -1, -1):\r\n        if float(A[i][i]) == 0.0:\r\n            print(\"no unique solution\")\r\n            break\r\n        x[i] = A[i][n]/A[i][i]\r\n        for k in range(i-1, -1, -1):\r\n            A[k][n] -= A[k][i] * x[i]\r\n    return x\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    #p372 6a\r\n    #A = [[0.0,1.0,-2.0,4],[1.0,-1.0,1.0,6.0],[1.0,0.0,-1.0,2.0]]\r\n    \r\n    #p372 6b\r\n    #A = [[1,-.5,1.0,0,3],[2.0,-1.0,-1.0,1,5.0],[1.0,1.0,0.5,0,2.0],[1,-.5,1,1,5]]\r\n\r\n    #p373 8d\r\n    #A = [[1,1,-1,1,-1,2],[2,2,1,-1,1,4],[3,1,-3,-2,3,8],[4,1,-1,4,-5,16],[16,-1,1,-1,-1,32]]\r\n\r\n    A = [[1,2,3,4],[2,1,-1,1],[-3,2,0,1],[0,5,2,6]]\r\n    # Print input\r\n    pprint(A)\r\n\r\n    # Calculate solution\r\n    x = gauss(A)\r\n\r\n    # Print result\r\n    line = \"Result:\\t\"\r\n    for i in range(0, len(A)):\r\n        line += str(x[i]) + \"\\t\"\r\n    print(line)\r\n"}
{"text": "import argparse\n\nparser = argparse.ArgumentParser(description='Load or create a Blekota model')\n\nparser.add_argument('file',\n                    help='Either a .pkl file containing an existing model or a .wav sound on which to train a new model. If a .wav is provided, other parameters can be used to specify model hyperparameters.')\n# parser.add_argument('--model-file',\n#                    +help='Load an existing model from this file. If unspecified, new model will be created')\n# parser.add_argument('--sound-file',\n#                    help='The sound file to train on. Should be a .wav')\nparser.add_argument('--model-name', default='unnamed_model',\n                    help='A name for the model, it will later be saved in files like so: [MODEL_NAME]_123.pkl')\nparser.add_argument('--layers', type=int, default=3,\n                    help='The number of layers of the model')\nparser.add_argument('--hidden', type=int, default=256,\n                    help='The size of the hidden vector of each layer')\nparser.add_argument('--seq-length', type=int, default=100,\n                    help='Number of steps to perform before updating parameters')\nparser.add_argument('--batch-size', type=int, default=80,\n                    help='Number of sequences to process in parallel')\nparser.add_argument('--alpha', type=float, default=0.002,\n                    help='Learning rate. Do not change if unsure')\n\nargs = parser.parse_args()\n\nimport numpy as np\nimport pickle\nimport audio_io\nimport rnn\nimport gru\nimport visual\n\n\nclf = None\n\nif args.file.endswith('.pkl'):  # load existing model\n    print('Loading model from', args.file)\n    with open(args.file, 'rb') as f:\n        clf = pickle.load(f)\nelse:\n    print('Creating new model')\n    if not args.file.endswith('.wav'):\n        exit('file should be either a .wav (create new model) or .pkl (load existing model)')\n\n    clf = gru.GRU(args.hidden, layer_n=args.layers, seq_length=args.seq_length, batches=args.batch_size,\n                  alpha=args.alpha, name=args.model_name, file=args.file)\n\n\ndef heatmap(start=0, length=1000):\n    \"\"\"After sampling a sound, visualise a part of it as a heatmap.\"\"\"\n    visual.heatmap(clf.p[start:(start + length)])\n\nshow = visual.show  # visualise a sound\nplay = audio_io.play  # play a sound\n\nprint(clf)\n"}
{"text": "# Purpose:\n#   Use the GUI, WASD or the arrow keys to control the robots movement.\n\n# Reason for updated version:\n#   Reason for an updated version is that using the setWidgetProperties, I would be able to eliminate a lot of repeated\n#   code and be able to set everything easily through one function therefore saving time & space.\n\n# Notes:\n#   This verison is just the barebones GUI with buttons not linked to actually move the robot.\n#   Linking the buttons to pi2go module wouldn't take long.\n\n# !/usr/bin/python\n\n# Imports\n__author__ = 'Erick Musembi'\n__version__ = \"1.2\"\n__date__ = \"28th February 2015\"\n\nfrom Tkinter import *\n# from tkMessageBox import showinfo#\n# #import pi2go as pi\n\n\nclass Application(Frame):\n    def __init__(self, window):\n        # Creates the main frame for the window.\n        Frame.__init__(self, window)\n        self.pack()\n\n        # Create the top  and bottom frames.\n        # Top is for the status bar & distance.\n        self.topFrame = Frame(self)\n        self.topFrame.pack()\n\n        # Bottom is for the buttons.\n        self.bottomFrame = Frame(self)\n        self.bottomFrame.pack()\n\n        # Frame geometry button.\n        self.btn_geometry = Button(self.bottomFrame)\n        self.btn_geometry['anchor'] = E\n        self.setWidgetProperties(self.btn_geometry,\n                                 text=\"Geometry\",\n                                 foreground=\"red\",\n                                 command=lambda: self.getGeometry())\n        self.btn_geometry.pack({'padx': 5})\n\n        # The status header label.\n        self.lbl_status_header = Label(self.topFrame)\n        self.setWidgetProperties(self.lbl_status_header,\n                                 text=\"Status:\",\n                                 foreground=\"black\")\n        self.lbl_status_header.grid(row=0, column=0)\n\n        # The status label that will change to show the status of the robot.\n        self.lbl_status = Label(self.topFrame)\n        self.setWidgetProperties(self.lbl_status,\n                                 text=\"Idle\",\n                                 foreground=\"red\")\n        self.lbl_status.grid(row=0, column=1, columnspan=2)\n\n        # The distance header label.\n        self.lbl_distance_header = Label(self.topFrame)\n        self.setWidgetProperties(self.lbl_distance_header,\n                                 text=\"Distance:\",\n                                 foreground=\"black\")\n        self.lbl_distance_header.grid(row=1, column=0)\n\n        # The distance label that will change to show the distance of the nearest object to the robot.\n        self.lbl_distance = Label(self.topFrame)\n        self.setWidgetProperties(self.lbl_distance,\n                                 text=\"# cm\",\n                                 foreground=\"red\")\n        self.lbl_distance.grid(row=1, column=1)\n\n        # Creating the up, down, left and right buttons and adding padding & unicode so it looks a little but prettier.\n        self.btn_left = Button(self.bottomFrame)\n        self.setWidgetProperties(self.btn_left, text=u\"\\u2190 Left\")\n        self.btn_left.pack({'side': 'left', 'padx': 5})\n\n        self.btn_right = Button(self.bottomFrame)\n        self.setWidgetProperties(self.btn_right, text=\"Right \" + u\"\\u2192\")\n        self.btn_right.pack({'side': 'right', 'padx': 5})\n\n        self.btn_down = Button(self.bottomFrame)\n        self.setWidgetProperties(self.btn_down, text=u\"\\u2193 Down\")\n        self.btn_down.pack({'side': 'bottom', 'pady': 5})\n\n        self.btn_up = Button(self.bottomFrame)\n        self.setWidgetProperties(self.btn_up, text=u\"\\u2191 Up\", padX=10)\n        self.btn_up.pack({'side': 'bottom', 'pady': 5})\n\n\n    def getGeometry(self,):\n        print self.winfo_geometry()\n\n    # These aren't used yet but I made them because I know I'll use them when it come to making the buttons\n    # work.\n    def setDistanceText(self, text):\n        self.lbl_distance['text'] = text\n\n    def setStatusText(self, text):\n        self.lbl_status['text'] = text\n\n    def setWidgetProperties(self, Widget, text=None, background=None, foreground=None, command=None, boarderWidth=None, padX=None, padY=None):\n        if text is not None:\n            Widget['text'] = text\n        if command is not None:\n            Widget['command'] = command\n        if boarderWidth is not None:\n            Widget['boarderWidth'] = True\n        if background is not None:\n            Widget['bg'] = background\n        if foreground is not None:\n            Widget['fg'] = foreground\n        if padX is not None:\n            Widget['padx'] = padX\n        if padY is not None:\n            Widget['pady'] = padY\n\n\nroot = Tk()\n# root.geometry(\"191x70\")\nApplication(root)\nmainloop()"}
{"text": "#!/usr/bin/env python\n\n\"\"\"\nDemos of MIMO time encoding and decoding algorithms that use IAF\nneurons with delays.\n\"\"\"\n\n# Copyright (c) 2009-2015, Lev Givon\n# All rights reserved.\n# Distributed under the terms of the BSD license:\n# http://www.opensource.org/licenses/bsd-license\n\nimport numpy as np\n\n# Set matplotlib backend so that plots can be generated without a\n# display:\nimport matplotlib\nmatplotlib.use('AGG')\n\nfrom bionet.utils.misc import func_timer\nimport bionet.utils.band_limited as bl\nimport bionet.utils.plotting as pl\nimport bionet.ted.iaf as iaf\n\n# For determining output plot file names:\noutput_name = 'iaf_delay_demo_'\noutput_count = 0\noutput_ext = '.png'\n\n# Define input signal generation parameters:\nT = 0.05\ndur = 2*T\ndt = 1e-6\nf = 100\nbw = 2*np.pi*f\n\nnp.random.seed(0)\nnoise_power = None\ncomps = 8\n\nif noise_power == None:\n    fig_title = 'IAF Input Signal with No Noise'\nelse:\n    fig_title = 'IAF Input Signal with %d dB of Noise' % noise_power\n\nM = 3 # number of input signals\nN = 9 # number of neurons\n\n# Starting and ending points of interval that is encoded:\nt_start = 0.02\nt_end = t_start+T\nif t_end > dur:\n    raise ValueError('t_start is too large')\nk_start = int(np.round(t_start/dt))\nk_end = int(np.round(t_end/dt))\nt_enc = np.arange(k_start, k_end, dtype=np.float)*dt\n\nu_list = []\nfor i in xrange(M):\n    fig_title_in = fig_title + ' (Signal #' + str(i+1) + ')'\n    print fig_title_in\n    u = func_timer(bl.gen_band_limited)(dur, dt, f, noise_power, comps)\n    u /= max(u)\n    u *= 1.5\n    pl.plot_signal(t_enc, u[k_start:k_end], fig_title_in,\n                   output_name + str(output_count) + output_ext)\n    u_list.append(u)\n    output_count += 1\n\nt = np.arange(len(u_list[0]), dtype=np.float)*dt\n\n# Define neuron parameters:\ndef randu(a, b, *d):\n    \"\"\"Create an array of the given shape and propagate it with random\n    samples from a uniform distribution over ``[a, b)``.\"\"\"\n\n    if a >= b:\n        raise ValueError('b must exceed a')\n    return a+(b-a)*np.random.rand(*d)\n\nb_list = list(randu(2.3, 3.3, N))\nd_list = list(randu(0.15, 0.25, N))\nk_list = list(0.01*np.ones(N))\na_list = map(list, np.reshape(np.random.exponential(0.003, N*M), (N, M)))\nw_list = map(list, np.reshape(randu(0.5, 1.0, N*M), (N, M)))\n\nfig_title = 'Signal Encoded Using Delayed IAF Encoder'\nprint fig_title\ns_list = func_timer(iaf.iaf_encode_delay)(u_list, t_start, dt, b_list, d_list,\n                                          k_list, a_list, w_list)\n\nfor i in xrange(M):\n    for j in xrange(N):\n        fig_title_out = fig_title + '\\n(Signal #' + str(i+1) + \\\n                        ', Neuron #' + str(j+1) + ')'\n        pl.plot_encoded(t_enc, u_list[i][k_start:k_end],\n                        s_list[j][np.cumsum(s_list[j])<T],\n                        fig_title_out,\n                        output_name + str(output_count) + output_ext)\n        output_count += 1\n    \nfig_title = 'Signal Decoded Using Delayed IAF Decoder'\nprint fig_title\nu_rec_list = func_timer(iaf.iaf_decode_delay)(s_list, T, dt,\n                                              b_list, d_list, k_list,\n                                              a_list, w_list)\n\nfor i in xrange(M):\n    fig_title_out = fig_title + ' (Signal #' + str(i+1) + ')'\n    pl.plot_compare(t_enc, u_list[i][k_start:k_end],\n                    u_rec_list[i][0:k_end-k_start], fig_title_out, \n                    output_name + str(output_count) + output_ext)\n    output_count += 1\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"\n    flask.testing\n    ~~~~~~~~~~~~~\n\n    Implements test support helpers. This module is lazily imported\n    and ususlly not used in production environments.\n\"\"\"\n\nfrom contextlib import contextmanager\n\nfrom werkzeug.test import Client, EnvironBuilder\n\nfrom .ctx import _request_ctx_stack\n\ntry:\n    from werkzeug.urls import url_parse\nexcept ImportError:\n    from urlparse import urlsplit as url_parse\n\n\ndef make_test_environ_builder(app, path='/', base_url=None, *args, **kwargs):\n    \"\"\"Creates a new test builder with some application defaults thrown in.\"\"\"\n    http_host = app.config.get('SERVER_NAME')\n    app_root = app.config.get('APPLICATION_ROOT')\n    if base_url is None:\n        url = url_parse(path)\n        base_url = 'http://%s/' % (url.netloc or http_host or 'localhost')\n        if app_root:\n            base_url += app_root.lstrip('/')\n        if url.netloc:\n            path = url.path\n    return EnvironBuilder(path, base_url, *args, **kwargs)\n\n\nclass FlaskClient(Client):\n    \"\"\"Works like a regular Werkzeug test client but has some knowledge about\n    how Flask words to defer the cleanup of the request context stack to the \n    end of a with body when used in a with statement.  For general information\n    about how to use the class refer to :class:`werkzeug.test.Client`.\n\n    Basic usage is outlined in the :ref:`testing` chapter.\n    \"\"\"\n\n    preserve_context = False\n\n    @contextmanager\n    def session_transaction(self, *args, **kwargs):\n        if self.cookie_jar is None:\n            raise RuntimeError('No Session without cookies enabled.')\n\n        app = self.application\n        environ_overrides = kwargs.setdefault('environ_overrides', {})\n        self.cookie_jar.inject_wsgi(environ_overrides)\n        outer_reqctx = _request_ctx_stack.top\n        with app.test_request_context(*args, **kwargs) as c:\n            sess = app.open_session(c.request)\n            if sess is None:\n                raise RuntimeError('Session not opened.')\n            _request_ctx_stack.push(outer_reqctx)\n            try:\n                yield sess\n            finally:\n                _request_ctx_stack.pop()\n            \n            resp = app.response_class()\n            if not app.session_interface.is_null_session(sess):\n                app.save_session(sess, resp)\n            headers = resp.get_wsgi_headers(c.request.environ)\n            self.cookie_jar.extract_wsgi(c.request.environ, headers)\n    \n    def open(self, *args, **kwargs):\n        kwargs.setdefault('environ_overrides', {}) \\\n            ['flask._preserve_context'] = self.preserve_context\n        as_tuple = kwargs.pop('as_tuple', False)\n        buffered = kwargs.pop('buffered', False)\n        follow_redirects = kwargs.pop('follow_redirects', False)\n        builder = make_test_environ_builder(self.application, *args, **kwargs)\n\n        return Client.open(self, builder,\n                           as_tuple=as_tuple,\n                           buffered=buffered,\n                           follow_redirects=follow_redirects)\n\n    def __enter__(self):\n        if self.preserve_context:\n            raise RuntimeError('Cannot nest client invocations')\n        self.preserve_context = True\n        return self\n\n    def __exit__(self, exc_type, exc_value, tb):\n        self.preserve_context = False\n        top = _request_ctx_stack.top\n        if top is not None and top.preserved:\n            top.pop()\n\n"}
{"text": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n#\nfrom airflow.contrib.hooks.discord_webhook_hook import DiscordWebhookHook\nfrom airflow.exceptions import AirflowException\nfrom airflow.operators.http_operator import SimpleHttpOperator\nfrom airflow.utils.decorators import apply_defaults\n\n\nclass DiscordWebhookOperator(SimpleHttpOperator):\n    \"\"\"\n    This operator allows you to post messages to Discord using incoming webhooks.\n    Takes a Discord connection ID with a default relative webhook endpoint. The\n    default endpoint can be overridden using the webhook_endpoint parameter\n    (https://discordapp.com/developers/docs/resources/webhook).\n\n    Each Discord webhook can be pre-configured to use a specific username and\n    avatar_url. You can override these defaults in this operator.\n\n    :param http_conn_id: Http connection ID with host as \"https://discord.com/api/\" and\n                         default webhook endpoint in the extra field in the form of\n                         {\"webhook_endpoint\": \"webhooks/{webhook.id}/{webhook.token}\"}\n    :type http_conn_id: str\n    :param webhook_endpoint: Discord webhook endpoint in the form of\n                             \"webhooks/{webhook.id}/{webhook.token}\"\n    :type webhook_endpoint: str\n    :param message: The message you want to send to your Discord channel\n                    (max 2000 characters). (templated)\n    :type message: str\n    :param username: Override the default username of the webhook. (templated)\n    :type username: str\n    :param avatar_url: Override the default avatar of the webhook\n    :type avatar_url: str\n    :param tts: Is a text-to-speech message\n    :type tts: bool\n    :param proxy: Proxy to use to make the Discord webhook call\n    :type proxy: str\n    \"\"\"\n\n    template_fields = ['username', 'message']\n\n    @apply_defaults\n    def __init__(self,\n                 http_conn_id=None,\n                 webhook_endpoint=None,\n                 message=\"\",\n                 username=None,\n                 avatar_url=None,\n                 tts=False,\n                 proxy=None,\n                 *args,\n                 **kwargs):\n        super().__init__(endpoint=webhook_endpoint,\n                         *args,\n                         **kwargs)\n\n        if not http_conn_id:\n            raise AirflowException('No valid Discord http_conn_id supplied.')\n\n        self.http_conn_id = http_conn_id\n        self.webhook_endpoint = webhook_endpoint\n        self.message = message\n        self.username = username\n        self.avatar_url = avatar_url\n        self.tts = tts\n        self.proxy = proxy\n        self.hook = None\n\n    def execute(self, context):\n        \"\"\"\n        Call the DiscordWebhookHook to post message\n        \"\"\"\n        self.hook = DiscordWebhookHook(\n            self.http_conn_id,\n            self.webhook_endpoint,\n            self.message,\n            self.username,\n            self.avatar_url,\n            self.tts,\n            self.proxy\n        )\n        self.hook.execute()\n"}
{"text": "# This Python file uses the following encoding: utf-8\n# coding: utf-8\n\nimport sys\n\n# Print iterations progress\ndef printProgress (iteration, total, prefix = '', suffix = '', decimals = 1, barLength = 20):\n    \"\"\"\n    Call in a loop to create terminal progress bar\n    @params:\n        iteration   - Required  : current iteration (Int)\n        total       - Required  : total iterations (Int)\n        prefix      - Optional  : prefix string (Str)\n        suffix      - Optional  : suffix string (Str)\n        decimals    - Optional  : positive number of decimals in percent complete (Int)\n        barLength   - Optional  : character length of bar (Int)\n    \"\"\"\n    formatStr = \"{0:.\" + str(decimals) + \"f}\"\n    percent = formatStr.format(100 * (iteration / float(total)))\n    filledLength = int(round(barLength * iteration / float(total)))\n    bar = '\u2588' * filledLength + '-' * (barLength - filledLength)\n    sys.stdout.write('\\r%s |%s| %s%s %s' % (prefix, bar, percent, '%', suffix)),\n    if iteration == total:\n        sys.stdout.write('\\n')\n    sys.stdout.flush()\n"}
{"text": "#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport sys\nfrom os import path, getenv\nfrom time import sleep\n\n# if PAPARAZZI_SRC not set, then assume the tree containing this\n# file is a reasonable substitute\nPPRZ_SRC = getenv(\"PAPARAZZI_SRC\", path.normpath(path.join(path.dirname(path.abspath(__file__)), '../../../')))\nsys.path.append(PPRZ_SRC + \"/sw/ext/pprzlink/lib/v1.0/python\")\n\nfrom ivy_msg_interface import IvyMessagesInterface\nfrom pprzlink.message import PprzMessage\n\n\nclass WaypointMover(object):\n    def __init__(self, verbose=False):\n        self.verbose = verbose\n        self._interface = IvyMessagesInterface(self.message_recv)\n\n    def message_recv(self, ac_id, msg):\n        if self.verbose:\n            print(\"Got msg %s\" % msg.name)\n\n    def shutdown(self):\n        print(\"Shutting down ivy interface...\")\n        self._interface.shutdown()\n\n    def __del__(self):\n        self.shutdown()\n\n    def move_waypoint(self, ac_id, wp_id, lat, lon, alt):\n        msg = PprzMessage(\"ground\", \"MOVE_WAYPOINT\")\n        msg['ac_id'] = ac_id\n        msg['wp_id'] = wp_id\n        msg['lat'] = lat\n        msg['long'] = lon\n        msg['alt'] = alt\n        print(\"Sending message: %s\" % msg)\n        self._interface.send(msg)\n\n\nif __name__ == '__main__':\n    try:\n        wm = WaypointMover()\n        # sleep shortly in oder to make sure Ivy is up, then message sent before shutting down again\n        sleep(0.1)\n        wm.move_waypoint(ac_id=202, wp_id=3, lat=43.563, lon=1.481, alt=172.0)\n        sleep(0.1)\n    except KeyboardInterrupt:\n        print(\"Stopping on request\")\n    wm.shutdown()\n"}
{"text": "# -*- coding: utf-8 -*-\n# Copyright (C) 2015  Red Hat, Inc.\n#\n# This copyrighted material is made available to anyone wishing to use,\n# modify, copy, or redistribute it subject to the terms and conditions of\n# the GNU General Public License v.2, or (at your option) any later version.\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY expressed or implied, including the implied warranties of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General\n# Public License for more details.  You should have received a copy of the\n# GNU General Public License along with this program; if not, write to the\n# Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n# 02110-1301, USA.  Any Red Hat trademarks that are incorporated in the\n# source code or documentation are not subject to the GNU General Public\n# License and may only be used or replicated with the express permission of\n# Red Hat, Inc.\n#\n# Red Hat Author(s): Anne Mulhern <amulhern@redhat.com>\n\n\"\"\"\n    tests.test_utils\n    ================\n\n    Tests utilities.\n\n    .. moduleauthor:: mulhern <amulhern@redhat.com>\n\"\"\"\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport pyblk\n\nfrom ._constants import GRAPH\n\nclass TestGraphUtils(object):\n    \"\"\"\n    Test utilities that work over networkx graphs.\n    \"\"\"\n    # pylint: disable=too-few-public-methods\n\n    def test_roots(self):\n        \"\"\"\n        Verify that roots are really roots.\n        \"\"\"\n        roots = pyblk.GraphUtils.get_roots(GRAPH)\n        in_degrees = GRAPH.in_degree(roots)\n\n        assert all(in_degrees[r] == 0 for r in roots)\n"}
{"text": "# -*- coding: utf-8 -*-\n\n\nfrom django.conf import settings\nfrom django.core.management.base import BaseCommand\nfrom plataforma.constants import ETHER_DIVISOR\nfrom plataforma.models import Saldo\nimport requests\n\n\ndef buscar_saldo(carteira):\n    try:\n        r = requests.get(\"https://api.etherscan.io/api?module=account&action=tokenbalance&contractaddress=%s&address=%s&tag=latest&apikey=%s\" % (settings.ETHERSCAN_CONTRACT_ADDRESS, carteira, settings.ETHERSCAN_APIKEY))\n        if r.status_code == 200:\n            data = r.json()\n            if data[\"status\"] == \"1\":\n                saldo = float(data[\"result\"]) / float(ETHER_DIVISOR)\n                _, created = Saldo.objects.update_or_create(carteira=carteira, defaults={\"total\": saldo})\n                print \"%s: %0.6f (%s)\" % (carteira, saldo, str(created))\n                return True\n        return False\n    except Exception, e:\n        print \"Nao consegui pegar o saldo da carteira %s\" % carteira\n        return None\n\n\nclass Command(BaseCommand):\n\n    help = u\"Atualiza o saldo de todas as carteiras de um contrato.\"\n\n    def handle(self, *args, **options):\n        url = \"https://api.etherscan.io/api?module=logs&action=getLogs&fromBlock=%s&toBlock=latest&address=%s&apikey=%s\" % (settings.ETHERSCAN_START_BLOCK_NUMBER, settings.ETHERSCAN_CONTRACT_ADDRESS, settings.ETHERSCAN_APIKEY)\n        r = requests.get(url)\n        data = r.json()\n        saldos_atualizados = []\n        for transacion in data[\"result\"]:\n            carteira_from = transacion[\"topics\"][1].replace(\"0x000000000000000000000000\", \"0x\")\n            if carteira_from not in saldos_atualizados:\n                if buscar_saldo(carteira_from):\n                    saldos_atualizados.append(carteira_from)\n            if len(transacion[\"topics\"]) >= 3:\n                carteira_to = transacion[\"topics\"][2].replace(\"0x000000000000000000000000\", \"0x\")\n                if carteira_to not in saldos_atualizados:\n                    if buscar_saldo(carteira_to):\n                        saldos_atualizados.append(carteira_to)\n        print \"Fim de processo!\"\n"}
{"text": "###############################################################################\n#\n# Tests for XlsxWriter.\n#\n# Copyright (c), 2013-2016, John McNamara, jmcnamara@cpan.org\n#\n\nfrom ..excel_comparsion_test import ExcelComparisonTest\nfrom ...workbook import Workbook\n\n\nclass TestCompareXLSXFiles(ExcelComparisonTest):\n    \"\"\"\n    Test file created by XlsxWriter against a file created by Excel.\n\n    \"\"\"\n\n    def setUp(self):\n        self.maxDiff = None\n\n        filename = 'chart_format08.xlsx'\n\n        test_dir = 'xlsxwriter/test/comparison/'\n        self.got_filename = test_dir + '_test_' + filename\n        self.exp_filename = test_dir + 'xlsx_files/' + filename\n\n        self.ignore_files = []\n        self.ignore_elements = {}\n\n    def test_create_file(self):\n        \"\"\"Test the creation of an XlsxWriter file with chart formatting.\"\"\"\n\n        workbook = Workbook(self.got_filename)\n\n        worksheet = workbook.add_worksheet()\n        chart = workbook.add_chart({'type': 'line'})\n\n        chart.axis_ids = [46164608, 46176128]\n\n        data = [\n            [1, 2, 3, 4, 5],\n            [2, 4, 6, 8, 10],\n            [3, 6, 9, 12, 15],\n\n        ]\n\n        worksheet.write_column('A1', data[0])\n        worksheet.write_column('B1', data[1])\n        worksheet.write_column('C1', data[2])\n\n        chart.add_series({\n            'categories': '=Sheet1!$A$1:$A$5',\n            'values': '=Sheet1!$B$1:$B$5',\n            'trendline': {'type': 'linear'},\n        })\n\n        chart.add_series({\n            'categories': '=Sheet1!$A$1:$A$5',\n            'values': '=Sheet1!$C$1:$C$5',\n        })\n\n        worksheet.insert_chart('E9', chart)\n\n        workbook.close()\n\n        self.assertExcelEqual()\n"}
{"text": "# Gufw 12.10.0 - http://gufw.tuxfamily.org\n# Copyright (C) 2008-2011 Raul Soriano https://launchpad.net/~gatoloko\n#                         Marcos Alvarez Costales https://launchpad.net/~costales\n#\n# Gufw is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 3 of the License, or\n# (at your option) any later version.\n# \n# Gufw is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with Gufw; if not, see http://www.gnu.org/licenses for more\n# information.\n\nimport os\n\n\n\nclass Validation:\n    \"\"\"Manages application instances & config file\"\"\"\n    def __init__(self):\n        self.pid_file = '/tmp/gufw.pid'\n        self._check_instance()\n        self._start_application()\n    \n    def _check_instance(self):\n        \"\"\"Check whether the app is running\"\"\"\n        if not os.path.isfile(self.pid_file):\n            return\n\n        # Read the pid from file\n        pid = 0\n        try:\n            file = open(self.pid_file, 'rt')\n            data = file.read()\n            file.close()\n            pid = int(data)\n        except:\n            pass\n        \n        # Check whether the process specified exists\n        if 0 == pid:\n            return\n        try:\n            os.kill(pid, 0) # exception if the pid is invalid\n        except:\n            return\n        \n        exit(0)\n    \n    def _start_application(self):\n        \"\"\"Called when there is no running instances, storing the new pid\"\"\"\n        file = open(self.pid_file, 'wt')\n        file.write(str(os.getpid()))\n        file.close()\n    \n    def exit_application(self):\n        \"\"\"Close app\"\"\"\n        try:\n            os.remove(self.pid_file)\n        except:\n            pass\n\n\n\nclass Path:\n    \"\"\"Return app paths\"\"\"\n    def get_ui_path(self, file_name):\n        \"\"\"Return Path GUI\"\"\"\n        path = os.path.join('/usr', 'share', 'gufw', 'ui', file_name)\n        if not os.path.exists(path):\n            path = os.path.join('data', 'ui', file_name)\n        return path\n    \n    def get_shield_path(self, incoming, outgoing):\n        \"\"\"Return Path Shields\"\"\"\n        file_name = incoming + '_' + outgoing + '.png'\n        path = os.path.join('/usr', 'share', 'gufw', 'media', file_name)\n        if not os.path.exists(path):\n            path = os.path.join('data', 'media', file_name)\n        return path\n        \n    def get_icon_path(self):\n        \"\"\"Return Icon app\"\"\"\n        path = os.path.join('/usr', 'share', 'icons', 'hicolor', '48x48', 'apps', 'gufw.png')\n        if not os.path.exists(path):\n            path = os.path.join('data', 'media', 'gufw.png')\n        return path\n"}
{"text": "import random\n\nfrom simulation import map_generator\nfrom simulation.simulation_runner import SequentialSimulationRunner\nfrom tests.test_simulation.mock_communicator import MockCommunicator\nfrom tests.test_simulation.dummy_avatar import DummyAvatarManager, MoveEastDummy\nfrom tests.test_simulation.mock_turn_collector import MockTurnCollector\n\nSETTINGS = {\"START_HEIGHT\": 5, \"START_WIDTH\": 5, \"OBSTACLE_RATIO\": 0}\n\n\nclass MockWorld(object):\n    \"\"\"\n    Creates an object that mocks the whole game and can be used in various testing.\n    It holds the map generator, avatar manager, game state and turn manager. Takes settings as a parameter,\n    if defaults are unsuitable.\n\n    By default, the first avatar added to the world will be a MoveEastDummy.\n    \"\"\"\n\n    def __init__(\n        self,\n        settings=SETTINGS,\n        dummies_list=None,\n        map_generator_class=map_generator.Main,\n        simulation_runner_class=SequentialSimulationRunner,\n    ):\n        random.seed(0)\n        if dummies_list is None:\n            dummies_list = [MoveEastDummy]\n\n        self.generator = map_generator_class(settings)\n        self.avatar_manager = DummyAvatarManager(dummies_list)\n        self.game_state = self.generator.get_game_state(self.avatar_manager)\n        self.simulation_runner = simulation_runner_class(\n            game_state=self.game_state, communicator=MockCommunicator()\n        )\n        self.turn_collector = MockTurnCollector()\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals, absolute_import\n\nfrom django.test import TestCase\nfrom django.utils.timezone import now, timedelta\n\nfrom .factories import BookingType30F, UserF\nfrom ..models import Booking, SlotTime\nfrom ..forms import BookingCreateForm\n\n\nclass BookingCreateFormTest(TestCase):\n\n    def setUp(self):\n        start = now()\n        self.booking_type = BookingType30F()\n        self.slottime = SlotTime.objects.create(\n            booking_type=self.booking_type,\n            start=start,\n            end=start + timedelta(minutes=self.booking_type.slot_length))\n        self.booker = UserF()\n        self.client.login(username=self.booker.username,\n                          password=self.booker.username)\n        self.data = {'slottime': self.slottime.pk,\n                     'notes': 'notes on booking',\n                     'telephone': '+399900990'}\n\n    def test_form_is_not_valid(self):\n        \"\"\"\n        Test that clean method raise ValidationError if slottime pk passed in\n        form is already linked to Booking object.\n        \"\"\"\n        Booking.objects.create(booker=self.booker, slottime=self.slottime)\n        form = BookingCreateForm(self.data)\n        self.assertFalse(form.is_valid())\n        self.assertIn('__all__', form.errors)\n        self.assertEqual(form.errors['__all__'][0],\n                         'Slot time selected is already assigned')\n\n    def test_form_is_valid(self):\n        form = BookingCreateForm(self.data)\n        self.assertTrue(form.is_valid())\n"}
{"text": "# -*- coding: utf-8 -*-\n\n# This module contains all the osid_error exception classes and can be\n# used by any python implementations that need to throw an osid error\n\n\"\"\"\nErrors are specified in each method specification. Only the specified\nerrors are permitted as error conditions in OSID method contracts with\nexceptions noted in the descriptions below. Provider Contract errors are\nnever specified but may be returned at any time by an OSID Runtime. An\nOSID Provider implementation is only permitted to return those errors\nspecified in a method contract however, some Consumer Contract errors may\nbe automatically handled in an OSID Runtime. Errors should result when the\ncontract of the interface as been violated or cannot be fulfilled and it\nis necessary to disrupt the flow of control for an OSID Consumer. Different\nerrors are specified where it is forseen that an OSID Consumer may wish\nto execute a different action without violating the encapsulation of\ninternal OSID Provider operations. Such actions do not include debugging\nor other detailed information which is the responsibility of the OSID\nProvider to manage. As such, the number of errors defined across all\nthe interfaces is kept to a minimum and the context of the error may vary\nfrom method to method in accordance with the OSID specification.\n\n\"\"\"\n\nfrom ...abstract_osid.osid import errors as abc_errors\n\n\"\"\" User Errors:\n\nUser errors are only permitted where specified in method signatures and\nshould be handled directly by a consumer application.\n\n\"\"\"\n\n\nclass AlreadyExists(abc_errors.AlreadyExists):\n    pass\n\n\nclass NotFound(abc_errors.NotFound):\n    pass\n\n\nclass PermissionDenied(abc_errors.PermissionDenied):\n    pass\n\n\"\"\" Operational Errors:\n\nOperational errors result from failures in the system. These errors are\nonly permitted where specified and should be handled directly by the\nconsumer application.\n\n\"\"\"\n\n\nclass ConfigurationError(abc_errors.ConfigurationError):\n    pass\n\n\nclass OperationFailed(abc_errors.OperationFailed):\n    pass\n\n\nclass TransactionFailure(abc_errors.TransactionFailure):\n    pass\n\n\"\"\" ConsumerContract:\n\nErrors in programming resulting from an incorrect use of the OSIDs.\nApplication code should be checked for accuracy. These errors should be\navoided by using the defined interoperability and flow control tests.\n\n\"\"\"\n\n\nclass IllegalState(abc_errors.IllegalState):\n    pass\n\n\nclass InvalidArgument(abc_errors.InvalidArgument):\n    pass\n\n\nclass InvalidMethod(abc_errors.InvalidMethod):\n    pass\n\n\nclass NoAccess(abc_errors.NoAccess):\n    pass\n\n\nclass NullArgument(abc_errors.NullArgument):\n    pass\n\n\nclass Unimplemented(abc_errors.Unimplemented):\n    pass\n\n\nclass Unsupported(abc_errors.Unsupported):\n    pass\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\n.. codeauthor:: C\u00e9dric Dumay <cedric.dumay@gmail.com>\n\n\n\"\"\"\nimport logging\nimport argparse\nfrom .lib import from_local, from_remote\n\n\nclass CommonParser(argparse.ArgumentParser):\n    \"\"\"CommonParser\"\"\"\n\n    def __init__(self, **kwargs):\n        argparse.ArgumentParser.__init__(self, **kwargs)\n        self.add_argument(\n            \"--output\", \"-o\", metavar=\"FILE\",\n            help=\"output file (default: stdout)\"\n        )\n        self.add_argument(\n            \"--verbose\", help=\"increase output verbosity\", action=\"store_true\"\n        )\n\n\ndef fromlocal():\n    \"\"\"description of fromlocal\"\"\"\n    parser = CommonParser()\n    parser.add_argument(\"file\", help=\"input file\")\n    args = parser.parse_args()\n    logging.basicConfig(\n        level=logging.DEBUG if args.verbose else logging.WARNING,\n        format=\"%(levelname)-8s: %(message)s\"\n    )\n    from_local(src=args.file, dst=args.output)\n\n\ndef fromurl():\n    \"\"\"description of fromurl\"\"\"\n    parser = CommonParser()\n    parser.add_argument(\"url\", help=\"url to fetch\")\n    args = parser.parse_args()\n    logging.basicConfig(\n        level=logging.DEBUG if args.verbose else logging.WARNING,\n        format=\"%(levelname)-8s: %(message)s\"\n    )\n    from_remote(src=args.url, dst=args.output)\n"}
{"text": "# encoding: utf-8\n\"\"\"\ncommunity.py\n\nCreated by Thomas Mangin on 2009-11-05.\nCopyright (c) 2009-2015 Exa Networks. All rights reserved.\n\"\"\"\n\nfrom struct import pack\nfrom struct import unpack\n\n\n# ==================================================================== Community\n#\n\nclass Community (object):\n\tNO_EXPORT            = pack('!L',0xFFFFFF01)\n\tNO_ADVERTISE         = pack('!L',0xFFFFFF02)\n\tNO_EXPORT_SUBCONFED  = pack('!L',0xFFFFFF03)\n\tNO_PEER              = pack('!L',0xFFFFFF04)\n\n\tcache = {}\n\tcaching = True\n\n\t__slots__ = ['community','_str']\n\n\tdef __init__ (self, community):\n\t\tself.community = community\n\t\tif community == self.NO_EXPORT:\n\t\t\tself._str = 'no-export'\n\t\telif community == self.NO_ADVERTISE:\n\t\t\tself._str = 'no-advertise'\n\t\telif community == self.NO_EXPORT_SUBCONFED:\n\t\t\tself._str = 'no-export-subconfed'\n\t\telse:\n\t\t\tself._str = \"%d:%d\" % unpack('!HH',self.community)\n\n\tdef __cmp__ (self, other):\n\t\tif not isinstance(other,self.__class__):\n\t\t\treturn -1\n\t\tif self.community != other.community:\n\t\t\treturn -1\n\t\treturn 0\n\n\tdef json (self):\n\t\treturn \"[ %d, %d ]\" % unpack('!HH',self.community)\n\n\tdef pack (self, negotiated=None):\n\t\treturn self.community\n\n\tdef __str__ (self):\n\t\treturn self._str\n\n\tdef __len__ (self):\n\t\treturn 4\n\n\tdef __eq__ (self, other):\n\t\treturn self.community == other.community\n\n\tdef __ne__ (self, other):\n\t\treturn self.community != other.community\n\n\t@classmethod\n\tdef unpack (cls, community, negotiated):\n\t\treturn cls(community)\n\n\t@classmethod\n\tdef cached (cls, community):\n\t\tif cls.caching and community in cls.cache:\n\t\t\treturn cls.cache[community]\n\t\tinstance = cls(community)\n\t\tif cls.caching:\n\t\t\tcls.cache[community] = instance\n\t\treturn instance\n\n# Always cache well-known communities, they will be used a lot\nif not Community.cache:\n\tCommunity.cache[Community.NO_EXPORT] = Community(Community.NO_EXPORT)\n\tCommunity.cache[Community.NO_ADVERTISE] = Community(Community.NO_ADVERTISE)\n\tCommunity.cache[Community.NO_EXPORT_SUBCONFED] = Community(Community.NO_EXPORT_SUBCONFED)\n\tCommunity.cache[Community.NO_PEER] = Community(Community.NO_PEER)\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# Copyright 2014 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Simple command-line sample for the Calendar API.\nCommand-line application that retrieves the list of the user's calendars.\"\"\"\n\nimport sys\n\nfrom oauth2client import client\nimport googleapiclient\n\ndef main(argv):\n  # Authenticate and construct service.\n  service, flags = sample_tools.init(\n      argv, 'calendar', 'v3', __doc__, __file__,\n      scope='https://www.googleapis.com/auth/calendar.readonly')\n\n  try:\n    page_token = None\n    while True:\n      calendar_list = service.calendarList().list(pageToken=page_token).execute()\n      for calendar_list_entry in calendar_list['items']:\n        print calendar_list_entry['summary']\n      page_token = calendar_list.get('nextPageToken')\n      if not page_token:\n        break\n\n  except client.AccessTokenRefreshError:\n    print ('The credentials have been revoked or expired, please re-run'\n      'the application to re-authorize.')\n\nif __name__ == '__main__':\n  main(sys.argv)\n"}
{"text": "#!flask/bin/python\nfrom flask_bcrypt import Bcrypt\nfrom flask_login import UserMixin\nfrom main import login_manager\nfrom main import mydb\nimport MySQLdb\n\n@login_manager.user_loader\ndef load_user(user_id):\n    user = User()\n    return user.getUserByID(int(user_id))\n\nclass User(UserMixin):\n    id = None\n    username = None\n    email = None\n    password = None\n\n    def __init__(self):\n        print \"Empty constructor\"\n\n    def getUserByID(self, id):\n        mycursor = mydb.connection.cursor(MySQLdb.cursors.DictCursor)\n        mycursor.execute(\"select * from Users u where u.userID = \" + str(id))\n        row = mycursor.fetchone()\n\n        if(row is None):\n            return None\n\n        self.id = row['userID']\n        self.username =  row['userName']\n        self.email = row['userEmail']\n        self.password = row['userPass']\n\n        return self\n\n    def getUserByEmail(self, email):\n        mycursor = mydb.connection.cursor(MySQLdb.cursors.DictCursor)\n        mycursor.execute(\"select * from Users u where u.userEmail = '\" + str(email) + \"'\")\n        row = mycursor.fetchone()\n\n        if(row is None):\n            return None\n\n        self.id = row['userID']\n        self.username =  row['userName']\n        self.email = row['userEmail']\n        self.password = row['userPass']\n\n        return self\n\n\n    def userAuthentication(self, email, password):\n        mycursor = mydb.connection.cursor(MySQLdb.cursors.DictCursor)\n        mycursor.execute(\"select * from Users u where u.userEmail = '\" + str(email) + \"'\")\n        row = mycursor.fetchone()\n\n        if(row is None):\n            return False\n\n        bcrypt = Bcrypt()\n        return  bcrypt.check_password_hash(row['userPass'], password)\n\n    def checkIfEmailExists(self, email):\n        mycursor = mydb.connection.cursor(MySQLdb.cursors.DictCursor)\n        mycursor.execute(\"select * from Users u where u.userEmail = '\" + str(email) + \"'\")\n        row = mycursor.fetchone()\n\n        if(row is None):\n            return False\n        return True\n\n    def addUser(self, name, email, password):\n        bcrypt = Bcrypt()\n        mycursor = mydb.connection.cursor(MySQLdb.cursors.DictCursor)\n        mycursor.execute(\"insert into Users (userName, userEmail, userPass) values ('\"+str(name)+\"','\"+str(email)+\"','\"+bcrypt.generate_password_hash(password)+\"')\")\n        mydb.connection.commit()\n"}
{"text": "import pytest\n\nimport modelx as mx\nfrom modelx import new_model, defcells\nfrom modelx.testing.testutil import SuppressFormulaError\n\n@pytest.fixture\ndef setitemsample():\n\n    space = new_model(name=\"samplemodel\").new_space(name=\"samplespace\")\n\n    funcdef = \"\"\"def func(x): return 2 * x\"\"\"\n\n    space.new_cells(formula=funcdef)\n\n    @defcells\n    def fibo(x):\n        if x == 0 or x == 1:\n            return x\n        else:\n            return fibo(x - 1) + fibo[x - 2]\n\n    @defcells\n    def double(x):\n        double[x] = 2 * x\n\n    @defcells\n    def return_last(x):\n        return return_last(x - 1)\n\n    @defcells\n    def balance(x):\n        return balance(x-1) + flow(x-1)\n\n    @defcells\n    def flow(x):\n        return 10\n\n    return space\n\n\ndef test_setitem(setitemsample):\n    setitemsample.fibo[0] = 1\n    setitemsample.return_last[4] = 5\n    assert setitemsample.fibo[2] == 2\n    assert setitemsample.return_last(5) == 5\n\n\ndef test_setitem_str(setitemsample):\n    cells = setitemsample.new_cells(formula=\"lambda s: 2 * s\")\n    cells[\"ABC\"] = \"DEF\"\n    assert cells[\"ABC\"] == \"DEF\"\n\n\ndef test_setitem_in_cells(setitemsample):\n    assert setitemsample.double[3] == 6\n\n\ndef test_setitem_in_formula_invalid_assignment_error(setitemsample):\n\n    def invalid_in_formula_assignment(x):\n        invalid_in_formula_assignment[x + 1] = 3 * x\n\n    setitemsample.new_cells(formula=invalid_in_formula_assignment)\n    with SuppressFormulaError():\n        with pytest.raises(KeyError):\n            setitemsample.invalid_in_formula_assignment[3]\n\n\ndef test_setitem_in_formula_duplicate_assignment_error(setitemsample):\n\n    def duplicate_assignment(x):\n        duplicate_assignment[x] = 4 * x\n        return 4 * x\n\n    setitemsample.new_cells(formula=duplicate_assignment)\n    with SuppressFormulaError():\n        with pytest.raises(ValueError):\n            setitemsample.duplicate_assignment[4]\n\n\n@pytest.mark.parametrize(\"recalc\", [True, False])\ndef test_setitem_recalc(setitemsample, recalc):\n\n    last_recalc = mx.get_recalc()\n\n    try:\n        mx.set_recalc(recalc)\n\n        setitemsample.balance[0] = 0\n        assert setitemsample.balance[10] == 100\n\n        setitemsample.balance[0] = 100\n\n        if recalc:\n            assert len(setitemsample.balance) == 11\n        else:\n            assert len(setitemsample.balance) == 1\n\n        assert setitemsample.balance[10] == 200\n\n    finally:\n        mx.set_recalc(last_recalc)\n"}
{"text": "#!/usr/bin/env python3\n\n#-------------------------------------------------------------------------------\n# Definition for singly-linked list.\n# class ListNode:\n#     def __init__(self, x):\n#         self.val = x\n#         self.next = None\n#-------------------------------------------------------------------------------\n# Merge Sort Solution\n#-------------------------------------------------------------------------------\n\nclass Solution:\n    def mergeKLists(self, lists):\n        \"\"\"\n        :type lists: List[ListNode]\n        :rtype: ListNode\n        \"\"\"\n        if not lists: return None\n        def mergeTwoLists(node1, node2):\n            dummy = ListNode(0)\n            cur, cur1, cur2 = dummy, node1, node2\n            while cur1 and cur2:\n                cur.next = cur1 if cur1.val < cur2.val else cur2\n                if cur.next == cur1:\n                    cur1 = cur1.next\n                else:\n                    cur2 = cur2.next\n                cur = cur.next\n            cur.next = cur1 or cur2\n            return [dummy.next]\n\n        def mergelists(Lists):\n            if len(Lists) == 1:\n                return Lists\n            elif len(Lists) == 2:\n                return mergeTwoLists(Lists[0], Lists[1])\n            else:\n                low, high = 0, len(Lists)\n                mid = (low+high)//2\n                return mergeTwoLists(mergelists(Lists[low:mid])[0], mergelists(Lists[mid:high])[0])\n\n        return mergelists(lists)[0]\n#-------------------------------------------------------------------------------\n# First Solution (Time Limit Exceeded)\n#-------------------------------------------------------------------------------\n\nclass Solution:\n    def mergeKLists(self, lists):\n        \"\"\"\n        :type lists: List[ListNode]\n        :rtype: ListNode\n        \"\"\"\n        if not lists:\n            return None\n        for i in range(len(lists)-1, -1, -1):\n                if not lists[i]:\n                    lists.pop(i)\n        dummy = ListNode(None)\n        curr = dummy\n        while lists:\n            smallest = float('inf')\n            idx = 0\n            for i in range(len(lists)-1, -1, -1):\n                if lists[i] and lists[i].val < smallest:\n                    smallest = lists[i].val\n                    idx = i\n            curr.next = ListNode(smallest)\n            curr = curr.next\n            lists[idx] = lists[idx].next\n            for i in range(len(lists)-1, -1, -1):\n                if not lists[i]:\n                    lists.pop(i)\n        return dummy.next\n\n#-------------------------------------------------------------------------------\n"}
{"text": "# encoding = utf-8\n\nimport random\n\nclass PetShop:\n    \"\"\"A pet shop\"\"\"\n\n    def __init__(self, animal_factory=None):\n        self.pet_factory = animal_factory\n\n    def show_pet(self):\n        pet = self.pet_factory.get_pet()\n        print(\"This is a lovely\", pet)\n        print(\"It says\", pet.speak())\n        print(\"It eats\", self.pet_factory.get_food())\n\n\n# Stuff that our factory makes\nclass Dog:\n    def speak(self):\n        return \"woof\"\n\n    def __str__(self):\n        return \"Dog\"\n\n\nclass Cat:\n    def speak(self):\n        return \"meow\"\n\n    def __str__(self):\n        return \"Cat\"\n\n\n# Factory classes\nclass DogFactory:\n    def get_pet(self):\n        return Dog()\n\n    def get_food(self):\n        return \"dog food\"\n\n\nclass CatFactory:\n    def get_pet(self):\n        return Cat()\n\n    def get_food(self):\n        return \"cat food\"\n\n\n# Create the proper family\ndef get_factory():\n    return random.choice([DogFactory, CatFactory])()\n\n\n# Show pets with various factories\nif __name__ == \"__main__\":\n    shop = PetShop()\n    for i in range(3):\n        shop.pet_factory = get_factory()\n        shop.show_pet()\n        print(\"=\" * 20)\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"\n    DARIAH Contribute - DARIAH-EU Contribute: edit your DARIAH contributions.\n\n    Copyright 2014 Data Archiving and Networked Services\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\"\"\"\n\nfrom south.utils import datetime_utils as datetime\nfrom south.db import db\nfrom south.v2 import SchemaMigration\nfrom django.db import models\n\n\nclass Migration(SchemaMigration):\n\n    def forwards(self, orm):\n        # Deleting field 'Country.iso3166_2'\n        db.delete_column(u'dariah_static_data_country', 'iso3166_2')\n\n        # Deleting field 'Country.uri'\n        db.delete_column(u'dariah_static_data_country', 'uri')\n\n        # Adding field 'Country.geonameid'\n        db.add_column(u'dariah_static_data_country', 'geonameid',\n                      self.gf('django.db.models.fields.PositiveIntegerField')(default=0),\n                      keep_default=False)\n\n\n    def backwards(self, orm):\n        # Adding field 'Country.iso3166_2'\n        db.add_column(u'dariah_static_data_country', 'iso3166_2',\n                      self.gf('django.db.models.fields.CharField')(default='', max_length=2),\n                      keep_default=False)\n\n        # Adding field 'Country.uri'\n        db.add_column(u'dariah_static_data_country', 'uri',\n                      self.gf('django.db.models.fields.URLField')(default='', max_length=200),\n                      keep_default=False)\n\n        # Deleting field 'Country.geonameid'\n        db.delete_column(u'dariah_static_data_country', 'geonameid')\n\n\n    models = {\n        u'dariah_static_data.activitygroupname': {\n            'Meta': {'object_name': 'ActivityGroupName'},\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '255'})\n        },\n        u'dariah_static_data.country': {\n            'Meta': {'object_name': 'Country'},\n            'geonameid': ('django.db.models.fields.PositiveIntegerField', [], {}),\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '255'})\n        },\n        u'dariah_static_data.tadirahactivity': {\n            'Meta': {'object_name': 'TADIRAHActivity'},\n            'activity_group_name': ('django.db.models.fields.related.ForeignKey', [], {'related_name': \"'tadirah_activities'\", 'to': u\"orm['dariah_static_data.ActivityGroupName']\"}),\n            'activity_name': ('django.db.models.fields.CharField', [], {'max_length': '255', 'blank': 'True'}),\n            'description': ('django.db.models.fields.TextField', [], {}),\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'uri': ('django.db.models.fields.URLField', [], {'max_length': '200'})\n        },\n        u'dariah_static_data.tadirahobject': {\n            'Meta': {'object_name': 'TADIRAHObject'},\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '255'}),\n            'uri': ('django.db.models.fields.URLField', [], {'max_length': '200'})\n        },\n        u'dariah_static_data.tadirahtechnique': {\n            'Meta': {'object_name': 'TADIRAHTechnique'},\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '255'}),\n            'uri': ('django.db.models.fields.URLField', [], {'max_length': '200'})\n        },\n        u'dariah_static_data.vcc': {\n            'Meta': {'object_name': 'VCC'},\n            'description': ('django.db.models.fields.TextField', [], {'max_length': '255'}),\n            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),\n            'name': ('django.db.models.fields.CharField', [], {'max_length': '255'}),\n            'uri': ('django.db.models.fields.URLField', [], {'max_length': '200'})\n        }\n    }\n\n    complete_apps = ['dariah_static_data']"}
{"text": "#\n# Copyright 2016 Import.io\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom __future__ import absolute_import\nimport logging\nfrom unittest import TestCase\nfrom importio2 import ApiCall\nimport json\n\nlogger = logging.getLogger(__name__)\n\n\nclass TestApiCall(TestCase):\n\n    def setUp(self):\n        self.api = ApiCall()\n\n    def test_constructor(self):\n        api = ApiCall()\n\n    # def test_http_delete(self):\n    #     self.api.api_host = 'httpbin.org'\n    #     self.api.path = '/delete'\n    #     request = self.api.api_request()\n    #     self.assertEqual(request.status_code, 200)\n    #\n    #     d = json.loads(request.text)\n    #     self.assertIsNotNone(d)\n\n    def test_http_get(self):\n\n        self.api.api_host = 'httpbin.org'\n        self.api.scheme = 'http'\n        self.api.path = \"get\"\n        self.api.headers = {\"Accept\": \"application/json\"}\n\n        self.api.api_request()\n\n        self.assertEqual(self.api.api_result.status_code, 200)\n\n        result = json.loads(self.api.api_result.text)\n        self.assertIsNotNone(result)\n        self.assertEqual('http://httpbin.org/get', result['url'])\n\n    def test_http_patch(self):\n        # self.assertFalse(True)\n        pass\n\n    def test_http_post(self):\n        # self.assertFalse(True)\n        pass\n"}
{"text": "#  Licensed to the Apache Software Foundation (ASF) under one\n#  or more contributor license agreements.  See the NOTICE file\n#  distributed with this work for additional information\n#  regarding copyright ownership.  The ASF licenses this file\n#  to you under the Apache License, Version 2.0 (the\n#  \"License\"); you may not use this file except in compliance\n#  with the License.  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing,\n#  software distributed under the License is distributed on an\n#  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n#  KIND, either express or implied.  See the License for the\n#  specific language governing permissions and limitations\n#  under the License.\n\n\nimport wx\nimport armid\nfrom BasePanel import BasePanel\nimport ConceptReference\nfrom Borg import Borg\n\nclass ConceptReferencePanel(BasePanel):\n  def __init__(self,parent):\n    BasePanel.__init__(self,parent,armid.CONCEPTREFERENCE_ID)\n    self.theId = None\n    b = Borg()\n    self.dbProxy = b.dbProxy\n    \n  def buildControls(self,isCreate,isUpdateable=True):\n    mainSizer = wx.BoxSizer(wx.VERTICAL)\n    mainSizer.Add(self.buildTextSizer('Name',(87,30),armid.CONCEPTREFERENCE_TEXTNAME_ID),0,wx.EXPAND)\n\n    dims = ['asset','attacker','countermeasure','domainproperty','environment','goal','misusecase','obstacle','persona','requirement','response','risk','role','task','threat','vulnerability']\n    mainSizer.Add(self.buildComboSizerList('Concept',(87,30),armid.CONCEPTREFERENCE_COMBODIMNAME_ID,dims),0,wx.EXPAND)\n    mainSizer.Add(self.buildComboSizerList('Object',(87,30),armid.CONCEPTREFERENCE_COMBOOBJTNAME_ID,[]),0,wx.EXPAND)\n    mainSizer.Add(self.buildMLTextSizer('Description',(87,30),armid.CONCEPTREFERENCE_TEXTDESCRIPTION_ID),1,wx.EXPAND)\n    mainSizer.Add(self.buildCommitButtonSizer(armid.CONCEPTREFERENCE_BUTTONCOMMIT_ID,isCreate),0,wx.CENTER)\n    wx.EVT_COMBOBOX(self,armid.CONCEPTREFERENCE_COMBODIMNAME_ID,self.onDimensionChange)\n    self.SetSizer(mainSizer)\n\n  def loadControls(self,objt,isReadOnly=False):\n    self.theId = objt.id()\n    nameCtrl = self.FindWindowById(armid.CONCEPTREFERENCE_TEXTNAME_ID)\n    dimCtrl = self.FindWindowById(armid.CONCEPTREFERENCE_COMBODIMNAME_ID)\n    objtCtrl = self.FindWindowById(armid.CONCEPTREFERENCE_COMBOOBJTNAME_ID)\n    descCtrl = self.FindWindowById(armid.CONCEPTREFERENCE_TEXTDESCRIPTION_ID)\n\n    nameCtrl.SetValue(objt.name())\n    dimCtrl.SetValue(objt.dimension())\n    objtCtrl.SetValue(objt.objectName())\n    descCtrl.SetValue(objt.description())\n\n  def onDimensionChange(self,evt):\n    dimName = evt.GetString()\n    objts = self.dbProxy.getDimensionNames(dimName)\n    objtCtrl = self.FindWindowById(armid.CONCEPTREFERENCE_COMBOOBJTNAME_ID)\n    objtCtrl.SetItems(objts)\n"}
{"text": "# Copyright 2021 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom .utils import get_model, get_optimizer, get_scheduler, LossTracker, AverageMeter, ProgressMeter, accuracy, balance_order_val,balance_order,get_pacing_function,run_cmd\nfrom .get_data import get_dataset\nfrom .cifar_label import CIFAR100N\n__all__ = [ \"get_dataset\", \"ImageMemFolder\", \"AverageMeter\", \"ProgressMeter\", \"accuracy\", \"get_optimizer\", \"get_scheduler\", \"get_model\", \"LossTracker\",\"cifar_label\",\"balance_order_val\",\"balance_order\",\"get_pacing_function\",\"run_cmd\"]\n "}
{"text": "# Copyright (C) 2010 Google Inc. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#    * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#    * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the\n# distribution.\n#    * Neither the name of Google Inc. nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport unittest\n\nfrom webkitpy.layout_tests.models.test_failures import *\n\n\nclass TestFailuresTest(unittest.TestCase):\n    def assert_loads(self, cls):\n        failure_obj = cls()\n        s = failure_obj.dumps()\n        new_failure_obj = TestFailure.loads(s)\n        self.assertTrue(isinstance(new_failure_obj, cls))\n\n        self.assertEqual(failure_obj, new_failure_obj)\n\n        # Also test that != is implemented.\n        self.assertFalse(failure_obj != new_failure_obj)\n\n    def test_unknown_failure_type(self):\n        class UnknownFailure(TestFailure):\n            def message(self):\n                return ''\n\n        failure_obj = UnknownFailure()\n        self.assertRaises(ValueError, determine_result_type, [failure_obj])\n\n    def test_message_is_virtual(self):\n        failure_obj = TestFailure()\n        self.assertRaises(NotImplementedError, failure_obj.message)\n\n    def test_loads(self):\n        for c in ALL_FAILURE_CLASSES:\n            self.assert_loads(c)\n\n    def test_equals(self):\n        self.assertEqual(FailureCrash(), FailureCrash())\n        self.assertNotEqual(FailureCrash(), FailureTimeout())\n        crash_set = set([FailureCrash(), FailureCrash()])\n        self.assertEqual(len(crash_set), 1)\n        # The hash happens to be the name of the class, but sets still work:\n        crash_set = set([FailureCrash(), \"FailureCrash\"])\n        self.assertEqual(len(crash_set), 2)\n\n    def test_crashes(self):\n        self.assertEquals(FailureCrash().message(), 'DumpRenderTree crashed')\n        self.assertEquals(FailureCrash(process_name='foo', pid=1234).message(), 'foo (pid 1234) crashed')\n"}
{"text": "import re\nquote_match = re.compile(r'(([\\\"\\']).*?\\2)')\ndouble_quote = re.compile(r'(\\A|[^\\\\])\\\"')\nsingle_quote = re.compile(r\"(\\A|[^\\\\])\\'\")\ndoc_name = re.compile(r'^[A-Za-z_][A-Za-z0-9_/]*$')\ntoken_separators=r'(?P<sep1>[\\s]+)|[\\s](?P<sep2>2>)[^&][^1]|[\\s](?P<sep3>!>)[\\s]|[\\s](?P<sep4>2>&1)|(?P<sep5>(?<![2!])>)|(?P<sep6>[;|])'\ntoken_separators = re.compile(token_separators)\nliteral = re.compile(r'.*') #rely on shlex.quote\n\ndef find_node(node_name, nodetypes, nodes):\n    if isinstance(nodetypes, str):\n        nodetypes = [nodetypes]\n    for nodetype in nodetypes:\n        for node_index, node in enumerate(nodes[nodetype]):\n            if node[\"name\"] == node_name:\n                return nodetype, node_index\n    raise NameError(node_name, nodetypes)\n\ndef append_node(nodes, nodetype, node):\n    for curr_node_index, curr_node in enumerate(nodes[nodetype]):\n        if curr_node:\n            if curr_node[\"name\"] == node[\"name\"]:\n                for field in curr_node:\n                    assert field in node, field #TODO: more informative message...\n                    assert node[field] == curr_node[field], field #TODO: more informative message...\n                for field in node:\n                    assert field in curr_node, field #TODO: more informative message...\n                return curr_node_index\n    for other_nodetype in nodes.keys():\n        if other_nodetype == nodetype:\n            continue\n        for curr_node in nodes[other_nodetype]:\n            assert curr_node[\"name\"] != node[\"name\"], (nodetype, other_nodetype, node[\"name\"]) #TODO: nicer error message\n    nodes[nodetype].append(node)\n    return len(nodes[nodetype]) - 1\n\ndef syntax_error(lineno, line, message):\n    message = \"    \" + \"\\n    \".join(message.splitlines())\n    msg = \"\"\"Line {0}:\n    {1}\nError message:\n{2}\"\"\".format(lineno, line, message)\n    raise SyntaxError(msg)\n\ndef tokenize(text, masked_text):\n    tokens = []\n    pos = 0\n    for match in token_separators.finditer(masked_text):\n        seps = match.groupdict()\n        split_tokens = [v for v in seps.values() if v is not None]\n        assert len(split_tokens) == 1, seps\n        split_token = split_tokens[0].strip()\n        newpos = match.start()\n        if pos != newpos:\n            tokens.append(text[pos:newpos])\n        pos = match.end()\n        if len(split_token):\n            tokens.append(split_token)\n    tokens.append(text[pos:])\n    return tokens\n"}
{"text": "import sys\nfrom datetime import date, timedelta\nfrom configuration import Configuration\nfrom service import raven_service, email_service\n\n\ndef main():\n    configuration_file_path = sys.argv[1]\n    configuration = Configuration(configuration_file_path)\n    _authenticate_services(configuration)\n    _make_user_bookings(configuration.users, 3)\n    _send_user_reports(configuration.users, 0)\n\n\ndef _authenticate_services(configuration):\n    \"\"\" Use `configuration` to authenticate raven_service and email_service.\n    :param configuration: Configuration instance for system configuration\n    \"\"\"\n    raven_service.set_default_credentials(configuration.default_crsid,\n                                          configuration.default_password)\n    email_service.set_email_credentials(configuration.gmail_username,\n                                        configuration.gmail_password)\n\n\ndef _make_user_bookings(users, days_in_advance):\n    \"\"\" Create bookings for each user in `users`.\n    :param users: list of Users to create bookings for\n    :param days_in_advance: how far in advance to book\n    :return: list of Booking instances containing all booked events\n    \"\"\"\n    date_to_book = date.today() + timedelta(days=days_in_advance)\n    bookings = []\n    for user in users:\n        bookings.append(user.create_booking(date_to_book))\n    return bookings\n\n\ndef _send_user_reports(users, days_in_advance):\n    \"\"\" Send reports to each user in `users`.\n    :param users: list of User instances to send reports to\n    :param days_in_advance: how many days in advance the reports should be for\n    \"\"\"\n    date_for_report = date.today() + timedelta(days=days_in_advance)\n    for user in users:\n        user.email_report(date_for_report)\n\n\nif __name__ == '__main__':\n    main()"}
{"text": "#!/usr/bin/python3\n# -*- coding: utf-8 -*-\n\n\"\"\"\nSnap! extension to support Raspberry Pi -- server component.\nCopyright (C) 2014  Paul C. Brown <p_brown@gmx.com>.\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"\n\nimport http.server\nimport os\nimport re\nimport socketserver\nimport urllib.request\nimport logging\n\nif __debug__:\n    import RPi.GPIO as GPIO\nelse:\n    import MockupRPi.GPIO as GPIO\n\n\nclass CORSHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):\n\n    regex = re.compile('.*pin=([0-9]*).*state=(LOW|HIGH)')\n    ospath = os.path.abspath('')\n        \n    def send_head(self):\n        path = self.path\n        \n        logging.info(path)\n        \n        # path looks like this:\n        # /pinwrite?pin=1&state=LOW\n        # or\n        # /pinread?pin=1&state=LOW\n\n        self.pin = 0\n        self.state = False\n\n        GPIO.setmode(GPIO.BCM)\n\n        m = self.regex.match(path)\n\n        if 'pinwrite' in path:  # write HIGH or LOW to pin\n\n            self.pin = int(m.group(1))\n            self.state = True\n            if m.group(2) == 'LOW':\n                self.state = False\n\n            GPIO.setup(self.pin, GPIO.OUT)\n            GPIO.output(self.pin, self.state)\n\n            #The Snap! block reports the body of the Web server\u2019s response \n            #(minus HTTP header), without interpretation.\n\n            #At a minimum, we must provide a header with a status line and a date.\n            self.send_response(200)\n            self.send_header('Date', self.date_time_string())\n            self.end_headers()\n            \n        elif 'pinread' in path:\n\n            # Read state of pin.\n\n            self.pin = int(m.group(1))\n            self.state = True\n            if m.group(2) == 'LOW':\n                self.state = False\n\n            f = open(self.ospath + '/return', 'w+')\n\n            GPIO.setup(self.pin, GPIO.IN)\n            if GPIO.input(self.pin) == self.state:\n                f.write(str(True))\n            else:\n                f.write(str(False))\n\n            f.close()\n            f = open(self.ospath + '/return', 'rb')\n            ctype = self.guess_type(self.ospath + '/rpireturn')\n            \n            #create minimal response\n            self.send_response(200)\n            self.send_header('Date', self.date_time_string())\n            self.send_header('Content-type', ctype)\n            fs = os.fstat(f.fileno())\n            self.send_header('Content-Length', str(fs[6]))\n            self.send_header('Last-Modified',\n                             self.date_time_string(fs.st_mtime))\n            self.send_header('Access-Control-Allow-Origin', '*')\n            self.end_headers()\n        \n\t\treturn f\n\nif __name__ == '__main__':\n    logging.basicConfig(level=logging.INFO, handlers=[logging.FileHandler(\"access.log\"), logging.StreamHandler()])\n    \n    PORT = 8280  # R+P in ASCII Decimal\n    Handler = CORSHTTPRequestHandler\n    \n    httpd = socketserver.TCPServer(('', PORT), Handler)\n    \n    logging.info('serving at port ' + str(PORT))\n    print('Go ahead and launch Snap!')\n    \n    httpd.serve_forever()\n"}
{"text": "\"\"\" The CS! (Configuration Service)\n\"\"\"\n\n__RCSID__ = \"$Id$\"\n\nfrom DIRAC.Core.Utilities.ReturnValues import S_OK, S_ERROR\nfrom DIRAC.ConfigurationSystem.private.ServiceInterface import ServiceInterface\nfrom DIRAC.Core.DISET.RequestHandler import RequestHandler\nfrom DIRAC.Core.Utilities import DErrno\n\ngServiceInterface = None\ngPilotSynchronizer = None\n\ndef initializeConfigurationHandler(serviceInfo):\n  global gServiceInterface\n  gServiceInterface = ServiceInterface(serviceInfo['URL'])\n  return S_OK()\n\n\nclass ConfigurationHandler(RequestHandler):\n  \"\"\" The CS handler\n  \"\"\"\n\n  types_getVersion = []\n\n  def export_getVersion(self):\n    return S_OK(gServiceInterface.getVersion())\n\n  types_getCompressedData = []\n\n  def export_getCompressedData(self):\n    sData = gServiceInterface.getCompressedConfigurationData()\n    return S_OK(sData)\n\n  types_getCompressedDataIfNewer = [basestring]\n\n  def export_getCompressedDataIfNewer(self, sClientVersion):\n    sVersion = gServiceInterface.getVersion()\n    retDict = {'newestVersion': sVersion}\n    if sClientVersion < sVersion:\n      retDict['data'] = gServiceInterface.getCompressedConfigurationData()\n    return S_OK(retDict)\n\n  types_publishSlaveServer = [basestring]\n\n  def export_publishSlaveServer(self, sURL):\n    gServiceInterface.publishSlaveServer(sURL)\n    return S_OK()\n\n  types_commitNewData = [basestring]\n\n  def export_commitNewData(self, sData):\n    global gPilotSynchronizer\n    credDict = self.getRemoteCredentials()\n    if 'DN' not in credDict or 'username' not in credDict:\n      return S_ERROR(\"You must be authenticated!\")\n    res = gServiceInterface.updateConfiguration(sData, credDict['username'])\n    if not res['OK']:\n      return res\n\n    # Check the flag for updating the pilot 3 JSON file\n    if self.srv_getCSOption('UpdatePilotCStoJSONFile', False) and gServiceInterface.isMaster():\n      if gPilotSynchronizer is None:\n        try:\n          # This import is only needed for the Master CS service, making it conditional avoids\n          # dependency on the git client preinstalled on all the servers running CS slaves\n          from DIRAC.WorkloadManagementSystem.Utilities.PilotCStoJSONSynchronizer import PilotCStoJSONSynchronizer\n        except ImportError as exc:\n          self.log.exception(\"Failed to import PilotCStoJSONSynchronizer\", repr(exc))\n          return S_ERROR(DErrno.EIMPERR, 'Failed to import PilotCStoJSONSynchronizer')\n        gPilotSynchronizer = PilotCStoJSONSynchronizer()\n      return gPilotSynchronizer.sync()\n\n    return res\n\n  types_writeEnabled = []\n\n  def export_writeEnabled(self):\n    return S_OK(gServiceInterface.isMaster())\n\n  types_getCommitHistory = []\n\n  def export_getCommitHistory(self, limit=100):\n    if limit > 100:\n      limit = 100\n    history = gServiceInterface.getCommitHistory()\n    if limit:\n      history = history[:limit]\n    return S_OK(history)\n\n  types_getVersionContents = [list]\n\n  def export_getVersionContents(self, versionList):\n    contentsList = []\n    for version in versionList:\n      retVal = gServiceInterface.getVersionContents(version)\n      if retVal['OK']:\n        contentsList.append(retVal['Value'])\n      else:\n        return S_ERROR(\"Can't get contents for version %s: %s\" % (version, retVal['Message']))\n    return S_OK(contentsList)\n\n  types_rollbackToVersion = [basestring]\n\n  def export_rollbackToVersion(self, version):\n    retVal = gServiceInterface.getVersionContents(version)\n    if not retVal['OK']:\n      return S_ERROR(\"Can't get contents for version %s: %s\" % (version, retVal['Message']))\n    credDict = self.getRemoteCredentials()\n    if 'DN' not in credDict or 'username' not in credDict:\n      return S_ERROR(\"You must be authenticated!\")\n    return gServiceInterface.updateConfiguration(retVal['Value'],\n                                                 credDict['username'],\n                                                 updateVersionOption=True)\n"}
{"text": "#    (c) Copyright 2014 Brocade Communications Systems Inc.\n#    All Rights Reserved.\n#\n#    Copyright 2014 OpenStack Foundation\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n#\n\nimport re\n\nfrom oslo_log import log\n\nfrom jacket.storage.i18n import _LI\n\nLOG = log.getLogger(__name__)\n\n\ndef get_friendly_zone_name(zoning_policy, initiator, target,\n                           host_name, storage_system, zone_name_prefix,\n                           supported_chars):\n    \"\"\"Utility function implementation of _get_friendly_zone_name.\n\n    Get friendly zone name is used to form the zone name\n    based on the details provided by the caller\n\n    :param zoning_policy - determines the zoning policy is either\n    initiator-target or initiator\n    :param initiator - initiator WWN\n    :param target - target WWN\n    :param host_name - Host name returned from Volume Driver\n    :param storage_system - Storage name returned from Volume Driver\n    :param zone_name_prefix - user defined zone prefix configured\n    in storage.conf\n    :param supported_chars - Supported character set of FC switch vendor.\n    Example: 'abc123_-$'. These are defined in the FC zone drivers.\n    \"\"\"\n    if host_name is None:\n        host_name = ''\n    if storage_system is None:\n        storage_system = ''\n    if zoning_policy == 'initiator-target':\n        host_name = host_name[:14]\n        storage_system = storage_system[:14]\n        if len(host_name) > 0 and len(storage_system) > 0:\n            zone_name = (host_name + \"_\"\n                         + initiator.replace(':', '') + \"_\"\n                         + storage_system + \"_\"\n                         + target.replace(':', ''))\n        else:\n            zone_name = (zone_name_prefix\n                         + initiator.replace(':', '')\n                         + target.replace(':', ''))\n            LOG.info(_LI(\"Zone name created using prefix because either \"\n                         \"host name or storage system is none.\"))\n    else:\n        host_name = host_name[:47]\n        if len(host_name) > 0:\n            zone_name = (host_name + \"_\"\n                         + initiator.replace(':', ''))\n        else:\n            zone_name = (zone_name_prefix\n                         + initiator.replace(':', ''))\n            LOG.info(_LI(\"Zone name created using prefix because host \"\n                         \"name is none.\"))\n\n    LOG.info(_LI(\"Friendly zone name after forming: %(zonename)s\"),\n             {'zonename': zone_name})\n    zone_name = re.sub('[^%s]' % supported_chars, '', zone_name)\n    return zone_name\n"}
{"text": "#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\n# \u4f4d\u8fd0\u7b97\u7b26\nprint \" & | ^ ~ << >>\"\n\n# \u903b\u8f91\u8fd0\u7b97\u7b26\na = 10\nb = 20\nif (a and b):\n    print 'a and b = ', a and b\n\nif (a or b):\n    print 'a or b = ', a or b\n\na = 0\nif (not a):\n    print 'not a = ', not a\n\n\na = 21\nb = 10\nc = 0\n\n# \u8d4b\u503c\u8fd0\u7b97\u7b26\nprint \"\u8d4b\u503c\u8fd0\u7b97\u7b26\"\nc = a + b\nc += a\nprint \"1 - c = \", c\n\nc *=  a\nprint \"2 - c = \", c\n\nc /= a\nprint \"3 - c = \", c\n\nc = 2\nc %= c\nprint \"4 - c = \", c\n\nc **= a\nprint \"5 - c = \", c\n\nc //= a\nprint \"6 - c = \", c\n\n# \u6bd4\u8f83\u8fd0\u7b97\u7b26\nprint \"\u6bd4\u8f83\u8fd0\u7b97\u7b26\"\nif (a == b):\n    print \"1 - a == b\"\nelse:\n    print \"1 - a != b\"\n\nif (a != b):\n    print \"2 - a != b\"\nelse:\n    print \"2 - a == b\" \n\nif (a <> b):\n    print \"3 - a != b\"\nelse:\n    print \"3 - a == b\"\n\nif (a < b):\n    print \"4 - a < b\"\nelse:\n    print \"4 - a >= b\"\n\nif (a > b):\n    print \"5 - a > b\"\nelse:\n    print \"5 - a <= b\"\n\nif (a >= b):\n    print \"6 - a >= b\"\nelse:\n    print \"6 - a < b\"\n\nif (a <= b):\n    print \"7 - a <= b\"\nelse:\n    print \"7 - a > b\"\n\n# + - * ** / // \u8fd0\u7b97\u7b26\nprint \"+ - * / ** //\"\nc = a + b\nprint \"1 - c = \", c # 31\n\nc = a - b\nprint \"2 - c = \", c # 11\n\nc = a * b\nprint \"3 - c = \", c # 210\n\nc = a / b\nprint \"4 - c = \", c # 2\n\na = 2\nb = 3\nc = a ** b\nprint \"6 - c = \", c # 8\n\na = 10\nb = 5\nc = a // b\nprint \"7 - c = \", c # 2\n"}
{"text": "# encoding: utf-8\n# module PyKDE4.kio\n# from /usr/lib/python2.7/dist-packages/PyKDE4/kio.so\n# by generator 1.135\n# no doc\n\n# imports\nimport PyKDE4.kdeui as __PyKDE4_kdeui\nimport PyQt4.QtCore as __PyQt4_QtCore\nimport PyQt4.QtGui as __PyQt4_QtGui\n\n\nclass KUriFilter(): # skipped bases: <type 'sip.wrapper'>\n    # no doc\n    def filteredUri(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def filterSearchUri(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def filterUri(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def loadPlugins(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def pluginNames(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def self(self, *args, **kwargs): # real signature unknown\n        pass\n\n    def __init__(self, *args, **kwargs): # real signature unknown\n        pass\n\n    __weakref__ = property(lambda self: object(), lambda self, v: None, lambda self: None)  # default\n    \"\"\"list of weak references to the object (if defined)\"\"\"\n\n\n    NormalTextFilter = 1\n    SearchFilterType = None # (!) real value is ''\n    SearchFilterTypes = None # (!) real value is ''\n    WebShortcutFilter = 2\n\n\n"}
{"text": "#!/usr/bin/env python\n\n# Copyright (c) 2016 Satya Mallick <spmallick@learnopencv.com>\n# All rights reserved. No warranty, explicit or implicit, provided.\n\nimport cv2\nimport numpy as np\nimport math\nimport random\n\nR = np.matrix([[0.6927,-0.7146,0.0978],[0.7165,0.6973,0.0198],[-0.0824,0.0564,0.995]])\n# Checks if a matrix is a valid rotation matrix.\ndef isRotationMatrix(R) :\n    Rt = np.transpose(R)\n    shouldBeIdentity = np.dot(Rt, R)\n    I = np.identity(3, dtype = R.dtype)\n    n = np.linalg.norm(I - shouldBeIdentity)\n    return n < 1e-6\n\n\n# Calculates rotation matrix to euler angles\n# The result is the same as MATLAB except the order\n# of the euler angles ( x and z are swapped ).\ndef rotationMatrixToEulerAngles(R) :\n\n   # assert(isRotationMatrix(R))\n    \n    sy = math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0])\n    \n    singular = sy < 1e-6\n\n    if  not singular :\n        x = math.atan2(R[2,1] , R[2,2])\n        y = math.atan2(-R[2,0], sy)\n        z = math.atan2(R[1,0], R[0,0])\n    else :\n        x = math.atan2(-R[1,2], R[1,1])\n        y = math.atan2(-R[2,0], sy)\n        z = 0\n\n    return np.array([x, y, z])\n\n# Calculates Rotation Matrix given euler angles.\ndef eulerAnglesToRotationMatrix(theta) :\n    \n    R_x = np.array([[1,         0,                  0                   ],\n                    [0,         math.cos(theta[0]), -math.sin(theta[0]) ],\n                    [0,         math.sin(theta[0]), math.cos(theta[0])  ]\n                    ])\n        \n        \n                    \n    R_y = np.array([[math.cos(theta[1]),    0,      math.sin(theta[1])  ],\n                    [0,                     1,      0                   ],\n                    [-math.sin(theta[1]),   0,      math.cos(theta[1])  ]\n                    ])\n                \n    R_z = np.array([[math.cos(theta[2]),    -math.sin(theta[2]),    0],\n                    [math.sin(theta[2]),    math.cos(theta[2]),     0],\n                    [0,                     0,                      1]\n                    ])\n                    \n                    \n    RR = np.dot(R_z, np.dot( R_y, R_x ))\n\n    return RR\n\n\nif __name__ == '__main__' :\n\n    # Randomly generate Euler angles\n #   e = np.random.rand(3) * math.pi * 2 - math.pi\n    \n    # Calculate rotation matrix\n #   RR = eulerAnglesToRotationMatrix(e)\n    \n    # Calculate Euler angles from rotation matrix\n    e1 = rotationMatrixToEulerAngles(R)\n\n    # Calculate rotation matrix\n    R1 = eulerAnglesToRotationMatrix(e1)\n\n    # Note e and e1 will be the same a lot of times\n    # but not always. R and R1 should be the same always.\n\n #   print \"\\nInput Euler angles :\\n{0}\".format(e)\n    print \"\\nR :\\n{0}\".format(R)\n    print \"\\nOutput Euler angles :\\n{0}\".format(e1)\n #   print \"\\nR1 :\\n{0}\".format(R1)\n\n"}
{"text": "# Copyright 2016 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Configurations.\"\"\"\n\nfrom lambada_lm.config_registry import config_registry\nconfig_registry.register('default', {\n  # Data\n  'read' : 'continuosly_with_extra_word',\n  'eval_read': 'continuosly_with_extra_word',\n  'num_steps' : 100,\n  'eval_num_steps' : 100,\n\n  # Schedule\n  'monitoring_frequency' : 100,\n  'saving_frequency' : 5000,\n  'max_batches_per_epoch' : 5000,\n  'num_epochs' : 100,\n  'start_annealing' : 20,\n  'lr_decay' : 0.8,\n\n  # Model\n  'init_scale' : 0.1,\n  'forget_bias' : 0.0,\n  'dim' : 128,\n  'architecture' : 'lstm',\n  'act' : 'relu',\n  'width' : -1,\n\n\n  # Optimization\n  'optimizer' : 'GradientDescentOptimizer',\n  'batch_size' : 32,\n  'learning_rate' : 1.0,\n  'lr_min': 0.000001,\n  'momentum' : 0.9,\n  'epsilon' : 1e-8,\n  'max_grad_norm': 5.0,\n  'next_worker_delay' : 1500,\n})\n\nc = config_registry['default']\nc['dim'] = 512\nc['read'] = 'shards_continuosly_with_bos'\nc['eval_read'] = 'padded_sentences_with_bos'\nc['eval_num_steps'] = 210\nconfig_registry.register('lambada', c)\n\nc = config_registry['lambada']\nc['optimizer'] = 'AdamOptimizer'\nc['learning_rate'] = 0.001\nconfig_registry.register('lambAdam', c)\n\nc = config_registry['lambAdam']\nc['architecture'] = 'conv'\nc['width'] = 5\nconfig_registry.register('lambAdamConv', c)\n"}
{"text": "import dryscrape, re, sys\nfrom bs4 import BeautifulSoup\n\nclass EmailCrawler:\n    \"\"\"\n    Takes a domain name and prints out a list of email addresses found on that web page\n    or a lower level web page with the same given domain name.\n    Stores emails, paths, and visited_paths as sets to avoid duplicates.\n    Uses Dryscrape to dynamically scrape text of JavaScript generated and static websites.\n    Uses BeautifulSoup to search for valid href's to continue crawling on.\n    \"\"\"\n    emailRE = re.compile(\"[\\w.+-]+@(?!\\dx)[\\w-]+\\.[\\w.-]+[\\w-]+\")\n\n    def __init__(self, domain):\n        if 'http' not in domain:\n            domain = 'http://' + domain\n        self.url = domain.lower()\n        self.session = dryscrape.Session(base_url=self.url)\n        self.emails = set()\n        self.paths = set()\n        self.visited_paths = set()\n        self.num_pages_limit = 50\n\n        self.session.set_attribute('auto_load_images', False)\n\n    def is_valid_tag(self, tag):\n        \"\"\"Checks if a tag contains a valid href that hasn't been visited yet.\"\"\"\n\n        if tag.has_attr('href') and len(tag['href']) > 0:\n            href = tag['href']\n            complete_href = self.session.complete_url(href)\n\n            is_relative = self.url in complete_href\n            is_visited = complete_href in self.visited_paths\n            is_style_sheet = tag.name == \"link\"\n            is_jumpTo = \"#\" in href\n            is_mailTo = \"mailto\" in href\n            is_js = \"javascript:\" in href\n            return is_relative and \\\n                not (is_visited or is_style_sheet or is_jumpTo or is_mailTo or is_js)\n        else:\n            return False\n\n    def find_emails_and_paths(self, path=None):\n        # Load the DOM\n        try:\n            self.session.visit(path)\n        except:\n            print(\"Error accessing the given URL\")\n            return\n\n        # Pass the DOM as HTML into the lxml parser\n        print(\"Crawling on:\\t\" + path)\n        response = self.session.body()\n        soup = BeautifulSoup(response, \"lxml\")\n\n        # Add new emails to `self.emails` \n        for email in re.findall(self.emailRE, response):\n            self.emails.add(email)\n\n        # Mark the current path as visited\n        self.visited_paths.add(path)\n\n        # Add new paths to `self.paths`\n        for tag in soup.find_all(self.is_valid_tag):\n            href = self.session.complete_url(tag['href']).lower()\n            self.paths.add(href)\n\n    def find(self):\n        \"\"\"\n        Crawls through new paths until the page limit has been reached or\n        there are no more discoverable paths.\n        \"\"\"\n        self.paths.add(self.url)\n        while len(self.visited_paths) < self.num_pages_limit and \\\n              len(self.paths) > 0:\n            self.find_emails_and_paths(path=self.paths.pop())\n\n    def print_emails(self):\n        # Print the emails found (if any)\n        if len(self.emails) > 0:\n            print(\"\\nFound these email addresses:\")\n            for email in self.emails:\n                print(\"\\t\" + email)\n        else:\n            print(\"\\nNo email addresses found.\")\n\ndef main():\n    \"\"\"\n    Initializes the crawler with the given domain name\n    and optional maximum number of pages to search.\n    Finds and prints any emails found.\n    \"\"\"\n    if len(sys.argv) >= 2:\n        crawler = EmailCrawler(sys.argv[1])\n        if len(sys.argv) >= 3 and sys.argv[2].isdigit():\n            crawler.num_pages_limit = int(sys.argv[2])\n\n        print(\"Beginning crawl with a limit of \" + str(crawler.num_pages_limit) + \" pages...\\n\")\n        crawler.find()\n        crawler.print_emails()\n    else:\n        print(\"Error: Please enter a domain to search on and an optional page limit (default=50).\")\n        print(\"Example: `python find_email_addresses.py jana.com 30`\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n"}
{"text": "# encoding: utf-8\n\n\"\"\"Classes used in scattering and gathering sequences.\n\nScattering consists of partitioning a sequence and sending the various\npieces to individual nodes in a cluster.\n\"\"\"\n\n# Copyright (c) IPython Development Team.\n# Distributed under the terms of the Modified BSD License.\n\nfrom __future__ import division\n\nimport sys\nfrom itertools import islice, chain\n\nnumpy = None\n\ndef is_array(obj):\n    \"\"\"Is an object a numpy array?\n    \n    Avoids importing numpy until it is requested\n    \"\"\"\n    global numpy\n    if 'numpy' not in sys.modules:\n        return False\n    \n    if numpy is None:\n        import numpy\n    return isinstance(obj, numpy.ndarray)\n\nclass Map(object):\n    \"\"\"A class for partitioning a sequence using a map.\"\"\"\n    \n    def getPartition(self, seq, p, q, n=None):\n        \"\"\"Returns the pth partition of q partitions of seq.\n        \n        The length can be specified as `n`,\n        otherwise it is the value of `len(seq)`\n        \"\"\"\n        n = len(seq) if n is None else n\n        # Test for error conditions here\n        if p<0 or p>=q:\n          raise ValueError(\"must have 0 <= p <= q, but have p=%s,q=%s\" % (p, q))\n        \n        remainder = n % q\n        basesize = n // q\n        \n        if p < remainder:\n            low = p * (basesize + 1)\n            high = low + basesize + 1\n        else:\n            low = p * basesize + remainder\n            high = low + basesize\n        \n        try:\n            result = seq[low:high]\n        except TypeError:\n            # some objects (iterators) can't be sliced,\n            # use islice:\n            result = list(islice(seq, low, high))\n            \n        return result\n           \n    def joinPartitions(self, listOfPartitions):\n        return self.concatenate(listOfPartitions)\n    \n    def concatenate(self, listOfPartitions):\n        testObject = listOfPartitions[0]\n        # First see if we have a known array type\n        if is_array(testObject):\n            return numpy.concatenate(listOfPartitions)\n        # Next try for Python sequence types\n        if isinstance(testObject, (list, tuple)):\n            return list(chain.from_iterable(listOfPartitions))\n        # If we have scalars, just return listOfPartitions\n        return listOfPartitions\n\nclass RoundRobinMap(Map):\n    \"\"\"Partitions a sequence in a round robin fashion.\n    \n    This currently does not work!\n    \"\"\"\n\n    def getPartition(self, seq, p, q, n=None):\n        n = len(seq) if n is None else n\n        return seq[p:n:q]\n\n    def joinPartitions(self, listOfPartitions):\n        testObject = listOfPartitions[0]\n        # First see if we have a known array type\n        if is_array(testObject):\n            return self.flatten_array(listOfPartitions)\n        if isinstance(testObject, (list, tuple)):\n            return self.flatten_list(listOfPartitions)\n        return listOfPartitions\n    \n    def flatten_array(self, listOfPartitions):\n        test = listOfPartitions[0]\n        shape = list(test.shape)\n        shape[0] = sum([ p.shape[0] for p in listOfPartitions])\n        A = numpy.ndarray(shape)\n        N = shape[0]\n        q = len(listOfPartitions)\n        for p,part in enumerate(listOfPartitions):\n            A[p:N:q] = part\n        return A\n    \n    def flatten_list(self, listOfPartitions):\n        flat = []\n        for i in range(len(listOfPartitions[0])):\n            flat.extend([ part[i] for part in listOfPartitions if len(part) > i ])\n        return flat\n\ndef mappable(obj):\n    \"\"\"return whether an object is mappable or not.\"\"\"\n    if isinstance(obj, (tuple,list)):\n        return True\n    if is_array(obj):\n        return True\n    return False\n\ndists = {'b':Map,'r':RoundRobinMap}\n\n"}
{"text": "#!/usr/bin/python\n# Copyright (c) 2011 The Chromium Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\nimport os\n\nimport pyauto_functional\nimport pyauto\n\n\nclass ChromeosSecurity(pyauto.PyUITest):\n  \"\"\"Security tests for chrome on ChromeOS.\n\n  Requires ChromeOS to be logged in.\n  \"\"\"\n\n  def ExtraChromeFlagsOnChromeOS(self):\n    \"\"\"Override default list of extra flags typicall used with automation.\n\n    See the default flags used with automation in pyauto.py.\n    Chrome flags for this test should be as close to reality as possible.\n    \"\"\"\n    return [\n       '--homepage=about:blank',\n    ]\n\n  def testCannotViewLocalFiles(self):\n    \"\"\"Verify that local files cannot be accessed from the browser.\"\"\"\n    urls_and_titles = {\n       'file:///': 'Index of /',\n       'file:///etc/': 'Index of /etc/',\n       self.GetFileURLForDataPath('title2.html'): 'Title Of Awesomeness',\n    }\n    for url, title in urls_and_titles.iteritems():\n      self.NavigateToURL(url)\n      self.assertNotEqual(title, self.GetActiveTabTitle(),\n          msg='Could access local file %s.' % url)\n\n\nif __name__ == '__main__':\n  pyauto_functional.Main()\n"}
{"text": "from recipe_scrapers.cookpad import CookPad\nfrom tests import ScraperTest\n\n\nclass TestCookPadScraper(ScraperTest):\n\n    scraper_class = CookPad\n\n    def test_host(self):\n        self.assertEqual(\"cookpad.com\", self.harvester_class.host())\n\n    def test_canonical_url(self):\n        self.assertEqual(\n            \"https://cookpad.com/recipe/4610651\", self.harvester_class.canonical_url()\n        )\n\n    def test_title(self):\n        self.assertEqual(self.harvester_class.title(), \"30\u5206\u3067\u7c21\u5358\u672c\u683c\u30d0\u30bf\u30fc\u30c1\u30ad\u30f3\u30ab\u30ec\u30fc\")\n\n    def test_yields(self):\n        self.assertEqual(\"4 serving(s)\", self.harvester_class.yields())\n\n    def test_image(self):\n        self.assertEqual(\n            \"https://img.cpcdn.com/recipes/4610651/640x640c/6de3ac788480ce2787e5e39714ef0856?u=6992401&p=1519025894\",\n            self.harvester_class.image(),\n        )\n\n    def test_ingredients(self):\n        self.assertCountEqual(\n            [\n                \"\u2665\u9d8f\u30e2\u30e2\u8089 500g\u524d\u5f8c\",\n                \"\u2665\u7389\u306d\u304e 2\u500b\",\n                \"\u2665\u306b\u3093\u306b\u304f\u30c1\u30e5\u30fc\u30d6 5cm\",\n                \"\u2665\u751f\u59dc\u30c1\u30e5\u30fc\u30d6 5cm(\u306a\u304f\u3066\u3082\u2661)\",\n                \"\u2665\u30ab\u30ec\u30fc\u7c89 \u5927\u3055\u30581\u30681/2\",\n                \"\u2665\u30d0\u30bf\u30fc \u5927\u3055\u30582+\u5927\u3055\u30583(60g)\",\n                \"\uff0a\u30c8\u30de\u30c8\u7f36 1\u7f36\",\n                \"\uff0a\u30b3\u30f3\u30bd\u30e1 \u5c0f\u3055\u30581\",\n                \"\uff0a\u5869 \u5c0f\u3055\u3058(1\u301c)2\u5f31\",\n                \"\uff0a\u7802\u7cd6 \u5c0f\u3055\u30582\",\n                \"\uff0a\u6c34 100ml\",\n                \"\uff0a\u30b1\u30c1\u30e3\u30c3\u30d7 \u5927\u3055\u30581\",\n                \"\u2665\u751f\u30af\u30ea\u30fc\u30e0 100ml\",\n            ],\n            self.harvester_class.ingredients(),\n        )\n\n    def test_instructions(self):\n        return self.assertEqual(\n            \"\u9d8f\u30e2\u30e2\u8089 \u306f\u4e00\u53e3\u5927\u306b\u3001 \u7389\u306d\u304e \u306f\u8584\u5207\u308a(or\u307f\u3058\u3093\u5207\u308a)\u306b\u3057\u307e\u3059\u266a\\n\u30d5\u30e9\u30a4\u30d1\u30f3\u306b \u30d0\u30bf\u30fc(\u5927\u3055\u30582) \u3092\u71b1\u3057\u3001\u9d8f\u8089 \u306b \u5869\u80e1\u6912 \u3092\u3075\u308a\u8868\u9762\u3092\u3053\u3093\u304c\u308a\u713c\u304d\u307e\u3059\u266a\\n\u304a\u934b\u306b \u30d0\u30bf\u30fc(\u5927\u3055\u30583) \u306b\u3093\u306b\u304f\u30c1\u30e5\u30fc\u30d6 \u751f\u59dc\u30c1\u30e5\u30fc\u30d6 \u7389\u306d\u304e \u3092\u5165\u308c\u3066\u3042\u3081\u8272\u306b\u306a\u308b\u307e\u3067\u3058\u3063\u304f\u308a\u7092\u3081\u307e\u3059\u266a\\n\u30ab\u30ec\u30fc\u7c89 \u3092\u52a0\u3048\u3066\u5f31\u706b\u30673\u5206\u304f\u3089\u3044\u7092\u3081\u307e\u3059\u266a\\n\uff0a \u3068 \u9d8f\u8089(\u6cb9\u5206\u3082) \u3092\u52a0\u3048\u3066\u6cb8\u9a30\u3057\u305f\u3089\u706b\u304c\u901a\u308b\u307e\u3067(10\u5206\u7a0b)\u716e\u307e\u3059\u266a\\n\u4ed5\u4e0a\u3052\u306b \u751f\u30af\u30ea\u30fc\u30e0 \u3092\u52a0\u3048\u3066\u6df7\u305c\u3001\u6e29\u307e\u3063\u305f\u3089\u3059\u3050\u706b\u3092\u6b62\u3081\u307e\u3059\u266a \u5b8c\u6210\u2661\u2661 \u66f4\u306b\u4ed5\u4e0a\u3052\u306b\u751f\u30af\u30ea\u30fc\u30e0\u3092\u30c8\u30c3\u30d4\u30f3\u30b0\u3057\u307e\u3057\u305f\u2661\\n\u5b50\u4f9b\u3054\u306f\u3093\u306f\u3053\u3093\u306a\u611f\u3058\u306e\u76db\u308a\u4ed8\u3051\u306b\u2661\u2665\",\n            self.harvester_class.instructions(),\n        )\n"}
{"text": "from __future__ import with_statement\nfrom alembic import context\nfrom sqlalchemy import engine_from_config, pool\nfrom logging.config import fileConfig\n\n# Hack for local models\nimport os, sys\nsys.path.append(os.getcwd())\nfrom ADSDeploy.models import Base\ntry:\n    from ADSDeploy.local_config import SQLALCHEMY_URL\nexcept ImportError:\n    from ADSDeploy.config import SQLALCHEMY_URL\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nfileConfig(config.config_file_name)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = Base.metadata\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline():\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url, target_metadata=target_metadata, literal_binds=True)\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n\n    alembic_config = config.get_section(config.config_ini_section)\n    alembic_config['sqlalchemy.url'] = SQLALCHEMY_URL\n\n    connectable = engine_from_config(\n        alembic_config,\n        prefix='sqlalchemy.',\n        poolclass=pool.NullPool)\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection,\n            target_metadata=target_metadata\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n"}
{"text": "\"\"\"\nSending RF signals with low-cost GPIO RF Modules on a Raspberry Pi.\n\"\"\"\n\nimport logging\nimport threading\nimport time\n\nfrom Adafruit_GPIO import GPIO\n\nfrom rfdevices.protocol import BasebandValue, Protocol, PulseOrder\n\nlog = logging.getLogger(__name__)\n\n\nclass Transmitter:\n    \"\"\"Representation of a GPIO RF chip.\"\"\"\n\n    def __init__(self, gpio: int, platform: GPIO.BaseGPIO=None, **kwargs):\n        \"\"\"Initialize the RF device.\"\"\"\n        self.gpio = gpio\n        self.platform = platform or GPIO.get_platform_gpio(**kwargs)\n\n        self.tx_enabled = False\n        self.lock = threading.Lock()\n\n        log.debug(\"Using GPIO \" + str(gpio))\n\n    def __enter__(self):\n        self.setup()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.cleanup()\n\n    def cleanup(self):\n        \"\"\"Disable TX and clean up GPIO.\"\"\"\n        with self.lock:\n            if self.tx_enabled:\n                self.platform.cleanup(pin=self.gpio)\n                self.tx_enabled = False\n                log.debug(\"Cleanup\")\n\n    def setup(self):\n        \"\"\"Enable TX, set up GPIO.\"\"\"\n        with self.lock:\n            if not self.tx_enabled:\n                self.platform.setup(pin=self.gpio, mode=GPIO.OUT)\n                self.tx_enabled = True\n                log.debug(\"TX enabled\")\n            return True\n\n    def tx_code(self, code: int, tx_proto: Protocol):\n        \"\"\"\n        Send a decimal code.\n        \"\"\"\n        bin_code = tx_proto.prepare_code(code)\n        with self.lock:\n            log.debug(\"TX code: \" + str(code))\n            return self._tx_bin_locked(bin_code, tx_proto)\n\n    def tx_bin(self, value: str, tx_proto: Protocol) -> bool:\n        \"\"\"Send a binary code.\"\"\"\n        with self.lock:\n            return self._tx_bin_locked(value, tx_proto)\n\n    def _tx_bin_locked(self, value: str, tx_proto: Protocol) -> bool:\n        if len(value) != tx_proto.message_length:\n            raise ValueError('Invalid value length, must be exactly {}'.format(tx_proto.message_length))\n\n        log.debug(\"TX bin: \" + str(value))\n        for _ in range(0, tx_proto.repeat):\n            for byte in range(0, tx_proto.message_length):\n                if value[byte] == '0':\n                    if not self._tx_zero_bit(tx_proto):\n                        return False\n                else:\n                    if not self._tx_one_bit(tx_proto):\n                        return False\n            if not self._tx_sync(tx_proto):\n                return False\n\n        return True\n\n    def _tx_zero_bit(self, tx_proto: Protocol):\n        \"\"\"Send a '0' bit.\"\"\"\n        return self._tx_waveform(tx_proto.pulse_length, tx_proto.pulse_order, tx_proto.zero)\n\n    def _tx_one_bit(self, tx_proto: Protocol):\n        \"\"\"Send a '1' bit.\"\"\"\n        return self._tx_waveform(tx_proto.pulse_length, tx_proto.pulse_order, tx_proto.one)\n\n    def _tx_sync(self, tx_proto: Protocol):\n        \"\"\"Send a sync.\"\"\"\n        return self._tx_waveform(tx_proto.pulse_length, tx_proto.pulse_order, tx_proto.sync)\n\n    def _tx_waveform(self, pulse_length: int, pulse_order: PulseOrder, value: BasebandValue):\n        \"\"\"Send basic waveform.\"\"\"\n        if not self.tx_enabled:\n            log.error(\"TX is not enabled, not sending data\")\n            return False\n\n        def low():\n            self.platform.output(pin=self.gpio, value=GPIO.LOW)\n            time.sleep((value.low * pulse_length) / 1000000)\n\n        def high():\n            self.platform.output(pin=self.gpio, value=GPIO.HIGH)\n            time.sleep((value.high * pulse_length) / 1000000)\n\n        if pulse_order is PulseOrder.LowHigh:\n            low()\n            high()\n            self.platform.output(pin=self.gpio, value=GPIO.LOW)\n        elif pulse_order is PulseOrder.HighLow:\n            high()\n            low()\n\n        return True\n"}
{"text": "\"\"\"\nWSGI config for lugflmembers project.\n\nThis module contains the WSGI application used by Django's development server\nand any production WSGI deployments. It should expose a module-level variable\nnamed ``application``. Django's ``runserver`` and ``runfcgi`` commands discover\nthis application via the ``WSGI_APPLICATION`` setting.\n\nUsually you will have the standard Django WSGI application here, but it also\nmight make sense to replace the whole Django WSGI application with a custom one\nthat later delegates to the Django one. For example, you could introduce WSGI\nmiddleware here, or combine a Django application with an application of another\nframework.\n\n\"\"\"\nimport os\n\n# We defer to a DJANGO_SETTINGS_MODULE already in the environment. This breaks\n# if running multiple sites in the same mod_wsgi process. To fix this, use\n# mod_wsgi daemon mode with each site in its own daemon process, or use\n# os.environ[\"DJANGO_SETTINGS_MODULE\"] = \"lugflmembers.settings\"\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"lugflmembers.local_settings\")\n\n# This application object is used by any WSGI server configured to use this\n# file. This includes Django's development server, if the WSGI_APPLICATION\n# setting points here.\nfrom django.core.wsgi import get_wsgi_application\napplication = get_wsgi_application()\n\n# Apply WSGI middleware here.\n# from helloworld.wsgi import HelloWorldApplication\n# application = HelloWorldApplication(application)\n"}
{"text": "# actor.py\r\n# Copyright (C) 2008-2010 Michael Trier (mtrier@gmail.com) and contributors\r\n#\r\n# This module is part of GitPython and is released under\r\n# the BSD License: http://www.opensource.org/licenses/bsd-license.php\r\n\r\nimport re\r\n\r\nclass Actor(object):\r\n    \"\"\"Actors hold information about a person acting on the repository. They\r\n    can be committers and authors or anything with a name and an email as\r\n    mentioned in the git log entries.\"\"\"\r\n    def __init__(self, name, email):\r\n        self.name = name\r\n        self.email = email\r\n\r\n    def __str__(self):\r\n        return self.name\r\n\r\n    def __repr__(self):\r\n        return '<git.Actor \"%s <%s>\">' % (self.name, self.email)\r\n\r\n    @classmethod\r\n    def from_string(cls, string):\r\n        \"\"\"\r\n        Create an Actor from a string.\r\n\r\n        ``str``\r\n            is the string, which is expected to be in regular git format\r\n\r\n        Format\r\n            John Doe <jdoe@example.com>\r\n\r\n        Returns\r\n            Actor\r\n        \"\"\"\r\n        if re.search(r'<.+>', string):\r\n            m = re.search(r'(.*) <(.+?)>', string)\r\n            name, email = m.groups()\r\n            return Actor(name, email)\r\n        else:\r\n            return Actor(string, None)\r\n"}
{"text": "# -*- coding: utf-8 -*-\n\n# Copyright (c) 2013 - 2015 CoNWeT Lab., Universidad Polit\u00e9cnica de Madrid\n\n# This file belongs to the business-charging-backend\n# of the Business API Ecosystem.\n\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom django.contrib.auth import logout as django_logout\nfrom django.http import HttpResponseRedirect\nfrom django.conf import settings\n\nfrom wstore.store_commons.utils.http import build_response\nfrom wstore.store_commons.utils.url import add_slash\n\n\nclass Http403(Exception):\n    pass\n\n\ndef logout(request):\n\n    django_logout(request)\n    response = None\n\n    if settings.PORTALINSTANCE:\n        # Check if the logout request is originated in a different domain\n        if 'HTTP_ORIGIN' in request.META:\n            origin = request.META['HTTP_ORIGIN']\n            origin = add_slash(origin)\n\n            from wstore.views import ACCOUNT_PORTAL_URL, CLOUD_PORTAL_URL, MASHUP_PORTAL_URL, DATA_PORTAL_URL\n\n            allowed_origins = [\n                add_slash(ACCOUNT_PORTAL_URL),\n                add_slash(CLOUD_PORTAL_URL),\n                add_slash(MASHUP_PORTAL_URL),\n                add_slash(DATA_PORTAL_URL)\n            ]\n\n            if origin in allowed_origins:\n                headers = {\n                    'Access-Control-Allow-Origin': origin,\n                    'Access-Control-Allow-Credentials': 'true'\n                }\n                response = build_response(request, 200, 'OK', headers=headers)\n            else:\n                response = build_response(request, 403, 'Forbidden')\n\n        else:\n            # If using the FI-LAB authentication and it is not a cross domain\n            # request redirect to the FI-LAB main page\n            response = build_response(request, 200, 'OK')\n\n    # If not using the FI-LAB authentication redirect to the login page\n    url = '/login?next=/'\n    response = HttpResponseRedirect(url)\n\n    return response\n"}
{"text": "import Tkinter as tk\n\nfrom itertools import cycle\nfrom Tkinter import *\nfrom PIL import Image, ImageTk # pip install pillow + sudo apt-get install python-imaging-tk\n\n# based on example found on \n# https://raspberrypi.stackexchange.com/questions/18261/how-do-i-display-an-image-file-png-in-a-simple-window\n\nclass SlideShow(tk.Frame):\n    \n    canvas = None\n    \n    current_image = 0\n    \n    stopShowing = False\n    \n    SLIDE_DURATION = 7500\n    NUMBER_OF_SLIDES = 1\n    \n    def __init__(self, parent, w, h):\n        tk.Frame.__init__(self, parent)\n        \n        # Set up the GUI window via Tk        \n        self.canvas = Canvas(self, background=\"black\", width=w, height=h)\n        self.canvas.pack(side=\"bottom\", fill=\"x\", padx=4)\n        \n        # pick an image file you have .bmp  .jpg  .gif.  .png\n        # load the file and covert it to a Tkinter image object\n        self.image1 = ImageTk.PhotoImage(Image.open('pictures/speelpong.jpg'))\n        \n        if self.NUMBER_OF_SLIDES >= 2:\n            self.image2 = ImageTk.PhotoImage(Image.open('pictures/ouderraad2.jpg'))\n        \n        if self.NUMBER_OF_SLIDES >= 3:  \n            self.image3 = ImageTk.PhotoImage(Image.open('pictures/ouderraad3.jpg'))\n\n        # make the root window the size of the image\n        #self.canvas.geometry(\"%dx%d+%d+%d\" % (w, h, 0, 0))\n\n        # root has no image argument, so use a label as a panel\n        self.panel1 = tk.Label(self.canvas, image=self.image1)\n        self.display = self.image1\n        self.panel1.pack(side=tk.TOP, fill=tk.BOTH, expand=tk.YES)\n        \n        print \"Display image1\"\n        \n        if self.NUMBER_OF_SLIDES > 1:\n            self.after(self.SLIDE_DURATION, self.update_image)\n        #self.root.mainloop()\n\n    def stop(self):\n        self.stopShowing = True\n        \n    def update_image(self):\n        if self.display == self.image1 and self.NUMBER_OF_SLIDES >= 2:\n            self.panel1.configure(image=self.image2)\n            print \"Display image2\"\n            self.display = self.image2\n        elif self.display == self.image2 and self.NUMBER_OF_SLIDES >= 3:\n            self.panel1.configure(image=self.image3)\n            print \"Display image3\"\n            self.display = self.image3\n        else:\n            self.panel1.configure(image=self.image1)\n            print \"Display image1\"\n            self.display = self.image1\n        \n        if self.stopShowing == False:\n            self.after(self.SLIDE_DURATION, self.update_image)       # Set to call again in 30 seconds"}
{"text": "from uuid import uuid4\nfrom rdflib import URIRef, RDF, RDFS\nfrom oldman.vocabulary import OLDM_CORRESPONDING_CLASS\n\n\nclass HydraSchemaAdapter(object):\n    \"\"\"Updates some Hydra patterns in the schema graph:\n\n          - hydra:Link: create a hydra:Class, subclass of the link range that support the same operations\n\n    \"\"\"\n\n    def update_schema_graph(self, graph):\n        graph = graph.skolemize()\n\n        graph = self._update_links(graph)\n\n        return graph\n\n    @staticmethod\n    def _update_links(graph):\n        links = list(graph.subjects(RDF.type, URIRef(u\"http://www.w3.org/ns/hydra/core#Link\")))\n\n        for link_property in links:\n            new_class_iri = URIRef(u\"http://localhost/.well-known/genid/link_class/%s\" % uuid4())\n            graph.add((new_class_iri, RDF.type, URIRef(u\"http://www.w3.org/ns/hydra/core#Class\")))\n            graph.add((link_property, URIRef(OLDM_CORRESPONDING_CLASS), new_class_iri))\n\n            # Ranges --> upper classes\n            ranges = list(graph.objects(link_property, RDFS.range))\n            for range in ranges:\n                graph.add((new_class_iri, RDFS.subClassOf, range))\n\n            # supported Operations\n            supported_operation_property = URIRef(u\"http://www.w3.org/ns/hydra/core#supportedOperation\")\n            operations = list(graph.objects(link_property, supported_operation_property))\n            for operation in operations:\n                graph.add((new_class_iri, supported_operation_property, operation))\n\n        return graph\n\n\n"}
{"text": "#import jsonlib2\nimport globalsObj\nimport logging\nimport traceback\nimport tornado.web\n#import ujson\n#import simplejson\nimport jsonpickle\nimport uuid\n\n\nclass Result(object):\n    def __init__(self, **kwargs):\n        #self.rootLogger = logging.getLogger('root')\n        for name, value in kwargs.items():\n            exec(\"self.\" + name + \" = value\")\n\n    def reload(self,**kwargs):\n        self.__init__(**kwargs)\n\nclass Error(object):\n    def __init__(self, **kwargs):\n        #self.rootLogger = logging.getLogger('root')\n        for name, value in kwargs.items():\n            exec(\"self.\" + name + \" = value\")\n\n    def setSection(self,section):\n        if globalsObj.errors_configuration.has_section(section):\n            errorsDict = dict(globalsObj.errors_configuration.items(section))\n            for key, val in enumerate(errorsDict.keys()):\n                exec(\"self.\" + val + \" = errorsDict[val]\")\n            #if self.code is not None:\n            #    self.code = int(self.code)\n            return True\n        else:\n            logging.getLogger(__name__).error(\"Error section %s not present\" % (section))\n            return False\n\n    def reload(self,**kwargs):\n        self.__init__(**kwargs)\n\n\nclass ResponseObj(object):\n    def __init__(self, ID = None, **kwargs):\n        #self.rootLogger = logging.getLogger('root')\n        self.apiVersion = globalsObj.configuration.get('version','version')\n        self.error = None\n        self.result = None\n        self.setID(ID)\n        self.error = Error(**kwargs)\n\n    def setResult(self, **kwargs):\n        self.result = Result(**kwargs)\n\n    def setError(self, section=None):\n        if section is not None:\n            if self.error.setSection(section):\n                return True\n            else:\n                return False\n\n    def setID(self, ID):\n        if ID is None or ID == \"\":\n            self.id = str(uuid.uuid4())\n        else:\n            self.id = ID\n\n    def jsonWrite(self):\n        try:\n            #jsonOut =  jsonlib2.write(self, default=lambda o: o.__dict__,sort_keys=False, indent=4,escape_slash=False)\n            jsonOut = jsonpickle.encode(self, unpicklable=False)\n            #jsonOut = ujson.dumps(self, ensure_ascii=False, indent=4)\n            #jsonOut2 = simplejson.dumps(pippo, ensure_ascii=False, indent=4)\n            return jsonOut\n        except BaseException as error:\n            logging.getLogger(__name__).error(\"Error on json encoding %s\" % (error.message))\n            return False\n\nclass RequestHandler(tornado.web.RequestHandler):\n\n    @property\n    def executor(self):\n        return self.application.executor\n\n    def compute_etag(self):\n        return None\n\n    def write_error(self, status_code, errorcode = '3', **kwargs):\n        self.set_header('Content-Type', 'application/json; charset=UTF-8')\n        self.set_status(status_code)\n\n        # debug info\n        if self.settings.get(\"serve_traceback\") and \"exc_info\" in kwargs:\n            debugTmp = \"\"\n            for line in traceback.format_exception(*kwargs[\"exc_info\"]):\n                debugTmp += line\n            getResponse = ResponseObj(debugMessage=debugTmp,httpcode=status_code,devMessage=self._reason)\n        else:\n            getResponse = ResponseObj(httpcode=status_code,devMessage=self._reason)\n\n        getResponse.setError(errorcode)\n        getResponse.setResult()\n\n        self.write(getResponse.jsonWrite())\n        self.finish()\n\nclass StaticFileHandler(tornado.web.StaticFileHandler):\n\n    def compute_etag(self):\n        return None\n\n    def write_error(self, status_code, errorcode = '3', **kwargs):\n        self.set_header('Content-Type', 'application/json; charset=UTF-8')\n        self.set_status(status_code)\n\n        # debug info\n        if self.settings.get(\"serve_traceback\") and \"exc_info\" in kwargs:\n            debugTmp = \"\"\n            for line in traceback.format_exception(*kwargs[\"exc_info\"]):\n                debugTmp += line\n            getResponse = ResponseObj(debugMessage=debugTmp,httpcode=status_code,devMessage=self._reason)\n        else:\n            getResponse = ResponseObj(httpcode=status_code,devMessage=self._reason)\n\n        getResponse.setError(errorcode)\n        getResponse.setResult()\n\n        self.write(getResponse.jsonWrite())\n        self.finish()\n\n"}
{"text": "import unittest\nfrom LibvirtQMFTest import *\n\n\nPROPERTIES = ('uuid', 'name', 'parentVolume', 'state', 'capacity', 'allocation',\n    'available', 'node')\n\n\nclass PoolTest(unittest.TestCase, LibvirtQMFTest):\n\n    @classmethod\n    def setUpClass(cls):\n        LibvirtQMFTest.setUpClass()\n        cls.cleanUp()\n\n    @classmethod\n    def tearDownClass(cls):\n        LibvirtQMFTest.tearDownClass()\n\n    def setUp(self):\n        self.node = self.getObjects(NODE)[0]\n        result = self.node.storagePoolDefineXML(getPoolXML())\n        pool_obj_id = result.outArgs['pool']\n        self.pool = self.getObject(pool_obj_id)\n\n    def tearDown(self):\n        if self.pool:\n            self.pool.destroy()\n            self.pool.delete()\n            self.pool.undefine()\n\n    def test_parentNode(self):\n        self.assertIn('node', (str(k) for k,v in self.pool.getProperties()))\n        self.assertEqual(self.pool.node, self.node.getObjectId())\n\n    def test_volumeCreate(self):\n        self.pool.build()\n        self.pool.create()\n        result = self.pool.createVolumeXML(getVolumeXML())\n        self.assertEqual(result.status, 0, result)\n        self.assertIn('volume', result.outArgs)\n        vol_obj_id = result.outArgs['volume']\n        self.assertTrue(vol_obj_id)\n        volume = self.getObject(vol_obj_id)\n        self.assertIsNotNone(volume)\n        volume.delete()\n\n    def test_getDescription(self):\n        result = self.pool.getXMLDesc()\n        self.assertIn('description', result.outArgs)\n        desc = result.outArgs['description']\n        nameXML = '<name>%s</name>' % POOL_PARAMS['name']\n        self.assertNotEqual(desc.find(nameXML), -1)\n\n    def test_refresh_error(self):\n        result = self.pool.refresh()\n        self.assertEqual(result.status, (1 << 16) | 55, result)\n        self.assertNotEqual(result.text.find('virStoragePoolRefresh'), -1)\n\n    def test_methods(self):\n        result = self.pool.build()\n        self.assertEqual(result.status, 0, result)\n        result = self.pool.create()\n        self.assertEqual(result.status, 0, result)\n\n        result = self.pool.refresh()\n        self.assertEqual(result.status, 0, result)\n\n        result = self.pool.destroy()\n        self.assertEqual(result.status, 0, result)\n        result = self.pool.delete()\n        self.assertEqual(result.status, 0, result)\n        result = self.pool.undefine()\n        self.assertEqual(result.status, 0, result)\n        self.pool = None\n\n    def test_properties(self):\n        properties = tuple(str(k) for k,v in self.pool.getProperties())\n        for p in PROPERTIES:\n            self.assertIn(p, properties,\n                          'Missing property \"%s\"' % p)\n\n\ndef suite():\n    return unittest.TestLoader().loadTestsFromTestCase(PoolTest)\n\n"}
{"text": "# -*- coding: utf-8 -*-\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom collections import OrderedDict\nfrom typing import Dict, Type\n\nfrom .base import ErrorGroupServiceTransport\nfrom .grpc import ErrorGroupServiceGrpcTransport\nfrom .grpc_asyncio import ErrorGroupServiceGrpcAsyncIOTransport\n\n\n# Compile a registry of transports.\n_transport_registry = OrderedDict()  # type: Dict[str, Type[ErrorGroupServiceTransport]]\n_transport_registry[\"grpc\"] = ErrorGroupServiceGrpcTransport\n_transport_registry[\"grpc_asyncio\"] = ErrorGroupServiceGrpcAsyncIOTransport\n\n__all__ = (\n    \"ErrorGroupServiceTransport\",\n    \"ErrorGroupServiceGrpcTransport\",\n    \"ErrorGroupServiceGrpcAsyncIOTransport\",\n)\n"}
{"text": "from django import forms\nfrom django.conf import settings\nfrom django.utils.encoding import force_unicode\nfrom django.utils.html import conditional_escape\nfrom django.utils.safestring import mark_safe\n\nfrom tower import ugettext as _\n\nfrom addons.models import Category\n\n\nclass IconWidgetRenderer(forms.RadioSelect.renderer):\n    \"\"\" Return radiobox as a list of images. \"\"\"\n\n    def render(self):\n        \"\"\" This will output radios as li>img+input. \"\"\"\n        output = []\n        for w in self:\n            value = w.choice_value\n            if value.split('/')[0] == 'icon' or value == '':\n                o = ((\"<li><a href='#' class='%s'>\"\n                      \"<img src='%simg/addon-icons/%s-32.png' alt=''>\"\n                      \"</a>%s</li>\") %\n                     ('active' if self.value == w.choice_value else '',\n                      settings.STATIC_URL, w.choice_label, w))\n            else:\n                o = \"<li class='hide'>%s</li>\" % w\n            output.append(o)\n        return mark_safe(u'\\n'.join(output))\n\n\nclass CategoriesSelectMultiple(forms.CheckboxSelectMultiple):\n    \"\"\"Widget that formats the Categories checkboxes.\"\"\"\n\n    def __init__(self, **kwargs):\n        super(self.__class__, self).__init__(**kwargs)\n\n    def render(self, name, value, attrs=None):\n        value = value or []\n        has_id = attrs and 'id' in attrs\n        final_attrs = self.build_attrs(attrs, name=name)\n\n        choices = []\n        other = None\n\n        miscs = Category.objects.filter(misc=True).values_list('id', flat=True)\n        for c in self.choices:\n            if c[0] in miscs:\n                other = (c[0],\n                         _(\"My add-on doesn't fit into any of the categories\"))\n            else:\n                choices.append(c)\n\n        choices = list(enumerate(choices))\n        choices_size = len(choices)\n\n        groups = [choices]\n        if other:\n            groups.append([(choices_size, other)])\n\n        str_values = set([force_unicode(v) for v in value])\n\n        output = []\n        for (k, group) in enumerate(groups):\n            cls = 'addon-misc-category' if k == 1 else 'addon-categories'\n            output.append(u'<ul class=\"%s checkbox-choices\">' % cls)\n\n            for i, (option_value, option_label) in group:\n                if has_id:\n                    final_attrs = dict(final_attrs, id='%s_%s' % (\n                            attrs['id'], i))\n                    label_for = u' for=\"%s\"' % final_attrs['id']\n                else:\n                    label_for = ''\n\n                cb = forms.CheckboxInput(\n                    final_attrs, check_test=lambda value: value in str_values)\n                option_value = force_unicode(option_value)\n                rendered_cb = cb.render(name, option_value)\n                option_label = conditional_escape(force_unicode(option_label))\n                output.append(u'<li><label%s>%s %s</label></li>' % (\n                        label_for, rendered_cb, option_label))\n\n            output.append(u'</ul>')\n\n        return mark_safe(u'\\n'.join(output))\n"}
{"text": "#\n# Copyright (C) 2013-2015 RoboIME\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\nfrom .goto import Goto\n\n\nclass GotoLooking(Goto):\n    def __init__(self, robot, lookpoint=None, **kwargs):\n        \"\"\"\n        lookpoint: Where you want it to look, what were you expecting?\n        \"\"\"\n        super(GotoLooking, self).__init__(robot, **kwargs)\n        self._lookpoint = lookpoint\n\n    @property\n    def lookpoint(self):\n        if callable(self._lookpoint):\n            return self._lookpoint()\n        else:\n            return self._lookpoint\n\n    @lookpoint.setter\n    def lookpoint(self, value):\n        self._lookpoint = value\n\n    def _step(self):\n        #print self.lookpoint\n        self.angle = self.robot.angle_to_point(self.lookpoint)\n        super(GotoLooking, self)._step()\n"}
{"text": "import json, sys, re, hashlib, smtplib, base64, urllib, os\n\nfrom auth import *\nfrom core.account import manager\nfrom django.http import *\nfrom django.shortcuts import render_to_response\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.core.context_processors import csrf\nfrom django.core.validators import email_re\nfrom django.db.utils import IntegrityError\nfrom django.utils.http import urlquote_plus\n\n'''\n@author: Anant Bhardwaj\n@date: Mar 21, 2013\n\nDatahub Web Handler\n'''\n\n@login_required\ndef user(request, username=None):\n\ttry:\n\t\tif(username):\n\t\t\tres = manager.list_databases(username)\n\t\t\treturn render_to_response(\"user.html\", {'username': username, 'db_names':res['db_names']})\n\t\telse:\n\t\t\tuser = request.session[kLogIn]\n\t\t\treturn HttpResponseRedirect(user)\n\texcept KeyError:\n\t\treturn HttpResponseRedirect('/login')\n\n\ndef new_database_form(request, username):\n\treturn render_to_response(\"new_database.html\", {'username': username})\n\n@login_required\ndef new_database(request, username, db_name):\n\tmanager.create_database(username, db_name)\n\treturn HttpResponseRedirect(\"/\"+username)\n\n@login_required\ndef database(request, username, db_name):\n\ttry:\n\t\tres = manager.list_tables(db_name)\n\t\treturn render_to_response(\"database.html\", {'username': username, 'db_name':db_name, 'table_names':res['table_names']})\n\texcept Exception, e:\n\t\treturn HttpResponse(request_error, mimetype=\"application/json\")\n\n@login_required\ndef table(request, username, db_name, table_name):\n\ttry:\n\t\treturn render_to_response(\"table.html\", {'username': username, 'db_name':db_name, 'table_name':table_name})\n\texcept Exception, e:\n\t\treturn HttpResponse(request_error, mimetype=\"application/json\")\n\n\n\n"}
{"text": "from __future__ import print_function\n\nimport requests\nimport os, sys\nimport subprocess\nimport json\nimport ipdb\n\nfrom Bio import Entrez\nfrom Bio import Medline\n\nclass PubmedGetter(object):\n    def __init__(self, pmid):\n        self.authors = []\n        self.title = \"\"\n        self.journal = \"\"\n        self.pmid = pmid\n        self.citation = \"\"\n        self.medline = None\n        self.date_pub = None\n    \n    def search_by_pmid(self):\n        self.title\n        text = subprocess.check_output(\n            \"efetch -db pubmed -id {0} -format MEDLINE\".format(self.pmid),\n            shell=True\n        )\n        self.parse_medline(text)\n\n    def parse_medline(self, text):\n        self.medline = Medline.read(text.split('\\n'))\n        self.title = self.medline['TI']\n        self.journal = self.medline['JT']\n        self.citation = self.medline['SO']\n        self.date_pub = self.medline['DP']\n        try:\n            self.authors = self.medline['AU']\n        except KeyError:\n            self.authors = self.medline['IR']\n\n    def dump_json(self):\n        return {\n            \"pmid\": self.pmid,\n            \"title\": self.title,\n            \"authors\": self.authors,\n            \"journal\": self.journal,\n            \"citation\": self.citation\n        }\n\ndef run_pubmed_getter(pmid):\n    p = PubmedGetter(pmid)\n    p.search_by_pmid()\n    return p.dump_json()\n\ndata = None\nif __name__==\"__main__\":\n    with open('../assets/data/genomes.json', 'r') as fp:\n        data = json.load(fp)\n    for d in data:\n        d['literature_reference'] = run_pubmed_getter(d['literature_reference']['pmid'])"}
{"text": "#!/home/pi/.virtualenvs/sunlight_rfid_doorman/bin/python\n\nimport pickle\nimport datetime\nimport time\nimport itertools\nfrom functools import wraps\nimport errno\nimport os\nimport signal\n\nimport redis\nimport gspread\n\nfrom settings import *\n\nACL_KEY = 'sunlight-doorman-acl'\nLOG_KEY = 'sunlight-doorman-log'\n\nclass TimeoutError(Exception):\n    pass\n\ndef timeout(seconds=10, error_message=os.strerror(errno.ETIME)):\n    def decorator(func):\n        def _handle_timeout(signum, frame):\n            raise TimeoutError(error_message)\n\n        def wrapper(*args, **kwargs):\n            signal.signal(signal.SIGALRM, _handle_timeout)\n            signal.alarm(seconds)\n            try:\n                result = func(*args, **kwargs)\n            finally:\n                signal.alarm(0)\n            return result\n\n        return wraps(func)(wrapper)\n\n    return decorator\n\n@timeout(10)\ndef refresh_access_control_list(gc=None):\t\n\tif gc is None:\n\t\tgc = gspread.login(SPREADSHEET_USER, SPREADSHEET_PASSWORD)\n\tsh = gc.open(SPREADSHEET_NAME)\t\n\tworksheet = sh.worksheet(SPREADSHEET_WORKSHEET)\n\tkey_cells = worksheet.col_values(1)\n\temail_cells = worksheet.col_values(2)\n\tactive_cells = worksheet.col_values(3)\n\t\n\tacl = {}\n\n\tfor (i, (key, email, active)) in enumerate(itertools.izip(key_cells, email_cells, active_cells)):\n\t\tif i==0:\n\t\t\tcontinue\n\t\tif active.upper().strip()=='Y':\n\t\t\tacl[key.strip()] = email.strip()\n\n\tr = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, db=0)\n\tr.set(ACL_KEY, pickle.dumps(acl))\n\n\ndef get_access_control_list():\n\tr = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, db=0)\n\tif not r.exists(ACL_KEY):\n\t\treturn False\n\telse:\n\t\treturn pickle.loads(r.get(ACL_KEY))\n\ndef log(status):\n\tr = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, db=0)\n\tr.rpush(LOG_KEY, pickle.dumps(status))\n\ndef _log_worksheet_name(timestamp):\n\tdt = datetime.datetime.fromtimestamp(float(timestamp))\n\treturn 'log - %d/%d' % (dt.month, dt.year)\n\t\n@timeout(30)\ndef store_log(gc=None):\n\tif gc is None:\n\t\tgc = gspread.login(SPREADSHEET_USER, SPREADSHEET_PASSWORD)\n\n\t# open spreadsheet\n\tss = gc.open(SPREADSHEET_NAME)\n\n\t# load log entries out of redis\n\tlog_items = []\n\tr = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, db=0)\n\twhile True:\n\t\tlog_item = r.lpop(LOG_KEY)\n\t\tif log_item is None:\n\t\t\tbreak\n\t\tlog_items.append(pickle.loads(log_item))\n\n\t# assemble data by month\n\tlog_worksheets = {}\n\tfor l in log_items:\n\t\ttimestamp = l[0]\n\t\tworksheet_name = _log_worksheet_name(timestamp)\n\t\tif not log_worksheets.has_key(worksheet_name):\n\t\t\tlog_worksheets[worksheet_name] = {'log_items': []}\n\t\tlog_worksheets[worksheet_name]['log_items'].append(l)\n\n\t# store log entries\n\tfor lw in log_worksheets:\n\t\t\n\t\t# create monthly worksheets as necessary\n\t\ttry:\n\t\t\tws = ss.worksheet(lw)\n\t\t\tws_offset = len(ws.col_values(1)) + 1\n\t\texcept:\n\t\t\tws = ss.add_worksheet(title=lw, rows=\"10000\", cols=\"3\")\n\t\t\tws_offset = 1\n\n\t\t# store log items\n\t\tcell_list = ws.range('A%(begin)d:C%(end)d' % {'begin': ws_offset, 'end': ws_offset + len(log_worksheets[lw]['log_items']) - 1})\n\t\tfor (i, log_item) in enumerate(log_worksheets[lw]['log_items']):\n\t\t\tcell_list[(3*i) + 0].value = datetime.datetime.fromtimestamp(float(log_item[0])).isoformat()\n\t\t\tcell_list[(3*i) + 1].value = log_item[1]\n\t\t\tcell_list[(3*i) + 2].value = log_item[2]\n\t\tws.update_cells(cell_list)\n\n\nif __name__ == '__main__':\n\tgc = gspread.login(SPREADSHEET_USER, SPREADSHEET_PASSWORD)\n\t\n\trefresh_access_control_list(gc)\n\tstore_log(gc)\n"}
{"text": "#\n# Copyright (c) 2013 Red Hat, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#           http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n\nclass DFSAState(object):\n    DISCONNECTING, DISCONNECTED, CONNECTING, \\\n    CONNECTED, UNAUTHORIZED, EXITING, COMMUNICATION_ERROR = range(7)\n\n    def __init__(self, Type):\n        self.value = Type\n\n    def __str__(self):\n        if self.value == DFSAState.DISCONNECTED:\n            return 'DISCONNECTED'\n        if self.value == DFSAState.CONNECTED:\n            return 'CONNECTED'\n        if self.value == DFSAState.UNAUTHORIZED:\n            return 'UNAUTHORIZED'\n        if self.value == DFSAState.EXITING:\n            return 'EXITING'\n        if self.value == DFSAState.DISCONNECTING:\n            return 'DISCONNECTING'\n        if self.value == DFSAState.CONNECTING:\n            return 'CONNECTING'\n        if self.value == DFSAState.COMMUNICATION_ERROR:\n            return 'COMMUNICATION_ERROR'\n\n    def __eq__(self, y):\n        return self.value == y.value\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"\nSimple example using LSTM recurrent neural network to classify IMDB\nsentiment dataset.\n\nReferences:\n    - Long Short Term Memory, Sepp Hochreiter & Jurgen Schmidhuber, Neural\n    Computation 9(8): 1735-1780, 1997.\n    - Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,\n    and Christopher Potts. (2011). Learning Word Vectors for Sentiment\n    Analysis. The 49th Annual Meeting of the Association for Computational\n    Linguistics (ACL 2011).\n\nLinks:\n    - http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n    - http://ai.stanford.edu/~amaas/data/sentiment/\n\n\"\"\"\nfrom __future__ import division, print_function, absolute_import\n\nimport tflearn\nfrom tflearn.data_utils import to_categorical, pad_sequences\nfrom tflearn.datasets import imdb\n\n# IMDB Dataset loading\ntrain, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,\n                                valid_portion=0.99)\ntrainX, trainY = train\n#testX, testY = test\ntestX, testY = train\n# Data preprocessing\n# Sequence padding\ntrainX = pad_sequences(trainX, maxlen=100, value=0.)\ntestX = pad_sequences(testX, maxlen=100, value=0.)\n# Converting labels to binary vectors\ntrainY = to_categorical(trainY, nb_classes=2)\ntestY = to_categorical(testY, nb_classes=2)\n\n# Network building\nnet = tflearn.input_data([None, 100])\nnet = tflearn.embedding(net, input_dim=10000, output_dim=128)\nnet = tflearn.lstm(net, 128, dropout=0.8)\nnet = tflearn.fully_connected(net, 2, activation='softmax')\nnet = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n                         loss='categorical_crossentropy')\n\n# Training\nmodel = tflearn.DNN(net, tensorboard_verbose=0)\nmodel.fit(trainX, trainY, n_epoch=1,validation_set=(testX, testY), show_metric=True,\n          batch_size=32)\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, unicode_literals\n\nfrom django import forms\nfrom django.contrib import admin\nfrom django.contrib.auth.admin import UserAdmin as AuthUserAdmin\nfrom django.contrib.auth.forms import UserChangeForm, UserCreationForm\n\nfrom .models import User\n\n\nclass MyUserChangeForm(UserChangeForm):\n    class Meta(UserChangeForm.Meta):\n        model = User\n\n\nclass MyUserCreationForm(UserCreationForm):\n\n    error_message = UserCreationForm.error_messages.update(\n        {\n            'duplicate_username': 'This username has already been taken.'\n        }\n    )\n\n    class Meta(UserCreationForm.Meta):\n        model = User\n\n    def clean_username(self):\n        username = self.cleaned_data['username']\n        try:\n            User.objects.get(username=username)\n        except User.DoesNotExist:\n            return username\n        raise forms.ValidationError(self.error_messages['duplicate_username'])\n\n\n@admin.register(User)\nclass UserAdmin(AuthUserAdmin):\n    form = MyUserChangeForm\n    add_form = MyUserCreationForm\n"}
{"text": "# SPDX-License-Identifier: AGPL-3.0-or-later\n\"\"\"\n FramaLibre (It)\n\"\"\"\n\nfrom html import escape\nfrom urllib.parse import urljoin, urlencode\nfrom lxml import html\nfrom searx.utils import extract_text\n\n# about\nabout = {\n    \"website\": 'https://framalibre.org/',\n    \"wikidata_id\": 'Q30213882',\n    \"official_api_documentation\": None,\n    \"use_official_api\": False,\n    \"require_api_key\": False,\n    \"results\": 'HTML',\n}\n\n# engine dependent config\ncategories = ['it']\npaging = True\n\n# search-url\nbase_url = 'https://framalibre.org/'\nsearch_url = base_url + 'recherche-par-crit-res?{query}&page={offset}'\n\n# specific xpath variables\nresults_xpath = '//div[@class=\"nodes-list-row\"]/div[contains(@typeof,\"sioc:Item\")]'\nlink_xpath = './/h3[@class=\"node-title\"]/a[@href]'\nthumbnail_xpath = './/img[@class=\"media-object img-responsive\"]/@src'\ncontent_xpath = './/div[@class=\"content\"]//p'\n\n\n# do search-request\ndef request(query, params):\n    offset = (params['pageno'] - 1)\n    params['url'] = search_url.format(query=urlencode({'keys': query}),\n                                      offset=offset)\n\n    return params\n\n\n# get response from search-request\ndef response(resp):\n    results = []\n\n    dom = html.fromstring(resp.text)\n\n    # parse results\n    for result in dom.xpath(results_xpath):\n        link = result.xpath(link_xpath)[0]\n        href = urljoin(base_url, link.attrib.get('href'))\n        # there's also a span (class=\"rdf-meta element-hidden\" property=\"dc:title\")'s content property for this...\n        title = escape(extract_text(link))\n        thumbnail_tags = result.xpath(thumbnail_xpath)\n        thumbnail = None\n        if len(thumbnail_tags) > 0:\n            thumbnail = extract_text(thumbnail_tags[0])\n            if thumbnail[0] == '/':\n                thumbnail = base_url + thumbnail\n        content = escape(extract_text(result.xpath(content_xpath)))\n\n        # append result\n        results.append({'url': href,\n                        'title': title,\n                        'img_src': thumbnail,\n                        'content': content})\n\n    # return results\n    return results\n"}
{"text": "import os\nfrom setuptools import setup\n\nwith open(os.path.join(os.path.dirname(__file__), 'README.md')) as readme:\n    README = readme.read()\n\n# allow setup.py to be run from any path\nos.chdir(os.path.normpath(os.path.join(os.path.abspath(__file__), os.pardir)))\n\nsetup(\n    name='django-form-validation',\n    version='0.1',\n    packages=['form_validation'],\n    include_package_data=True,\n    license='MIT License',  # example license\n    description=\"A simple Django app to handle form's client validation .\",\n    long_description=README,\n    url='https://github.com/kiahosseini/django-form-validation',\n    author='Kiarash Hosseini',\n    author_email='kia.hosseini7@gmail.com',\n    classifiers=[\n        'Environment :: Web Environment',\n        'Framework :: Django',\n        'Intended Audience :: Developers',\n        'License :: MIT License', # example license\n        'Operating System :: OS Independent',\n        'Programming Language :: Python',\n        # Replace these appropriately if you are stuck on Python 2.\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 2.6',\n        'Topic :: Internet :: WWW/HTTP',\n        'Topic :: Internet :: WWW/HTTP :: Dynamic Content',\n    ],\n)\n"}
{"text": "\"\"\"adam.adam_1\n\"\"\"\n\n# pylint: disable=C0111\nimport operator\n\nfrom math import *  # noqa: F401,F403\n\n# Add mutation points for comparison operators.\n\n\ndef constant_number():\n    return 42\n\n\ndef constant_true():\n    return True\n\n\ndef constant_false():\n    return False\n\n\ndef bool_and():\n    return object() and None\n\n\ndef bool_or():\n    return object() or None\n\n\ndef bool_expr_with_not():\n    return not object()\n\n\ndef bool_if():\n    if object():\n        return True\n\n    raise Exception(\"bool_if() failed\")\n\n\ndef if_expression():\n    return True if object() else None\n\n\ndef assert_in_func():\n    assert object()\n    return True\n\n\ndef unary_sub():\n    return -1\n\n\ndef unary_add():\n    return +1\n\n\ndef binary_add():\n    return 5 + 6\n\n\ndef equals(vals):\n    def constraint(x, y):\n        return operator.xor(x == y, x != y)\n\n    return all([constraint(x, y) for x in vals for y in vals])\n\n\ndef use_break(limit):\n    for x in range(limit):\n        break\n    return x\n\n\ndef use_continue(limit):\n    for x in range(limit):\n        continue\n    return x\n\n\ndef use_star_args(*args):\n    pass\n\n\ndef use_extended_call_syntax(x):\n    use_star_args(*x)\n\n\ndef use_star_expr(x):\n    a, *b = x\n"}
{"text": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n_author_            =   \"Lars van der Voorden\"\n_version_           =   \"1.0\"\n_date_              =   \"03/02/2015\"                #dd/mm/yyyy\n\nimport urllib2                                      #urllib2 for downloading measurement table\nfrom HTMLParser import HTMLParser                   #for parsing measurement table\n\nclass Steca():\n\n    #The IP-adres of the Steca inverter (webserver)\n    ip_adres = \"\"\n\n    def __init__(self, ip_adres):\n        self.ip_adres = ip_adres\n\n  \n    def getMeasurementTable(self):\n        steca_webserver_request = urllib2.Request('http://%s/gen.measurements.table.js' \n                                    % self.ip_adres )\n        steca_webserver_respone = urllib2.urlopen(steca_webserver_request)\n        steca_webserver_html = steca_webserver_respone.read()[16:-3]        #cut off the Javascript\n        \n        #Create a new parser and feed it\n        steca_parser = StecaParser()\n        steca_parser.feed(steca_webserver_html)\n\n        #Return the parsed dict\n        return steca_parser.getParsedData()\n\n    def getEnergyGenerated(self):\n        steca_webserver_energy_request = urllib2.Request('http://%s/gen.yield.day.chart.js' \n                                            % self.ip_adres )\n        steca_webserver_energy_respone = urllib2.urlopen(steca_webserver_energy_request)\n        steca_webserver_energy_html = steca_webserver_energy_respone.read()\n\n        kWhLocation = steca_webserver_energy_html.index('kWh')\n\n        energy_generated = steca_webserver_energy_html[(kWhLocation-6):kWhLocation].replace(\" \", \"\")\n\n        return int(float(energy_generated)*1000)\n\n\n\n\n\n\n#Class for parsing the Steca HTML-page to an dict\nclass StecaParser(HTMLParser):\n    \n    temp_steca_data     =   []\n\n    def handle_data(self, data):\n       self.temp_steca_data.append(data)\n\n\n    def parseToDict(self):\n        parsed_steca_data   =   {}\n        \n        if self.temp_steca_data:   \n\n            #Start by 4. The first (0,1,2,3) items are headers, no data         \n            i = 4\n\n            #place all the data from the measurement table into an dict\n            while i < len(self.temp_steca_data):\n                parsed_steca_data[self.temp_steca_data[i]] = self.temp_steca_data[(i+1)]\n                i += 3\n        else:\n            print \"No data from Steca inverter\"\n        return parsed_steca_data\n\n\n    def getParsedData(self):\n        data_dict = self.parseToDict()\n        return data_dict\n\n"}
{"text": "import logging\nfrom wellspring.models import VestSection, VestSubSection\n\nLOGGER = logging.getLogger(__name__)\n\nVEST_SECTIONS = {\n                    \"EQUILIBRIUM\" : [\"SCHOOL\", \"SELF\", \"HOME\", \"WORK\"],\n                    \"SUPPORT\" : [\"PROFESSIONALS\", \"FAMILY\", \"FRIENDS\", \"COLLEAGUES\"],\n                    \"LIFESTYLE\" : [\"DIET\", \"EXERCISE\", \"MEDITATION\", \"RECREATION\"]\n                    }\n\ndef add_section(name):\n    LOGGER.debug(\"Adding VestSection: \" + name)\n    result = VestSection(section_name=name)\n    result.save()\n    return result\n    \ndef add_subsection(section_name, subsection_name):\n    LOGGER.debug(\"Adding VestSubSection: \" + section_name + \":\" + subsection_name)\n    vest_section = get_by_name_vest_section(section_name)\n    result = VestSubSection(vest_section=vest_section, subsection_name=subsection_name)\n    result.save()\n    return result\n\ndef get_all_vest_section():\n    LOGGER.debug(\"Getting all VestSections\")\n    return list(VestSection.objects.all())\n\ndef get_all_vest_subsection():\n    LOGGER.debug(\"Getting all VestSubSections\")\n    return list(VestSubSection.objects.all())\n\ndef get_by_name_vest_section(name):\n    LOGGER.debug(\"Getting VestSection by name: \" + name)\n    return VestSection.objects.get(section_name = name)\n\ndef get_by_name_vest_subsection(name):\n    LOGGER.debug(\"Getting VestSubSection by name: \" + name)\n    return VestSubSection.objects.get(subsection_name = name)"}
{"text": "# uncompyle6 version 2.9.10\n# Python bytecode 2.7 (62211)\n# Decompiled from: Python 3.6.0b2 (default, Oct 11 2016, 05:27:10) \n# [GCC 6.2.0 20161005]\n# Embedded file name: Mcl_Cmd_Banner_Tasking.py\n\n\ndef TaskingMain(namespace):\n    import mcl.imports\n    import mcl.target\n    import mcl.tasking\n    from mcl.object.Message import MarshalMessage\n    mcl.imports.ImportWithNamespace(namespace, 'mca.network.cmd.banner', globals())\n    mcl.imports.ImportWithNamespace(namespace, 'mca.network.cmd.banner.tasking', globals())\n    lpParams = mcl.tasking.GetParameters()\n    tgtParams = mca.network.cmd.banner.Params()\n    tgtParams.targetAddr = lpParams['targetAddress']\n    tgtParams.broadcast = lpParams['broadcast']\n    tgtParams.wait = lpParams['wait']\n    tgtParams.dstPort = lpParams['dstPort']\n    tgtParams.srcPort = lpParams['srcPort']\n    if lpParams['protocol'] == 1:\n        protocol = 'TCP'\n        tgtParams.socketType = mca.network.cmd.banner.SOCKET_TYPE_TCP\n    elif lpParams['protocol'] == 2:\n        protocol = 'UDP'\n        tgtParams.socketType = mca.network.cmd.banner.SOCKET_TYPE_UDP\n    elif lpParams['protocol'] == 3:\n        protocol = 'ICMP'\n        tgtParams.socketType = mca.network.cmd.banner.SOCKET_TYPE_ICMP\n    else:\n        mcl.tasking.OutputError('Invalid protocol type (%u)' % lpParams['protocol'])\n        return False\n    if tgtParams.dstPort == 0 and tgtParams.socketType != mca.network.cmd.banner.SOCKET_TYPE_ICMP:\n        mcl.tasking.OutputError('A port must be specified for this type of connection')\n        return False\n    else:\n        if lpParams['...'] != None:\n            if not _bufferScrubber(lpParams['...'], tgtParams.data):\n                mcl.tasking.OutputError('Invalid send buffer')\n                return False\n        taskXml = mcl.tasking.Tasking()\n        taskXml.SetTargetRemote('%s' % tgtParams.targetAddr)\n        taskXml.SetType(protocol)\n        if tgtParams.dstPort != 0:\n            taskXml.AddSearchMask('%u' % tgtParams.dstPort)\n        mcl.tasking.OutputXml(taskXml.GetXmlObject())\n        rpc = mca.network.cmd.banner.tasking.RPC_INFO_BANNER\n        msg = MarshalMessage()\n        tgtParams.Marshal(msg)\n        rpc.SetData(msg.Serialize())\n        rpc.SetMessagingType('message')\n        res = mcl.tasking.RpcPerformCall(rpc)\n        if res != mcl.target.CALL_SUCCEEDED:\n            mcl.tasking.RecordModuleError(res, 0, mca.network.cmd.banner.errorStrings)\n            return False\n        return True\n\n\ndef _bufferScrubber(input, data):\n    i = 0\n    while i < len(input):\n        try:\n            if input[i] != '\\\\':\n                charToAdd = ord(input[i])\n            else:\n                if input[i + 1] == 'a':\n                    charToAdd = ord('\\x07')\n                elif input[i + 1] == 'b':\n                    charToAdd = ord('\\x08')\n                elif input[i + 1] == 'f':\n                    charToAdd = ord('\\x0c')\n                elif input[i + 1] == 'n':\n                    charToAdd = ord('\\n')\n                elif input[i + 1] == 'r':\n                    charToAdd = ord('\\r')\n                elif input[i + 1] == 't':\n                    charToAdd = ord('\\t')\n                elif input[i + 1] == 'v':\n                    charToAdd = ord('\\x0b')\n                elif input[i + 1] == '?':\n                    charToAdd = ord('\\\\?')\n                elif input[i + 1] == \"'\":\n                    charToAdd = ord(\"'\")\n                elif input[i + 1] == '\"':\n                    charToAdd = ord('\"')\n                elif input[i + 1] == '\\\\':\n                    charToAdd = ord('\\\\')\n                elif input[i + 1] == '0' or input[i + 1] == '1' or input[i + 1] == '2' or input[i + 1] == '3':\n                    sum = 0\n                    j = i + 1\n                    while j <= i + 3:\n                        if j >= len(input):\n                            return False\n                        charval = ord(input[j]) - ord('0')\n                        if charval >= 0 and charval <= 7:\n                            sum = 8 * sum + charval\n                        else:\n                            return False\n                        j = j + 1\n\n                    charToAdd = sum\n                    i = i + 2\n                elif input[i + 1] == 'X' or input[i + 1] == 'x':\n                    sum = 0\n                    i = i + 2\n                    j = i\n                    while j <= i + 1:\n                        if j >= len(input):\n                            return False\n                        charval = ord(input[j].upper()) - ord('0')\n                        if charval >= 0 and charval <= 9:\n                            sum = 16 * sum + charval\n                        elif charval + ord('0') >= ord('A') and charval + ord('0') <= ord('F'):\n                            sum = 16 * sum + charval - 7\n                        else:\n                            return False\n                        charToAdd = sum\n                        j = j + 1\n\n                else:\n                    return False\n                i = i + 1\n            data.append(charToAdd)\n        finally:\n            i = i + 1\n\n    return True\n\n\nif __name__ == '__main__':\n    import sys\n    if TaskingMain(sys.argv[1]) != True:\n        sys.exit(-1)"}
{"text": "import sys\nimport os\nimport datetime\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\nfrom pprint import pprint\nimport sqlite3\nimport calendar\nfrom datetime import datetime\nfrom datetime import timedelta\nimport math\nimport numpy.polynomial.polynomial as poly\n\n\n\n\n#mass fg, pk_ht, UNCORR\nAL_HG_incand_calib = [\n[0.23173,25.17577  ],\n[0.41398,48.99595  ],\n[1.26106,186.48122 ],\n[2.88282,489.41296 ],\n[5.43241,880.95554 ],\n[8.94784,1347.39537],\n]\n\n\nHG_pkht = np.array([row[1] for row in AL_HG_incand_calib])\nHG_mass = np.array([row[0] for row in AL_HG_incand_calib])\nHG_mass_corr = np.array([row[0]/0.7 for row in AL_HG_incand_calib])\nHG_fit = poly.polyfit(HG_pkht, HG_mass_corr, 1)\t\nprint 'HG fit', HG_fit\n\n\nfor line in AL_HG_incand_calib:\n\t\n\tincand_pk_ht = line[1]\n\tuncorr_mass_fg = line[0]\n\tAD_corr_fit = HG_fit[0] + HG_fit[1]*incand_pk_ht \n\t\n\tline.append(AD_corr_fit)\n\n\t\nHG_pk_ht = [row[1] for row in AL_HG_incand_calib]\nHG_uncorr_mass = [row[0] for row in AL_HG_incand_calib]\nHG_uncorr_fit = [row[2]*0.7 for row in AL_HG_incand_calib]\nHG_ADcorr_fit = [row[2] for row in AL_HG_incand_calib]\n\n\nfig = plt.figure(figsize=(12,10))\nax = fig.add_subplot(111)\nax.scatter(HG_pk_ht,HG_uncorr_mass,color='r', label = 'Uncorrected calibration')\nax.plot(HG_pk_ht,HG_ADcorr_fit, '--r', label = 'Aquadag correction applied')\nax.plot(HG_pk_ht,HG_uncorr_fit, '-r')\nplt.xlabel('Incandescent pk height (a.u.)')\nplt.ylabel('rBC mass (fg)')\nplt.text(250,10, 'Aquadag corrected fit:\\nrBC mass = -0.017584 + 9.2453E-3*pkht')\nax.set_ylim(0,14)\nax.set_xlim(0,2000)\n\nplt.legend()\nos.chdir('C:/Users/Sarah Hanna/Documents/Data/Alert Data/SP2 Calibrations/')\nplt.savefig('Alert SP2#17 Aquadag calibration curves.png', bbox_inches='tight')\n\nplt.show()"}
{"text": "#!/usr/bin/env python\n\nimport re\nfrom setuptools import setup, find_packages\n\ndef read_version():\n    with open(\"pandas_td/version.py\") as f:\n        m = re.match(r'__version__ = \"([^\\\"]*)\"', f.read())\n        return m.group(1)\n\nrequires = [\n    \"certifi\",\n    \"pytz\",\n    \"tzlocal\",\n    \"pandas>=0.16.0\",\n    \"requests>=2.21.0\",\n    \"td-client>=0.4.0\",\n]\n\nsetup(\n    name=\"pandas-td\",\n    version=read_version(),\n    description=\"Pandas extension for Treasure Data\",\n    author=\"Treasure Data, Inc.\",\n    author_email=\"support@treasure-data.com\",\n    url=\"https://github.com/treasure-data/pandas-td\",\n    install_requires=requires,\n    extras_require={\n        \"testing\": [\"pytest>=3.6\", \"pytest-cov\"],\n        \"dev\": [\"black==19.3b0\", \"isort\"],\n    },\n    packages=find_packages(),\n    license=\"Apache License 2.0\",\n    platforms=\"Posix; MacOS X; Windows\",\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Environment :: Console\",\n        \"Framework :: IPython\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Topic :: Software Development\",\n    ],\n)\n"}
{"text": "from circuits import Event\n\nclass ReceiveInput(Event):\n    \"\"\" Some input arrived (fieldcode or other input) \"\"\"\n\nclass StartGame(Event):\n    \"\"\" Start a new game. \"\"\"\n\nclass GameInitialized(Event):\n    \"\"\" A new game was started. \"\"\"\n\nclass DartStuck(Event):\n    \"\"\" A dart is stuck. May be fired many times in a row. \"\"\"\n\nclass SkipPlayer(Event):\n    \"\"\" The button for skipping a player was pressed. \"\"\"\n\nclass CodeNotImplemented(Event):\n    \"\"\" This part of the code is not implemented. \"\"\"\n\nclass Hit(Event):\n    \"\"\" A dart hit the board. \"\"\"\n\nclass HitBust(Event):\n    \"\"\" A dart hit the board, but busted. \"\"\"\n\nclass HitWinner(Event):\n    \"\"\" A dart hit the board, and the player won. \"\"\"\n\nclass EnterHold(Event):\n    \"\"\" Wait for the player to hit start. \"\"\"\n\nclass LeaveHold(Event):\n    \"\"\" Player has pressed start to continue. \"\"\"\n\nclass FrameStarted(Event):\n    \"\"\" A new frame was started. \"\"\"\n\nclass FrameFinished(Event):\n    \"\"\" A player has thrown three darts (or the round was skipped, etc.) \"\"\"\n\nclass GameOver(Event):\n    \"\"\" The game is over. \"\"\"\n\nclass ManualNextPlayer(Event):\n    \"\"\" Manual request to advance to next player \"\"\"\n\nclass ChangeLastRound(Event):\n    \"\"\" Change the history (last round) of the player. \"\"\"\n\nclass GameStateChanged(Event):\n    \"\"\" Something (general) in the player's history changed. \"\"\"\n\nclass UpdateSettings(Event):\n    \"\"\" Set come config \"\"\"\n\nclass SettingsChanged(Event):\n    \"\"\" Something in the config has changed, new config is attached. \"\"\"\n\nclass ErrorMessage(Event):\n    \"\"\" Some general error occured. \"\"\"\n\nclass PerformSelfUpdate(Event):\n    \"\"\" Command to update the running python file. \"\"\"\n\nclass UndoLastFrame(Event):\n    \"\"\" Remove a player's last frame from the history. \"\"\"\n\nclass UpdatePlayers(Event):\n    \"\"\" Change list of current players. \"\"\"\n"}
{"text": "import glob\nimport json\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport torch as T\n\n\ndef flat_items(d, prefix):\n    \"\"\"Recurses through a dict-of-dicts yielding ((key_0..key_n), value).\"\"\"\n    if isinstance(d, dict):\n        for k, v in d.items():\n            yield from flat_items(v, prefix + (k,))\n    else:\n        yield prefix, d\n\n\nclass Log:\n    def __init__(self, path, header):\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        self._log = open(path, 'w')\n        self.write(header, flush=True)\n\n    def write(self, item, flush=False):\n        self._log.write(f'{json.dumps(item)}\\n')\n        if flush:\n            self._log.flush()\n\n    @staticmethod\n    def load(path):\n        def flat_dict(d):\n            return {'_'.join(k): v for k, v in flat_items(d, ())}\n        with open(path, 'r') as f:\n            header = flat_dict(json.loads(next(f)))\n            df = pd.DataFrame.from_dict(flat_dict(json.loads(line)) for line in f)\n            return df.assign(**header)\n\n    @classmethod\n    def load_dir(cls, path):\n        return pd.concat([cls.load(f) for f in glob.glob(f'{path}/*')], sort=False).reset_index()\n\n\ndef sample_multinomial(probs):\n    \"\"\"Generate a multinomial sample for each row of `probs`.\"\"\"\n    samples = T.rand(probs.shape[0], device=probs.device)\n    indices = (T.cumsum(probs, -1) < samples[:, np.newaxis]).sum(-1)\n    return T.clamp(indices, 0, probs.shape[1])\n\n\nclass SubsetOutputs(T.nn.Module):\n    def __init__(self, model, start=None, end=None):\n        super().__init__()\n        self._model = model\n        self.start = start\n        self.end = end\n\n    def forward(self, x):\n        return self._model(x)[..., self.start:self.end]\n\n\nclass Max2d(T.nn.Module):\n    def forward(self, x):\n        return x.view(*x.shape[:-2], -1).max(-1)[0]\n\n\nclass Avg2d(T.nn.Module):\n    def forward(self, x):\n        return x.mean((-2, -1))\n"}
{"text": "#! /usr/bin/env python\n# Copyright (C) 2014 ZhiQiang Fan <aji.zqfan@gmail.com>\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\n\n# https://oj.leetcode.com/problems/find-minimum-in-rotated-sorted-array-ii/\n\nclass Solution:\n    # @param num, a list of integer\n    # @return an integer\n    def findMin(self, num):\n        if not num: return\n        left, mid, right = 0, 0, len(num) - 1\n        while left < right:\n            # it is sorted, such case means no rotated at all\n            if num[left] < num[right]:\n                return num[left]\n            mid = (left + right + 1) / 2\n            # such case means left is ordered, so rotated part is on the right.\n            if num[mid] > num[left]:\n                left = mid\n            elif num[mid] < num[left]:\n                right = mid\n                # mid can be equal to right, we need to increase left to avoid\n                # infinite loop, since num[left] >= num[right], it is safe to\n                # do so.\n                left += 1\n            else:\n                # we can not know the mininum in which side, so just increase\n                # left, in worst case, it is linear\n                left += 1\n        return num[mid]\n"}
{"text": "\"\"\"\nThis script is used to create a training database based on GPDSSynth signatures.\nImages are scaled to 192x96;\nPaper-like backgrounds are added\n\"\"\"\n\nimport argparse\nimport numpy as np\nfrom scipy.misc import imread, imresize, imshow, imsave\nfrom PIL import Image\nimport os\nfrom skimage.transform import rotate\nimport time\n\nimport prepimage\n\n\ndef load_backgrounds(folder):\n    \"\"\"read image file and convert to grayscale\"\"\"\n    return [imresize(\n                np.dot(imread(os.path.join(folder, bg_file))[..., :3],\n                       [0.299, 0.587, 0.144]),\n            0.5)\n            for bg_file in os.listdir(folder)\n            if '.jpg' in bg_file or '.png' in bg_file]\n\n\ndef get_background(img, size):\n    \"\"\"crop a random piece of desired size from the given image\"\"\"\n    y = np.random.randint(0, img.shape[0]-size[0])\n    x = np.random.randint(0, img.shape[1]-size[1])\n    return imresize(img[y:y+size[0], x:x+size[1]], (size[0], size[1]))\n\n\ndef get_signatures(data_dir, no_forgeries=False):\n    for (path, _, files) in os.walk(data_dir):\n        for f in files:\n            if '.png' in f and not (no_forgeries and 'cf' in f):\n                yield os.path.join(path, f)\n\n\ndef get_signatures_(data_dir, no_forgeries=False):\n    for f in os.listdir(data_dir):\n        if '.png' in f and not (no_forgeries and 'cf' in f):\n            yield os.path.join(data_dir, f)\n\n\ndef get_roi(image, pad=20):\n    roix, roiy = prepimage.min_max(prepimage.binarize(image))\n    roix = (max(0, roix[0] - pad), min(roix[1] + pad, image.shape[1]))\n    roiy = (max(0, roiy[0] - pad), min(roiy[1] + pad, image.shape[0]))\n    return roiy, roix\n\n\ndef process_signature(sig_path):\n    sig = imread(sig_path).astype(np.float32) / 255.0\n    sig = rotate(sig, np.random.randint(-25, 25), cval=1.0, resize=True)\n\n    roiy, roix = get_roi(sig)\n    shape = (roiy[1] - roiy[0], roix[1] - roix[0])\n    bg = get_background(np.random.choice(backgrounds), shape).astype(np.float32) / 255.0\n\n    img = bg + sig[roiy[0]:roiy[1], roix[0]:roix[1]]\n    img = imresize(img, target_size, mode='L').astype(np.float32)\n    img *= 1.0/img.max()\n    # return np.minimum(img, 1.0)\n    return img\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('signatures',\n                        help='Path to extracted GPDS data')\n    parser.add_argument('backgrounds',\n                        help='Path to background files (jpg or png)')\n    parser.add_argument('--out', '-o', default='images',\n                        help='Path to save output images')\n    parser.add_argument('--start', '-s', default=1, type=int,\n                        help='User to start with (for resumes)')\n    args = parser.parse_args()\n\n    target_size = (384, 768)\n    # signatures = list(get_signatures(args.signatures))\n    backgrounds = load_backgrounds(args.backgrounds)\n    print(\"Loaded {} backgrounds\".format(len(backgrounds)))\n\n    for user in range(args.start, 20):\n        user_str = \"{}\".format(user)\n        print(\"processing user \" + user_str)\n        os.makedirs(os.path.join(args.out, user_str), exist_ok=True)\n        count = 0\n        start = time.clock()\n        for sig in get_signatures_(os.path.join(args.signatures, user_str)):\n            fname, _ = os.path.splitext(os.path.basename(sig))\n            for i in range(1, 21):\n                outname = os.path.join(args.out, user_str, \"{}-{:02d}.png\".format(fname, i))\n                imsave(outname, process_signature(sig), 'png')\n                count += 1\n        print(\"{} images in {:3f} sec\".format(count, time.clock() - start))\n"}
{"text": "#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom datetime import datetime\n\nfrom mcrouter.test.MCProcess import Memcached\nfrom mcrouter.test.McrouterTestCase import McrouterTestCase\n\n\nclass TestLatencyInjectionRoute(McrouterTestCase):\n    config_latency_before = './mcrouter/test/test_latency_injection_before.json'\n    config_latency_after = './mcrouter/test/test_latency_injection_before.json'\n    config_latency_total = './mcrouter/test/test_latency_injection_before.json'\n\n    def setUp(self) -> None:\n        self.mc = self.add_server(Memcached())\n\n        self.mcrouter_latency_before =\\\n            self.add_mcrouter(self.config_latency_before)\n        self.mcrouter_latency_after =\\\n            self.add_mcrouter(self.config_latency_after)\n        self.mcrouter_latency_total =\\\n            self.add_mcrouter(self.config_latency_total)\n\n    def test_latency_before(self) -> None:\n        self.mc.set(\"key1\", \"value1\")\n\n        t_start = datetime.now()\n        self.assertEqual(\"value1\", self.mcrouter_latency_before.get(\"key1\"))\n        t_end = datetime.now()\n\n        duration = t_end - t_start\n        self.assertGreaterEqual(duration.total_seconds(), 2)\n\n    def test_latency_after(self) -> None:\n        self.mc.set(\"key2\", \"value2\")\n\n        t_start = datetime.now()\n        self.assertTrue(\"value2\", self.mcrouter_latency_after.get(\"key2\"))\n        t_end = datetime.now()\n\n        duration = t_end - t_start\n        self.assertGreaterEqual(duration.total_seconds(), 1)\n\n    def test_latency_total(self) -> None:\n        self.mc.set(\"key3\", \"value3\")\n\n        t_start = datetime.now()\n        self.assertTrue(\"value3\", self.mcrouter_latency_total.get(\"key3\"))\n        t_end = datetime.now()\n\n        duration = t_end - t_start\n        self.assertGreaterEqual(duration.total_seconds(), 1)\n"}
{"text": "import ast\nimport logging\nimport time\nimport unittest\n\nfrom malcolm.profiler import Profiler\n\n\n# https://github.com/bdarnell/plop/blob/master/plop/test/collector_test.py\nclass ProfilerTest(unittest.TestCase):\n    def filter_stacks(self, results):\n        # Kind of hacky, but this is the simplest way to keep the tests\n        # working after the internals of the collector changed to support\n        # multiple formatters.\n        stack_counts = ast.literal_eval(results)\n        counts = {}\n        for stack, count in stack_counts.items():\n            filtered_stack = [\n                frame[2] for frame in stack if frame[0].endswith(\"test_profiler.py\")\n            ]\n            if filtered_stack:\n                counts[tuple(filtered_stack)] = count\n\n        return counts\n\n    def check_counts(self, counts, expected):\n        failed = False\n        output = []\n        for stack, count in expected.items():\n            # every expected frame should appear in the data, but\n            # the inverse is not true if the signal catches us between\n            # calls.\n            self.assertTrue(stack in counts)\n            ratio = float(counts[stack]) / float(count)\n            output.append(\n                \"%s: expected %s, got %s (%s)\" % (stack, count, counts[stack], ratio)\n            )\n            if not (0.70 <= ratio <= 1.25):\n                failed = True\n        if failed:\n            for line in output:\n                logging.warning(line)\n            for key in set(counts.keys()) - set(expected.keys()):\n                logging.warning(\"unexpected key: %s: got %s\" % (key, counts[key]))\n            self.fail(\"collected data did not meet expectations\")\n\n    def test_collector(self):\n        start = time.time()\n\n        def a(end):\n            while time.time() < end:\n                pass\n            c(time.time() + 0.1)\n\n        def b(end):\n            while time.time() < end:\n                pass\n            c(time.time() + 0.1)\n\n        def c(end):\n            while time.time() < end:\n                pass\n\n        profiler = Profiler(\"/tmp\")\n        profiler.start(interval=0.01)\n        a(time.time() + 0.1)\n        b(time.time() + 0.2)\n        c(time.time() + 0.3)\n        end = time.time()\n        profiler.stop(\"profiler_test.plop\")\n        elapsed = end - start\n        self.assertTrue(0.8 < elapsed < 0.9, elapsed)\n\n        with open(\"/tmp/profiler_test.plop\") as f:\n            results = f.read()\n        counts = self.filter_stacks(results)\n\n        expected = {\n            (\"a\", \"test_collector\"): 10,\n            (\"c\", \"a\", \"test_collector\"): 10,\n            (\"b\", \"test_collector\"): 20,\n            (\"c\", \"b\", \"test_collector\"): 10,\n            (\"c\", \"test_collector\"): 30,\n        }\n        self.check_counts(counts, expected)\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom enum import Enum\n\n\nclass Card(object):\n\n    class Type(Enum):\n        NUMBER = \"card_number\"\n        JACK = \"card_jack\"\n        QUEEN = \"card_queen\"\n        KING = \"card_king\"\n        ACE = \"card_ace\"\n\n    symbols = [\"\u2665\", \"\u2666\", \"\u2663\", \"\u2660\"]\n    value_str = [\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"Jack\", \"Queen\", \"King\", \"Ace\"]\n\n    def __init__(self, card_id):\n        self.card_id = card_id\n\n    def is_ace(self):\n        return self.value == 11\n\n    @property\n    def symbol(self):\n        return self.symbols[self.card_id // 13]\n\n    @property\n    def value(self):\n        values = [2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11]\n        return values[self.card_id % 13]\n\n    @property\n    def face(self):\n        return self.value_str[self.card_id % 13]\n\n    @property\n    def type(self):\n        if (self.card_id % 13) in range(0, 9):\n            return Card.Type.NUMBER\n        elif (self.card_id % 13) == 9:\n            return Card.Type.JACK\n        elif (self.card_id % 13) == 10:\n            return Card.Type.QUEEN\n        elif (self.card_id % 13) == 11:\n            return Card.Type.KING\n        elif (self.card_id % 13) == 12:\n            return Card.Type.ACE\n        else:\n            raise ValueError(\"card_id '{}' can't be mapped to card type!\".format(self.card_id))\n\n    @property\n    def str_id(self):\n        str_ids = [\"card_2\", \"card_3\", \"card_4\", \"card_5\", \"card_6\",\n                   \"card_7\", \"card_8\", \"card_9\", \"card_10\",\n                   \"card_jack\", \"card_queen\", \"card_king\", \"card_ace\"]\n        return str_ids[self.card_id % 13]\n\n    def __str__(self):\n        return \"{} {}\".format(self.symbol, self.face)\n\n    def __repr__(self):\n        return self.__str__()\n"}
{"text": "from math import floor\ndef sqrt(S):\n    \"\"\"Given an integer, return the square root.\n\n    A continued fraction expansion implementation.\n        https://en.wikipedia.org/wiki/Methods_of_computing_square_roots#Continued_fraction_expansion\n\n        Args:\n            S: Any natural number\n    \"\"\"\n    i = 0\n    s = 1\n    if S == 0 or S == 1: return S\n    while s ** 2 < S:\n        if i ** 2 == S:\n            return i\n        s = s * 2\n        i += 1\n    return __search((s / 2), s, S)\n\ndef __search(i, k, S):\n    j = i + ((k - i) / 2)\n    s = j ** 2\n    if s == S:\n        return j\n    elif k == i + 1:\n        return __continued_fraction(S, [j], 1, 0)\n    elif s > S:\n        return __search(i, j, S)\n    elif s < S:\n        return __search(j, k, S)\n\ndef __continued_fraction(S, a, d_n, m_n):\n    n = len(a) - 1\n    m_1 = (d_n * a[n]) - m_n\n    d_1 = (S - m_1 ** 2) / d_n\n    a_1 = int(floor((a[0] + m_1) / d_1))\n    a.append(a_1)\n    if a_1 != 2 * a[0] and len(a) < 11:\n        return __continued_fraction(S, a, d_1, m_1)\n    else:\n        result = 1.0\n        while len(a):\n            result = a.pop() + (1 / result)\n        return result"}
{"text": "# coding=utf-8\n# --------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n#\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is\n# regenerated.\n# --------------------------------------------------------------------------\n\nfrom msrest.serialization import Model\n\n\nclass EffectiveNetworkSecurityGroupListResult(Model):\n    \"\"\"Response for list effective network security groups API service call.\n\n    Variables are only populated by the server, and will be ignored when\n    sending a request.\n\n    :param value: A list of effective network security groups.\n    :type value:\n     list[~azure.mgmt.network.v2017_08_01.models.EffectiveNetworkSecurityGroup]\n    :ivar next_link: The URL to get the next set of results.\n    :vartype next_link: str\n    \"\"\"\n\n    _validation = {\n        'next_link': {'readonly': True},\n    }\n\n    _attribute_map = {\n        'value': {'key': 'value', 'type': '[EffectiveNetworkSecurityGroup]'},\n        'next_link': {'key': 'nextLink', 'type': 'str'},\n    }\n\n    def __init__(self, *, value=None, **kwargs) -> None:\n        super(EffectiveNetworkSecurityGroupListResult, self).__init__(**kwargs)\n        self.value = value\n        self.next_link = None\n"}
{"text": "import unittest\nimport datetime\n\nimport pandas as pd\n\nfrom simple_ranker import Ranker\n\n\nclass RankerTest(unittest.TestCase):\n    def setUp(self):\n        self.current_year = datetime.datetime.now().year\n\n    def test_rank_by_PE_returns_lowest_first(self):\n        pe_rank = {\n            'name': 'pe',\n            'ascending': True\n        }\n        data = pd.DataFrame({\n            'code': ['ANZ', 'CBA', 'NAB'],\n            'pe': [3.0, 1.0, 2.0],\n        }, index=pd.to_datetime(\n            [datetime.date(self.current_year, 6, 20)] * 3), dtype=float\n        )\n\n        ranker = Ranker(data, [pe_rank], [], limit=50)\n        results = ranker.process()\n\n        self.assertTrue(results[0:1]['code'][0] == 'CBA')\n\n    def test_rank_by_ROE_return_highest_first_after_filtering(self):\n        roe_rank = {\n            'name': 'roe',\n            'max': 0.70,\n            'ascending': False\n        }\n        data = pd.DataFrame({\n            'code': ['ANZ', 'CBA', 'NAB'],\n            'roe': [0.70, 0.71, 0.69]},\n            index=pd.to_datetime(\n                [datetime.date(self.current_year, 6, 20)] * 3\n            ), dtype=float\n        )\n\n        ranker = Ranker(data, [roe_rank], [], limit=50)\n        results = ranker.process()\n\n        self.assertTrue(results[0:1]['code'][0] == 'ANZ')\n\n    def test_rank_and_filter_removes_too_small_companies(self):\n        market_cap_filter = {\n            'name': 'market_cap',\n            'min': 5000000\n        }\n        roe_rank = {\n            'name': 'roe',\n            'max': 0.70,\n            'ascending': False\n        }\n        data = pd.DataFrame({\n            'code': ['SMALL', 'ANZ', 'CBA', 'NAB'],\n            'roe': [0.50, 0.40, 0.41, 0.39],\n            'market_cap': [1000000] + [6000000] * 3},\n            index=pd.to_datetime(\n                [datetime.date(self.current_year, 6, 20)] * 4\n            ), dtype=float\n        )\n\n        ranker = Ranker(data, [roe_rank], [market_cap_filter], limit=50)\n        results = ranker.process()\n\n        self.assertTrue(results[0:1]['code'][0] == 'CBA')\n\n    def test_rank_ROE_and_PE_returns_correct_top(self):\n        roe_rank = {\n            'name': 'roe',\n            'ascending': False\n        }\n        pe_rank = {\n            'name': 'pe',\n            'ascending': True\n        }\n\n        data = pd.DataFrame({\n            'code': ['ANZ', 'CBA', 'NAB', 'WST'],\n            'pe': [3, 4, 5, 6],\n            'roe': [0.30, 0.50, 0.80, 0.70]},\n            index=pd.to_datetime(\n                [datetime.date(self.current_year, 6, 20)] * 4\n            ), dtype=float\n        )\n\n        ranker = Ranker(data, [pe_rank, roe_rank], [], limit=50)\n        results = ranker.process()\n\n        # Output should look like this:\n        # code  pe_rank  roe_rank  total_rank\n        # ANZ    1         4        5\n        # CBA    2         3        5\n        # NAB    3         1        4  -- first pick\n        # WST    4         2        6  -- last pick\n        self.assertTrue(results[0:1]['code'][0] == 'NAB')\n        self.assertTrue(results[-1:]['code'][0] == 'WST')\n\n    def test_rank_ROE_avg_3_returns_correct_top(self):\n        roe_rank = {\n            'name': 'roe',\n            'max': 0.8,\n            'average': 3,\n            'ascending': False\n        }\n\n        # Push last 3 years into a list\n        date_array = [\n            datetime.date(self.current_year - i, 6, 20) for i in range(3)]\n\n        data = pd.DataFrame({\n            'code': ['ANZ'] * 3 + ['CBA'] * 3 + ['NAB'] * 3,\n            'roe': [0.1, 0.2, 0.5] + [0.7, 0.1, 0.2] + [0.1, 0.2, 0.4]},\n            index=pd.to_datetime(date_array * 3), dtype=float\n        )\n\n        ranker = Ranker(data, [roe_rank], [], limit=50)\n        results = ranker.process()\n\n        self.assertTrue(results[0:1]['code'][0] == 'CBA')\n        self.assertTrue(results[-1:]['code'][0] == 'NAB')\n\nif __name__ == '__main__':\n    unittest.run()\n"}
{"text": "#!/usr/bin/env python\n\"\"\"Read the contents of a directory containing DFT output and create a csv style file of information\"\"\"\nfrom __future__ import print_function\nimport sys\nimport dill as pickle\nimport PDielec.__init__\nversion = PDielec.__init__.__version__\n\ndef print_help():\n    print('pickled_reader filenames', file=sys.stderr)\n    print('  Read in a pickled (actually using dill to pickle the object) reader            ', file=sys.stderr)\n    print('  The pickled file should have been created using preader -pickle                ', file=sys.stderr)\n    print('  Version ',version,file=sys.stderr)\n    exit()\n\ndef main():\n    #\n    # Print out the help file if there is nothing else on the command line\n    #\n    if len(sys.argv) <= 1 :\n        print_help()\n    #\n    # Read in the pickled reader objects from the dump file\n    #\n    picklefile = sys.argv[1]\n    #\n    # store each reader in a list\n    #\n    readers = []\n    #\n    # Open the pickled file as binary and for reading only\n    # keep reading until we reach an end of file\n    #\n    with open(picklefile,'rb') as f:\n        try:\n            while True:\n                readers.append(pickle.load(f))\n        except EOFError:\n            pass\n    #\n    print('Read in {} readers'.format(len(readers)))\n    #\n    # Loop over the readers and print out some information - assign a variable\n    #\n    for reader in readers:\n        print('NEW READER type={}, file={}'.format(reader.type,reader.names[0]))\n        reader.print_info()\n        print('LAST CELL')\n        lastcell = reader.unit_cells[-1]\n        lastcell.print_info()\n    # End of for loop over readers\n# end of def main\n\nif __name__ == \"__main__\":\n    main()\n"}
{"text": "# Copyright 2020 Tensorforce Team. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport unittest\n\nimport numpy as np\n\nfrom test.unittest_base import UnittestBase\n\n\nclass TestSeed(UnittestBase, unittest.TestCase):\n\n    def test_seed(self):\n        self.start_tests()\n\n        states = dict(\n            int_state=dict(type='int', shape=(2,), num_values=4),\n            float_state=dict(type='float', shape=(2,), min_value=1.0, max_value=2.0),\n        )\n        actions = dict(\n            int_action=dict(type='int', shape=(2,), num_values=4),\n            float_action=dict(type='float', shape=(2,), min_value=1.0, max_value=2.0),\n        )\n\n        agent, environment = self.prepare(\n            states=states, actions=actions, config=dict(\n                seed=0, device='CPU', eager_mode=True, create_debug_assertions=True,\n                tf_log_level=20\n            )\n        )\n\n        print_environment = False\n        print_agent = False\n\n        states = environment.reset()\n        if print_environment:\n            print(states['int_state'])\n            print(states['float_state'])\n        else:\n            self.assertTrue(expr=np.allclose(a=states['int_state'], b=np.asarray([2, 3])))\n            self.assertTrue(expr=np.allclose(\n                a=states['float_state'], b=np.asarray([1.33350747, 1.92415877])\n            ))\n\n        actions = agent.act(states=states)\n        if print_agent:\n            print(actions['int_action'])\n            print(actions['float_action'])\n        else:\n            self.assertTrue(expr=np.allclose(a=actions['int_action'], b=np.asarray([0, 0])))\n            self.assertTrue(expr=np.allclose(\n                a=actions['float_action'], b=np.asarray([1.5049707, 1.4608247])\n            ))\n\n        states, terminal, reward = environment.execute(actions=actions)\n        updated = agent.observe(terminal=terminal, reward=reward)\n        if print_environment:\n            print(states['int_state'])\n            print(states['float_state'])\n            print(terminal, reward, updated)\n        else:\n            self.assertTrue(expr=np.allclose(a=states['int_state'], b=np.asarray([1, 2])))\n            self.assertTrue(expr=np.allclose(\n                a=states['float_state'], b=np.asarray([1.71033683, 1.0078841])\n            ))\n            self.assertFalse(expr=terminal)\n            self.assertEqual(first=reward, second=0.6888437030500962)\n            self.assertFalse(expr=updated)\n\n        actions = agent.act(states=states)\n        if print_agent:\n            print(actions['int_action'])\n            print(actions['float_action'])\n        else:\n            self.assertTrue(expr=np.allclose(a=actions['int_action'], b=np.asarray([3, 3])))\n            self.assertTrue(expr=np.allclose(\n                a=actions['float_action'], b=np.asarray([1.5072203, 1.6714704])\n            ))\n\n        states, terminal, reward = environment.execute(actions=actions)\n        updated = agent.observe(terminal=terminal, reward=reward)\n        if print_environment:\n            print(states['int_state'])\n            print(states['float_state'])\n            print(terminal, reward, updated)\n        else:\n            self.assertTrue(expr=np.allclose(a=states['int_state'], b=np.asarray([1, 3])))\n            self.assertTrue(expr=np.allclose(\n                a=states['float_state'], b=np.asarray([1.60039224, 1.58873961])\n            ))\n            self.assertFalse(expr=terminal)\n            self.assertEqual(first=reward, second=0.515908805880605)\n            self.assertFalse(expr=updated)\n\n        actions = agent.act(states=states)\n        if print_agent:\n            print(actions['int_action'])\n            print(actions['float_action'])\n        else:\n            self.assertTrue(expr=np.allclose(a=actions['int_action'], b=np.asarray([0, 3])))\n            self.assertTrue(expr=np.allclose(\n                a=actions['float_action'], b=np.asarray([1.6693485, 1.7339616])\n            ))\n\n        states, terminal, reward = environment.execute(actions=actions)\n        updated = agent.observe(terminal=terminal, reward=reward)\n        if print_environment:\n            print(states['int_state'])\n            print(states['float_state'])\n            print(terminal, reward, updated)\n        else:\n            self.assertTrue(expr=np.allclose(a=states['int_state'], b=np.asarray([1, 0])))\n            self.assertTrue(expr=np.allclose(\n                a=states['float_state'], b=np.asarray([1.13346147, 1.98058013])\n            ))\n            self.assertFalse(expr=terminal)\n            self.assertEqual(first=reward, second=-0.15885683833831)\n            self.assertFalse(expr=updated)\n"}
{"text": "import unittest\nfrom taric_challange.core.books_manager import BooksManager\nfrom taric_challange.core.models.book import Book\nfrom mock import Mock\nimport mock\n\n\ndata = {\"author_data\" : [\n        {\n            \"name\": \"Richards, Rowland\",\n            \"id\": \"richards_rowland\"\n        }],\n        \"awards_text\": \"\",\n        \"marc_enc_level\": \"4\",\n        \"subject_ids\": [\n        \"mechanics_applied\",\n        \"physics\"\n        ],\n        \"summary\": \"\",\n        \"isbn13\": \"9780849303159\",\n        \"dewey_normal\": \"620.105\",\n        \"title_latin\": \"Principles of solid mechanics\",\n        \"publisher_id\": \"crc_press\",\n        \"dewey_decimal\": \"620/.1/05\",\n        \"publisher_text\": \"Boca Raton, FL : CRC Press, 2001.\",\n        \"language\": \"eng\",\n        \"physical_description_text\": \"446 p. : ill. ; 24 cm.\",\n        \"isbn10\": \"084930315X\",\n        \"edition_info\": \"(alk. paper)\",\n        \"urls_text\": \"\",\n        \"lcc_number\": \"TA350\",\n        \"publisher_name\": \"CRC Press\",\n        \"book_id\": \"principles_of_solid_mechanics\",\n        \"notes\": \"Includes bibliographical references and index.\",\n        \"title\": \"Principles of solid mechanics\",\n        \"title_long\": \"\"}\n\n\nclass BooksManagerTest(unittest.TestCase):\n\n    def simple_local_test(self):\n        manager = BooksManager()\n        book = Book(data)\n\n        manager._remote_request_data = Mock(return_value=[[data]])\n        manager.update_books('fake keyword')\n\n        self.assertEqual(book.title, manager.books_titles[0])\n        self.assertEqual(book.publisher, manager.books[0].publisher)\n        self.assertEqual(book.subjects, manager.books[0].subjects)\n        self.assertEqual(book.author, manager.books[0].author)\n        self.assertEqual(book.isbn10, manager.books[0].isbn10)\n        self.assertEqual(book.isbn13, manager.books[0].isbn13)\n        self.assertEqual(book.edition, manager.books[0].edition)\n        self.assertEqual(book.language, manager.books[0].language)\n"}
{"text": "import bz2\nimport os\nimport time\nfrom urllib.request import urlopen, Request, urlretrieve\n\n\ndef request_(req_url, sleep_time=1):\n    print(\"Requesting: %s\" % req_url)\n    request = Request(req_url)\n    request.add_header('User-Agent', 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36')\n    response = urlopen(request)\n    out = response.read().decode('utf8') # cos python3 is kind of stupid http://stackoverflow.com/questions/6862770/python-3-let-json-object-accept-bytes-or-let-urlopen-output-strings\n    time.sleep(sleep_time)  # obey api rate limits\n    return out\n\n\ndef postjson_request_(req_url, data):\n    \"\"\"\n    no sleep time as these are for my own server\n    :param req_url:\n    :param data:\n    :return:\n    \"\"\"\n    print(\"Requesting: %s\" % req_url)\n    request = Request(req_url, data=data.encode('ascii'))\n    request.add_header('Content-Type', 'application/json')\n    response = urlopen(request)\n    out = response.read().decode('utf8') # cos python3 is kind of stupid http://stackoverflow.com/questions/6862770/python-3-let-json-object-accept-bytes-or-let-urlopen-output-strings\n    return out\n\n\ndef download(match):\n    succeeded = False\n    tries = 0\n    # TODO it would be nice to debug this.\n    # Seems to be connection issues. maybe having parallel downloads is just frowned upon and I should remove pooling downloads\n    sleeper = 2 ** tries  # whatever\n    while not succeeded and tries < 10:\n        try:\n            urlretrieve(match.replay_url, match.download_path)\n            succeeded = True\n        except:\n            tries += 1\n            time.sleep(sleeper)\n            continue\n\n\ndef extract(match):\n    with open(match.file_path, 'wb') as newf, bz2.BZ2File(match.download_path) as oldf:\n        for data in iter(lambda: oldf.read(100 * 1024), b''):\n            newf.write(data)\n            if not data:\n                return\n\n    print(\"Match successfully downloaded: %d\" % match.id)\n    os.remove(match.download_path)  # Delete the old compressed replay file\n\n\ndef download_and_extract(match):\n    \"\"\"\n\n    :param match:\n    :return: ...why did I do return 1? I dont think thats necessary\n    \"\"\"\n\n    if match.already_have_replay:\n        return\n\n    download(match)\n    time.sleep(0.1)\n    extract(match)\n    return"}
{"text": "#!/usr/bin/python2.7\n\n# Part of the WiFi Harvester project.\n# AUTHOR: Harvey Phillips\n# Project Home: https://github.com/xcellerator/wifi_harvester\n# Released under GPL v2\n\nimport csv\nimport sys\nimport requests\n\nif ( len(sys.argv) != 2 ):\n\tprint (\"Usage: \" + str(sys.argv[0]) + \" <kismet csv>\")\n\texit\nelse:\n\tfile = str(sys.argv[1])\n\n\nwith open(file, 'rU') as f:\n\treader = csv.reader(f)\n\tdata = list(list(rec) for rec in csv.reader(f, delimiter=','))\n\tf.close()\n\nsplit =  data.index(['Station MAC', ' First time seen', ' Last time seen', ' Power', ' # packets', ' BSSID', ' Probed ESSIDs'])\nap_content = []\ncl_content = []\nserver = \"192.168.1.235\"\n\nfor x in range(2, split - 1):\n\tESSID = data[x][13]\n\tBSSID = data[x][0]\n\tenc_string = data[x][5]\n\n\tif (enc_string == \" OPN\"): enc_type = 0\n\tif (enc_string == \" WEP\"): enc_type = 1\n\tif (enc_string == \" WPA2\"): enc_type = 2\n\tif (enc_string == \" WPA2 WPA\"): enc_type = 2\n\n\tap_content.append( [ 0, ESSID, BSSID, enc_type ] )\n\nfor x in range(split + 1, int(len(data)) - 1):\n\tBSSID =  data[x][0]\n\n\tcl_content.append( [1, \"NULL\", BSSID, \"NULL\" ] )\n\ncontent = ap_content + cl_content\n\nfor entry in content:\n\turl = \"http://\" + server + \"/update.php?a=\" + str(entry[0]) + \"&b=\" + entry[1] + \"&c=\" + entry[2] + \"&d=\" + str(entry[3])\n\tr = requests.get(url)\n\nprint \"Captures uploaded successfully.\\n\"\n"}
{"text": "__author__ = 'Amin'\n\n# COMPLETED\n# PYTHON 3.x\n\nimport sys\nimport math\n\n\nclass Floor:\n    def __init__(self, width, contains_exit=False, exit_position=-1):\n        self.width = width\n        self.__contains_elevator = False\n        self.__elevator_position = -1\n        self.__contains_exit = contains_exit\n        self.__exit_position = exit_position\n\n    def add_exit(self, exit_position):\n        self.__contains_exit = True\n        self.__exit_position = exit_position\n\n    def add_elevator(self, elevator_position):\n        self.__contains_elevator = True\n        self.__elevator_position = elevator_position\n\n    def should_be_blocked(self, position, direction):\n        flag_should_be_blocked = False\n\n        if self.__contains_elevator:\n            if position > self.__elevator_position and direction == \"RIGHT\" or \\\n                    position < self.__elevator_position and direction == \"LEFT\":\n                flag_should_be_blocked = True\n        elif self.__contains_exit:\n            if position > self.__exit_position and direction == \"RIGHT\" or \\\n                    position < self.__exit_position and direction == \"LEFT\":\n                flag_should_be_blocked = True\n\n        return flag_should_be_blocked\n\n\nclass Drive:\n    def __init__(self):\n        self.floors = []\n\n        self.load_from_input()\n\n    def load_from_input(self):\n        # nb_floors: number of floors\n        # width: width of the area\n        # nb_rounds: maximum number of rounds\n        # exit_floor: floor on which the exit is found\n        # exit_pos: position of the exit on its floor\n        # nb_total_clones: number of generated clones\n        # nb_additional_elevators: ignore (always zero)\n        # nb_elevators: number of elevators\n        nb_floors, width, nb_rounds, exit_floor, exit_pos, nb_total_clones, nb_additional_elevators, nb_elevators = [int(i) for i in input().split()]\n\n        for i in range(nb_floors):\n            self.floors.append(Floor(width))\n\n        self.floors[exit_floor].add_exit(exit_pos)\n\n        for i in range(nb_elevators):\n            # elevator_floor: floor on which this elevator is found\n            # elevator_pos: position of the elevator on its floor\n            elevator_floor, elevator_pos = [int(j) for j in input().split()]\n            self.floors[elevator_floor].add_elevator(elevator_pos)\n\n\nif __name__ == '__main__':\n    drive = Drive()\n\n    flag_do_the_blocking = False\n\n    # game loop\n    while 1:\n        # clone_floor: floor of the leading clone\n        # clone_pos: position of the leading clone on its floor\n        # direction: direction of the leading clone: LEFT or RIGHT\n        clone_floor, clone_pos, direction = input().split()\n        clone_floor = int(clone_floor)\n        clone_pos = int(clone_pos)\n\n        flag_do_the_blocking = drive.floors[clone_floor].should_be_blocked(clone_pos, direction)\n\n        # Write an action using print\n        # To debug: print(\"Debug messages...\", file=sys.stderr)\n\n        # action: WAIT or BLOCK\n        if flag_do_the_blocking:\n            print(\"BLOCK\")\n        else:\n            print(\"WAIT\")\n"}
{"text": "#!/usr/bin/env python\n\nimport os\nimport pytest\nimport json\nimport boutiques as bosh\nimport boutiques.creator as bc\nfrom boutiques import __file__ as bfile\nfrom boutiques.localExec import ExecutorError\nfrom argparse import ArgumentParser\nfrom unittest import TestCase\nimport mock\nfrom boutiques_mocks import mock_zenodo_search, MockZenodoRecord\n\n\ndef mock_get(*args, **kwargs):\n    query = args[0].split(\"=\")[1]\n    query = query[:query.find(\"&\")]\n    query = query.replace(\"*\", '')\n\n    mock_records = []\n    # Return an arbitrary list of results\n    for i in range(0, 10):\n        mock_records.append(MockZenodoRecord(i, \"Example Tool %s\" % i))\n    return mock_zenodo_search(mock_records)\n\n\nclass TestLogger(TestCase):\n\n    def get_examples_dir(self):\n        return os.path.join(os.path.dirname(bfile),\n                            \"schema\", \"examples\")\n\n    def test_raise_error(self):\n        example1_dir = os.path.join(self.get_examples_dir(), \"example1\")\n        invocationStr = open(os.path.join(example1_dir,\n                                          \"invocation_invalid.json\")).read()\n        with pytest.raises(ExecutorError) as e:\n            bosh.execute(\"launch\",\n                         os.path.join(example1_dir,\n                                      \"example1_docker.json\"),\n                         invocationStr)\n        assert(\"[ ERROR ]\" in str(e))\n\n    @mock.patch('requests.get', side_effect=mock_get)\n    def test_print_info(self, mock_get):\n        bosh.search(\"-v\")\n        out, err = self.capfd.readouterr()\n        assert(\"[ INFO ]\" in out)\n        assert(\"[ INFO (200) \" in out)\n\n    def test_print_warning(self):\n        parser = ArgumentParser(description=\"my tool description\")\n        parser.add_argument(\"--myarg\", \"-m\", action=\"store\",\n                            help=\"my help\", dest=\"==SUPPRESS==\")\n        creatorObj = bc.CreateDescriptor(parser,\n                                         execname='/path/to/myscript.py',\n                                         verbose=True,\n                                         tags={\"purpose\": \"testing-creator\",\n                                               \"foo\": \"bar\"})\n        out, err = self.capfd.readouterr()\n        assert(\"[ WARNING ]\" in out)\n\n    def test_evaloutput(self):\n        example1_dir = os.path.join(self.get_examples_dir(), \"example1\")\n        desc = os.path.join(example1_dir, \"example1_docker.json\")\n        invo = os.path.join(example1_dir, \"invocation.json\")\n        query = bosh.evaluate(desc, invo, \"invalid-query\")\n        out, err = self.capfd.readouterr()\n        assert(\"[ ERROR ]\" in out)\n\n    # Captures the stdout and stderr during test execution\n    # and returns them as a tuple in readouterr()\n    @pytest.fixture(autouse=True)\n    def capfd(self, capfd):\n        self.capfd = capfd\n"}
{"text": "\"\"\"\nExamples\n========\n\nmy_app/models.py\n----------------\n\n    from django.db import models\n\n    class CustomerType(models.Model):\n        name = models.CharField(max_length=50)\n\n        def __unicode__(self):\n            return self.name\n\n    class Customer(models.Model):\n        name = models.CharField(max_length=50)\n        type = models.ForeignKey('CustomerType')\n        is_active = models.BooleanField(default=True, blank=True)\n        employer = models.CharField(max_length=100)\n\n        def __unicode__(self):\n            return self.name\n\nanother_app/models.py\n---------------------\n\n    from django.db import models\n    from django.contrib.auth.models import User\n\n    from djangoplus.modify_models import ModifiedModel\n\n    class City(models.Model):\n        name = models.CharField(max_length=50)\n\n        def __unicode__(self):\n            return self.name\n\n    class HelperCustomerType(ModifiedModel):\n        class Meta:\n            model = 'my_app.CustomerType'\n\n        description = models.TextField()\n\n    class HelperCustomer(ModifiedModel):\n        class Meta:\n            model = 'my_app.Customer'\n            exclude = ('employer',)\n\n        type = models.CharField(max_length=50)\n        address = models.CharField(max_length=100)\n        city = models.ForeignKey(City)\n\n        def __unicode__(self):\n            return '%s - %s'%(self.pk, self.name)\n\n    class HelperUser(ModifiedModel):\n        class Meta:\n            model = User\n\n        website = models.URLField(blank=True, verify_exists=False)\n\n\"\"\"\nimport types\n\nfrom django.db import models\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.db.models import get_model\nfrom django.db.models.fields import FieldDoesNotExist\n\nclass ModifiedModelMetaclass(type):\n    def __new__(cls, name, bases, attrs):\n        new_class = super(ModifiedModelMetaclass, cls).__new__(cls, name, bases, attrs)\n\n        if name == 'ModifiedModel' and bases[0] == object:\n            return new_class\n\n        try:\n            meta = attrs['Meta']()\n        except KeyError:\n            raise ImproperlyConfigured(\"Helper class %s hasn't a Meta subclass!\" % name)\n\n        # Find model class for this helper\n        if isinstance(getattr(meta, 'model', None), basestring):\n            model_class = get_model(*meta.model.split('.'))\n        elif issubclass(getattr(meta, 'model', None), models.Model):\n            model_class = meta.model\n        else:\n            raise ImproperlyConfigured(\"Model informed by Meta subclass of %s is improperly!\" % name)\n\n        def remove_field(f_name):\n            # Removes the field form local fields list\n            model_class._meta.local_fields = [f for f in model_class._meta.local_fields\n                    if f.name != f_name]\n\n            # Removes the field setter if exists\n            if hasattr(model_class, f_name):\n                delattr(model_class, f_name)\n\n        # Removes fields setted in attribute 'exclude'\n        if isinstance(getattr(meta, 'exclude', None), (list,tuple)):\n            for f_name in meta.exclude:\n                remove_field(f_name)\n\n        # Calls 'contribute_to_class' from field to sender class\n        for f_name, field in attrs.items():\n            if isinstance(field, models.Field):\n                # Removes the field if it already exists\n                remove_field(f_name)\n\n                # Appends the new field to model class\n                field.contribute_to_class(model_class, f_name)\n\n        # Attaches methods\n        for m_name, func in attrs.items():\n            if callable(func) and type(func) == types.FunctionType:\n                setattr(model_class, m_name, func)\n\n        new_class._meta = meta\n\n        return new_class\n\nclass ModifiedModel(object):\n    \"\"\"\n    Make your inheritance from this class and set a Meta subclass with attribute\n    'model' with the model class you want to modify: add/replace/exclude fields\n    and/or add/replace methods.\n    \"\"\"\n    __metaclass__ = ModifiedModelMetaclass\n\n"}
{"text": "import RPi.GPIO as GPIO\nimport time\nimport curses\n\n\n#added motor enable to pi\n#pin 16 ->enable\n#pin 18 ->enable\nclass NiksRobot:\n\tdef __init__(self):\n\t\t#setup\n\t\tGPIO.setmode(GPIO.BOARD)\n\t\tGPIO.setup(7,GPIO.OUT)\n\t\tGPIO.setup(11,GPIO.OUT)\n\t\tGPIO.setup(13,GPIO.OUT)\n\t\tGPIO.setup(15,GPIO.OUT)\n\n\t\t#GPIO.setup(16,GPIO.OUT)\n\t\t#GPIO.setup(18,GPIO.OUT)\n\t\t#pwm0=GPIO.PWM(16,100)\n\t\t#pwm1=GPIO.PWM(18,100)\n\t\tprint 'setup complete'\n\tdef forward(self):#, dutyCycle):\n\t\tGPIO.output(7,True)\n\t\tGPIO.output(11,False)\n\t\tGPIO.output(15,True)\n\t\tGPIO.output(13,False)\n\t\t#pwm0.start(dutyCycle)\n\t\t#pwm1.start(dutyCycle)\n\t\t#GPIO.output(16,True)\n                #GPIO.output(18,True)\n\tdef backward(self):\n\t\tGPIO.output(7,False)\n\t\tGPIO.output(11,True)\n\t\tGPIO.output(15,False)\n\t\tGPIO.output(13,True)\n        def right(self):\n                GPIO.output(7,False)\n                GPIO.output(11,True)\n                GPIO.output(15,True)\n                GPIO.output(13,False)\n        def left(self):\n                GPIO.output(7,True)\n                GPIO.output(11,False)\n                GPIO.output(15,False)\n                GPIO.output(13,True)\n        def stop(self):\n                GPIO.output(7,False)\n                GPIO.output(11,False)\n                GPIO.output(15,False)\n                GPIO.output(13,False)\n               \t#GPIO.output(16,False)\n                #GPIO.output(18,False)\n                #pwm0.stop()\n                #pwm1.stop()\n\tdef cleanup(self):\n\t\tGPIO.cleanup()\n\t\t#pwm0.stop()\n\t\t#pwm1.stop()\n\nnr = NiksRobot()\nscreen = curses.initscr()\ncurses.noecho() \ncurses.cbreak()\nscreen.keypad(True)\n\ntry:\n        while True:   \n            char = screen.getch()\n            if char == ord('q'):\n                break\n            elif char == curses.KEY_UP:\n\t\tnr.forward()\n            elif char == curses.KEY_DOWN:\n                nr.backward()\n            elif char == curses.KEY_RIGHT:\n                nr.right()\n            elif char == curses.KEY_LEFT:\n                nr.left()\n            elif char == ord('p'):\n                nr.stop()\n             \nfinally:\n    #Close down curses properly, inc turn echo back on!\n    curses.nocbreak(); screen.keypad(0); curses.echo()\n    curses.endwin()\n    nr.cleanup()\n    \n\n"}
{"text": "import argparse\nimport sys\n\nfrom openpyxl import load_workbook\n\nimport config\nfrom sample import Sample\nfrom family import update_family, family_ped\n\n# CLI\n\nparser = argparse.ArgumentParser(description=\"Convert OGT xlsx to PED file\")\n\nparser.add_argument(\"orderform\", \n    help=\"OGT order form with sample ID, status and family groups.\")\nparser.add_argument(\"outfile\", help=\"Output PED file\", nargs='?')\nparser.add_argument(\"-D\", \"--debug\", help=\"Enable DEBUG output.\", \n                    action=\"store_true\")\n\nargs = parser.parse_args()\n\nif config.debug:\n    print(sys.stderr, \"DEBUG output turned on.\")\n    config.debug = True\n\nconfig.outfile = args.outfile\n\n# Truncate output ped file\n\nif config.outfile is not None:\n    out = open(config.outfile, 'w')\nelse:\n    out = sys.stdout\n\n# Open workbook\n\nwb = load_workbook(filename = args.orderform)\nws = wb.active\n\n# Sanity checks\n\nif ws.title != \"material form\": \n    print(sys.stderr, \"WARNING: Non standard active sheet name \", ws.title)\n\nif (ws['B12'].value != \"Customer Sample ID\" \n        or ws['M12'].value != \"Additional Experimental Comments\" \n        or ws['C12'].value != \"Source (cells, tissue etc)\"):\n    print(sys.stderr, (\"Unexpected table / cell layout: check to see\"\n        \"that sheet is ok, and ask to have the script updated.\"))\n\n    exit(1)\n\n# Main\n# Iterate over all rows, parse row blocks \nin_sample_section = False\nin_family = False\n\nmax_rows = 1024\n\nsamples_found = 0\nfamily = []\nfamily_count = 0 \n\nfor rownum in range(1,max_rows+1):\n    cell=ws[\"B\" + str(rownum)]\n\n    if not in_sample_section:\n        if cell.value == \"Customer Sample ID\":\n            if config.debug:\n                print(sys.stderr, \"Found sample ID tag.\")\n            in_sample_section = True\n    else:\n        if cell.value is not None:\n            # Found a new sample row.\n            sample_id = cell.value\n            sample_id.rstrip()\n\n            if not in_family:\n                if config.debug:\n                    print(sys.stderr, (\"New family, starting with sample \"\n                                         \"'{}'\").format(sample_id))\n                family_count += 1\n\n            info_cell = ws[\"M\" + str(rownum)]\n            info = info_cell.value\n            if info is None:\n                info = \"NA\"\n            info.rstrip()\n\n            tissue_cell =  ws[\"C\" + str(rownum)]\n            tissue = tissue_cell.value\n            if tissue is None:\n                tissue = \"NA\"\n            tissue.rstrip()\n\n            sample = Sample(sample_id, info, tissue)\n            in_family = True\n            family.append(sample)\n            \n            if sample.info.find(\"singleton\") != -1:          \n                # Found a singleton!\n                sample.affected = True\n                update_family(family)\n                print >> out, family_ped(family, family_count).rstrip()\n                # This ends the current family.\n                if config.debug:\n                    print(sys.stderr, \"Found a singleton. Family complete.\")\n                family = []\n                in_family = False\n                # Note that the next row may be a None or a new family member..\n\n            samples_found += 1\n\n        elif cell.value is None: \n            # Value None means an empty row.\n            if in_family:\n                # This ends the current family.\n                if config.debug:\n                    print(sys.stderr, \"Family complete.\")\n\n                update_family(family)\n                print >> out, family_ped(family, family_count).rstrip()\n\n                family = []\n                in_family = False\n"}
{"text": "## pythonFlu - Python wrapping for OpenFOAM C++ API\n## Copyright (C) 2010- Alexey Petrov\n## Copyright (C) 2009-2010 Pebble Bed Modular Reactor (Pty) Limited (PBMR)\n## \n## This program is free software: you can redistribute it and/or modify\n## it under the terms of the GNU General Public License as published by\n## the Free Software Foundation, either version 3 of the License, or\n## (at your option) any later version.\n##\n## This program is distributed in the hope that it will be useful,\n## but WITHOUT ANY WARRANTY; without even the implied warranty of\n## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n## GNU General Public License for more details.\n## \n## You should have received a copy of the GNU General Public License\n## along with this program.  If not, see <http://www.gnu.org/licenses/>.\n## \n## See http://sourceforge.net/projects/pythonflu\n##\n## Author : Alexey PETROV, Andrey SIMURZIN\n##\n\n\n#--------------------------------------------------------------------------------------\nattr2interface={ 'explicitSolve' : 'Foam.src.finiteVolume.fvMatrices.solvers.MULES.MULES.MULES_explicitSolve' }\n                 \n\n"}
{"text": "from functools import wraps\nfrom hyperspy.component import Component\n\n_CLASS_DOC = \\\n    \"\"\"%s component (created with Expression).\n\n.. math::\n\n    f(x) = %s\n\n\"\"\"\n\n\ndef _fill_function_args(fn):\n    @wraps(fn)\n    def fn_wrapped(self, x):\n        return fn(x, *[p.value for p in self.parameters])\n\n    return fn_wrapped\n\n\nclass Expression(Component):\n\n    def __init__(self, expression, name, position=None, module=\"numpy\",\n                 autodoc=True, **kwargs):\n        \"\"\"Create a component from a string expression.\n\n        It automatically generates the partial derivatives and the\n        class docstring.\n\n        Parameters\n        ----------\n        expression: str\n            Component function in SymPy text expression format. See the SymPy\n            documentation for details. The only additional constraint is that\n            the variable must be `x`. Also, in `module` is \"numexpr\" the\n            functions are limited to those that numexpr support. See its\n            documentation for details.\n        name : str\n            Name of the component.\n        position: str, optional\n            The parameter name that defines the position of the component if\n            applicable. It enables adjusting the position of the component\n            interactively in a model.\n        module: {\"numpy\", \"numexpr\"}, default \"numpy\"\n            Module used to evaluate the function. numexpr is often faster but\n            it supports less functions.\n\n        **kwargs\n             Keyword arguments can be used to initialise the value of the\n             parameters.\n\n        Methods\n        -------\n        recompile: useful to recompile the function and gradient with a\n            a different module.\n\n        Examples\n        --------\n\n        The following creates a Gaussian component and set the initial value\n        of the parameters:\n\n        >>> hs.model.components.Expression(\n        ... expression=\"height * exp(-(x - x0) ** 2 * 4 * log(2)/ fwhm ** 2)\",\n        ... name=\"Gaussian\",\n        ... height=1,\n        ... fwhm=1,\n        ... x0=0,\n        ... position=\"x0\",)\n\n        \"\"\"\n\n        import sympy\n        self._str_expression = expression\n        self.compile_function(module=module)\n        # Initialise component\n        Component.__init__(self, self._parameter_strings)\n        self._whitelist['expression'] = ('init', expression)\n        self._whitelist['name'] = ('init', name)\n        self._whitelist['position'] = ('init', position)\n        self._whitelist['module'] = ('init', module)\n        self.name = name\n        # Set the position parameter\n        if position:\n            self._position = getattr(self, position)\n        # Set the initial value of the parameters\n        if kwargs:\n            for kwarg, value in kwargs.items():\n                setattr(getattr(self, kwarg), 'value', value)\n\n        if autodoc:\n            self.__doc__ = _CLASS_DOC % (\n                name, sympy.latex(sympy.sympify(expression)))\n\n    def function(self, x):\n        return self._f(x, *[p.value for p in self.parameters])\n\n    def compile_function(self, module=\"numpy\"):\n        import sympy\n        from sympy.utilities.lambdify import lambdify\n        expr = sympy.sympify(self._str_expression)\n\n        rvars = sympy.symbols([s.name for s in expr.free_symbols], real=True)\n        real_expr = expr.subs(\n            {orig: real_ for (orig, real_) in zip(expr.free_symbols, rvars)})\n        # just replace with the assumption that all our variables are real\n        expr = real_expr\n\n        eval_expr = expr.evalf()\n        # Extract parameters\n        parameters = [\n            symbol for symbol in expr.free_symbols if symbol.name != \"x\"]\n        parameters.sort(key=lambda x: x.name)  # to have a reliable order\n        # Extract x\n        x, = [symbol for symbol in expr.free_symbols if symbol.name == \"x\"]\n        # Create compiled function\n        self._f = lambdify([x] + parameters, eval_expr,\n                           modules=module, dummify=False)\n        parnames = [symbol.name for symbol in parameters]\n        self._parameter_strings = parnames\n        for parameter in parameters:\n            grad_expr = sympy.diff(eval_expr, parameter)\n            setattr(self,\n                    \"_f_grad_%s\" % parameter.name,\n                    lambdify([x] + parameters,\n                             grad_expr.evalf(),\n                             modules=module,\n                             dummify=False)\n                    )\n\n            setattr(self,\n                    \"grad_%s\" % parameter.name,\n                    _fill_function_args(\n                        getattr(\n                            self,\n                            \"_f_grad_%s\" %\n                            parameter.name)).__get__(\n                        self,\n                        Expression)\n                    )\n"}
{"text": "import os\nfrom boxbranding import getImageVersion\n\ndef enumFeeds():\n\tfor fn in os.listdir('/etc/opkg'):\n\t\tif fn.endswith('-feed.conf'):\n\t\t\tfile = open(os.path.join('/etc/opkg', fn))\n\t\t\tfeedfile = file.readlines()\n\t\t\tfile.close()\n\t\t\ttry:\n\t\t\t\tfor feed in feedfile:\n\t\t\t\t\tyield feed.split()[1]\n\t\t\texcept IndexError:\n\t\t\t\tpass\n\t\t\texcept IOError:\n\t\t\t\tpass\n\ndef enumPlugins(filter_start=''):\n\tlist_dir = listsDirPath()\n\tfor feed in enumFeeds():\n\t\tpackage = None\n\t\ttry:\n\t\t\tfor line in open(os.path.join(list_dir, feed), 'r'):\n\t\t\t\tif line.startswith('Package:'):\n\t\t\t\t\tpackage = line.split(\":\",1)[1].strip()\n\t\t\t\t\tversion = ''\n\t\t\t\t\tdescription = ''\n\t\t\t\t\tif package.startswith(filter_start) and not package.endswith('-dev') and not package.endswith('-staticdev') and not package.endswith('-dbg') and not package.endswith('-doc') and not package.endswith('-src'):\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tpackage = None\n\t\t\t\tif package is None:\n\t\t\t\t\tcontinue\n\t\t\t\tif line.startswith('Version:'):\n\t\t\t\t\tversion = line.split(\":\",1)[1].strip()\n\t\t\t\telif line.startswith('Description:'):\n\t\t\t\t\tdescription = line.split(\":\",1)[1].strip()\n\t\t\t\telif description and line.startswith(' '):\n\t\t\t\t\tdescription += line[:-1]\n\t\t\t\telif len(line) <= 1:\n\t\t\t\t\td = description.split(' ',3)\n\t\t\t\t\tif len(d) > 3:\n\t\t\t\t\t\t# Get rid of annoying \"version\" and package repeating strings\n\t\t\t\t\t\tif d[1] == 'version':\n\t\t\t\t\t\t\tdescription = d[3]\n\t\t\t\t\t\tif description.startswith('gitAUTOINC'):\n\t\t\t\t\t\t\tdescription = description.split(' ',1)[1]\n\t\t\t\t\tyield package, version, description.strip()\n\t\t\t\t\tpackage = None\n\t\texcept IOError:\n\t\t\tpass\n\ndef listsDirPath():\n        try:\n                for line in open('/etc/opkg/opkg.conf', \"r\"):\n                        if line.startswith('option'):\n                                line = line.split(' ', 2)\n                                if len(line) > 2 and line[1] == ('lists_dir'):\n                                        return line[2].strip()\n                        elif line.startswith('lists_dir'):\n                                return line.replace('\\n','').split(' ')[2]\n        except Exception, ex:\n                print \"[opkg]\", ex\n        return '/var/lib/opkg/lists'\n\nif __name__ == '__main__':\n\tfor p in enumPlugins('enigma'):\n\t\tprint p\n"}
{"text": "import os\nfrom netCDF4 import Dataset as NCDataset\nfrom dateutil import parser as dateparser\nfrom datetime import datetime\n\nfrom birdfeeder.utils import humanize_filesize\n\nimport logging\nlogger = logging.getLogger(__name__)\n\nSPATIAL_VARIABLES =  [\n    'longitude', 'lon',\n    'latitude', 'lat',\n    'altitude', 'alt', 'level', 'height',\n    'rotated_pole',\n    'rotated_latitude_longitude',\n    'time']\n\n\nclass Dataset(object):\n    def __init__(self, filepath, basedir='/'):\n        self.filepath = filepath\n        self.path = os.path.sep + os.path.relpath(filepath, basedir)\n        self.bytes = os.path.getsize(filepath)\n        self.size = humanize_filesize(self.bytes) \n        self.name = os.path.basename(filepath)\n        self.url = 'file://' + filepath\n        self.content_type = 'application/netcdf'\n        self.resourcename = filepath\n        self._last_modified = None\n        self.attributes = {}\n        self._parse(filepath)\n\n    def __str__(self):\n        return \"attributes={0}\".format(self.attributes)\n\n    @property\n    def last_modified(self):\n        if self._last_modified is None:\n            mtime = os.path.getmtime(self.filepath)\n            self._last_modified = datetime.fromtimestamp(mtime).strftime('%Y-%m-%dT%H:%M:%SZ')\n        return self._last_modified\n\n    @property\n    def variable(self):\n        return self.attributes.get('variable')\n\n    @property\n    def variable_long_name(self):\n        return self.attributes.get('variable_long_name')\n\n    @property\n    def cf_standard_name(self):\n        return self.attributes.get('cf_standard_name')\n\n    @property\n    def units(self):\n        return self.attributes.get('units')\n\n    @property\n    def comments(self):\n        return self.attributes.get('comments')\n\n    @property\n    def institute(self):\n        return self.attributes.get('institute_id')\n\n    @property\n    def experiment(self):\n        return self.attributes.get('experiment_id')\n\n    @property\n    def project(self):\n        return self.attributes.get('project_id')\n\n    @property\n    def model(self):\n        return self.attributes.get('model_id')\n    \n    @property\n    def frequency(self):\n        return self.attributes.get('frequency')\n\n    @property\n    def creation_date(self):\n        if 'creation_date' in self.attributes:\n            return self.attributes['creation_date'][0]\n        else:\n            return None\n\n    def _add_attribute(self, key, value):\n        if not key in self.attributes:\n            self.attributes[key] = [] \n        self.attributes[key].append(value)\n\n    def _parse(self, filepath):\n        filepath = os.path.abspath(filepath)\n        logger.debug(\"parse %s\", filepath)\n\n        try:\n            ds = NCDataset(filepath, 'r')\n\n            # loop over global attributes\n            for attname in ds.ncattrs():\n                attvalue = getattr(ds, attname)\n                if 'date' in attname.lower():\n                    # must format dates in Solr format, if possible\n                    try:\n                        solr_dt = dateparser.parse(attvalue)\n                        self._add_attribute(attname, solr_dt.strftime('%Y-%m-%dT%H:%M:%SZ') )\n                    except:\n                        pass # disregard this attribute\n                else:\n                    self._add_attribute(attname, attvalue)\n\n            # loop over dimensions\n            for key, dim in ds.dimensions.items():\n                self._add_attribute('dimension', \"%s:%s\" % (key, len(dim)) )\n\n            # loop over variable attributes\n            for key, variable in ds.variables.items():\n                if key.lower() in ds.dimensions:\n                    # skip dimension variables\n                    continue\n                if '_bnds' in key.lower():\n                    continue\n                if key.lower() in SPATIAL_VARIABLES:\n                    continue\n                self._add_attribute('variable', key)\n                self._add_attribute('variable_long_name', getattr(variable, 'long_name', None) )\n                cf_standard_name = getattr(variable, 'standard_name', None)\n                if cf_standard_name is not None:\n                    self._add_attribute('cf_standard_name', getattr(variable, 'standard_name', None) )\n                self._add_attribute('units', getattr(variable, 'units', None) )\n\n        except Exception as e:\n            logging.error(e)\n        finally:\n            try:\n                ds.close()\n            except:\n                pass\n\n \ndef crawl(start_dir):\n    if not os.path.isdir(start_dir):\n        raise Exception(\"Invalid start directory: %s\", start_dir)\n\n    logger.info('start directory = %s', start_dir)\n\n    for directory, subdirs, files in os.walk(start_dir):\n        # loop over files in this directory\n        for filename in files:\n            # only parse .nc files\n            if filename.endswith('.nc'):\n                filepath = os.path.join(directory, filename)\n                yield Dataset(filepath, basedir=start_dir)\n\n\n        \n            \n\n\n   \n\n    \n        \n\n\n"}
{"text": "from setuptools import setup\nimport os\n\n\nhere = os.path.abspath(os.path.dirname(__file__))\nREADME = open(os.path.join(here, 'README.rst')).read()\nHISTORY = open(os.path.join(here, 'HISTORY.rst')).read()\n\n\nversion = \"1.2.1.dev0\"\n\n\nsetup(\n    version=version,\n    description=\"Plugin for ploy to provision virtual machines using VirtualBox.\",\n    long_description=README + \"\\n\\n\" + HISTORY,\n    name=\"ploy_virtualbox\",\n    author='Florian Schulze',\n    author_email='florian.schulze@gmx.net',\n    license=\"BSD 3-Clause License\",\n    url='http://github.com/ployground/ploy_virtualbox',\n    classifiers=[\n        'Environment :: Console',\n        'Intended Audience :: System Administrators',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3.3',\n        'Topic :: System :: Installation/Setup',\n        'Topic :: System :: Systems Administration'],\n    include_package_data=True,\n    zip_safe=False,\n    packages=['ploy_virtualbox'],\n    install_requires=[\n        'setuptools',\n        'ploy >= 1.2.0, < 2dev',\n        'lazy'],\n    entry_points=\"\"\"\n        [ploy.plugins]\n        virtualbox = ploy_virtualbox:plugin\n    \"\"\")\n"}
{"text": "import os\nimport math\nimport numpy as np\nfrom PlummerGalaxy import PlummerGalaxy\nfrom InitialConditions import InitialConditions\nimport imp\ntry:\n    imp.find_module('matplotlib')\n    matplotlibAvailable = True\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D\nexcept ImportError:\n    matplotlibAvailable = False\n\n\n# Settings:\n\nInitialConditionsFolder = \"data/initialconditions/\"\nOutputResultsFolder = \"data/results/\"\n\nGravitationalConst = 1.0\n\nTotalNumPts = 6000\n\ncreateNewInitialConditions = True\n\nMakePositionsVideo = True\nMakeDistributionsVideo = False\n\nUseImageMagickForFancierVideo = False\n\n#=================================================================================\nif createNewInitialConditions:\n\t\n\tgalaxy1 = PlummerGalaxy()\n\tgalaxy1.npts = (TotalNumPts/2)\n\tgalaxy1.R = 1.0\n\tgalaxy1.ZeroVelocities_Bool = False\n\tgalaxy1.GenerateInitialConditions(-2.5, -2.5, 0)\n\t\n\tgalaxy2 =PlummerGalaxy()\n\tgalaxy2.npts = (TotalNumPts/2)\n\tgalaxy2.R = 1.0\n\tgalaxy2.ZeroVelocities_Bool = False\n\tgalaxy2.GenerateInitialConditions(2.5, 2.5, 0)\n\t\n\ttimeStep = 0.15\n\ttimeMax = 30.0\n\tepssqd = 0.1\n\t\n\tbothGalaxies = InitialConditions()\n\tbothGalaxies.extend(galaxy1)\n\tbothGalaxies.extend(galaxy2)\n\tAarsethHeader = str(TotalNumPts)+\"  0.05  0.15  30.0  0.10\\n\"\n\tAarsethHeader = str(TotalNumPts)+\" 0.01 \"+str(timeStep)+\" \"+str(timeMax)+\" \"+str(epssqd)+\" \"+str(GravitationalConst)+\"\\n\"\n\tbothGalaxies.WriteInitialConditionsToFile(InitialConditionsFolder+\"two_plummers_collision.data\", AarsethHeader)\n\t\n\tprint(\"compiling Aarseth c code...\")\n\tos.system(\"(cd Aarseth && make)\")\n\t\n\tprint(\"Running compiled Aarseth nbody code on Plummer initial conditions file\")\n\tos.system(\"./Aarseth/aarseth \"+InitialConditionsFolder+\"two_plummers_collision.data \"+OutputResultsFolder+\"out_aarseth_npts_\"+str(TotalNumPts)+\".data\")\n\n\nif matplotlibAvailable and (MakePositionsVideo or MakeDistributionsVideo):\n\tif MakePositionsVideo:\n\t\tMakeVideo(galaxyNumPts, OutputResultsFolder+\"out_aarseth_npts_\"+str(TotalNumPts)+\".data\", \"video_two_plummer_collision.avi\", True)\n\tif MakeDistributionsVideo:\n\t\tMakeVideo(galaxyNumPts, OutputResultsFolder+\"out_aarseth_npts_\"+str(TotalNumPts)+\".data\", \"video_two_plummer_collision_distributions.avi\", False)\n\n\n\n\n"}
{"text": "#!/usr/bin/env python\n# coding: utf-8\n\n#-------------------------------------------------------------------------------\n\nfrom numpy.random import uniform, normal\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\n#-------------------------------------------------------------------------------\n\nnx = 20\nnoise = 0.02\nniter = 60\nkmin, kmax = 0.3, 2.0\n\n#-------------------------------------------------------------------------------\n\nfrom numpy import exp\nf = lambda x,k: exp(-x*k)\nfk = lambda x,k: -x * exp(-x*k)\nfkk = lambda x,k: x**2 * exp(-x*k)\n\n#-------------------------------------------------------------------------------\n\nK0 = 1.0 # uniform(kmin,kmax)\nx = uniform(0,10,nx)\nD = f(x,K0) + normal(0, noise, nx)\n\n#-------------------------------------------------------------------------------\n\nfrom numpy import ndarray\nk = ndarray(niter)\nk[0] = 2.0 # uniform(kmin,kmax)\na = 1.0\nprint \"{:6} {:11} {:11} {:6} {:11}\".format(\"K\",\"Z\",\"a\",\"Knext\",\"K-K0\")\nfor i in range(1,niter):\n    dY = f(x, k[i-1]) - D\n    Z = sum(fk(x,k[i-1]) * dY)\n    # Zk = sum(fkk(x,k[i-1]) * dY + fk(x,k[i-1])**2)\n    # a = float(i) / niter\n    k[i] = k[i-1] - Z * a\n    print \"{:6.3f} {:11.3e} {:11.3e} {:6.3f} {:11.3e}\".format(k[i-1], Z, a, k[i], k[i] - K0)\n    # a = (k[i] - k[i-1]) / abs(sum(fk(x,k[i]) * dY) - Z)\n\nprint \"real K = {:.3f}\".format(K0)\n\n#-------------------------------------------------------------------------------\n\nfrom numpy import linspace\nk0 = linspace(kmin, kmax + 0.5 * (kmax - kmin), 256)\nE = ndarray(k0.shape[0])\nZ = ndarray(k0.shape[0])\nZk = ndarray(k0.shape[0])\nfor i in range(k0.shape[0]):\n    dY = f(x, k0[i]) - D\n    E[i] = sum(dY**2) / 2\n    Z[i] = sum(fk(x,k0[i]) * dY)\n    Zk[i] = sum(fkk(x,k0[i]) * dY + fk(x,k0[i])**2)\n\nplt.figure()\n\nax = plt.subplot(211)\nax.set_xlim(k0.min(),k0.max())\nax.plot(k0,E)\nax.set_yscale('log')\n\nax = plt.subplot(212)\nax.set_xlim(k0.min(),k0.max())\nax.plot(k0,Z,'-')\nax.plot(k0,Zk,'--')\n# plt.plot(k0,-Z/Zk)\nax.set_yscale('symlog', linthreshy = 0.1)\nfor i in range(niter):\n    ax.axvline(k[i], color = cm.inferno(1-float(i) / niter))\nax.axvline(K0, linewidth = 1.6, linestyle = '--', color = 'black')\nax.grid()\n\nplt.savefig('/tmp/optim2k.png')\n# plt.show()\n\n#-------------------------------------------------------------------------------\n#-------------------------------------------------------------------------------\n#-------------------------------------------------------------------------------\n#-------------------------------------------------------------------------------\n#-------------------------------------------------------------------------------\n#-------------------------------------------------------------------------------\n\n\nx0 = linspace(0,x.max(),200)\nplt.figure()\nplt.scatter(x,D)\nplt.plot(x0, f(x0,K0), color = '#9C9C9C')\nfor i in range(niter): plt.plot(x0, f(x0,k[i]), color = cm.inferno(1-float(i) / niter))\nplt.savefig('/tmp/optim2.png')\n\n#-------------------------------------------------------------------------------\n#-------------------------------------------------------------------------------\n"}
{"text": "# This file is part of Lerot.\n#\n# Lerot is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Lerot is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with Lerot.  If not, see <http://www.gnu.org/licenses/>.\n\nimport unittest\nimport cStringIO\nimport numpy as np\n\nimport lerot.query as query\nimport lerot.utils as utils\n\n\nclass TestUtils(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def testSplitArgStr(self):\n        split = utils.split_arg_str(\"--a 10 --b foo --c \\\"--d bar --e 42\\\"\")\n        self.assertEqual(split, [\"--a\", \"10\", \"--b\", \"foo\", \"--c\",\n            \"--d bar --e 42\"], \"wrong split (1): %s\" % \", \".join(split))\n        split = utils.split_arg_str(\"\\\"--a\\\" 10 --b foo --c --d bar --e 42\")\n        self.assertEqual(split, [\"--a\", \"10\", \"--b\", \"foo\", \"--c\", \"--d\",\n            \"bar\", \"--e\", \"42\"], \"wrong split (2): %s\" % \", \".join(split))\n        split = utils.split_arg_str(\"\\\"--a\\\"\\\" 10\\\"--b foo --c --d bar --e 42\")\n        self.assertEqual(split, [\"--a\", \" 10\", \"--b\", \"foo\", \"--c\", \"--d\",\n            \"bar\", \"--e\", \"42\"], \"wrong split (2): %s\" % \", \".join(split))\n\n    def testRank(self):\n        scores = [2.1, 2.9, 2.3, 2.3, 5.5]\n        self.assertIn(utils.rank(scores, ties=\"random\"),\n                      [[0, 3, 1, 2, 4], [0, 3, 2, 1, 4]])\n        self.assertIn(utils.rank(scores, reverse=True, ties=\"random\"),\n                      [[4, 1, 3, 2, 0], [4, 1, 2, 3, 0]])\n        self.assertEqual(utils.rank(scores, reverse=True, ties=\"first\"),\n                         [4, 1, 2, 3, 0])\n        self.assertEqual(utils.rank(scores, reverse=True, ties=\"last\"),\n                         [4, 1, 3, 2, 0])\n\n        scores = [2.1, 2.9, 2.3, 2.3, 5.5, 2.9]\n        self.assertIn(utils.rank(scores, ties=\"random\"),\n                      [[0, 4, 2, 1, 5, 3],\n                       [0, 3, 2, 1, 5, 4],\n                       [0, 4, 1, 2, 5, 3],\n                       [0, 3, 1, 2, 5, 4]])\n        self.assertIn(utils.rank(scores, reverse=True, ties=\"random\"),\n                      [[5, 1, 3, 4, 0, 2],\n                       [5, 2, 3, 4, 0, 1],\n                       [5, 1, 4, 3, 0, 2],\n                       [5, 2, 4, 3, 0, 1]])\n        self.assertEqual(utils.rank(scores, reverse=True, ties=\"first\"),\n                         [5, 1, 3, 4, 0, 2])\n        self.assertEqual(utils.rank(scores, reverse=True, ties=\"last\"),\n                         [5, 2, 4, 3, 0, 1])\n\n    def test_create_ranking_vector(self):\n        feature_count = 5\n        # Create queries to test with\n        test_queries = \"\"\"\n            1 qid:373 1:0.080000 2:0.500000 3:0.500000 4:0.500000 5:0.160000\n            0 qid:373 1:0.070000 2:0.180000 3:0.000000 4:0.250000 5:0.080000\n            0 qid:373 1:0.150000 2:0.016000 3:0.250000 4:0.250000 5:0.150000\n            0 qid:373 1:0.100000 2:0.250000 3:0.500000 4:0.750000 5:0.130000\n            0 qid:373 1:0.050000 2:0.080000 3:0.250000 4:0.250000 5:0.060000\n            0 qid:373 1:0.050000 2:1.000000 3:0.250000 4:0.250000 5:0.160000\n        \"\"\"\n        hard_gamma = [1, 0.63092975357, 0.5, 0.43067655807, 0.38685280723,\n                      0.3562071871]\n        hard_ranking_vector = [0.27938574, 1.11639191, 1.02610328, 1.29150486,\n                               0.42166665]\n        query_fh = cStringIO.StringIO(test_queries)\n        this_query = query.Queries(query_fh, feature_count)['373']\n        query_fh.close()\n        fake_ranking = sorted(this_query.get_docids())\n        # gamma, ranking_vector = utils.create_ranking_vector(\n        ranking_vector = utils.create_ranking_vector(\n            this_query, fake_ranking)\n        # self.assertEqual(len(gamma), len(hard_gamma))\n        self.assertEqual(feature_count, len(ranking_vector))\n        # for i in xrange(0, len(gamma)):\n        #     self.assertAlmostEqual(gamma[i], hard_gamma[i])\n\n        for j in xrange(0, feature_count):\n            self.assertAlmostEqual(ranking_vector[j], hard_ranking_vector[j])\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "# Creates a rough gold layer with a rough dielectric coating containing an\n# anisotropic scattering medium\n\nimport sys\nsys.path.append('.')\n\nfrom utils.materials import gold\nfrom utils.cie import get_rgb\n\nimport layerlab as ll\n\neta_top = 1.5\n\n# This step integrates the spectral IOR against the CIE XYZ curves to obtain\n# equivalent sRGB values. This may seem fairly approximate but turns out to\n# yield excellent agreement with spectral reference renders\nprint('Computing gold IOR parameters')\neta_bot = get_rgb(gold)\n\nalpha_top = 0.1  # Beckmann roughness of top layer (coating)\nalpha_bot = 0.1  # Beckmann roughness of bottom layer (gold)\n\n# Medium parameters\ng = 0.5  # Scattering anisotropy\nalbedo = [0.25, 0.0, 0.95]  # Single scattering albedo\ntau = 0.5  # Optical depth\n\n# Construct quadrature scheme suitable for the material\nn_top, m_top = ll.parameterHeuristicMicrofacet(eta=eta_top, alpha=alpha_top)\nn_bot, m_bot = ll.parameterHeuristicMicrofacet(eta=eta_bot[0], alpha=alpha_bot)\nn_med, m_med = ll.parameterHeuristicHG(g=g)\n\nn = max(n_top, n_bot)  # Max of zenith angle discretization\nm = m_top              # Number of Fourier orders determined by top layer\nmu, w = ll.quad.gaussLobatto(n)\nprint(\"# of nodes = %i, fourier orders = %i\" % (n, m))\n\n# Construct coating layer\nprint(\"Creating coating layer\")\ncoating = ll.Layer(mu, w, m)\ncoating.setMicrofacet(eta=eta_top, alpha=alpha_top)\n\noutput = []\nfor channel in range(3):\n    # Construct diffuse bottom layer for each channel\n    print(\"Creating metal layer\")\n    l = ll.Layer(mu, w, m)\n    l.setMicrofacet(eta=eta_bot[channel], alpha=alpha_bot)\n\n    # Construct medium layer\n    print(\"Creating medium layer\")\n    l2 = ll.Layer(mu, w, m)\n    l2.setHenyeyGreenstein(g=g, albedo=albedo[channel])\n    l2.expand(tau)\n\n    # Apply medium layer\n    print(\"Applying medium ..\")\n    l.addToTop(l2)\n\n    # Apply coating\n    print(\"Applying coating..\")\n    l.addToTop(coating)\n    output.append(l)\n\n# .. and write to disk\nprint(\"Writing to disk..\")\nstorage = ll.BSDFStorage.fromLayerRGB(\"output.bsdf\", *output)\nstorage.close()\n"}
{"text": "import unittest\nimport unittest.mock\n\nfrom GitManager.commands import state\nfrom GitManager.repo import description\nfrom GitManager.utils import format\nfrom GitManager.repo import implementation\n\n\nclass TestState(unittest.TestCase):\n    \"\"\" Tests that the state command works properly \"\"\"\n\n    @unittest.mock.patch(\n        'GitManager.repo.implementation.LocalRepository')\n    @unittest.mock.patch(\n        'builtins.print')\n    def test_run(self,\n                 builtins_print: unittest.mock.Mock,\n                 implementation_LocalRepository: unittest.mock.Mock):\n        # create a repository\n        repo = description.RepositoryDescription('/path/to/source',\n                                                 '/path/to/clone')\n\n        # create a line\n        line = format.TerminalLine()\n\n        # and a command instance\n        cmd = state.State(line, [repo], \"--no-update\")\n\n        # if we are up-to-date, nothing should have been printed\n        implementation_LocalRepository.return_value.exists.return_value = True\n        implementation_LocalRepository.return_value.remote_status \\\n            .return_value = implementation.RemoteStatus.UP_TO_DATE\n        self.assertTrue(cmd.run(repo))\n        implementation_LocalRepository.return_value.remote_status \\\n            .assert_called_with(False)\n        builtins_print.assert_not_called()\n\n        # reset the mock\n        implementation_LocalRepository.reset_mock()\n        builtins_print.reset_mock()\n\n        # create another command instance\n        cmd = state.State(line, [repo], \"--update\")\n\n        # if the local repository does not exist, we\n        implementation_LocalRepository.return_value.exists.return_value = False\n        self.assertFalse(cmd.run(repo))\n\n        # reset the mock\n        implementation_LocalRepository.reset_mock()\n        builtins_print.reset_mock()\n\n        # if we are up-to-date, nothing should have been printed\n        implementation_LocalRepository.return_value.exists.return_value = True\n        implementation_LocalRepository.return_value.remote_status \\\n            .return_value = implementation.RemoteStatus.UP_TO_DATE\n        self.assertTrue(cmd.run(repo))\n        implementation_LocalRepository.return_value.remote_status\\\n            .assert_called_with(True)\n        builtins_print.assert_not_called()\n\n        # reset the mock\n        implementation_LocalRepository.reset_mock()\n        builtins_print.reset_mock()\n\n        # we need to pull\n        implementation_LocalRepository.return_value.exists.return_value = True\n        implementation_LocalRepository.return_value.remote_status \\\n            .return_value = implementation.RemoteStatus.REMOTE_NEWER\n        self.assertFalse(cmd.run(repo))\n        implementation_LocalRepository.return_value.remote_status \\\n            .assert_called_with(True)\n        builtins_print.assert_called_with(\n            format.Format.yellow('Upstream is ahead of your branch, '\n                                 'pull required. '))\n\n        # reset the mock\n        implementation_LocalRepository.reset_mock()\n        builtins_print.reset_mock()\n\n        # we need to push\n        implementation_LocalRepository.return_value.exists.return_value = True\n        implementation_LocalRepository.return_value.remote_status \\\n            .return_value = implementation.RemoteStatus.LOCAL_NEWER\n        self.assertFalse(cmd.run(repo))\n        implementation_LocalRepository.return_value.remote_status \\\n            .assert_called_with(True)\n        builtins_print.assert_called_with(\n            format.Format.green('Your branch is ahead of upstream, '\n                                'push required.'))\n\n        # reset the mock\n        implementation_LocalRepository.reset_mock()\n        builtins_print.reset_mock()\n\n        # divergence\n        implementation_LocalRepository.return_value.exists.return_value = True\n        implementation_LocalRepository.return_value.remote_status \\\n            .return_value = implementation.RemoteStatus.DIVERGENCE\n        self.assertFalse(cmd.run(repo))\n        implementation_LocalRepository.return_value.remote_status \\\n            .assert_called_with(True)\n        builtins_print.assert_called_with(\n            format.Format.red('Your branch and upstream have diverged, '\n                              'merge or rebase required. '))\n"}
{"text": "__author__ = 'User'\nfrom django.conf.urls import url\nfrom . import views\n\nurlpatterns = [\n    url(r'^myfollowers/$', views.beingFollowedByView, name=\"my_followers\"),\n    url(r'^circle/new/$', views.CreateCircleView.as_view(), name=\"new_circle\"),\n    url(r'^circle/(?P<slug>[0-9]+)/$', views.circleDetails, name=\"circle_details\"),\n    url(r'^circle/(?P<circle_id>[0-9]+)/addusers/$', views.add_users_to_circle, name=\"add_users_to_circle\"),\n    url(r'^circle/(?P<user_id>[0-9]+)/adduser/$', views.add_user_to_circles, name=\"add_user_to_circles\"),\n    url(r'^circle/(?P<user_id>[0-9]+)/(?P<circle_id>[0-9]+)/removeuser/$', views.remove_user_from_circle, name=\"remove_user_from_circle\"),\n    url(r'^circle/(?P<slug>[0-9]+)/delete/$', views.RemoveCircleView, name=\"delete_circle\"),\n    url(r'^circles/$', views.CircleOverviewView.as_view(), name=\"circle_overview\"),\n    url(r'^follows/$', views.listfollows, name=\"list_follows\"),\n    url(r'^circlemessage/new/$', views.postCirclemessage, name=\"new_circlemessage\"),\n    url(r'^circlemessage/(?P<message_id>[0-9]+)/delete/$', views.delete_circle_message, name=\"delete_circlemessage\"),\n    url(r'^follow/(?P<user_id>[0-9]+)/$', views.follow, name=\"follow\"),\n    url(r'^unfollow/(?P<user_id>[0-9]+)/$', views.unfollow, name=\"unfollow\"),\n    # new since sprint 4\n    url(r'^are_there_new_notifications/$', views.information_about_new_directmessages, name=\"notification_polling\"),\n    url(r'^circle/message/(?P<slug>[0-9]+)/$', views.PostDetailsView.as_view(), name=\"one_circlemessage\"),\n    #url(r'^circle/message/(?P<message_id>[0-9]+)/answer$', views.answer_to_circlemessage, name=\"answer_circlemessage\"),\n    url(r'^circle/message/(?P<message_id>[0-9]+)/repost$', views.repost, name=\"repost_circlemessage\"),\n    url(r'^chat/(?P<sender_id>[a-zA-Z0-9]+)/$', views.direct_messages_details, name=\"chat\"),\n    url(r'^chat/(?P<username>[a-zA-Z0-9]+)/poll/json$', views.chat_polling, name=\"chat_polling\"),\n    url(r'^chats/$', views.direct_messages_overview, name=\"all_chats\"),\n    url(r'^search/user/(?P<query>[a-zA-Z0-9]+)/json$', views.search_user_json, name=\"search_user_json\"),\n    url(r'^search/theme/(?P<query>[a-zA-Z0-9]+)/json$', views.search_theme_json, name=\"search_theme_json\"),\n    url(r'^search/theme/(?P<theme>[a-zA-Z0-9]+)', views.showPostsToTheTheme, name=\"search_theme\"),\n    #new sind sprint 5\n    url(r'^circlemessage/new/json$', views.postCirclemessage_json, name=\"new_circlemessage_json\"),\n    url(r'^circlemessage/getall/json$', views.get_all_circlemessages_json, name=\"get_circlemessages_json\"),\n]\n"}
{"text": "\"\"\"\nCopyright (C) 2014, \u7533\u745e\u73c9 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"\n\n\ndef epsilon(config, problem):\n    if type(problem).__name__ == 'DTLZ1':\n        table = {\n            3: 0.033,\n            4: 0.052,\n            5: 0.059,\n            6: 0.0554,\n            8: 0.0549,\n            10: 0.0565,\n        }\n        _epsilon = table[problem.GetNumberOfObjectives()]\n        epsilon = [_epsilon] * problem.GetNumberOfObjectives()\n        return [epsilon]\n    elif type(problem).__name__ == 'DTLZ2':\n        table = {\n            2: 0.006,\n            3: 0.06,\n            4: 0.1312,\n            5: 0.1927,\n            6: 0.234,\n            8: 0.29,\n            10: 0.308,\n        }\n        _epsilon = table[problem.GetNumberOfObjectives()]\n        epsilon = [_epsilon] * problem.GetNumberOfObjectives()\n        return [epsilon]\n    elif type(problem).__name__ == 'DTLZ3':\n        table = {\n            3: 0.06,\n            4: 0.1385,\n            5: 0.2,\n            6: 0.227,\n            8: 0.1567,\n            10: 0.85,\n        }\n        _epsilon = table[problem.GetNumberOfObjectives()]\n        epsilon = [_epsilon] * problem.GetNumberOfObjectives()\n        return [epsilon]\n    elif type(problem).__name__ == 'DTLZ4':\n        table = {\n            3: 0.06,\n            4: 0.1312,\n            5: 0.1927,\n            6: 0.234,\n            8: 0.29,\n            10: 0.308,\n        }\n        _epsilon = table[problem.GetNumberOfObjectives()]\n        epsilon = [_epsilon] * problem.GetNumberOfObjectives()\n        return [epsilon]\n    elif type(problem).__name__ == 'DTLZ5':\n        table = {\n            3: 0.0052,\n            4: 0.042,\n            5: 0.0785,\n            6: 0.11,\n            8: 0.1272,\n            10: 0.1288,\n        }\n        _epsilon = table[problem.GetNumberOfObjectives()]\n        epsilon = [_epsilon] * problem.GetNumberOfObjectives()\n        return [epsilon]\n    elif type(problem).__name__ == 'DTLZ6':\n        table = {\n            3: 0.0227,\n            4: 0.12,\n            5: 0.3552,\n            6: 0.75,\n            8: 1.15,\n            10: 1.45,\n        }\n        _epsilon = table[problem.GetNumberOfObjectives()]\n        epsilon = [_epsilon] * problem.GetNumberOfObjectives()\n        return [epsilon]\n    elif type(problem).__name__ == 'DTLZ7':\n        table = {\n            2: 0.005,\n            3: 0.048,\n            4: 0.105,\n            5: 0.158,\n            6: 0.15,\n            8: 0.225,\n            10: 0.46,\n        }\n        _epsilon = table[problem.GetNumberOfObjectives()]\n        epsilon = [_epsilon] * problem.GetNumberOfObjectives()\n        return [epsilon]\n    elif type(problem).__name__ == 'ConvexDTLZ2':\n        table = {\n            2: 0.0075,\n            3: 0.035,\n            4: 0.039,\n            5: 0.034,\n            6: 0.0273,\n            8: 0.0184,\n            10: 0.0153,\n        }\n        _epsilon = table[problem.GetNumberOfObjectives()]\n        epsilon = [_epsilon] * problem.GetNumberOfObjectives()\n        return [epsilon]\n    elif type(problem).__name__ == 'DTLZ5I':\n        if problem.GetNumberOfObjectives() == 10:\n            table = {\n                3: 0.06,\n                4: 0.12,\n                5: 0.16,\n                6: 0.2,\n                7: 0.24,\n                8: 0.25,\n                9: 0.26,\n            }\n            _epsilon = table[problem.GetManifold() + 1]\n            epsilon = [_epsilon] * problem.GetNumberOfObjectives()\n            return [epsilon]\n    raise Exception(type(problem).__name__, problem.GetNumberOfObjectives())\n"}
{"text": "#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n# vim: ts=4 sw=4 expandtab:\r\n\r\n\"\"\"\r\n\u5b8c\u6210\u4f5c\u4e1a\u4ee5\u540e\uff0c\u8fd0\u884c\u672c\u6587\u4ef6\uff0c\u5b83\u4f1a\u5c06\u4f60\u7684\u4ee3\u7801\u3001\u5b9e\u9a8c\u62a5\u544a\u4e00\u8d77\u6253\u5305\u4e3a submit.zip\u3002\r\n\"\"\"\r\n\r\nfrom __future__ import print_function # Requires Python 2.7+\r\n\r\nimport locale\r\nimport os\r\nimport re\r\nimport sys\r\nimport zipfile\r\n\r\n\r\n# Python3's input == Python2's raw_input\r\ntry:\r\n    input_compat = raw_input\r\nexcept NameError:\r\n    input_compat = input\r\n\r\n\r\ndef S(s):\r\n    # F*** systems that still refuse to use UTF-8 by default in the 21st century\r\n    if 'decode' in dir(s):\r\n        return s.decode('utf-8').encode(locale.getpreferredencoding()) # Python 2 is naughty\r\n    else:\r\n        return s # Python 3 is good\r\n    # FIXME: Can anybody please tell me whether there is a simpler way to make Chinese\r\n    # characters display correctly in both Python 2 and 3, and in all three major OSes\r\n    # (Win/Lin/Mac)?\r\n\r\n\r\ndef main():\r\n\r\n    # Preparations\r\n    locale.setlocale(locale.LC_ALL, '')\r\n\r\n    # Check whether decaf.jar exists\r\n    decaf_jar = os.path.join('result', 'decaf.jar')\r\n    if not os.path.exists(decaf_jar):\r\n        print(S('\u672a\u627e\u5230 decaf.jar \u6587\u4ef6\u3002\u8bf7\u91cd\u65b0\u7f16\u8bd1\u3002'), file=sys.stderr)\r\n        return 1\r\n    print(S('\u5df2\u627e\u5230 {}'.format(decaf_jar)))\r\n\r\n    # Check whether report exists\r\n    for report_file in ['report.txt', 'report.doc', 'report.docx', 'report.pdf', 'report.odf']:\r\n        if os.path.exists(report_file):\r\n            break\r\n    else:\r\n        print(S('\u672a\u627e\u5230\u5b9e\u9a8c\u62a5\u544a\u3002\u8bf7\u786e\u8ba4\u5b83\u7684\u6587\u4ef6\u540d\u6b63\u786e (report.txt/doc/docx/pdf/odf)\u3002'), file=sys.stderr)\r\n        return 1\r\n    print(S('\u5df2\u627e\u5230\u5b9e\u9a8c\u62a5\u544a {}'.format(report_file)))\r\n\r\n    # Ask for E-mail\r\n    email = input_compat(S('\u8bf7\u8f93\u5165\u60a8\u7684 Email: '))\r\n    while not re.match(r'[^@]+@\\w+\\.[\\w.]+', email):\r\n        email = input_compat(S('Email \u683c\u5f0f\u4e0d\u6b63\u786e\uff0c\u8bf7\u91cd\u65b0\u8f93\u5165: '))\r\n\r\n    # Creating submit.zip\r\n    with zipfile.ZipFile('submit.zip', 'w') as submit_zip:\r\n        submit_zip.write(decaf_jar, 'decaf.jar', zipfile.ZIP_STORED)\r\n        submit_zip.write(report_file, report_file, zipfile.ZIP_DEFLATED)\r\n        submit_zip.writestr('email.txt', email.encode('utf-8'), zipfile.ZIP_STORED)\r\n\r\n    # Finished\r\n    print(S('\u5b8c\u6210\u3002\u8bf7\u5c06 submit.zip \u6587\u4ef6\u4e0a\u4f20\u5230\u7f51\u7edc\u5b66\u5802\u3002'))\r\n    return 0\r\n\r\n\r\nif __name__ == '__main__':\r\n    retcode = main()\r\n    if os.name == 'nt':\r\n        input_compat('Press Enter to continue...')\r\n    sys.exit(retcode)\r\n"}
{"text": "# -*- coding: utf-8 -*-\n# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom google.cloud.kms_v1.services.key_management_service.client import KeyManagementServiceClient\nfrom google.cloud.kms_v1.services.key_management_service.async_client import KeyManagementServiceAsyncClient\n\nfrom google.cloud.kms_v1.types.resources import CryptoKey\nfrom google.cloud.kms_v1.types.resources import CryptoKeyVersion\nfrom google.cloud.kms_v1.types.resources import CryptoKeyVersionTemplate\nfrom google.cloud.kms_v1.types.resources import ExternalProtectionLevelOptions\nfrom google.cloud.kms_v1.types.resources import ImportJob\nfrom google.cloud.kms_v1.types.resources import KeyOperationAttestation\nfrom google.cloud.kms_v1.types.resources import KeyRing\nfrom google.cloud.kms_v1.types.resources import PublicKey\nfrom google.cloud.kms_v1.types.resources import ProtectionLevel\nfrom google.cloud.kms_v1.types.service import AsymmetricDecryptRequest\nfrom google.cloud.kms_v1.types.service import AsymmetricDecryptResponse\nfrom google.cloud.kms_v1.types.service import AsymmetricSignRequest\nfrom google.cloud.kms_v1.types.service import AsymmetricSignResponse\nfrom google.cloud.kms_v1.types.service import CreateCryptoKeyRequest\nfrom google.cloud.kms_v1.types.service import CreateCryptoKeyVersionRequest\nfrom google.cloud.kms_v1.types.service import CreateImportJobRequest\nfrom google.cloud.kms_v1.types.service import CreateKeyRingRequest\nfrom google.cloud.kms_v1.types.service import DecryptRequest\nfrom google.cloud.kms_v1.types.service import DecryptResponse\nfrom google.cloud.kms_v1.types.service import DestroyCryptoKeyVersionRequest\nfrom google.cloud.kms_v1.types.service import Digest\nfrom google.cloud.kms_v1.types.service import EncryptRequest\nfrom google.cloud.kms_v1.types.service import EncryptResponse\nfrom google.cloud.kms_v1.types.service import GetCryptoKeyRequest\nfrom google.cloud.kms_v1.types.service import GetCryptoKeyVersionRequest\nfrom google.cloud.kms_v1.types.service import GetImportJobRequest\nfrom google.cloud.kms_v1.types.service import GetKeyRingRequest\nfrom google.cloud.kms_v1.types.service import GetPublicKeyRequest\nfrom google.cloud.kms_v1.types.service import ImportCryptoKeyVersionRequest\nfrom google.cloud.kms_v1.types.service import ListCryptoKeysRequest\nfrom google.cloud.kms_v1.types.service import ListCryptoKeysResponse\nfrom google.cloud.kms_v1.types.service import ListCryptoKeyVersionsRequest\nfrom google.cloud.kms_v1.types.service import ListCryptoKeyVersionsResponse\nfrom google.cloud.kms_v1.types.service import ListImportJobsRequest\nfrom google.cloud.kms_v1.types.service import ListImportJobsResponse\nfrom google.cloud.kms_v1.types.service import ListKeyRingsRequest\nfrom google.cloud.kms_v1.types.service import ListKeyRingsResponse\nfrom google.cloud.kms_v1.types.service import LocationMetadata\nfrom google.cloud.kms_v1.types.service import RestoreCryptoKeyVersionRequest\nfrom google.cloud.kms_v1.types.service import UpdateCryptoKeyPrimaryVersionRequest\nfrom google.cloud.kms_v1.types.service import UpdateCryptoKeyRequest\nfrom google.cloud.kms_v1.types.service import UpdateCryptoKeyVersionRequest\n\n__all__ = ('KeyManagementServiceClient',\n    'KeyManagementServiceAsyncClient',\n    'CryptoKey',\n    'CryptoKeyVersion',\n    'CryptoKeyVersionTemplate',\n    'ExternalProtectionLevelOptions',\n    'ImportJob',\n    'KeyOperationAttestation',\n    'KeyRing',\n    'PublicKey',\n    'ProtectionLevel',\n    'AsymmetricDecryptRequest',\n    'AsymmetricDecryptResponse',\n    'AsymmetricSignRequest',\n    'AsymmetricSignResponse',\n    'CreateCryptoKeyRequest',\n    'CreateCryptoKeyVersionRequest',\n    'CreateImportJobRequest',\n    'CreateKeyRingRequest',\n    'DecryptRequest',\n    'DecryptResponse',\n    'DestroyCryptoKeyVersionRequest',\n    'Digest',\n    'EncryptRequest',\n    'EncryptResponse',\n    'GetCryptoKeyRequest',\n    'GetCryptoKeyVersionRequest',\n    'GetImportJobRequest',\n    'GetKeyRingRequest',\n    'GetPublicKeyRequest',\n    'ImportCryptoKeyVersionRequest',\n    'ListCryptoKeysRequest',\n    'ListCryptoKeysResponse',\n    'ListCryptoKeyVersionsRequest',\n    'ListCryptoKeyVersionsResponse',\n    'ListImportJobsRequest',\n    'ListImportJobsResponse',\n    'ListKeyRingsRequest',\n    'ListKeyRingsResponse',\n    'LocationMetadata',\n    'RestoreCryptoKeyVersionRequest',\n    'UpdateCryptoKeyPrimaryVersionRequest',\n    'UpdateCryptoKeyRequest',\n    'UpdateCryptoKeyVersionRequest',\n)\n"}
{"text": "# -*- Mode: python; indent-tabs-mode: nil; c-basic-offset: 2; tab-width: 2 -*-\n\n\"\"\"\nFriendly Python SSH2 interface.\nCopied from http://media.commandline.org.uk//code/ssh.txt\nModified by James Yoneda to include command-line arguments.\n\"\"\"\n\nimport getopt\nimport os\nimport time\nimport paramiko\nimport sys\nimport tempfile\n\nfrom foldersync.storage import Status\n\nclass SSHStorage(object):\n  \"\"\"Connects and logs into the specified hostname. \n  Arguments that are not given are guessed from the environment.\"\"\" \n\n  def __init__(self,\n         host,\n         username = None,\n         private_key = None,\n         password = None,\n         port = 22,\n         ):\n\n    if port == None:\n      port = 22\n    else:\n      port = int(port);\n\n    self._sftp_live = False\n    self._sftp = None\n    if not username:\n      username = os.environ['LOGNAME']\n\n    # Log to a temporary file.\n    templog = tempfile.mkstemp('.txt', 'con-')[1]\n    paramiko.util.log_to_file(templog)\n\n    # Begin the SSH transport.\n    self._transport = paramiko.Transport((host, port))\n    self._tranport_live = True\n    # Authenticate the transport.\n    \n    if password:\n      # Using Password.\n      self._transport.connect(username = username, password = password)\n    else:\n      ## Use Private Key.\n      #if not private_key:\n      #  # Try to use default key.\n      #  if os.path.exists(os.path.expanduser('~/.con/id_rsa')):\n      #    private_key = '~/.con/id_rsa'\n      #  elif os.path.exists(os.path.expanduser('~/.con/id_dsa')):\n      #    private_key = '~/.con/id_dsa'\n      #  else:\n      #    raise TypeError, \"You have not specified a password or key.\"\n\n      private_key_file = os.path.expanduser(private_key)\n      rsa_key = paramiko.RSAKey.from_private_key_file(private_key_file)\n      self._transport.connect(username = username, pkey = rsa_key)\n\n    self._sftp_connect()\n    self._time_offset = 0\n\n    try:\n      remote_time = int(self._execute(\"date +%s\")[0].strip())\n      self._time_offset = time.time() - remote_time\n    except:\n      pass\n\n  def _sftp_connect(self):\n    \"\"\"Establish a SFTP connection.\"\"\"\n    if not self._sftp_live:\n      self._sftp = paramiko.SFTPClient.from_transport(self._transport)\n      self._sftp_live = True\n\n  def put(self, localpath, remotepath = None):\n    \"\"\"Copies a file between the local host and the remote host.\"\"\"\n    if not remotepath:\n      remotepath = os.path.split(localpath)[1]\n    if not os.path.exists(localpath):\n      return\n    self._sftp_connect()\n    if os.path.isdir(localpath):\n      try:\n        self._sftp.mkdir(remotepath)\n      except IOError:\n        pass\n    else:\n      self._sftp.put(localpath, remotepath)\n\n  def stat(self, remotepath):\n    \"\"\"Provides information about the remote file.\"\"\"\n    self._sftp_connect()\n    try:\n      status = self._sftp.stat(remotepath)\n      return Status(status.st_mtime + self._time_offset, status.st_size)\n    except IOError:\n      return None\n\n  def _execute(self, command):\n    \"\"\"Execute a given command on a remote machine.\"\"\"\n    channel = self._transport.open_session()\n    channel.exec_command(command)\n    output = channel.makefile('rb', -1).readlines()\n    if output:\n      return output\n    else:\n      return channel.makefile_stderr('rb', -1).readlines()\n\n  def _get(self, remotepath, localpath = None):\n    \"\"\"Copies a file between the remote host and the local host.\"\"\"\n    if not localpath:\n      localpath = os.path.split(remotepath)[1]\n    self._sftp_connect()\n    self._sftp.get(remotepath, localpath)\n\n  def close(self):\n    \"\"\"Closes the connection and cleans up.\"\"\"\n    # Close SFTP Connection.\n    if self._sftp_live:\n      self._sftp.close()\n      self._sftp_live = False\n    # Close the SSH Transport.\n    if self._tranport_live:\n      self._transport.close()\n      self._tranport_live = False\n\n  def __del__(self):\n    \"\"\"Attempt to clean up if not explicitly closed.\"\"\"\n    self.close()\n\n"}
{"text": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for lists module.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.contrib.py2tf import utils\nfrom tensorflow.contrib.py2tf.converters import converter_test_base\nfrom tensorflow.contrib.py2tf.converters import lists\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops import tensor_array_ops\nfrom tensorflow.python.platform import test\n\n\nclass ListTest(converter_test_base.TestCase):\n\n  def test_empty_annotated_list(self):\n\n    def test_fn():\n      l = []\n      utils.set_element_type(l, dtypes.int32)\n      l.append(1)\n      return l\n\n    node = self.parse_and_analyze(test_fn, {'dtypes': dtypes, 'utils': utils})\n    node = lists.transform(node, self.ctx)\n\n    with self.compiled(node, tensor_array_ops.TensorArray,\n                       dtypes.int32) as result:\n      # TODO(mdan): Attach these additional modules automatically.\n      result.utils = utils\n      result.dtypes = dtypes\n      with self.test_session() as sess:\n        self.assertEqual(test_fn(), sess.run(result.test_fn().stack()))\n\n\nif __name__ == '__main__':\n  test.main()\n"}
{"text": "#!/usr/bin/env python\n# Licensed to Cloudera, Inc. under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  Cloudera, Inc. licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport re\n\nfrom nose.tools import assert_true, assert_equal, assert_false\nfrom django.contrib.auth.models import User\n\nfrom desktop.lib.django_test_util import make_logged_in_client\nfrom beeswax.models import SavedQuery, QueryHistory\nfrom beeswax.server import dbms\nfrom beeswax.design import hql_query\n\n\nclass MockDbms:\n  def get_databases(self):\n    return ['db1', 'db2']\n\n  def get_tables(self, database):\n    return ['table1', 'table2']\n\n\nclass TestMockedImpala:\n  def setUp(self):\n    self.client = make_logged_in_client()\n\n    # Mock DB calls as we don't need the real ones\n    self.prev_dbms = dbms.get\n    dbms.get = lambda a, b: MockDbms()\n\n  def tearDown(self):\n    # Remove monkey patching\n    dbms.get = self.prev_dbms\n\n  def test_basic_flow(self):\n    response = self.client.get(\"/impala/\")\n    assert_true(re.search('<li id=\"impalaIcon\"\\W+class=\"active', response.content), response.content)\n    assert_true('Query Editor' in response.content)\n\n    response = self.client.get(\"/impala/execute/\")\n    assert_true('Query Editor' in response.content)\n\n\n  def test_saved_queries(self):\n    user = User.objects.get(username='test')\n\n    response = self.client.get(\"/impala/list_designs\")\n    assert_equal(len(response.context['page'].object_list), 0)\n\n    try:\n      beewax_query = create_saved_query('beeswax', user)\n      response = self.client.get(\"/impala/list_designs\")\n      assert_equal(len(response.context['page'].object_list), 0)\n\n      impala_query = create_saved_query('impala', user)\n      response = self.client.get(\"/impala/list_designs\")\n      assert_equal(len(response.context['page'].object_list), 1)\n\n      response = self.client.get(\"/impala/execute_parameterized/%s\" % impala_query.id)\n      assert_true('specify parameters' in response.content)\n\n      # Test my query page\n      QueryHistory.objects.create(owner=user, design=impala_query, query='', last_state=QueryHistory.STATE.available.index)\n\n      resp = self.client.get('/impala/my_queries')\n      assert_equal(len(resp.context['q_page'].object_list), 1)\n      assert_equal(len(resp.context['h_page'].object_list), 1)\n    finally:\n      if beewax_query is not None:\n        beewax_query.delete()\n      if impala_query is not None:\n        impala_query.delete()\n\n\n# Can be refactored with SavedQuery.create_empty() in Hue 2.3\ndef create_saved_query(app_name, owner):\n    query_type = SavedQuery.TYPES_MAPPING[app_name]\n    design = SavedQuery(owner=owner, type=query_type)\n    design.name = SavedQuery.DEFAULT_NEW_DESIGN_NAME\n    design.desc = ''\n    design.data = hql_query('show $tables', database='db1').dumps()\n    design.is_auto = False\n    design.save()\n    return design\n"}
{"text": "\"\"\"A caching template loader that allows disk-based templates to be used.\"\"\"\n\n\nimport os, sys\nfrom abc import ABCMeta, abstractmethod\n\n\nclass TemplateDoesNotExist(Exception):\n    \n    \"\"\"A named template could not be found.\"\"\"\n\n\nclass Source(metaclass=ABCMeta):\n    \n    \"\"\"A source of template data.\"\"\"\n    \n    __slots__ = ()\n    \n    @abstractmethod\n    def load_source(self, template_name):\n        \"\"\"\n        Loads the template source code for the template of the given name.\n        \n        If no source code can be found, returns None.\n        \"\"\"\n        \n        \nclass MemorySource(Source):\n\n    \"\"\"A template loader that loads from memory.\"\"\"\n    \n    __slots__ = (\"templates\",)\n    \n    def __init__(self, templates):\n        \"\"\"Initializes the MemorySource from a dict of template source strings.\"\"\"\n        self.templates = templates\n        \n    def load_source(self, template_name):\n        \"\"\"Loads the source from the memory template dict.\"\"\"\n        return self.templates.get(template_name)\n        \n    def __str__(self):\n        \"\"\"Returns a string representation.\"\"\"\n        return \"<memory>\"\n        \n        \nclass DirectorySource(Source):\n    \n    \"\"\"A template loader that loads from a directory on disk.\"\"\"\n    \n    __slots__ = (\"dirname\")\n    \n    def __init__(self, dirname):\n        \"\"\"\n        Initializes the DirectorySource.\n        \n        On windows, the dirname should be specified using forward-slashes.\n        \"\"\"\n        self.dirname = dirname\n        \n    def load_source(self, template_name):\n        \"\"\"Loads the source from disk.\"\"\"\n        template_path = os.path.normpath(os.path.join(self.dirname, template_name))\n        if os.path.exists(template_path):\n            with open(template_path, \"r\") as template_file:\n                return template_file.read()\n        return None\n        \n    def __str__(self):\n        \"\"\"Returns a string representation.\"\"\"\n        return self.dirname\n\n\nclass DebugLoader:\n\n    \"\"\"\n    A template loader that doesn't cache compiled templates.\n    \n    Terrible performance, but great for debugging.\n    \"\"\"\n    \n    __slots__ = (\"_sources\", \"_parser\",)\n    \n    def __init__(self, sources, parser):\n        \"\"\"\n        Initializes the Loader.\n        \n        When specifying template_dirs on Windows,the forward slash '/' should be used as a path separator.\n        \"\"\"\n        self._sources = list(reversed(sources))\n        self._parser = parser\n    \n    def compile(self, template, name=\"__string__\", params=None, meta=None):\n        \"\"\"Compiles the given template source.\"\"\"\n        default_meta = {\n            \"__loader__\": self\n        }\n        default_meta.update(meta or {})\n        return self._parser.compile(template, name, params, default_meta)\n    \n    def _load_all(self, template_name):\n        \"\"\"Loads and returns all the named templates from the sources.\"\"\"\n        # Load from all the template sources.\n        templates = []\n        for source in self._sources:\n            template_src = source.load_source(template_name)\n            if template_src is not None:\n                meta = {\n                    \"__super__\": templates and templates[-1] or None,\n                }\n                templates.append(self.compile(template_src, template_name, {}, meta))\n        return templates\n    \n    def load(self, *template_names):        \n        \"\"\"\n        Loads and returns the named template.\n        \n        If more than one template name is given, then the first template that exists will be used.\n        \n        On Windows, the forward slash '/' should be used as a path separator.\n        \"\"\"\n        if not template_names:\n            raise ValueError(\"You must specify at least one template name.\")\n        for template_name in template_names:\n            templates = self._load_all(template_name)\n            if templates:\n                return templates[-1]\n        # Raise an error.\n        template_name_str = \", \".join(repr(template_name) for template_name in template_names)\n        source_name_str = \", \".join(str(source) for source in self._sources)\n        raise TemplateDoesNotExist(\"Could not find a template named {} in any of {}.\".format(template_name_str, source_name_str))\n        \n    def render(self, *template_names, **params):\n        \"\"\"\n        Loads and renders the named template.\n        \n        If more than one template name is given, then the first template that exists will be used.\n        \n        On Windows, the forward slash '/' should be used as a path separator.\n        \"\"\"\n        return self.load(*template_names).render(**params)\n\n\nclass Loader(DebugLoader):\n    \n    \"\"\"\n    A template loader.\n    \n    Compiled templates are cached for performance.\n    \"\"\"\n    \n    __slots__ = (\"_cache\",)\n    \n    def __init__(self, sources, parser):\n        \"\"\"Initializes the loader.\"\"\"\n        super(Loader, self).__init__(sources, parser)\n        self._cache = {}\n    \n    def clear_cache(self, ):\n        \"\"\"Clears the template cache.\"\"\"\n        self._cache.clear()\n        \n    def _load_all(self, template_name):\n        \"\"\"A caching version of the debug loader's load method.\"\"\"\n        if template_name in self._cache:\n            return self._cache[template_name]\n        template = super(Loader, self)._load_all(template_name)\n        self._cache[template_name] = template\n        return template"}
{"text": "WIDTH = 640\r\nHEIGHT = 480\r\n\r\nclass Ball(ZRect): pass\r\n#\r\n# The ball is a red square halfway across the game screen\r\n#\r\nball = Ball(0, 0, 30, 30)\r\nball.center = WIDTH / 2, HEIGHT / 2\r\nball.colour = \"red\"\r\n#\r\n# The ball moves one step right and one step down each tick\r\n#\r\nball.direction = 1, 1\r\n#\r\n# The ball moves at a speed of 3 steps each tick\r\n#\r\nball.speed = 3\r\n\r\nclass Bat(ZRect): pass\r\n#\r\n# The bat is a green oblong which starts just along the bottom\r\n# of the screen and halfway across.\r\n#\r\nBAT_W = 150\r\nBAT_H = 15\r\nbat = Bat(WIDTH / 2, HEIGHT - BAT_H, BAT_W, BAT_H)\r\nbat.colour = \"green\"\r\n\r\ndef draw():\r\n    #\r\n    # Clear the screen and place the ball at its current position\r\n    #\r\n    screen.clear()\r\n    screen.draw.filled_rect(ball, ball.colour)\r\n    screen.draw.filled_rect(bat, bat.colour)\r\n\r\ndef on_mouse_move(pos):\r\n    #\r\n    # Make the bat follow the horizontal movement of the mouse.\r\n    #\r\n    x, y = pos\r\n    bat.centerx = x\r\n\r\ndef update():\r\n    #\r\n    # Move the ball along its current direction at its current speed\r\n    #\r\n    dx, dy = ball.direction\r\n    ball.move_ip(ball.speed * dx, ball.speed * dy)\r\n\r\n    #\r\n    # Bounce the ball off the bat\r\n    #\r\n    if ball.colliderect(bat):\r\n        ball.direction = dx, -dy\r\n\r\n    #\r\n    # Bounce the ball off the left or right walls\r\n    #\r\n    if ball.right >= WIDTH or ball.left <= 0:\r\n        ball.direction = -dx, dy\r\n\r\n    #\r\n    # If the ball hits the bottom of the screen, you lose\r\n    #\r\n    if ball.bottom >= HEIGHT:\r\n        exit()\r\n\r\n    #\r\n    # Bounce the ball off the top wall\r\n    #\r\n    if ball.top <= 0:\r\n        ball.direction = dx, -dy"}
{"text": "# -*- coding: utf-8 -*-\n\nfrom django.conf import settings\n\n\nclass Conf:\n    \"\"\" \u041a\u043b\u0430\u0441\u0441 \u043a\u043e\u043d\u0444\u0438\u0433\u0443\u0440\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0440\u043e\u0431\u043e\u043a\u0430\u0441\u0441\u044b, \u0431\u0435\u0440\u0451\u0442 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u0438\u0437 settings.ROBOKASSA_CONF\n    \"\"\"\n    # todo: \u0432 \u0431\u043e\u043b\u044c\u0448\u0438\u043d\u0441\u0442\u0432\u0435 \u0441\u043b\u0443\u0447\u0430\u0435\u0432 1 \u043c\u0430\u0433\u0430\u0437\u0438\u043d \u043d\u0430 1 \u0441\u0430\u0439\u0442 - \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043d\u0435\u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 token\n\n    # \u043e\u0431\u044f\u0437\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b - \u0440\u0435\u043a\u0432\u0438\u0437\u0438\u0442\u044b \u043c\u0430\u0433\u0430\u0437\u0438\u043d\u0430\n    LOGIN = ''\n    PASSWORD1 = ''\n    PASSWORD2 = ''\n\n    # url, \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u043e\u043c\u0443 \u0431\u0443\u0434\u0435\u0442 \u0438\u0434\u0442\u0438 \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0430 \u0444\u043e\u0440\u043c\n    FORM_TARGET = 'https://merchant.roboxchange.com/Index.aspx'\n    # \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043b\u0438 \u043c\u0435\u0442\u043e\u0434 POST \u043f\u0440\u0438 \u043f\u0440\u0438\u0435\u043c\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u043e\u0432\n    USE_POST = True\n    # \u0442\u0440\u0435\u0431\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0443\u0432\u0435\u0434\u043e\u043c\u043b\u0435\u043d\u0438\u044f \u043d\u0430 ResultURL\n    STRICT_CHECK = True\n\n    # \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0439 \u0440\u0435\u0436\u0438\u043c\n    TEST_MODE = False\n    # \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u0441\u043a\u0438\u0445 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 (\"shp\" \u043a \u043d\u0438\u043c \u043f\u0440\u0438\u043f\u0438\u0441\u044b\u0432\u0430\u0442\u044c \u043d\u0435 \u043d\u0443\u0436\u043d\u043e)\n    EXTRA_PARAMS = []\n\n    def __init__(self, token):\n        if token not in settings.ROBOKASSA_CONF:\n            raise ValueError('Can not find \"{}\" in settings.ROBOKASSA_CONF'.format(token))\n        config = settings.ROBOKASSA_CONF[token]\n\n        self.LOGIN = config['ROBOKASSA_LOGIN']\n        self.PASSWORD1 = config['ROBOKASSA_PASSWORD1']\n        self.PASSWORD2 = config.get('ROBOKASSA_PASSWORD2', None)\n\n        self.USE_POST = config.get('ROBOKASSA_USE_POST', True)\n        self.STRICT_CHECK = config.get('ROBOKASSA_STRICT_CHECK', True)\n        self.TEST_MODE = config.get('ROBOKASSA_TEST_MODE', False)\n        self.EXTRA_PARAMS = sorted(config.get('ROBOKASSA_EXTRA_PARAMS', []))\n"}
{"text": "# coding: utf-8\n\nfrom __future__ import unicode_literals\n\nimport pytest\n\nfrom mock import Mock\nfrom boxsdk.config import API\nfrom boxsdk.object.device_pinner import DevicePinner\nfrom boxsdk.network.default_network import DefaultNetworkResponse\n\n\n@pytest.fixture(scope='module')\ndef delete_device_pin_response():\n    # pylint:disable=redefined-outer-name\n    mock_network_response = Mock(DefaultNetworkResponse)\n    mock_network_response.ok = True\n    return mock_network_response\n\n\ndef test_get(test_device_pin, mock_box_session):\n    created_at = '2016-05-18T17:38:03-07:00',\n    expected_url = '{0}/device_pinners/{1}'.format(API.BASE_API_URL, test_device_pin.object_id)\n    mock_box_session.get.return_value.json.return_value = {\n        'type': 'device_pinner',\n        'id': test_device_pin.object_id,\n        'created_at': created_at\n    }\n    device_pin = test_device_pin.get()\n    mock_box_session.get.assert_called_once_with(expected_url, headers=None, params=None)\n    assert isinstance(device_pin, DevicePinner)\n    assert device_pin.created_at == created_at\n\n\ndef test_delete_device_pin_return_the_correct_response(\n        test_device_pin,\n        mock_box_session,\n        delete_device_pin_response,\n):\n    # pylint:disable=redefined-outer-name\n    mock_box_session.delete.return_value = delete_device_pin_response\n    response = test_device_pin.delete()\n    # pylint:disable=protected-access\n    expected_url = test_device_pin.get_url()\n    # pylint:enable = protected-access\n    mock_box_session.delete.assert_called_once_with(expected_url, params={}, expect_json_response=False, headers=None)\n    assert response is True\n"}
{"text": "\"\"\"Tests for the correct reading of the queue config.\"\"\"\nfrom copy import deepcopy\n\nfrom amqpeek.cli import build_queue_data\n\n\nclass TestFormatQueues:\n    \"\"\"Tests parsing of queue config.\"\"\"\n\n    def test_dedup_queue_config(self, config_data: dict) -> None:\n        \"\"\"Tests handling of duplicate config entries in different formats.\n\n        my_queue is defined twice, both in queues and queue_limits\n        build_queue_data should dedup queues defined twice if their limits\n        are the same\n        \"\"\"\n        result = build_queue_data(config_data)\n\n        assert isinstance(result, list)\n        assert len(result) == 2\n\n        expected_queues = [(\"my_queue\", 0), (\"my_other_queue\", 1)]\n        for excepted_queue in expected_queues:\n            assert excepted_queue in result\n\n    def test_just_queue_config(self, config_data: dict) -> None:\n        \"\"\"Test that queue config is parsed correctly.\"\"\"\n        config_data = deepcopy(config_data)\n        del config_data[\"queue_limits\"]\n\n        result = build_queue_data(config_data)\n\n        assert result == [(\"my_queue\", 0)]\n\n    def test_just_queue_limits_config(self, config_data: dict) -> None:\n        \"\"\"Test that queue limits config is parsed correctly.\"\"\"\n        config_data = deepcopy(config_data)\n        del config_data[\"queues\"]\n\n        result = build_queue_data(config_data)\n\n        assert len(result) == 2\n\n        expected_queues = [(\"my_queue\", 0), (\"my_other_queue\", 1)]\n        for excepted_queue in expected_queues:\n            assert excepted_queue in result\n\n    def test_no_queue_config(self) -> None:\n        \"\"\"Test handling of no queue config.\"\"\"\n        result = build_queue_data({})\n\n        assert result == []\n"}
{"text": "from rest_framework.pagination import PageNumberPagination\nfrom rest_framework.response import Response\nfrom collections import OrderedDict\n\nclass CustomPagination(PageNumberPagination):\n    def get_paginated_posts(self, data, request):\n        result = OrderedDict([\n            ('count', self.page.paginator.count),\n            ('next', self.get_next_link()),\n            ('previous', self.get_previous_link()),\n            ('posts', data),\n            ])\n\n        if ('size' in request.GET):\n            result['size'] = request.GET['size']\n            return Response(result)\n        result['size'] = 20;\n        return Response(result)\n    \n    def get_paginated_comments(self, data, request):\n        result = OrderedDict([\n            ('count', self.page.paginator.count),\n            ('next', self.get_next_link()),\n            ('previous', self.get_previous_link()),\n            ('comments', data),\n            ])\n\n        if ('size' in request.GET):\n            result['size'] = request.GET['size']\n            return Response(result)\n        result['size'] = 20;\n        return Response(result)"}
{"text": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\n\n\n\"\"\"\n[path]\ncd /Users/brunoflaven/Documents/01_work/blog_articles/extending_streamlit_usage/001_nlp_spacy_python_realp/\n\n\n[file]\npython 002c_nlp_spacy_python.py\n\n\n\n\n# source\nSource: https://realpython.com/natural-language-processing-spacy-python/\n\n# required\npip install spacy-langdetect\n\n# validation\npython -m spacy validate\n\n\"\"\"\nfrom spacy_langdetect import LanguageDetector\nfrom spacy.language import Language\nimport spacy\nimport spacy_streamlit\n\n# Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.\n\ndef set_custom_boundaries(doc):\n       # Adds support to use `   ` as the delimiter for sentence detection\n       for token in doc[:-1]:\n            if token.text == '   ':\n                doc[token.i+1].is_sent_start = True\n                return doc\n\n\nellipsis_text = ('Gus, can you,     never mind, I forgot'\n                 ' what I was saying. So, do you think'\n                 ' we should    ')\n\ncustom_nlp = spacy.load('en_core_web_sm')\n\nLanguage.component(\"the_set_custom_boundaries\", func=set_custom_boundaries)\ncustom_nlp.add_pipe(\"the_set_custom_boundaries\", last=True)\ncustom_ellipsis_doc = custom_nlp(ellipsis_text)\ncustom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n\nprint(\"\\n --- result_5\")\nfor sentence in custom_ellipsis_sentences:\n    print(sentence)\n\n# Gus, can you,     never mind, I forgot what I was saying.\n# So, do you think we should\n"}
{"text": "# Copyright (C) 2015 Andrey Antukh <niwi@niwi.be>\n# Copyright (C) 2015 Jes\u00fas Espino <jespinog@gmail.com>\n# Copyright (C) 2015 David Barrag\u00e1n <bameda@dbarragan.com>\n# Copyright (C) 2015 Anler Hern\u00e1ndez <hello@anler.me>\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nimport pytest\nfrom django.core.urlresolvers import reverse\n\nfrom .. import factories as f\n\npytestmark = pytest.mark.django_db\n\n\ndef test_upvote_issue(client):\n    user = f.UserFactory.create()\n    issue = f.create_issue(owner=user)\n    f.MembershipFactory.create(project=issue.project, user=user, is_owner=True)\n    url = reverse(\"issues-upvote\", args=(issue.id,))\n\n    client.login(user)\n    response = client.post(url)\n\n    assert response.status_code == 200\n\n\ndef test_downvote_issue(client):\n    user = f.UserFactory.create()\n    issue = f.create_issue(owner=user)\n    f.MembershipFactory.create(project=issue.project, user=user, is_owner=True)\n    url = reverse(\"issues-downvote\", args=(issue.id,))\n\n    client.login(user)\n    response = client.post(url)\n\n    assert response.status_code == 200\n\n\ndef test_list_issue_voters(client):\n    user = f.UserFactory.create()\n    issue = f.create_issue(owner=user)\n    f.MembershipFactory.create(project=issue.project, user=user, is_owner=True)\n    f.VoteFactory.create(content_object=issue, user=user)\n    url = reverse(\"issue-voters-list\", args=(issue.id,))\n\n    client.login(user)\n    response = client.get(url)\n\n    assert response.status_code == 200\n    assert response.data[0]['id'] == user.id\n\ndef test_get_issue_voter(client):\n    user = f.UserFactory.create()\n    issue = f.create_issue(owner=user)\n    f.MembershipFactory.create(project=issue.project, user=user, is_owner=True)\n    vote = f.VoteFactory.create(content_object=issue, user=user)\n    url = reverse(\"issue-voters-detail\", args=(issue.id, vote.user.id))\n\n    client.login(user)\n    response = client.get(url)\n\n    assert response.status_code == 200\n    assert response.data['id'] == vote.user.id\n\ndef test_get_issue_votes(client):\n    user = f.UserFactory.create()\n    issue = f.create_issue(owner=user)\n    f.MembershipFactory.create(project=issue.project, user=user, is_owner=True)\n    url = reverse(\"issues-detail\", args=(issue.id,))\n\n    f.VotesFactory.create(content_object=issue, count=5)\n\n    client.login(user)\n    response = client.get(url)\n\n    assert response.status_code == 200\n    assert response.data['total_voters'] == 5\n\n\ndef test_get_issue_is_voted(client):\n    user = f.UserFactory.create()\n    issue = f.create_issue(owner=user)\n    f.MembershipFactory.create(project=issue.project, user=user, is_owner=True)\n    f.VotesFactory.create(content_object=issue)\n    url_detail = reverse(\"issues-detail\", args=(issue.id,))\n    url_upvote = reverse(\"issues-upvote\", args=(issue.id,))\n    url_downvote = reverse(\"issues-downvote\", args=(issue.id,))\n\n    client.login(user)\n\n    response = client.get(url_detail)\n    assert response.status_code == 200\n    assert response.data['total_voters'] == 0\n    assert response.data['is_voter'] == False\n\n    response = client.post(url_upvote)\n    assert response.status_code == 200\n\n    response = client.get(url_detail)\n    assert response.status_code == 200\n    assert response.data['total_voters'] == 1\n    assert response.data['is_voter'] == True\n\n    response = client.post(url_downvote)\n    assert response.status_code == 200\n\n    response = client.get(url_detail)\n    assert response.status_code == 200\n    assert response.data['total_voters'] == 0\n    assert response.data['is_voter'] == False\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\n  Copyright 2014 Message Bus\n\n  Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n  not use this file except in compliance with the License. You may obtain\n  a copy of the License at\n\n      http://www.apache.org/licenses/LICENSE-2.0\n\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n  License for the specific language governing permissions and limitations\n  under the License.\n\"\"\"\nimport sys\nimport os\n\npath = os.path.abspath(\n    os.path.join(os.path.dirname(__file__), '..', '..', '..'))\nif not path in sys.path:\n    sys.path.insert(1, path)\ndel path\n\nfrom messagebus import MessageBusWebhooksClient, MessageBusResponseError\n\n\napi_key = '7215ee9c7d9dc229d2921a40e899ec5f'\nwebhook_key = '2ff80e9159b517704ce43f0f74e6e247'\n\ndef delete_webhook():\n    try:\n        webhook_client = MessageBusWebhooksClient(api_key)\n        results = webhook_client.delete_webhook(webhook_key)\n    except MessageBusResponseError, error:\n        raise error\n    else:\n        print \"Successfully deleted webhook %s\" % webhook_key\n\n\ndef get_webhooks():\n    try:\n        webhooks_client = MessageBusWebhooksClient(api_key)\n        results = webhooks_client.get_webhooks()\n    except MessageBusResponseError, error:\n        print error.message\n    else:\n        for webhook in results['webhooks']:\n            print \"Webhook Key: %s\" % webhook['webhook_key']\n\nif __name__ == '__main__':\n    get_webhooks()\n    delete_webhook()\n    get_webhooks()\n"}
{"text": "from __future__ import print_function\nimport os\nimport sys\nimport sh\n\ntry: from distutils.core import setup\nexcept ImportError: from setuptools import setup\n\n\nsetup(\n    name=\"sh\",\n    version=sh.__version__,\n    description=\"Python subprocess interface\",\n    author=\"Andrew Moffat\",\n    author_email=\"andrew.robert.moffat@gmail.com\",\n    url=\"https://github.com/amoffat/sh\",\n    license=\"MIT\",\n    py_modules=[\"sh\"],\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Environment :: Console\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: System Administrators\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python\",\n        \"Programming Language :: Python :: 2\",\n        \"Programming Language :: Python :: 2.6\",\n        \"Programming Language :: Python :: 2.7\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.1\",\n        \"Programming Language :: Python :: 3.2\",\n        \"Programming Language :: Python :: 3.3\",\n        \"Topic :: Software Development :: Build Tools\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n    ],\n)\n"}
{"text": "__author__ = 'Sergio Sicari'\n__email__  = \"sergiosicari@gmail.com\"\n\nfrom evernote.api.client import EvernoteClient\nimport evernote.edam.type.ttypes as Types\nimport evernote.edam.notestore.NoteStore as NoteStore\nimport evernote.edam.userstore.UserStore as UserStore\n\nclass EverCode:\n\n     def __init__(self, Settings):\n          self._settings = Settings\n          self._token = self._settings.developer_token\n\n     def setClient(self):\n          self._client = EvernoteClient(token=self._token, sandbox=self._settings.sandbox)\n          self._userStore = self._client.get_user_store()\n          self._user = self._userStore.getUser()\n          self._noteStore = self._client.get_note_store()\n          self._notebooks = self._noteStore.listNotebooks()\n     '''\n     def login(self):\n          self._client = EvernoteClient(\n               consumer_key = self._settings.consumer_key,\n               consumer_secret = self._settings.consumer_secret,\n               sandbox=self._settings.sandbox\n          )\n          request_token = self._client.get_request_token('YOUR CALLBACK URL')\n          self._client.get_authorize_url(request_token)\n          access_token = self._client.get_access_token(\n               request_token['oauth_token'],\n               request_token['oauth_token_secret'],\n               request_token.GET.get('oauth_verifier', '')\n          )\n          self._token = access_token\n     '''\n\n     def makeNote(self, noteTitle, noteBody, parentNotebook=None):\n \n          result = noteBody.replace('class=\"highlight\"', '')\n      \n          nBody =  \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\"?>\"\n          nBody += \"<!DOCTYPE en-note SYSTEM \\\"http://xml.evernote.com/pub/enml2.dtd\\\">\"\n          nBody += \"<en-note>%s</en-note>\" % result\n      \n          ## Create note object\n          ourNote = Types.Note()\n          ourNote.title = noteTitle\n          ourNote.content = nBody\n      \n          ## parentNotebook is optional; if omitted, default notebook is used\n          if parentNotebook and hasattr(parentNotebook, 'guid'):\n               ourNote.notebookGuid = parentNotebook.guid\n      \n          ## Attempt to create note in Evernote account\n          try:\n               note = self._noteStore.createNote(self._token, ourNote)\n          except Errors.EDAMUserException, edue:\n               ## Something was wrong with the note data\n               ## See EDAMErrorCode enumeration for error code explanation\n               ## http://dev.evernote.com/documentation/reference/Errors.html#Enum_EDAMErrorCode\n               print \"EDAMUserException:\", edue\n               return None\n          except Errors.EDAMNotFoundException, ednfe:\n               ## Parent Notebook GUID doesn't correspond to an actual notebook\n               print \"EDAMNotFoundException: Invalid parent notebook GUID\"\n               return None\n               ## Return created note object\n          return note"}
{"text": "# GUI object/properties browser. \r\n# Copyright (C) 2011 Matiychuk D.\r\n#\r\n# This library is free software; you can redistribute it and/or\r\n# modify it under the terms of the GNU Lesser General Public License\r\n# as published by the Free Software Foundation; either version 2.1\r\n# of the License, or (at your option) any later version.\r\n#\r\n# This library is distributed in the hope that it will be useful,\r\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\r\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n# See the GNU Lesser General Public License for more details.\r\n#\r\n# You should have received a copy of the GNU Lesser General Public\r\n# License along with this library; if not, write to the\r\n#    Free Software Foundation, Inc.,\r\n#    59 Temple Place,\r\n#    Suite 330,\r\n#    Boston, MA 02111-1307 USA\r\n\r\n#Boa:App:BoaApp\r\n\r\nimport sys\r\nimport traceback\r\nimport wx\r\n\r\nimport _mainframe\r\nimport tools\r\n\r\n\r\ndef hook(exctype, value, tb):\r\n\r\n    \"\"\"\r\n    Handle all unexpected exceptions. Show the msgbox then close main window.\r\n    \"\"\"\r\n    traceback_text = ''.join(traceback.format_exception(exctype, value, tb, 5))\r\n    tools.show_error_message('ERROR', traceback_text)\r\n\r\n\r\nif not __debug__:\r\n    # Catch all unhandled exceptions and show the details in a msgbox.\r\n    sys.excepthook = hook\r\n\r\n\r\nmodules ={'_mainframe': [0, '', '_mainframe.py'], 'proxy': [0, '', 'proxy.py']}\r\n\r\n\r\nclass BoaApp(wx.App):\r\n    def OnInit(self):\r\n        self.main = _mainframe.create(None)\r\n        self.main.Center()\r\n        self.main.Show()\r\n        self.SetTopWindow(self.main)\r\n        return True\r\n\r\n\r\ndef main():\r\n    application = BoaApp(0)\r\n    application.MainLoop()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n"}
{"text": "from django.db import models\n\nSTATE_CHOICES = (\n    ('pa', 'Pennsylvania'),\n    ('nj', 'New Jersey'),\n)\n\nITEM_CHOICES = (\n    ('event','Event'),\n    ('food','Food'),\n    ('goods','Goods'),\n    ('service','Service'),\n    ('other','Other'),\n)\n\nPAYMENT_CHOICES = (\n    ('none','None'),\n    ('cash','Cash'),\n    ('check','Check'),\n    ('credit','Credit'),\n)\n\nclass Auction(models.Model):\n\n    ''' Model to represent an Auction '''\n\n    name = models.CharField(max_length=255)\n    date = models.DateField()\n\n    def __unicode__(self):\n        return self.name + ' ' + str(self.date)\n\nclass AuctionUser(models.Model):\n\n    ''' Model to represent an Auction User; i.e. someone who donates\n        or bids on users '''\n\n    name = models.CharField(max_length=255)\n    address_1 = models.CharField(max_length=255, blank=True)\n    address_2 = models.CharField(max_length=255, blank=True)\n    city = models.CharField(max_length=255, blank=True)\n    state = models.CharField(max_length=2, choices=STATE_CHOICES,\n        blank=True)\n    zip = models.CharField(max_length=10, blank=True)\n    phone = models.CharField(max_length=255, blank=True)\n    email = models.EmailField(max_length=255, blank=True)\n\n    def __unicode__(self):\n        return self.name\n\nclass AuctionParticipant(models.Model):\n\n    ''' Model to represent an Auction Participant; i.e. someone who\n        will be bidding on items in the auction '''\n\n    auction = models.ForeignKey(Auction)\n    user = models.ForeignKey(AuctionUser)\n    paddle = models.PositiveIntegerField()\n    payment_method = models.CharField(max_length=10, choices=PAYMENT_CHOICES,\n        default='none')\n    payment_notes = models.TextField(blank=True)\n\n    def __unicode__(self):\n        return str(self.user) + ' (' + str(self.paddle) + ')'\n\nclass AuctionEvent(models.Model):\n\n    ''' Model to represent an Auction Event; i.e. a collection of items\n        that will be bid on during the auction '''\n\n    name = models.CharField(max_length=255)\n    abbreviation = models.CharField(max_length=10)\n    auction = models.ForeignKey(Auction)\n\n    def __unicode__(self):\n        return self.name + ' ' + self.abbreviation\n\nclass AuctionItem(models.Model):\n\n    ''' Model to represent an Item to be bid on in an auction '''\n\n    name = models.CharField(max_length=255)\n    item_type = models.CharField(max_length=20, choices=ITEM_CHOICES)\n    item_number = models.IntegerField()\n    description = models.TextField(blank=True)\n    image = models.ImageField(max_length=255,upload_to='images',blank=True)\n    valid_winners = models.PositiveIntegerField(default=1)\n    auction = models.ForeignKey(Auction)\n    auction_event = models.ForeignKey(AuctionEvent)\n    donor = models.ForeignKey(AuctionUser)\n    starting_bid = models.FloatField()\n    conditions = models.TextField(blank=True)\n    time_and_location = models.TextField(blank=True)\n\n    def __unicode__(self):\n        return str(self.auction_event.abbreviation) + str(self.item_number) + ' ' + self.name\n\nclass AuctionBid(models.Model):\n\n    ''' Model to represent an individual Bid in an Auction '''\n\n    auction = models.ForeignKey(Auction)\n    bidder = models.ForeignKey(AuctionParticipant)\n    item = models.ForeignKey(AuctionItem)\n    ammount = models.FloatField()\n\n    def __unicode__(self):\n        return str(self.bidder) + ' ' + str(self.ammount)\n"}
{"text": "# -*- coding: utf-8 -*-\n#\n#\n#    Author: Romain Deheele\n#    Copyright 2015 Camptocamp SA\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n#\n\nfrom openerp import models, fields, api\n\n\nclass CrmLeadLost(models.TransientModel):\n\n    \"\"\" Ask a reason for the opportunity lost.\"\"\"\n    _name = 'crm.lead.lost'\n    _description = __doc__\n\n    def _default_reason(self):\n        active_id = self._context.get('active_id')\n        active_model = self._context.get('active_model')\n        if active_id and active_model == 'crm.lead':\n            lead = self.env['crm.lead'].browse(active_id)\n            return lead.lost_reason_id.id\n\n    reason_id = fields.Many2one(\n        'crm.lead.lost.reason',\n        string='Reason',\n        required=True,\n        default=_default_reason)\n\n    @api.one\n    def confirm_lost(self):\n        act_close = {'type': 'ir.actions.act_window_close'}\n        lead_ids = self._context.get('active_ids')\n        if lead_ids is None:\n            return act_close\n        assert len(lead_ids) == 1, \"Only 1 lead ID expected\"\n        lead = self.env['crm.lead'].browse(lead_ids)\n        lead.lost_reason_id = self.reason_id.id\n        lead.case_mark_lost()\n        return act_close\n"}
{"text": "class Config(object):\n    \n    def __init__(self):\n        self.__sqlLiteDatabasePath = \"./aquarius.db\"\n        self.__webServerAddress = \"192.168.0.9\"\n        self.__webServerPort = 9090\n        self.__harvestPaths = []\n        \n    @property\n    def harvest_paths(self):\n        return self.__harvestPaths\n    \n    @harvest_paths.setter\n    def harvest_paths(self, value):\n        self.__harvestPaths = value\n    \n    @property\n    def sqllite_database_path(self):\n        return self.__sqlLiteDatabasePath\n    \n    @sqllite_database_path.setter\n    def sqllite_database_path(self, value):\n        self.__sqlLiteDatabasePath = value\n        \n    @property\n    def web_server_address(self):\n        return self.__webServerAddress\n    \n    @web_server_address.setter\n    def web_server_address(self, value):\n        self.__webServerAddress = value\n        \n    @property\n    def web_server_port(self):\n        return self.__webServerPort\n    \n    @web_server_port.setter\n    def web_server_port(self, value):\n        self.__webServerPort = value\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Mar 18 13:07:18 2016\n\n@author: Administrator\n\"\"\"\n\nfrom urllib import request\nimport json,re,time\n\n\ntime1=''\nupdate=''\nwith request.urlopen('http://api.money.126.net/data/feed/1160220,money.api') as f:\n        data = f.read()\n        myjson = re.search(r'\\(.*\\)',data.decode('utf-8'))\n        myjson1 = myjson.group(0)\n        myjson3 = myjson1[1:-1]\n        decode1 = json.loads(myjson3)\n        decode2 = decode1\nwhile True :\n    with request.urlopen('http://api.money.126.net/data/feed/1160220,money.api') as f:\n        data = f.read()\n        myjson = re.search(r'\\(.*\\)',data.decode('utf-8'))\n        myjson1 = myjson.group(0)\n        myjson3 = myjson1[1:-1]\n        decode1 = json.loads(myjson3)\n      #  del decode1['1160220'][]\n        time1 = decode1['1160220']['time']\n        update1 = decode1['1160220']['update']\n        del decode1['1160220']['time']\n        del decode1['1160220']['update']       \n        if decode2 != decode1:\n            print(decode1['1160220']['name'],time1,update1,decode1['1160220']['price'])            \n            for (d,x) in decode1['1160220'].items():\n                if x != decode2['1160220'][d]:\n                    print(d,x)\n                    print(d,decode2['1160220'][d])\n            decode2 = decode1  \n        time.sleep(1)\n#data = []\n#data['code'] = decode1   "}
{"text": "#!/usr/bin/python\n\nimport ConfigParser\nimport feedparser\nimport logging\nimport os\nimport pywapi\nimport string\nimport sys\nimport time\nimport unicodedata\n\nfrom core.alive import alive\n\nfrom pygeocoder import Geocoder\nfrom core.aprsfi import AprsFi\nfrom core.voicesynthetizer import VoiceSynthetizer\nfrom core.phonetic import Phonetic\n\ndays = {'Monday': 'Lunes', 'Tuesday': 'Martes', 'Wednesday': 'Miercoles',\n'Thursday': 'Jueves', 'Friday': 'Viernes', 'Saturday': 'Sabado',\n'Sunday': 'Domingo',\n}\n\nmonths = {'January': 'Enero', 'February': 'Febrero', 'March': 'Marzo',\n'April': 'Abril', 'May': 'Mayo', 'June': 'Junio',\n'July': 'Julio', 'August': 'Agosto', 'September': 'Septiembre',\n'October': 'Octubre', 'November' : 'Noviembre', 'December': 'Diciembre'\n}\n\nclass AprsTracker(object):\n\n    def __init__(self, voicesynthetizer, callsign='XE1GYP-9'):\n        logging.info('[AprsTracker]')\n        self.speaker = voicesynthetizer\n        self.callsign = callsign\n        self.modulename = 'AprsTracker'\n\n        self.phonetic = Phonetic()\n        self.aprsfi = AprsFi()\n\n        self.conf = ConfigParser.ConfigParser()\n        self.path = \"configuration/general.config\"\n        self.conf.read(self.path)\n\n    def time(self, aprstime):\n        logging.info('[AprsTracker] Time')\n        weekday = days[time.strftime(\"%A\", time.gmtime(int(aprstime)))]\n        day = time.strftime(\"%d\", time.gmtime(int(aprstime))).lstrip('0')\n        month = months[time.strftime(\"%B\", time.gmtime(int(aprstime)))]\n        year = time.strftime(\"%Y\", time.gmtime(int(aprstime)))\n        return weekday, day, month, year\n\n    def localize(self):\n        logging.info('[AprsTracker] Localize')\n        self.speaker.speechit(\"Localizacion de estaciones a traves de a p r s punto f i\")\n        self.aprsfi.callsignset(self.callsign)\n        self.aprsfi.dataset('loc')\n        data = self.aprsfi.query()\n        logging.info(data)\n\n        station = \"Estacion \" + self.callsign\n        stationdecoded = \"Estacion \" + ' '.join(self.phonetic.decode(self.callsign))\n\n        if data.get('entries'):\n            for entry in data['entries']:\n                weekday, day, month, year = self.time(entry['lasttime'])\n                message = \", Ultima vez visto \" + weekday + ' ' + day + ' de ' + month + ' del ' + year\n                try:\n                    message =  message + \", Velocidad \" + str(entry['speed']) + \" Km/h\"\n                except:\n                    pass\n                try:\n                    message =  message + \", Altitud \" + str(entry['altitude']) + \" metros\"\n                except:\n                    pass\n                results = Geocoder.reverse_geocode(float(entry['lat']), float(entry['lng']))\n                logging.info(results)\n                try:\n                    message = message + \", Calle \" + results[0].route\n                    message = message + \", Colonia \" + results[0].political\n                    if results[0].administrative_area_level_2:\n                        message = message + \", Municipio \" + results[0].administrative_area_level_2\n                    elif results[0].locality:\n                        message =  message + \", Municipio \" + results[0].locality\n                    message =  message + \", \" + results[0].administrative_area_level_1\n                    message =  message + \", \" + results[0].country\n                except:\n                    pass\n\n                speechmessage = stationdecoded + message\n                self.speaker.speechit(speechmessage)\n\n            modulemessage = station + message\n            alive(modulename=self.modulename, modulemessage=modulemessage)\n\n        else:\n            self.speaker.speechit(stationdecoded + \" no ha reportado ubicacion!\")\n\n# End of File\n"}
{"text": "\"\"\"\nDjango settings for cockroach_example project.\n\nGenerated by 'django-admin startproject' using Django 2.2.6.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/2.2/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/2.2/ref/settings/\n\"\"\"\n\nimport os\n\nfrom urllib.parse import urlparse\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/2.2/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = '0pld^66i)iv4df8km5vc%1^sskuqjf16jk&z=c^rk--oh6i0i^'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'cockroach_example',\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'cockroach_example.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'cockroach_example.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/2.2/ref/settings/#databases\n\nport = 26257\naddr = os.getenv('ADDR')\nif addr is not None:\n    url = urlparse(addr)\n    port = url.port\n\nDATABASES = {\n    'default': {\n        'ENGINE' : 'django_cockroachdb',\n        'NAME' : 'company_django',\n        'USER' : 'root',\n        'PASSWORD': '',\n        'HOST' : 'localhost',\n        'PORT' : port,\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/2.2/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/2.2/topics/i18n/\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'UTC'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/2.2/howto/static-files/\n\nSTATIC_URL = '/static/'\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\nSystem functions\n\"\"\"\n\nfrom __future__ import unicode_literals\nfrom __future__ import absolute_import\n\nfrom mathics.core.expression import Expression, String, strip_context\nfrom mathics.builtin.base import Builtin, Predefined\nfrom mathics import version_string\n\n\nclass Version(Predefined):\n    \"\"\"\n    <dl>\n    <dt>'$Version'\n        <dd>returns a string with the current Mathics version and the versions of relevant libraries.\n    </dl>\n\n    >> $Version\n     = Mathics ...\n    \"\"\"\n\n    name = '$Version'\n\n    def evaluate(self, evaluation):\n        return String(version_string.replace('\\n', ' '))\n\n\nclass Names(Builtin):\n    \"\"\"\n    <dl>\n    <dt>'Names[\"$pattern$\"]'\n        <dd>returns the list of names matching $pattern$.\n    </dl>\n\n    >> Names[\"List\"]\n     = {List}\n\n    The wildcard '*' matches any character:\n    >> Names[\"List*\"]\n     = {List, ListLinePlot, ListPlot, ListQ, Listable}\n\n    The wildcard '@' matches only lowercase characters:\n    >> Names[\"List@\"]\n     = {Listable}\n\n    >> x = 5;\n    >> Names[\"Global`*\"]\n     = {x}\n\n    The number of built-in symbols:\n    >> Length[Names[\"System`*\"]]\n     = ...\n\n    #> Length[Names[\"System`*\"]] > 350\n     = True\n    \"\"\"\n\n    def apply(self, pattern, evaluation):\n        'Names[pattern_]'\n\n        pattern = pattern.get_string_value()\n        if pattern is None:\n            return\n\n        names = set([])\n        for full_name in evaluation.definitions.get_matching_names(pattern):\n            short_name = strip_context(full_name)\n            names.add(short_name if short_name not in names else full_name)\n\n        # TODO: Mathematica ignores contexts when it sorts the list of\n        # names.\n        return Expression('List', *[String(name) for name in sorted(names)])\n\n\nclass Aborted(Predefined):\n    \"\"\"\n    <dl>\n    <dt>'$Aborted'\n        <dd>is returned by a calculation that has been aborted.\n    </dl>\n    \"\"\"\n\n    name = '$Aborted'\n\n\nclass Failed(Predefined):\n    \"\"\"\n    <dl>\n    <dt>'$Failed'\n        <dd>is returned by some functions in the event of an error.\n    </dl>\n\n    >> Get[\"nonexistent_file.m\"]\n     : Cannot open nonexistent_file.m.\n     = $Failed\n    \"\"\"\n\n    name = '$Failed'\n"}
{"text": "import os\nfrom unittest import TestCase\n\nfrom rosie.cli.actions import InitConfigAction\n\n\nclass TestInitConfigAction(TestCase):\n    config_file_path = 'test_file.py'\n\n    def tearDown(self):\n        if os.path.exists(self.config_file_path):\n            os.remove(self.config_file_path)\n\n    def test_action_gets_file_name_from_args(self):\n        # given\n        action = InitConfigAction()\n\n        # when\n        action.run(server_url='http://test.server.org',\n                   username='test_user',\n                   password='secret_pass',\n                   params_file=self.config_file_path)\n\n        # then\n        self.assertTrue(os.path.exists(self.config_file_path))\n\n    def test_action_stores_given_server_url(self):\n        # given\n        server_url = 'http://test.server.org'\n        expected_config_part = 'JENKINS_URL = \\'%s\\'' % server_url\n        action = InitConfigAction()\n\n        # when\n        action.run(server_url=server_url,\n                   username='test_user',\n                   password='secret_pass',\n                   params_file=self.config_file_path)\n        with open(self.config_file_path) as config_file:\n            s = config_file.read()\n\n        # then\n        self.assertTrue(s.find(expected_config_part) > -1,\n                        'Could not find configuration for server URL %s'\n                        % server_url)\n\n    def test_action_stores_given_user_name(self):\n        # given\n        user_name = 'test_user'\n        expected_config_part = 'USER_NAME = \\'%s\\'' % user_name\n        action = InitConfigAction()\n\n        # when\n        action.run(server_url='http://test.server.org',\n                   username=user_name,\n                   password='secret_pass',\n                   params_file=self.config_file_path)\n        with open(self.config_file_path) as config_file:\n            s = config_file.read()\n\n        # then\n        self.assertTrue(s.find(expected_config_part) > -1,\n                        'Could not find configuration part for user name %s'\n                        % user_name)\n\n    def test_action_stores_given_password(self):\n        # given\n        password = 'secret_pass'\n        expected_config_part = 'PASSWORD = \\'%s\\'' % password\n        action = InitConfigAction()\n\n        # when\n        action.run(server_url='http://test.server.org',\n                   username='test_user',\n                   password=password,\n                   params_file=self.config_file_path)\n        with open(self.config_file_path) as config_file:\n            s = config_file.read()\n\n        # then\n        self.assertTrue(s.find(expected_config_part) > -1,\n                        'Could not find configuration part for password %s'\n                        % password)\n"}
{"text": "from flask import request, make_response\n\nimport os\n\nfrom pwd import getpwnam\nfrom grp import getgrnam\n\nfrom ConfigParser import ConfigParser\n\n# TODO: why not use SafeConfigParser() ??\nconfig_parser = ConfigParser()\n\nCONFFILE = os.getenv('DIRECTOR_CFG')\n\nif not CONFFILE:\n    CONFFILE = \"/etc/cpsdirector/director.cfg\"\n\n# Config values for unit testing\nif os.getenv('DIRECTOR_TESTING'):\n    CONFFILE = 'director.cfg.example'\n\nconfig_parser.read(CONFFILE)\n\n# from conpaas.core import https\n# https.client.conpaas_init_ssl_ctx('/etc/cpsdirector/certs', 'director')\n\nif os.getenv('DIRECTOR_TESTING'):\n    if not config_parser.has_section('iaas'):\n        config_parser.add_section('iaas')\n    if not config_parser.has_section('director'):\n        config_parser.add_section('director')\n\n    # dummy cloud\n    config_parser.set(\"iaas\", \"DRIVER\", \"dummy\")\n    config_parser.set(\"iaas\", \"USER\", \"dummy\")\n\n    # separate database\n    config_parser.set(\"director\", \"DATABASE_URI\", \"sqlite:///director-test.db\")\n    config_parser.set(\"director\", \"DIRECTOR_URL\", \"\")\n\n    # dummy data dir for manifests\n    config_parser.set(\"director\", \"USERDATA_DIR\", \"/tmp/\")\n\ndef get_director_url():\n    return config_parser.get(\"director\", \"DIRECTOR_URL\")\n\ndef get_userdata_dir():\n    return config_parser.get(\"director\", \"USERDATA_DIR\")\n\ndef chown(path, username, groupname):\n    os.chown(path, getpwnam(username).pw_uid, getgrnam(groupname).gr_gid)\n\ndef log(msg):\n    try:\n        print >> request.environ['wsgi.errors'], msg\n    except RuntimeError:\n        print msg\n\ndef log_error(msg):\n    log('ERROR: %s' % msg)\n\ndef build_response(data):\n    response = make_response(data)\n    response.headers['Access-Control-Allow-Origin'] = '*'\n    response.headers['Connection'] = 'close'\n    return response\n\ndef error_response(message):\n    return { 'error': message }\n"}
{"text": "\r\n\r\nimport sys\r\nfrom solution import Solution\r\n# from classes import ?\r\n\r\n\r\nclass TestSuite:\r\n    \r\n    def run(self):\r\n        self.test000()\r\n        self.test001()\r\n        self.test002()\r\n        self.test003()\r\n        self.test004()\r\n\r\n    def test000(self):\r\n        \r\n        print 'test 000\\n'\r\n        \r\n        n = 0\r\n        r = Solution().climbStairs(n)\r\n        print '  input:\\t', n\r\n        print '  expect:\\t', 0\r\n        print '  output:\\t', r\r\n        print\r\n\r\n    def test001(self):\r\n        \r\n        print 'test 001\\n'\r\n        \r\n        n = 1\r\n        r = Solution().climbStairs(n)\r\n        print '  input:\\t', n\r\n        print '  expect:\\t', 1\r\n        print '  output:\\t', r\r\n        print\r\n\r\n    def test002(self):\r\n        \r\n        print 'test 002\\n'\r\n        \r\n        n = 2\r\n        r = Solution().climbStairs(n)\r\n        print '  input:\\t', n\r\n        print '  expect:\\t', 2\r\n        print '  output:\\t', r\r\n        print\r\n\r\n    def test003(self):\r\n        \r\n        print 'test 003\\n'\r\n        \r\n        n = 3\r\n        r = Solution().climbStairs(n)\r\n        print '  input:\\t', n\r\n        print '  expect:\\t', 3\r\n        print '  output:\\t', r\r\n        print\r\n\r\n    def test004(self):\r\n        \r\n        print 'test 003\\n'\r\n        \r\n        n = 4\r\n        r = Solution().climbStairs(n)\r\n        print '  input:\\t', n\r\n        print '  expect:\\t', 5\r\n        print '  output:\\t', r\r\n        print\r\n\r\n        \r\ndef main(argv):\r\n    TestSuite().run()\r\n\r\nif __name__ == '__main__':\r\n    main(sys.argv)\r\n    "}
{"text": "# coding=utf-8\n# --------------------------------------------------------------------------\n# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT License. See License.txt in the project root for\n# license information.\n#\n# Code generated by Microsoft (R) AutoRest Code Generator.\n# Changes may cause incorrect behavior and will be lost if the code is\n# regenerated.\n# --------------------------------------------------------------------------\n\nfrom msrest.serialization import Model\n\n\nclass ExpressRouteCircuitSku(Model):\n    \"\"\"Contains SKU in an ExpressRouteCircuit.\n\n    :param name: The name of the SKU.\n    :type name: str\n    :param tier: The tier of the SKU. Possible values are 'Standard' and\n     'Premium'. Possible values include: 'Standard', 'Premium'\n    :type tier: str or\n     ~azure.mgmt.network.v2017_11_01.models.ExpressRouteCircuitSkuTier\n    :param family: The family of the SKU. Possible values are: 'UnlimitedData'\n     and 'MeteredData'. Possible values include: 'UnlimitedData', 'MeteredData'\n    :type family: str or\n     ~azure.mgmt.network.v2017_11_01.models.ExpressRouteCircuitSkuFamily\n    \"\"\"\n\n    _attribute_map = {\n        'name': {'key': 'name', 'type': 'str'},\n        'tier': {'key': 'tier', 'type': 'str'},\n        'family': {'key': 'family', 'type': 'str'},\n    }\n\n    def __init__(self, name=None, tier=None, family=None):\n        super(ExpressRouteCircuitSku, self).__init__()\n        self.name = name\n        self.tier = tier\n        self.family = family\n"}
{"text": "import argparse\nfrom typing import Any\n\nimport sqlalchemy as sa\n\nfrom drillsrs import db, util\nfrom drillsrs.cmd.command_base import CommandBase\n\n\ndef _print_single_tag(session: Any, index: int, tag: db.Tag) -> None:\n    tag_usages = (\n        session.query(sa.func.count(db.CardTag.tag_id))\n        .filter(db.CardTag.tag_id == tag.id)\n        .scalar()\n    ) or 0\n    print(\"Tag #%d\" % (index + 1))\n    print(\"Name:    %s\" % tag.name)\n    print(\"Color:   %s\" % tag.color)\n    print(\"Preview: [%s]\" % util.format_card_tag(tag))\n    print(\"Usages:  %d\" % tag_usages)\n    print()\n\n\nclass ListTagsCommand(CommandBase):\n    names = [\"list-tags\"]\n    description = \"print all tags in a deck\"\n\n    def decorate_arg_parser(self, parser: argparse.ArgumentParser) -> None:\n        parser.add_argument(\"deck\", nargs=\"?\", help=\"choose the deck name\")\n\n    def run(self, args: argparse.Namespace) -> None:\n        deck_name: str = args.deck\n\n        with db.session_scope() as session:\n            deck = db.get_deck_by_name(session, deck_name)\n            tags = (\n                session.query(db.Tag).filter(db.Tag.deck_id == deck.id).all()\n            )\n\n            if not tags:\n                print(\"No tags to show.\")\n                return\n\n            for i, tag in enumerate(tags):\n                _print_single_tag(session, i, tag)\n"}
{"text": "import copy\nfrom corehq.apps.accounting.models import Subscription\nfrom corehq.apps.domain.models import Domain\nfrom corehq.pillows.base import HQPillow\nfrom corehq.pillows.mappings.domain_mapping import DOMAIN_MAPPING, DOMAIN_INDEX\nfrom dimagi.utils.decorators.memoized import memoized\nfrom django.conf import settings\nfrom django_countries.countries import OFFICIAL_COUNTRIES\n\n\nclass DomainPillow(HQPillow):\n    \"\"\"\n    Simple/Common Case properties Indexer\n    \"\"\"\n    document_class = Domain\n    couch_filter = \"domain/domains_inclusive\"\n    es_alias = \"hqdomains\"\n    es_type = \"hqdomain\"\n    es_index = DOMAIN_INDEX\n    default_mapping = DOMAIN_MAPPING\n    es_meta = {\n        \"settings\": {\n            \"analysis\": {\n                \"analyzer\": {\n                    \"default\": {\n                        \"type\": \"custom\",\n                        \"tokenizer\": \"whitespace\",\n                        \"filter\": [\"lowercase\"]\n                    },\n                    \"comma\": {\n                        \"type\": \"pattern\",\n                        \"pattern\": \"\\s*,\\s*\"\n                    },\n                }\n            }\n        }\n    }\n\n    def get_unique_id(self):\n        return DOMAIN_INDEX\n\n    @memoized\n    def calc_meta(self):\n        \"\"\"\n        override of the meta calculator since we're separating out all the types,\n        so we just do a hash of the \"prototype\" instead to determined md5\n        \"\"\"\n        return self.calc_mapping_hash({\"es_meta\": self.es_meta,\n                                       \"mapping\": self.default_mapping})\n\n    def change_transform(self, doc_dict):\n        doc_ret = copy.deepcopy(doc_dict)\n        sub =  Subscription.objects.filter(\n                subscriber__domain=doc_dict['name'],\n                is_active=True)\n        doc_dict['deployment'] = doc_dict.get('deployment', None) or {}\n        countries = doc_dict['deployment'].get('countries', [])\n        doc_ret['deployment']['countries'] = []\n        if sub:\n            doc_ret['subscription'] = sub[0].plan_version.plan.edition\n        for country in countries:\n            doc_ret['deployment']['countries'].append(OFFICIAL_COUNTRIES[country])\n        return doc_ret\n"}
{"text": "{% if cookiecutter.use_celery == 'y' %}\nimport os\nfrom celery import Celery\nfrom django.apps import apps, AppConfig\nfrom django.conf import settings\n\n\nif not settings.configured:\n    # set the default Django settings module for the 'celery' program.\n    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings.local')  # pragma: no cover\n\n\napp = Celery('{{cookiecutter.project_slug}}')\n\n\nclass CeleryAppConfig(AppConfig):\n    name = '{{cookiecutter.project_slug}}.taskapp'\n    verbose_name = 'Celery Config'\n\n    def ready(self):\n        # Using a string here means the worker will not have to\n        # pickle the object when using Windows.\n        # - namespace='CELERY' means all celery-related configuration keys\n        #   should have a `CELERY_` prefix.\n        app.config_from_object('django.conf:settings', namespace='CELERY')\n        installed_apps = [app_config.name for app_config in apps.get_app_configs()]\n        app.autodiscover_tasks(lambda: installed_apps, force=True)\n\n        {% if cookiecutter.use_sentry == 'y' -%}\n        if hasattr(settings, 'RAVEN_CONFIG'):\n            # Celery signal registration\n{% if cookiecutter.use_pycharm == 'y' -%}\n\t    # Since raven is required in production only,\n            # imports might (most surely will) be wiped out\n            # during PyCharm code clean up started\n            # in other environments.\n            # @formatter:off\n{%- endif %}\n            from raven import Client as RavenClient\n            from raven.contrib.celery import register_signal as raven_register_signal\n            from raven.contrib.celery import register_logger_signal as raven_register_logger_signal\n{% if cookiecutter.use_pycharm == 'y' -%}\n            # @formatter:on\n{%- endif %}\n\n            raven_client = RavenClient(dsn=settings.RAVEN_CONFIG['dsn'])\n            raven_register_logger_signal(raven_client)\n            raven_register_signal(raven_client)\n        {%- endif %}\n\n\n@app.task(bind=True)\ndef debug_task(self):\n    print(f'Request: {self.request!r}')  # pragma: no cover\n{% else %}\n# Use this as a starting point for your project with celery.\n# If you are not using celery, you can remove this app\n{% endif -%}\n"}
{"text": "#Some Codes from \"Rename tracks to take source filename\", -- Set pan for selected track(s) (SPK77)\n#This script by junh1024 sets pans of tracks according to their suffixes. Useful for say, film dialogue. See global variables below for pans.\n\nfrom reaper_python import *\nfrom contextlib import contextmanager\n# import os\n\n@contextmanager\ndef undoable(message):\n\tRPR_Undo_BeginBlock2(0)\n\ttry:\n\t\tyield\n\tfinally:\n\t\tRPR_Undo_EndBlock2(0, message, -1)\n\ndebug = True #disable for using\n\ndef msg(m):\n\tif 'debug' in globals():\n\t\tRPR_ShowConsoleMsg(m)\n\n#Track suffixes\nL=-0.15 #initial Left pan, everything else is derived modularly. Change to taste.\nR=-L\nLW=2*L\nRW=-LW\nLC=(2*L)/3\nRC=-LC\n\n#these have parenting set later on\nSL=-1\nSR=-SL\n\nRR=C=0 #last 2 of naRR,center pans\n\n# msg(L)\n# msg(R)\n# msg(LW)\n# msg(RW)\n# msg(LC)\n# msg(RC)\n\nwith undoable(\"Set Pan According To Track Suffix\"):\n\t# for i in range(RPR_CountTracks(0)): #for all tracks, get track\n\t\t# trackId = RPR_GetTrack(0, i)\n\n\tfor i in range(RPR_CountSelectedTracks(0)): #for selected tracks, get track\n\t\ttrackId = RPR_GetSelectedTrack(0, i)\n\t\t\n\t\tsuffix = str(RPR_GetSetMediaTrackInfo_String(trackId, \"P_NAME\", \"\", False )[3] )[-2:].lstrip().upper() #get actual track name, last 2 chars, remove whitespace)\n\t\t\n\t\tif(suffix==''):\n\t\t\tcontinue\n\t\t\n\t\tif(suffix[0] == 'S'): #anything rear/surround. I'm not doing else cuz there may be top pans.\n\t\t\tRPR_SetMediaTrackInfo_Value(trackId, \"C_MAINSEND_OFFS\", 4) #set parent ch 5/6 rear/surround\n\t\t\n\t\tif(suffix in globals()): #if a suffix is one of the global preset pans, see global variables above\n\t\t\tRPR_SetMediaTrackInfo_Value(trackId, \"D_PAN\", eval(suffix)) #set it according to the pan designated by the suffix REFLECTION USED\n\t\t\n\t\t\n\t\tif(suffix in ('C,RR,L,R,LW,RW,LC,RC'.split(','))): #anything front\n\t\t\tRPR_SetMediaTrackInfo_Value(trackId, \"C_MAINSEND_OFFS\", 0)\n\n\n"}
{"text": "# -*- coding: utf-8 -*-\n#\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nfrom airflow.contrib.hooks.segment_hook import SegmentHook\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\n\nclass SegmentTrackEventOperator(BaseOperator):\n    \"\"\"\n    Send Track Event to Segment for a specified user_id and event\n\n    :param user_id: The ID for this user in your database\n    :type user_id: string\n    :param event: The name of the event you're tracking\n    :type event: string\n    :param properties: A dictionary of properties for the event.\n    :type properties: dict\n    :param segment_conn_id: The connection ID to use when connecting to Segment.\n    :type segment_conn_id: string\n    :param segment_debug_mode: Determines whether Segment should run in debug mode.\n        Defaults to False\n    :type segment_debug_mode: boolean\n    \"\"\"\n    template_fields = ('user_id', 'event', 'properties')\n    ui_color = '#ffd700'\n\n    @apply_defaults\n    def __init__(self,\n                 user_id,\n                 event,\n                 properties=None,\n                 segment_conn_id='segment_default',\n                 segment_debug_mode=False,\n                 *args,\n                 **kwargs):\n        super(SegmentTrackEventOperator, self).__init__(*args, **kwargs)\n        self.user_id = user_id\n        self.event = event\n        properties = properties or {}\n        self.properties = properties\n        self.segment_debug_mode = segment_debug_mode\n        self.segment_conn_id = segment_conn_id\n\n    def execute(self, context):\n        hook = SegmentHook(segment_conn_id=self.segment_conn_id,\n                           segment_debug_mode=self.segment_debug_mode)\n\n        self.log.info(\n            'Sending track event ({0}) for user id: {1} with properties: {2}'.\n            format(self.event, self.user_id, self.properties))\n\n        hook.track(self.user_id, self.event, self.properties)\n"}
{"text": "#!/usr/bin/python\n'''\nCreates text file of Cocoa superclasses in given filename or in\n./cocoa_indexes/classes.txt by default.\n'''\nimport os, re\nfrom cocoa_definitions import write_file, find\nfrom commands import getoutput\n\ndef find_headers(frameworks):\n    '''Returns a dictionary of the headers for each given framework.'''\n    headers_and_frameworks = {}\n    for framework in frameworks:\n        headers_and_frameworks[framework] = \\\n                ' '.join(find('/System/Library/Frameworks/%s.framework'\n                         % framework, '.h'))\n    return headers_and_frameworks\n\ndef get_classes(header_files_and_frameworks):\n    '''Returns list of Cocoa Protocols classes & their framework.'''\n    classes = {}\n    for framework, files in header_files_and_frameworks:\n        for line in getoutput(r\"grep -ho '@\\(interface\\|protocol\\) [A-Z]\\w\\+' \"\n                                       + files).split(\"\\n\"):\n            cocoa_class = re.search(r'[A-Z]\\w+', line)\n            if cocoa_class and not classes.has_key(cocoa_class.group(0)):\n                classes[cocoa_class.group(0)] = framework\n    classes = classes.items()\n    classes.sort()\n    return classes\n\ndef get_superclasses(classes_and_frameworks):\n    '''\n    Given a list of Cocoa classes & their frameworks, returns a list of their\n    superclasses in the form: \"class\\|superclass\\|superclass\\|...\".\n    '''\n    args = ''\n    for classname, framework in classes_and_frameworks:\n        args += classname + ' ' + framework + ' '\n    return getoutput('./superclasses ' + args).split(\"\\n\")\n\ndef output_file(fname=None):\n    '''Output text file of Cocoa classes to given filename.'''\n    if fname is None:\n        fname = './cocoa_indexes/classes.txt'\n    if not os.path.isdir(os.path.dirname(fname)):\n        os.mkdir(os.path.dirname(fname))\n\n    frameworks = ('Foundation', 'AppKit', 'AddressBook', 'CoreData',\n                  'PreferencePanes', 'QTKit', 'ScreenSaver', 'SyncServices',\n                  'WebKit')\n    headers_and_frameworks = find_headers(frameworks).items()\n\n    superclasses = get_superclasses(get_classes(headers_and_frameworks))\n    write_file(fname, superclasses)\n\nif __name__ == '__main__':\n    from sys import argv\n    output_file(argv[1] if len(argv) > 1 else None)\n"}
{"text": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\ndef query_script(client):\n    # [START bigquery_query_script]\n    # TODO(developer): Import the client library.\n    # from google.cloud import bigquery\n\n    # TODO(developer): Construct a BigQuery client object.\n    # client = bigquery.Client()\n\n    # Run a SQL script.\n    sql_script = \"\"\"\n    -- Declare a variable to hold names as an array.\n    DECLARE top_names ARRAY<STRING>;\n\n    -- Build an array of the top 100 names from the year 2017.\n    SET top_names = (\n    SELECT ARRAY_AGG(name ORDER BY number DESC LIMIT 100)\n    FROM `bigquery-public-data.usa_names.usa_1910_2013`\n    WHERE year = 2000\n    );\n\n    -- Which names appear as words in Shakespeare's plays?\n    SELECT\n    name AS shakespeare_name\n    FROM UNNEST(top_names) AS name\n    WHERE name IN (\n    SELECT word\n    FROM `bigquery-public-data.samples.shakespeare`\n    );\n    \"\"\"\n    parent_job = client.query(sql_script)\n\n    # Wait for the whole script to finish.\n    rows_iterable = parent_job.result()\n    print(\"Script created {} child jobs.\".format(parent_job.num_child_jobs))\n\n    # Fetch result rows for the final sub-job in the script.\n    rows = list(rows_iterable)\n    print(\n        \"{} of the top 100 names from year 2000 also appear in Shakespeare's works.\".format(\n            len(rows)\n        )\n    )\n\n    # Fetch jobs created by the SQL script.\n    child_jobs_iterable = client.list_jobs(parent_job=parent_job)\n    for child_job in child_jobs_iterable:\n        child_rows = list(child_job.result())\n        print(\n            \"Child job with ID {} produced {} row(s).\".format(\n                child_job.job_id, len(child_rows)\n            )\n        )\n\n    # [END bigquery_query_script]\n"}
{"text": "'''\nThe MIT License\n\nCopyright (c) Bo Lopker, http://blopker.com\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n'''\n'''\nModule to determine the correct downloader to use.\nBy @blopker\n'''\nfrom . import requests\nfrom . import null\nfrom . import wget\nfrom ... import logger\nlog = logger.get(__name__)\n\n# Check if this OS supports SSL\ntry:\n    import ssl\n    SSL = True\nexcept ImportError:\n    SSL = False\n\n\ndef get():\n    if not SSL and wget.is_available():\n        log.debug('Using WGET downloader.')\n        return wget.WgetDownloader()\n    if SSL:\n        log.debug('Using Requests downloader.')\n        return requests.RequestsDownloader()\n    log.error('No suitable downloader found. Everything is terrible.')\n    return null.NullDownloader()\n"}
{"text": "from process_base import *\nfrom targets import *\n\nimport subprocess\nimport os\n\n\nclass ProcessDeviceIo(ProcessBase):\n\tdef __init__(self, Controller, crashdump_folder, breakpoint_handler, pid, ph, unique_identifier, verbose, logger):\n\t\t# Specific options\n\t\tself.path_to_exe = b\"C:\\\\Windows\\\\System32\\\\notepad.exe\"\n\t\tself.command_line = b\"notepad.exe\"\n\t\tself.logger = logger\n\t\t\n\t\t# Initialize\n\t\tself.initialize(Controller, self.__class__.__name__, crashdump_folder, breakpoint_handler, pid, ph, unique_identifier, verbose)\n\t\t\n\tdef on_debugger_attached(self, Engine):\n\t\t# Set the types\n\t\tself.Engine = Engine\n\t\tself.types = meddle_types(Engine)\n\t\t\n\t\t# Add the targets\n\t\tEngine.AddTarget(Target_Handles)\n\t\tEngine.AddTarget(Target_DeviceIoControl)\n\t\t\n\t\t# Handle process loaded\n\t\tEngine.HandleProcessLoaded()\n\n\t\t# Start an auto-it script\n\t\ttry:\n\t\t\tsubprocess.Popen(['autoit3.exe', os.path.join(os.path.dirname(__file__), \"..\", \"autoit\", \"notepad_print.au3\"), str(self.pid), \">nul\"], shell=True)\n\t\texcept:\n\t\t\tprint \"Warning: autoit3.exe not found on path. Please install it and add it to path to increase the attack surface.\"\n\t\t\n\t\t# Resume the process that we created suspended. This is called just after the debugger has been attached.\n\t\tif self.start_th >= 0:\n\t\t\twindll.kernel32.ResumeThread(self.start_th);\n\n\tdef log_csv(self, fields):\n\t\tself.logger.log_event(fields)\n\t\t\n\t\n\t\n\t\t\n\t\t\n\t"}
{"text": "'''Arsenal audit UI.'''\n#  Copyright 2015 CityGrid Media, LLC\n#\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport logging\nfrom pyramid.view import view_config\nfrom arsenalweb.views import (\n    _api_get,\n    get_authenticated_user,\n    get_nav_urls,\n    get_pag_params,\n    site_layout,\n    )\n\nLOG = logging.getLogger(__name__)\n\n@view_config(route_name='data_centers_audit', permission='view', renderer='arsenalweb:templates/all_audit.pt')\n@view_config(route_name='hardware_profiles_audit', permission='view', renderer='arsenalweb:templates/all_audit.pt')\n@view_config(route_name='ip_addresses_audit', permission='view', renderer='arsenalweb:templates/all_audit.pt')\n@view_config(route_name='network_interfaces_audit', permission='view', renderer='arsenalweb:templates/all_audit.pt')\n@view_config(route_name='node_groups_audit', permission='view', renderer='arsenalweb:templates/all_audit.pt')\n@view_config(route_name='nodes_audit', permission='view', renderer='arsenalweb:templates/all_audit.pt')\n@view_config(route_name='operating_systems_audit', permission='view', renderer='arsenalweb:templates/all_audit.pt')\n@view_config(route_name='statuses_audit', permission='view', renderer='arsenalweb:templates/all_audit.pt')\n@view_config(route_name='tags_audit', permission='view', renderer='arsenalweb:templates/all_audit.pt')\ndef view_all_audit(request):\n    '''Handle requests for the overall object type audit UI route.'''\n\n    page_title_type = 'objects/'\n    auth_user = get_authenticated_user(request)\n    (perpage, offset) = get_pag_params(request)\n\n    meta = {\n        'data_centers_audit': {\n            'page_type': 'Data Centers',\n            'object_type': 'data_centers',\n        },\n        'hardware_profiles_audit': {\n            'page_type': 'Hardware Profiles',\n            'object_type': 'hardware_profiles',\n        },\n        'ip_addresses_audit': {\n            'page_type': 'IpAddress',\n            'object_type': 'ip_addresses',\n        },\n        'network_interfaces_audit': {\n            'page_type': 'NetworkInterface',\n            'object_type': 'network_interfaces',\n        },\n        'nodes_audit': {\n            'page_type': 'Node',\n            'object_type': 'nodes',\n        },\n        'node_groups_audit': {\n            'page_type': 'Node Group',\n            'object_type': 'node_groups',\n        },\n        'operating_systems_audit': {\n            'page_type': 'Operating Systems',\n            'object_type': 'operating_systems',\n        },\n        'statuses_audit': {\n            'page_type': 'Status',\n            'object_type': 'statuses',\n        },\n        'tags_audit': {\n            'page_type': 'Tags',\n            'object_type': 'tags',\n        },\n    }\n\n    params = meta[request.matched_route.name]\n    page_title_name = '{0}_audit'.format(params['object_type'])\n    uri = '/api/{0}_audit'.format(params['object_type'])\n\n    payload = {}\n    for k in request.GET:\n        payload[k] = request.GET[k]\n\n    # Force the UI to 50 results per page\n    if not perpage:\n        perpage = 50\n\n    payload['perpage'] = perpage\n\n    LOG.info('UI requesting data from API={0},payload={1}'.format(uri, payload))\n\n    resp = _api_get(request, uri, payload)\n\n    total = 0\n    objects_audit = []\n\n    if resp:\n        total = resp['meta']['total']\n        objects_audit = resp['results']\n\n    nav_urls = get_nav_urls(request.path, offset, perpage, total, payload)\n\n    # Used by the columns menu to determine what to show/hide.\n    column_selectors = [\n        {'name': 'created', 'pretty_name': 'Date Created'},\n        {'name': 'field', 'pretty_name': 'Field'},\n        {'name': 'new_value', 'pretty_name': 'New Value'},\n        {'name': 'node_audit_id', 'pretty_name': 'Audit ID'},\n        {'name': 'object_id', 'pretty_name': '{0} ID'.format(params['page_type'])},\n        {'name': 'old_value', 'pretty_name': 'Old Value'},\n        {'name': 'updated_by', 'pretty_name': 'Updated By'},\n    ]\n\n    return {\n        'au': auth_user,\n        'column_selectors': column_selectors,\n        'layout': site_layout('max'),\n        'nav_urls': nav_urls,\n        'objects_audit': objects_audit,\n        'offset': offset,\n        'page_title_name': page_title_name,\n        'page_title_type': page_title_type,\n        'params': params,\n        'perpage': perpage,\n        'total': total,\n    }\n"}
{"text": "import numpy as np\nfrom matplotlib import pyplot as plt\nfrom pandas import Series\nfrom mpl_toolkits.mplot3d import axes3d\n\n\ndef plotData(X, y):\n    pos = X[np.where(y == 1, True, False).flatten()]\n    neg = X[np.where(y == 0, True, False).flatten()]\n    plt.plot(pos[:, 0], pos[:, 1], 'X', markersize=7, markeredgecolor='black', markeredgewidth=2)\n    plt.plot(neg[:, 0], neg[:, 1], 'D', markersize=7, markeredgecolor='black', markerfacecolor='springgreen')\n\n\ndef plotDecisionBoundary(theta, X, y):\n    \"\"\"\n    Plots the data points X and y into a new figure with the decision boundary defined by theta\n      PLOTDECISIONBOUNDARY(theta, X,y) plots the data points with + for the\n      positive examples and o for the negative examples. X is assumed to be\n      a either\n      1) Mx3 matrix, where the first column is an all-ones column for the\n         intercept.\n      2) MxN, N>3 matrix, where the first column is all-ones\n    \"\"\"\n\n    # Plot Data\n    plt.figure(figsize=(15, 10))\n    plotData(X[:, 1:], y)\n\n    if X.shape[1] <= 3:\n        # Only need 2 points to define a line, so choose two endpoints\n        plot_x = np.array([min(X[:, 2]),  max(X[:, 2])])\n\n        # Calculate the decision boundary line\n        plot_y = (-1. / theta[2]) * (theta[1] * plot_x + theta[0])\n\n        # Plot, and adjust axes for better viewing\n        plt.plot(plot_x, plot_y)\n\n    else:\n        # Here is the grid range\n        u = np.linspace(-1, 1.5, 50)\n        v = np.linspace(-1, 1.5, 50)\n        z = [\n                np.array([mapFeature2(u[i], v[j]).dot(theta) for i in range(len(u))])\n                for j in range(len(v))\n            ]\n        print(z[0])\n        plt.contour(u, v, z, levels=[0.0])\n\n    # Legend, specific for the exercise\n    # axis([30, 100, 30, 100])\n\n\ndef mapFeature(X, degree=6):\n    \"\"\"\n    Feature mapping function to polynomial features\n\n    MAPFEATURE(X, degree) maps the two input features\n    to quadratic features used in the regularization exercise.\n\n    Returns a new feature array with more features, comprising of\n    X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n    \"\"\"\n    quads = Series([X.iloc[0] ** (i-j) * X.iloc[1] ** j for i in range(1, degree + 1) for j in range(i + 1)])\n    return Series([1]).append([X, quads])\n\n\ndef mapFeature2(X1, X2, degree=6):\n    \"\"\"\n    Feature mapping function to polynomial features\n\n    MAPFEATURE(X, degree) maps the two input features\n    to quadratic features used in the regularization exercise.\n\n    Returns a new feature array with more features, comprising of\n    X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n    \"\"\"\n    quads = Series([X1 ** (i - j) * X2 ** j for i in range(1, degree + 1) for j in range(i + 1)])\n    return Series([1]).append([Series(X1), Series(X2), quads])\n"}
{"text": "\"\"\"\nDjango settings for oervoer project.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.7/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.7/ref/settings/\nbojan: 06 21543084\n\"\"\"\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nimport os\nfrom django.conf.global_settings import TEMPLATE_CONTEXT_PROCESSORS\nBASE_DIR = os.path.dirname(os.path.dirname(__file__))\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/1.7/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = 're78rq!(q%1zvygez@83+9wu+$ew$!hy(v&)4_wkctte-qhyhe'\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = True\n\nTEMPLATE_DEBUG = True\n\nALLOWED_HOSTS = []\n\n\n# Application definition\n\nINSTALLED_APPS = (\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'django_tables2',\n    'wizard',\n)\n\nMIDDLEWARE_CLASSES = (\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n)\n\nROOT_URLCONF = 'oervoer.urls'\n\nWSGI_APPLICATION = 'oervoer.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/1.7/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.sqlite3',\n        'NAME': os.path.join(BASE_DIR, 'oervoer.db'),\n    }\n}\n#TEMPLATE_CONTEXT_PROCESSORS =\n#(\n#    'django.core.context_processors.request',\n#)\n# Internationalization\n# https://docs.djangoproject.com/en/1.7/topics/i18n/\ntcp = list(TEMPLATE_CONTEXT_PROCESSORS)\ntcp.append('django.core.context_processors.request')\nTEMPLATE_CONTEXT_PROCESSORS = tuple(tcp)\n#TEMPLATE_CONTEXT_PROCESSORS =(\"django.contrib.auth.context_processors.auth\",\n#\"django.core.context_processors.debug\",\n#\"django.core.context_processors.i18n\",\n#\"django.core.context_processors.media\",\n#\"django.core.context_processors.static\",\n#\"django.core.context_processors.tz\",\n#\"django.contrib.messages.context_processors.messages\",\n#'django.core.context_processors.request',)\n\nLANGUAGE_CODE = 'en-us'\n\nTIME_ZONE = 'CET'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = True\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.7/howto/static-files/\n\nSTATIC_URL = '/static/'\n"}
{"text": "\"\"\"\n\n\"\"\"\n# Prevent Python2 from getting confused about 'bson' and 'pymarshal.bson'\nfrom __future__ import absolute_import\nimport datetime\nimport time\n\n# Will raise an import error if the user hasn't installed 'bson'\nimport bson\n\nfrom .json import JSON_TYPES\nfrom .util.marshal import *\nfrom .util.pm_assert import pm_assert\nfrom .util.type import *\n\n\n__all__ = [\n    'bson',\n    'ExtraKeysError',\n    'InitArgsError',\n    'marshal_bson',\n    'MongoDocument',\n    'pm_assert',\n    'type_assert',\n    'type_assert_dict',\n    'type_assert_iter',\n    'unmarshal_bson',\n]\n\n\nBSON_TYPES = (\n    bool,\n    bson.ObjectId,\n    datetime.datetime,\n    dict,\n    float,\n    int,\n    list,\n    tuple,\n    str,\n    type(None),\n)\n\n\nclass MongoDocument:\n    \"\"\" Abstract class to facilitate inserting into MongoDB.\n        Inherit this for classes that represent BSON documents.\n\n        Assumes that you assigned the ObjectId in your\n        class like:\n\n            def __init__(\n                self,\n                ...,\n                _id=None,\n                ...,\n            ):\n\n            self._id = type_assert(\n                _id,\n                bson.ObjectId,\n                allow_none=True,\n            )\n    \"\"\"\n    # If you need to override this, just add '_id' to the new list\n    _marshal_exclude_none_keys = ['_id']\n\n    def json(\n        self,\n        include_id=False,\n        date_fmt=None,\n        object_id_fmt=str,\n    ):\n        \"\"\" Helper method to convert to MongoDB documents to JSON\n\n            This includes helpers to convert non-JSON compatible types\n            to valid JSON types.  HOWEVER, it cannot recurse into nested\n            classes.\n\n        Args:\n            include_id:    bool, True to cast _id to a str,\n                           False to omit from the result\n            date_fmt:      str-or-None:  None to cast to UNIX timestamp,\n                           str (strftime format) to convert to string,\n                           for example: '%Y-%m-%d_%H:%M:%S'\n            object_id_fmt: type, Cast the bson.ObjectId's to this format,\n                           or None to exclude.  This only applies to\n                           ObjectId variables other than _id.\n        Returns:\n            dict\n        \"\"\"\n        has_slots, d = _get_dict(self)\n        _id = self._id\n        if not include_id:\n            self._id = None\n\n        object_ids = {\n            k: v for k, v in d.items()\n            if isinstance(v, bson.ObjectId)\n        }\n\n        for k, v in object_ids.items():\n            if object_id_fmt is None:\n                setattr(self, k, None)\n            else:\n                setattr(self, k, object_id_fmt(v))\n\n        datetimes = {\n            k: v for k, v in d.items()\n            if isinstance(v, datetime.datetime)\n        }\n\n        for k, v in datetimes.items():\n            if date_fmt is None:\n                ts = (\n                    time.mktime(\n                        v.timetuple(),\n                    )\n                    + v.microsecond\n                    / 1e6\n                )\n                setattr(self, k, ts)\n            else:\n                setattr(self, k, v.strftime(date_fmt))\n\n        j = marshal_dict(\n            self,\n            JSON_TYPES,\n            'json',\n            include_id=include_id,\n            date_fmt=date_fmt,\n            object_id_fmt=object_id_fmt,\n        )\n        self._id = _id\n\n        for k, v in object_ids.items():\n            setattr(self, k, v)\n\n        for k, v in datetimes.items():\n            setattr(self, k, v)\n\n        return j\n\n\ndef marshal_bson(\n    obj,\n    types=BSON_TYPES,\n    fields=None,\n):\n    \"\"\" Recursively marshal a Python object to a BSON-compatible dict\n        that can be passed to PyMongo, Motor, etc...\n\n    Args:\n        obj:    object, It's members can be nested Python\n                objects which will be converted to dictionaries\n        types:  tuple-of-types, The BSON primitive types, typically\n                you would not change this\n        fields: None-list-of-str, Explicitly marshal only these fields\n    Returns:\n        dict\n    \"\"\"\n    return marshal_dict(\n        obj,\n        types,\n        fields=fields,\n    )\n\n\ndef unmarshal_bson(\n    obj,\n    cls,\n    allow_extra_keys=True,\n    ctor=None,\n):\n    \"\"\" Unmarshal @obj into @cls\n\n    Args:\n        obj:              dict, A BSON object\n        cls:              type, The class to unmarshal into\n        allow_extra_keys: bool, False to raise an exception when extra\n                          keys are present, True to ignore\n        ctor:             None-or-static-method: Use this method as the\n                          constructor instead of __init__\n    Returns:\n        instance of @cls\n    Raises:\n        ExtraKeysError: If allow_extra_keys == False, and extra keys\n                        are present in @obj and not in @cls.__init__\n        ValueError:     If @cls.__init__ does not contain a self argument\n    \"\"\"\n    return unmarshal_dict(\n        obj,\n        cls,\n        allow_extra_keys,\n        ctor=ctor,\n    )\n"}
{"text": "# -*- coding: utf-8 -*-\n\n\"\"\"\n***************************************************************************\n    ExtendLines.py\n    --------------------\n    Date                 : October 2016\n    Copyright            : (C) 2016 by Nyall Dawson\n    Email                : nyall dot dawson at gmail dot com\n***************************************************************************\n*                                                                         *\n*   This program is free software; you can redistribute it and/or modify  *\n*   it under the terms of the GNU General Public License as published by  *\n*   the Free Software Foundation; either version 2 of the License, or     *\n*   (at your option) any later version.                                   *\n*                                                                         *\n***************************************************************************\n\"\"\"\n\n__author__ = 'Nyall Dawson'\n__date__ = 'October 2016'\n__copyright__ = '(C) 2016, Nyall Dawson'\n\n# This will get replaced with a git SHA1 when you do a git archive323\n\n__revision__ = '$Format:%H$'\n\nfrom qgis.core import (QgsProcessingParameterNumber,\n                       QgsProcessingException,\n                       QgsProcessing)\nfrom processing.algs.qgis.QgisAlgorithm import QgisFeatureBasedAlgorithm\n\n\nclass ExtendLines(QgisFeatureBasedAlgorithm):\n\n    START_DISTANCE = 'START_DISTANCE'\n    END_DISTANCE = 'END_DISTANCE'\n\n    def group(self):\n        return self.tr('Vector geometry')\n\n    def __init__(self):\n        super().__init__()\n        self.start_distance = None\n        self.end_distance = None\n\n    def initParameters(self, config=None):\n        self.addParameter(QgsProcessingParameterNumber(self.START_DISTANCE,\n                                                       self.tr('Start distance'), defaultValue=0.0))\n        self.addParameter(QgsProcessingParameterNumber(self.END_DISTANCE,\n                                                       self.tr('End distance'), defaultValue=0.0))\n\n    def name(self):\n        return 'extendlines'\n\n    def displayName(self):\n        return self.tr('Extend lines')\n\n    def outputName(self):\n        return self.tr('Extended')\n\n    def inputLayerTypes(self):\n        return [QgsProcessing.TypeVectorLine]\n\n    def prepareAlgorithm(self, parameters, context, feedback):\n        self.start_distance = self.parameterAsDouble(parameters, self.START_DISTANCE, context)\n        self.end_distance = self.parameterAsDouble(parameters, self.END_DISTANCE, context)\n        return True\n\n    def processFeature(self, feature, context, feedback):\n        input_geometry = feature.geometry()\n        if input_geometry:\n            output_geometry = input_geometry.extendLine(self.start_distance, self.end_distance)\n            if not output_geometry:\n                raise QgsProcessingException(\n                    self.tr('Error calculating extended line'))\n\n            feature.setGeometry(output_geometry)\n\n        return feature\n"}
{"text": "# set command to set global variables\nfrom lib.utils import *\n\n\ndef _help():\n    usage = '''\nUsage: set [options] (var) [value]\n\n[options]:\n-h                Print this help.\n\n-del (var)        Delete variable\n                  (var) if defined.\n\nwhere (var) is a valid\nglobal variable\nif [value] is not given,\ncurrent value is returned\n'''\n    print(usage)\n\n\ndef main(argv):\n    if '-h' in argv:\n        _help()\n        return\n    # The shell doesnt send the\n    # command name in the arg list\n    # so the next line is not needed\n    # anymore\n    # argv.pop(0) #remove arg\n\n    # to show all vars\n    if len(argv) < 1:\n        for i in prop.vars():\n            print(i, ' = ', prop.get(i))\n        return\n    if '-del' in argv:\n        try:\n            var = argv[1]\n            # detect system vars\n            if var == 'save_state' or var == 'c_char':\n                err(4, add='Cant delete system variable \"' + var + '\"')\n                return\n            prop.delete(var)\n            return\n        except IndexError:\n            err(4, add='variable name was missing')\n            return\n\n    var = argv[0]\n\n    if len(argv) < 2:\n        val = prop.get(var)\n        if val == NULL:\n            err(4, var)\n            return\n        print(val)\n        return\n\n    # remove name of var\n    argv.pop(0)\n    # make the rest the val\n    val = make_s(argv)\n    try:\n        prop.set(var, val)\n    except ValueError:\n        err(4, add=\"can't create this variable\")\n"}
{"text": "import unittest\nimport requests\nimport deepl\n\nparagraph_text = \"\"\"This is a text with multiple paragraphs. This is still the first one.\nThis is the second one.\n\nThis is the third paragraph.\"\"\"\n\nparagraph_list = [\n    'This is a text with multiple paragraphs. This is still the first one.',\n    'This is the second one.',\n    'This is the third paragraph.'\n]\n\nsentence_list = [\n    'This is a text with multiple paragraphs.',\n    'This is still the first one.',\n    'This is the second one.',\n    'This is the third paragraph.'\n]\n\n\nclass TestOfflineMethods(unittest.TestCase):\n    def test_split_paragraphs(self):\n        self.assertListEqual(deepl.translator._split_paragraphs(paragraph_text), paragraph_list)\n\n    @unittest.skip(\"Not yet implemented\")\n    def test_insert_translation(self):\n        pass\n\n\nclass TestOnlineMethods(unittest.TestCase):\n    def setUp(self):\n        try:\n            requests.get(\"https://www.deepl.com/jsonrpc\")\n        except ConnectionError:\n            self.skipTest(\"Can't contact deepl API. Skipping online tests\")\n\n    def test_split_sentences(self):\n        self.assertListEqual(deepl.translator._request_split_sentences(paragraph_list, \"EN\", [\"EN\"]),\n                             sentence_list)\n\n    def test_translate(self):\n        self.assertListEqual(\n            deepl.translator._request_translate([\"This is a test\"], \"EN\", \"DE\", [\"EN\", \"DE\"])[\"translations\"],\n            [\"Das ist ein Test\"])\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "__source__ = 'https://leetcode.com/problems/subarray-product-less-than-k/'\n# Time:  O(N)\n# Space: O(1)\n#\n# Description: Leetcode # 713. Subarray Product Less Than K\n#\n# Your are given an array of positive integers nums.\n#\n# Count and print the number of (contiguous) subarrays\n# where the product of all the elements in the subarray is less than k.\n#\n# Example 1:\n# Input: nums = [10, 5, 2, 6], k = 100\n# Output: 8\n# Explanation: The 8 subarrays that have product less than 100 are:\n# [10], [5], [2], [6], [10, 5], [5, 2], [2, 6], [5, 2, 6].\n# Note that [10, 5, 2] is not included as the product of 100 is not strictly less than k.\n# Note:\n#\n# 0 < nums.length <= 50000.\n# 0 < nums[i] < 1000.\n# 0 <= k < 10^6.\n#\nimport unittest\n\n# 284ms 29.27%\nclass Solution(object):\n    def numSubarrayProductLessThanK(self, nums, k):\n        \"\"\"\n        :type nums: List[int]\n        :type k: int\n        :rtype: int\n        \"\"\"\n        if k <= 1: return 0\n        prod = 1\n        ans = left = 0\n        for right, val in enumerate(nums):\n            prod *= val\n            while prod >= k:\n                prod /= nums[left]\n                left += 1\n            if prod < k:\n                ans += right - left + 1\n        return ans\n\nclass TestMethods(unittest.TestCase):\n    def test_Local(self):\n        self.assertEqual(1, 1)\n\nif __name__ == '__main__':\n    unittest.main()\n\nJava = '''\n# Thought: https://leetcode.com/problems/subarray-product-less-than-k/solution/\n\nApproach #2: Sliding Window [Accepted]\nTime Complexity: O(N), where N is the length of nums. left can only be incremented at most N times.\nSpace Complexity: O(1), the space used by prod, left, and ans.\n\n# 18ms 38.88%\nclass Solution {\n    public int numSubarrayProductLessThanK(int[] nums, int k) {\n        if ( k <= 1) return 0;\n        int prod = 1, ans = 0, left = 0;\n        for (int right = 0; right < nums.length; right++) {\n            prod *= nums[right];\n            while (prod >= k) prod /= nums[left++];\n            ans += right - left + 1;\n        }\n        return ans;\n    }\n}\n\n# only for reference\nBecause \\log(\\prod_i x_i) = \\sum_i \\log x_i, we can reduce the problem to subarray sums instead of subarray products.\n\nAlgorithm\nAfter this transformation where every value x becomes log(x),\nlet us take prefix sums prefix[i+1] = nums[0] + nums[1] + ... + nums[i].\nNow we are left with the problem of finding, for each i,\nthe largest j so that nums[i] + ... + nums[j] = prefix[j] - prefix[i] < k.\nBecause prefix is a monotone increasing array, this can be solved with binary search.\nWe add the width of the interval [i, j] to our answer, which counts all subarrays [i, k] with k <= j.\n\nComplexity Analysis\nTime Complexity: O(NlogN), where N is the length of nums.\nInside our for loop, each binary search operation takes O(logN) time.\nSpace Complexity: O(N), the space used by prefix.\n\n# 89ms 0.98%\nclass Solution {\n    public int numSubarrayProductLessThanK(int[] nums, int k) {\n        if (k == 0) return 0;\n        double logk = Math.log(k);\n        double[] prefix = new double[nums.length + 1];\n        for (int i = 0; i < nums.length; i++) {\n            prefix[i+1] = prefix[i] + Math.log(nums[i]);\n        }\n\n        int ans = 0;\n        for (int i = 0; i < prefix.length; i++) {\n            int lo = i + 1, hi = prefix.length;\n            while (lo < hi) {\n                int mi = lo + (hi - lo) / 2;\n                if (prefix[mi] < prefix[i] + logk - 1e-9) lo = mi + 1;\n                else hi = mi;\n            }\n            ans += lo - i - 1;\n        }\n        return ans;\n    }\n}\n'''\n"}
{"text": "from __future__ import absolute_import\nfrom __future__ import print_function\nfrom forecast import make_job_file, JobState, process_arguments, load_sys_cfg\nimport json\nimport logging\nimport sys,glob\nimport os.path as osp\n\nsimulations_path = osp.abspath('simulations')    # copy here job_id/wfc\ncatalog_path = osp.join(simulations_path,'catalog.json')\ntry:\n    catalog = json.load(open(catalog_path,'r'))\nexcept:\n    print(('Cannot open catalog at %s, creating new.' % catalog_path))\n    catalog={}\n\nif __name__ == '__main__':\n\n    if len(sys.argv) < 2:\n        print('Usage: ./recover_catalog.sh 1.json 2.json ....')\n        print('x.json are inputs to forecast.sh as from wrfxctrl, starting with /, no spaces')\n        print('Example: ./recover_catalog.sh ~/Projects/wrfxctrl/jobs/*.json')\n        print('Important: must be run from the wrfxpy directory. Before using:')\n        print('In wrfxweb/fdds/simulations: tar cvfz ~/c.tgz <simulations to recove>/*.json catalog.json')\n        print('Transfer the file c.tgz and  untar here')\n        print('On exit, ./simulations/catalog.json will be updated')\n        sys.exit(1)\n\n    sims = glob.glob(osp.join(simulations_path,'wfc-*'))\n    state = {}\n    desc = {}\n    for s in sims:\n        job_id = osp.basename(s)\n        if job_id in catalog:\n            state[job_id]='Already in catalog'\n            desc[job_id] = catalog[job_id]['description'] \n        else:\n            state[job_id]='No catalog entry'\n            desc[job_id] = 'Unknown' \n     \n    for js_path in sys.argv[1:]:\n        # print js_path\n        js = json.load(open(js_path,'r'))\n        description = js['postproc']['description']\n        #print json.dumps(js, indent=4, separators=(',', ': '))\n        jsb=osp.basename(js_path)\n        m=glob.glob(osp.join(simulations_path,'*','wfc-'+jsb))\n        print(('%s simulations found for %s file %s' % \n            (len(m), description, js_path)))\n        for mf in m: \n            #print mf\n            manifest=osp.basename(mf)\n            job_id=osp.basename(osp.split(mf)[0])\n            print('%s found in %s' % (manifest, job_id))\n            manifest_js=json.load(open(mf,'r'))\n            from_utc = '999-99-99_99:99:99'\n            to_utc = '0000-00-00_00:00:00'\n            for domain in manifest_js:\n                for time in manifest_js[domain]:\n                    from_utc = min(from_utc,time)\n                    to_utc = max(to_utc,time)\n            new={'manifest_path':osp.join(job_id,manifest),\n                            'description':js['postproc']['description'],\n                            'from_utc':from_utc,\n                            'to_utc':to_utc}\n            if job_id in catalog:\n                if catalog[job_id]==new:\n                    print('Catalog entry already exists, no change.')\n                    state[job_id] = 'No change'\n                else:\n                    print(('Replacing catalog entry %s' % job_id))\n                    print(('Old value:\\n%s' % \n                        json.dumps(catalog[job_id], indent=4, separators=(',', ': '))))\n                    print(('New value:\\n%s' % \n                        json.dumps(new, indent=4, separators=(',', ': '))))\n                    state[job_id] = 'Replaced'\n            else:\n                    print(('Creating catalog entry %s' % job_id))\n                    print(('New value:\\n%s' % \n                        json.dumps(new, indent=4, separators=(',', ': '))))\n                    state[job_id] = 'Recovered'\n            catalog[job_id]=new\n            desc[job_id] = catalog[job_id]['description'] \n    print('Writing catalog at %s' % catalog_path)\n    json.dump(catalog, open(catalog_path,'w'), indent=4, separators=(',', ': '))\n    for job_id in sorted(state):\n        print('%s: %s: %s' % (job_id, desc[job_id], state[job_id]))\n"}
{"text": "# -*- coding: utf-8 -*- #\n\nfrom __future__ import unicode_literals\n\nimport os\nimport sys\nSITE_ROOT = os.path.realpath(os.path.dirname(__file__))\nsys.path.append(SITE_ROOT)\nimport local_settings as ls\n\n\nAUTHOR = ls.AUTHOR\nSITENAME = ls.SITENAME\nSITEURL = ls.SITEURL\nPATH = ls.PATH\nTIMEZONE = ls.TIMEZONE\nLOCALE = ls.LOCALE\nDEFAULT_LANG = ls.DEFAULT_LANG\n\nARTICLE_URL = 'articles/{lang}/{slug}.html'\nARTICLE_SAVE_AS = ARTICLE_URL\nARTICLE_LANG_URL = ARTICLE_URL\nARTICLE_LANG_SAVE_AS = ARTICLE_URL\n\n\n# Feed generation is usually not desired when developing\nFEED_ALL_ATOM = None\nCATEGORY_FEED_ATOM = None\nTRANSLATION_FEED_ATOM = None\n\n# Blogroll\nLINKS = (\n    ('Pelican', 'http://getpelican.com/'),\n    ('Python.org', 'http://python.org/'),\n    ('Jinja2', 'http://jinja.pocoo.org/'),\n    ('ReStructuredText', 'http://docutils.sourceforge.net/rst.html'),\n)\n\n# Social widget\nSOCIAL = (\n    ('linkedin', 'http://ua.linkedin.com/pub/dmitry-semenov/5/994/a6a', ''),\n    ('github', 'https://github.com/dmisem', ''),\n    ('bitbucket', 'https://bitbucket.org/dmisem', ''),\n    ('e-mail', 'mailto:dmitry.5674@gmail.com', 'envelope'),\n)\n\nSTATIC_PATHS = ['images', 'img']\n\nDEFAULT_PAGINATION = 10\n\n# Uncomment following line if you want document-relative URLs when developing\nRELATIVE_URLS = True\n\nTHEME = \"themes/pelican-bootstrap3\"\nPYGMENTS_STYLE = \"default\"\n\nFAVICON = 'img/favicon.ico'\nSITELOGO = 'img/dsm.png'\nHIDE_SITENAME = True\n\nDISPLAY_TAGS_ON_SIDEBAR = True\nDISPLAY_TAGS_INLINE = False\nTAG_LEVELS_COUNT = 3  # My settings\nTAGS_URL = 'tags.html'\nDISPLAY_CATEGORIES_ON_SIDEBAR = False\nDISPLAY_RECENT_POSTS_ON_SIDEBAR = False\n\n# PLUGIN_PATHS = [SITE_ROOT + '/plugins']\nPLUGIN_PATHS = ['plugins']\nPLUGINS = ['tag_cloud']\nUSE_FOLDER_AS_CATEGORY = True\n\nif __name__ == \"__main__\":\n    d = globals()\n    for k in dir():\n        print('{0} => {1}'.format(k, d[k]))\n"}
{"text": "def extractKeztranslationsWordpressCom(item):\n\t'''\n\tParser for 'keztranslations.wordpress.com'\n\t'''\n\n\tvol, chp, frag, postfix = extractVolChapterFragmentPostfix(item['title'])\n\tif not (chp or vol) or \"preview\" in item['title'].lower():\n\t\treturn None\n\n\ttagmap = [\n\t\t('FOD',     'Quickly Wear the Face of the Devil',      'translated'), \n\t\t('ABO',     'ABO Cadets',                              'translated'), \n\t\t('dfc',     'The First Dragon Convention',             'translated'), \n\t\t('ogu',     'My Family\u2019s Omega Has Just Grown Up',     'translated'), \n\t]\n\n\tfor tagname, name, tl_type in tagmap:\n\t\tif tagname in item['tags']:\n\t\t\treturn buildReleaseMessageWithType(item, name, vol, chp, frag=frag, postfix=postfix, tl_type=tl_type)\n\n\n\ttitlemap = [\n\t\t('ABO Vol',      'ABO Cadets',                              'translated'), \n\t\t('FOD Chapter',  'Quickly Wear the Face of the Devil',      'translated'), \n\t\t('FOD Chap',     'Quickly Wear the Face of the Devil',      'translated'), \n\t]\n\n\tfor titlecomponent, name, tl_type in titlemap:\n\t\tif titlecomponent.lower() in item['title'].lower():\n\t\t\treturn buildReleaseMessageWithType(item, name, vol, chp, frag=frag, postfix=postfix, tl_type=tl_type)\n\n\treturn False"}
{"text": "from multiprocessing import Process, Queue, Value, RawValue, Lock\nimport time\n\n\nclass INT():\n    def __init__(self, n):\n        self.i = n\n        self.v = RawValue('i', n)\n\n    def increment(self):\n        self.v.value += 1\n        self.i += 1\n\n    def getV(self):\n        return self.v.value\n\n    def getI(self):\n        return self.i\n\n\nclass QueueProcess(Process):\n    def __init__(self, q):\n        Process.__init__(self)\n        self.q = q\n        q.put('MessageSentToQueue 1')\n    def run(self):\n        self.q.put('MessageSentToQueue 2')\n        return 0\n\nclass INTProcess(Process):\n    def __init__(self, i, vOut1, vOut2):\n        Process.__init__(self);\n        self.i = i\n        self.vOut1 = vOut1\n        self.vOut2 = vOut2\n        \n    def run(self):\n        self.i.increment();\n        self.vOut1.value = self.i.getI()\n        self.vOut2.value = self.i.getV()\n        return 0;\n\nclass LockProcess(Process):\n    def __init__(self, v, l, useLock):\n        Process.__init__(self);\n        self.l = l\n        self.v = v\n        self.useLock = useLock\n        \n    def run(self):\n        for i in range(100000):\n            if self.useLock==True:\n                with self.l:\n                    self.v.value += 1\n            else:\n                self.v.value += 1\n        return 0;\n\nif __name__ == \"__main__\":\n    q = Queue()\n    p = QueueProcess(q)\n    p.start()\n    p.join()\n    if not(q.empty()):\n        print q.get()\n    print q.get_nowait()\n    try:\n        print q.get_nowait()\n    except:\n        print \"Queue empty !\"\n    \n    i = INT(0);\n    v1=RawValue('i', 0)\n    v2=RawValue('i', 0)\n    p2 = INTProcess(i, v1, v2);\n    p2.start()\n    p2.join()\n    print i.getI()\n    print i.getV()\n    print v1.value\n    print v2.value\n\n    v = RawValue('i', 0)\n    l = Lock()\n    nbrProcs = 10\n    procs = [LockProcess(v, l, False) for j in range(nbrProcs)]\n    for j in range(nbrProcs): procs[j].start()\n    for j in range(nbrProcs): procs[j].join()\n    print v.value\n    v.value = 0\n    procs2 = [LockProcess(v, l, True) for j in range(nbrProcs)]\n    for j in range(nbrProcs): procs2[j].start()\n    for j in range(nbrProcs): procs2[j].join()\n    print v.value\n\n\n\n\n\n\n\n"}
{"text": "# -*- coding: utf-8 -*-\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n\nimport os\n\nfrom quodlibet import app\nfrom tests import TestCase, destroy_fake_app, init_fake_app\n\nfrom senf import mkstemp\n\nfrom quodlibet.player.nullbe import NullPlayer\nfrom quodlibet.qltk.info import SongInfo\nfrom quodlibet.library import SongLibrary\n\n\nSOME_PATTERN = \"foo\\n[big]<title>[/big] - <artist>\"\n\n\nclass FakePatternEdit(object):\n    @property\n    def text(self):\n        return SOME_PATTERN\n\n\nclass TSongInfo(TestCase):\n\n    def setUp(self):\n        init_fake_app()\n        fd, self.filename = mkstemp()\n        os.close(fd)\n        self.info = SongInfo(SongLibrary(), NullPlayer(), self.filename)\n\n    def test_save(self):\n        fake_edit = FakePatternEdit()\n        self.info._on_set_pattern(None, fake_edit, app.player)\n        with open(self.filename, \"r\") as f:\n            contents = f.read()\n            self.failUnlessEqual(contents, SOME_PATTERN + \"\\n\")\n\n    def tearDown(self):\n        destroy_fake_app()\n        self.info.destroy()\n        os.unlink(self.filename)\n"}
{"text": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# aiopening documentation build configuration file, created by\n# sphinx-quickstart on Sun Jun  4 11:36:56 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('../'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = ['sphinx.ext.autodoc',\n    'sphinx.ext.todo',\n    'sphinx.ext.imgmath',\n    'sphinx.ext.viewcode',\n    'sphinx.ext.githubpages']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = 'aiopening'\ncopyright = '2017, Sven Mika'\nauthor = 'Sven Mika'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = '1.0.1'\n# The full version, including alpha/beta/rc tags.\nrelease = '1.0.1'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'alabaster'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'aiopenerdoc'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, 'aiopening.tex', 'aiopening Documentation',\n     'Sven Mika', 'manual'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 'aiopening', 'aiopening Documentation',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, 'aiopening', 'aiopening Documentation',\n     author, 'aiopening', 'One line description of project.',\n     'Miscellaneous'),\n]\n\n\n\n"}
{"text": "'''Trains a simple CNN on the MNIST dataset.\nGets to 97.14% test accuracy after 2 epochs\n(there is *a lot* of margin for parameter tuning).\n'''\n\nfrom __future__ import print_function\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom keras.optimizers import RMSprop\nfrom keras import backend as K\n\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import normalize\n\nimport matplotlib.pyplot as plt\n\nbatch_size = 128\nnum_classes = 10\nepochs = 10\n\n# the data, shuffled and split between train and test sets\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n#print(x_train[0, :,:])\n#plt.imshow(x_train[0, :,:], cmap='gray')\n#plt.show()\n\n#x_train = x_train.reshape(60000, 784)\n\n#x_test = x_test.reshape(10000, 784)\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\nprint(y_test)\nprint(y_test.shape)\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\"\"\"2 hidden layers each with 256 neurons\"\"\"\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.summary()\n\nmodel.compile(loss='mean_squared_error',\n              optimizer=RMSprop(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=(x_test, y_test))\n\ndef confusionMatrix(Model):\n\n\tprobMatrix = Model.predict(x_test)\n\tprint(probMatrix.shape)\n\n\tpredictedVals = np.zeros((1, 10000))\n\tprint(predictedVals.shape)\n\n\trealVals = np.zeros((10000, 1))\n\n\t# Get the matrix for the test values\n\tfor row in range(10000):\n\t\tfor col in range(10):\n\t\t\tif y_test[(row, col)] == 1.0:\n\t\t\t\trealVals[(row, 0)] = col\n\n\t# Normalize the probMatrix\n\tprobMatrix = normalize(probMatrix, axis=1, norm='l1')\n\n\t# Get the matrix for the predicated test values\n\tfor row in range(probMatrix.shape[0]):\n\t\tmaxIndex = 0\n\t\tmaxVal = 0\n\t\tfor col in range(probMatrix.shape[1]):\n\t\t\tif probMatrix[(row, col)] > maxVal:\n\t\t\t\tmaxVal = probMatrix[(row, col)]\n\t\t\t\tmaxIndex = col \n\t\tpredictedVals[(0, row)] = maxIndex\n\t\tmaxIndex = 0\n\t\tmaxVal = 0\n\n\tpredictedVals = np.transpose(predictedVals)\n\tpredictedVals = predictedVals.reshape((10000, ))\n\tpredictedVals = predictedVals.astype(np.uint8)\n\n\trealVals = realVals.reshape((10000, ))\n\trealVals = realVals.astype(np.uint8)\n\n\treturn confusion_matrix(realVals, predictedVals)\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"modelCNN.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"modelCNN.h5\")\nprint(\"Saved model to disk\")\n"}
{"text": "import ctypes\r\nimport netsnmpapi\r\n\r\nclass fd_set(ctypes.Structure):\r\n    _fields_ = [('fds_bits', ctypes.c_long * 32)]\r\n\r\nclass Timeval(ctypes.Structure):\r\n    _fields_ = [(\"tv_sec\", ctypes.c_long), (\"tv_usec\", ctypes.c_long)]\r\n\r\ndef FD_SET(fd, fd_set):\r\n    \"\"\"Set fd in fd_set, where fd can may be in range of 0..FD_SETSIZE-1 (FD_SETSIZE is 1024 on Linux).\"\"\"\r\n    l64_offset = fd / 64\r\n    bit_in_l64_idx = fd % 64;\r\n    fd_set.fds_bits[l64_offset] = fd_set.fds_bits[l64_offset] | (2**bit_in_l64_idx)\r\n\r\ndef FD_ISSET(fd, fd_set):\r\n    \"\"\"Check if fd is in fd_set.\"\"\"\r\n    l64_offset = fd / 64\r\n    bit_in_l64_idx = fd % 64;\r\n    if fd_set.fds_bits[l64_offset] & (2**bit_in_l64_idx) > 0:\r\n        return True\r\n    return False\r\n\r\ndef netsnmp_event_fd():\r\n    \"\"\"Return each netsnmp file descriptor by number.\"\"\"\r\n    maxfd = ctypes.c_int(0)\r\n    fdset = fd_set()\r\n    timeval = Timeval(0, 0)\r\n    fakeblock = ctypes.c_int(1)\r\n    netsnmpapi.libnsa.snmp_select_info(\r\n        ctypes.byref(maxfd),\r\n        ctypes.byref(fdset),\r\n        ctypes.byref(timeval),\r\n        ctypes.byref(fakeblock)\r\n    )\r\n    for fd in range(0, maxfd.value):\r\n        if FD_ISSET(fd, fdset):\r\n            yield fd\r\n"}
{"text": "# -*- coding: utf-8 -*-\n#\n# Copyright (c) 2010-2012 Cidadania S. Coop. Galega\n#\n# This file is part of e-cidadania.\n#\n# e-cidadania is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# e-cidadania is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with e-cidadania. If not, see <http://www.gnu.org/licenses/>.\n\nfrom django.contrib import admin\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.http import HttpResponseRedirect\nfrom django.shortcuts import render_to_response\nfrom django.contrib.auth.models import User\n\nfrom apps.ecidadania.accounts.models import UserProfile\n\nclass ProfileAdmin(admin.ModelAdmin):\n\n    \"\"\"\n    This is a minimal view for Django administration interface. It shows the\n    user and the website.\n    \"\"\"\n    list_display = ('user', 'website')\n    actions = ['mass_mail']\n    \n    def mass_mail(self, request, queryset):\n        \"\"\"\n        This function exports the selected ovjects to a new view to manipulate\n        them properly.\n        \"\"\"\n        #selected = request.POST.getlist(admin.ACTION_CHECKBOX_NAME)\n        # ct = ContentType.objects.get_for_model(queryset.model)\n        if request.method == \"POST\":\n            for obj in queryset:\n                get_user = get_object_or_404(User, id=obj.id)\n                send.mail(request.POST['massmail_subject'], request.POST['message'], 'noreply@ecidadania.org', [get_user.email])\n            return HttpResponseRedirect(request.get_full_path())\n        \n        selected = request.POST.getlist(admin.ACTION_CHECKBOX_NAME)\n        ct = ContentType.objects.get_for_model(queryset.model)\n        return render_to_response('/mail/massmail.html', { 'people': selected })\n    mass_mail.short_description = 'Send a global mail to the selected users'\n    \nadmin.site.register(UserProfile, ProfileAdmin)\n"}
{"text": "#!/usr/bin/env python\n#\n# Copyright (c) 2001 - 2016 The SCons Foundation\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to\n# the following conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY\n# KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n# LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n#\n\n__revision__ = \"test/Batch/up_to_date.py rel_2.5.1:3735:9dc6cee5c168 2016/11/03 14:02:02 bdbaddog\"\n\n\"\"\"\nVerify simple use of $SOURCES with batch builders correctly decide\nthat files are up to date on a rebuild.\n\"\"\"\n\nimport TestSCons\n\ntest = TestSCons.TestSCons()\n\n_python_ = TestSCons._python_\n\ntest.write('batch_build.py', \"\"\"\\\nimport os\nimport sys\ndir = sys.argv[1]\nfor infile in sys.argv[2:]:\n    inbase = os.path.splitext(os.path.split(infile)[1])[0]\n    outfile = os.path.join(dir, inbase+'.out')\n    open(outfile, 'wb').write(open(infile, 'rb').read())\nsys.exit(0)\n\"\"\")\n\ntest.write('SConstruct', \"\"\"\nenv = Environment()\nenv['BATCH_BUILD'] = 'batch_build.py'\nenv['BATCHCOM'] = r'%(_python_)s $BATCH_BUILD ${TARGET.dir} $SOURCES'\nbb = Action('$BATCHCOM', batch_key=True)\nenv['BUILDERS']['Batch'] = Builder(action=bb)\nenv1 = env.Clone()\nenv1.Batch('out1/f1a.out', 'f1a.in')\nenv1.Batch('out1/f1b.out', 'f1b.in')\nenv2 = env.Clone()\nenv2.Batch('out2/f2a.out', 'f2a.in')\nenv3 = env.Clone()\nenv3.Batch('out3/f3a.out', 'f3a.in')\nenv3.Batch('out3/f3b.out', 'f3b.in')\n\"\"\" % locals())\n\ntest.write('f1a.in', \"f1a.in\\n\")\ntest.write('f1b.in', \"f1b.in\\n\")\ntest.write('f2a.in', \"f2a.in\\n\")\ntest.write('f3a.in', \"f3a.in\\n\")\ntest.write('f3b.in', \"f3b.in\\n\")\n\nexpect = test.wrap_stdout(\"\"\"\\\n%(_python_)s batch_build.py out1 f1a.in f1b.in\n%(_python_)s batch_build.py out2 f2a.in\n%(_python_)s batch_build.py out3 f3a.in f3b.in\n\"\"\" % locals())\n\ntest.run(stdout = expect)\n\ntest.must_match(['out1', 'f1a.out'], \"f1a.in\\n\")\ntest.must_match(['out1', 'f1b.out'], \"f1b.in\\n\")\ntest.must_match(['out2', 'f2a.out'], \"f2a.in\\n\")\ntest.must_match(['out3', 'f3a.out'], \"f3a.in\\n\")\ntest.must_match(['out3', 'f3b.out'], \"f3b.in\\n\")\n\ntest.up_to_date(options = '--debug=explain', arguments = '.')\n\ntest.pass_test()\n\n# Local Variables:\n# tab-width:4\n# indent-tabs-mode:nil\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n"}
{"text": "# coding: utf-8\n\n\"\"\"\n    KubeVirt API\n\n    This is KubeVirt API an add-on for Kubernetes.\n\n    OpenAPI spec version: 1.0.0\n    Contact: kubevirt-dev@googlegroups.com\n    Generated by: https://github.com/swagger-api/swagger-codegen.git\n\"\"\"\n\n\nfrom pprint import pformat\nfrom six import iteritems\nimport re\n\n\nclass K8sIoApimachineryPkgApisMetaV1WatchEvent(object):\n    \"\"\"\n    NOTE: This class is auto generated by the swagger code generator program.\n    Do not edit the class manually.\n    \"\"\"\n\n\n    \"\"\"\n    Attributes:\n      swagger_types (dict): The key is attribute name\n                            and the value is attribute type.\n      attribute_map (dict): The key is attribute name\n                            and the value is json key in definition.\n    \"\"\"\n    swagger_types = {\n        'object': 'K8sIoApimachineryPkgRuntimeRawExtension',\n        'type': 'str'\n    }\n\n    attribute_map = {\n        'object': 'object',\n        'type': 'type'\n    }\n\n    def __init__(self, object=None, type=None):\n        \"\"\"\n        K8sIoApimachineryPkgApisMetaV1WatchEvent - a model defined in Swagger\n        \"\"\"\n\n        self._object = None\n        self._type = None\n\n        self.object = object\n        self.type = type\n\n    @property\n    def object(self):\n        \"\"\"\n        Gets the object of this K8sIoApimachineryPkgApisMetaV1WatchEvent.\n        Object is:  * If Type is Added or Modified: the new state of the object.  * If Type is Deleted: the state of the object immediately before deletion.  * If Type is Error: *Status is recommended; other types may make sense    depending on context.\n\n        :return: The object of this K8sIoApimachineryPkgApisMetaV1WatchEvent.\n        :rtype: K8sIoApimachineryPkgRuntimeRawExtension\n        \"\"\"\n        return self._object\n\n    @object.setter\n    def object(self, object):\n        \"\"\"\n        Sets the object of this K8sIoApimachineryPkgApisMetaV1WatchEvent.\n        Object is:  * If Type is Added or Modified: the new state of the object.  * If Type is Deleted: the state of the object immediately before deletion.  * If Type is Error: *Status is recommended; other types may make sense    depending on context.\n\n        :param object: The object of this K8sIoApimachineryPkgApisMetaV1WatchEvent.\n        :type: K8sIoApimachineryPkgRuntimeRawExtension\n        \"\"\"\n        if object is None:\n            raise ValueError(\"Invalid value for `object`, must not be `None`\")\n\n        self._object = object\n\n    @property\n    def type(self):\n        \"\"\"\n        Gets the type of this K8sIoApimachineryPkgApisMetaV1WatchEvent.\n\n        :return: The type of this K8sIoApimachineryPkgApisMetaV1WatchEvent.\n        :rtype: str\n        \"\"\"\n        return self._type\n\n    @type.setter\n    def type(self, type):\n        \"\"\"\n        Sets the type of this K8sIoApimachineryPkgApisMetaV1WatchEvent.\n\n        :param type: The type of this K8sIoApimachineryPkgApisMetaV1WatchEvent.\n        :type: str\n        \"\"\"\n        if type is None:\n            raise ValueError(\"Invalid value for `type`, must not be `None`\")\n\n        self._type = type\n\n    def to_dict(self):\n        \"\"\"\n        Returns the model properties as a dict\n        \"\"\"\n        result = {}\n\n        for attr, _ in iteritems(self.swagger_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else x,\n                    value\n                ))\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n        \"\"\"\n        Returns the string representation of the model\n        \"\"\"\n        return pformat(self.to_dict())\n\n    def __repr__(self):\n        \"\"\"\n        For `print` and `pprint`\n        \"\"\"\n        return self.to_str()\n\n    def __eq__(self, other):\n        \"\"\"\n        Returns true if both objects are equal\n        \"\"\"\n        if not isinstance(other, K8sIoApimachineryPkgApisMetaV1WatchEvent):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n        \"\"\"\n        Returns true if both objects are not equal\n        \"\"\"\n        return not self == other\n"}
{"text": "#!/usr/bin/env python3\n\n# -*- coding: utf-8 -*-\n\n# Copyright (c) 2015-2016, Thierry Lemeunier <thierry at lemeunier dot net>\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without modification,\n# are permitted provided that the following conditions are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice, this \n# list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright notice,\n# this list of conditions and the following disclaimer in the documentation\n# and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" \n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n# ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport ssl\nimport socket\nimport pprint\nimport argparse\n\nargparser = argparse.ArgumentParser(description='Client to test a X.509 certificat')\n\nargparser.add_argument('-c', '--cert', metavar='certificat', type=str, \n                       required=True, help=\"the PEM X509 certificat file\")\n                       \nargparser.add_argument('-p', '--port' , type=int, nargs='?', default=62230,\n                       metavar='port', help='the server connexion port')\n\noptions = argparser.parse_args()\n\nprint(\"Using port number \" + str(options.port))\nprint(\"Taking '\" + options.cert + \"' for certificat file\")\n\ncontext = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\ncontext.verify_mode = ssl.CERT_OPTIONAL\ncontext.check_hostname = False\ncontext.options |= ssl.OP_NO_SSLv2 # SSL v2 not allowed\ncontext.options |= ssl.OP_NO_SSLv3 # SSL v3 not allowed\ncontext.load_verify_locations(cafile=options.cert)\n\nconn = context.wrap_socket(socket.socket(socket.AF_INET))\n\ntry:\n    conn.connect((\"localhost\", options.port))\nexcept ConnectionRefusedError as e:\n    print(e)\n    print(\"The server seems not running or verify the port number.\")\n    exit(1)\nexcept ssl.SSLError as e:\n    print(e)\n    print(\"There is a problem with this certificat.\")\n    exit(1)\nexcept Exception as e:\n    print(e)\n    exit(1)\n\nprint(conn.cipher())\n\ncert = conn.getpeercert()\npprint.pprint(cert)\n\nconn.send(b'Hello World!')\n\nprint(\"The certificat seems compatible.\")\n"}
{"text": "# Largest palindrome product\n# https://projecteuler.net/problem=4\n# A palindromic number reads the same both ways. \n# The largest palindrome made from the product of two 2-digit numbers is 9009 = 91 \u00d7 99.\n# Find the largest palindrome made from the product of two 3-digit numbers.\n\ndef get_digits_array(num):\n    q1 = num\n    digits = []\n    digits.append(int(q1 % 10))\n    while q1 != 0:\n        q1 = int(q1 / 10)\n        if (q1 == 0): break\n        digits.append(int(q1 % 10))\n    return digits\n\ndef is_palindrome(num):\n    digs = get_digits_array(num)\n    i = 0\n    j = len(digs) - 1\n\n    while j >= i:\n        # print(\"%d vs %d\" % (digs[j], digs[i]))\n        if digs[j] != digs[i]:\n            return False\n        j -= 1\n        i += 1\n    return True\n\n\n# digits_array = get_digits_array(1234567)\n# for digit in digits_array:\n#     print(digit)\n\n# print(is_palindrome(9019))\n# print(is_palindrome(9009))\n# print(is_palindrome(90009))\n\nnum1 = 999\nnum2 = 999\nbiggest = 0\nwhile num1 > 99:\n    while num2 > 99:\n        test = num1 * num2\n        # print(test)\n        if is_palindrome(test) and test > biggest:\n            biggest = test\n        num2 -= 1\n    num1 -= 1\n    num2 = num1\n\nprint(\"Biggest %d\" % biggest)"}
{"text": "\nfrom flask_restful import fields, marshal_with\nfrom flask_restful.reqparse import RequestParser\n\nfrom cebulany.auth import token_required\nfrom cebulany.queries.payment_summary import PaymentSummaryQuery\nfrom cebulany.resources.model import ModelListResource\n\nresource_fields = {\n    'payments': fields.List(fields.Nested({\n        'cost': fields.Price(decimals=2),\n        'is_positive': fields.Boolean(),\n        'payment_type_id': fields.Integer(),\n        'budget_id': fields.Integer(),\n    })),\n    'balances': fields.Nested({\n        'curr_start_year': fields.Price(decimals=2),\n        'curr_end_year': fields.Price(decimals=2),\n        'prev_start_year': fields.Price(decimals=2),\n        'prev_end_year': fields.Price(decimals=2),\n        'diff_start_year': fields.Price(decimals=2),\n        'diff_end_year': fields.Price(decimals=2),\n        'diff_prev_start_year': fields.Price(decimals=2),\n        'diff_prev_end_year': fields.Price(decimals=2),\n    }),\n    'outstanding_cost': fields.Price(decimals=2),\n}\n\nquery_summary_parser = RequestParser()\nquery_summary_parser.add_argument('year', type=int)\n\n\nclass PaymentSummaryResource(ModelListResource):\n\n    @token_required\n    @marshal_with(resource_fields)\n    def get(self):\n        args = query_summary_parser.parse_args()\n        year = args['year']\n\n        return {\n            'payments': PaymentSummaryQuery.get_payment_data(year),\n            'balances': PaymentSummaryQuery.get_balances(year),\n            'outstanding_cost': PaymentSummaryQuery.get_outstanding_cost(year),\n        }\n\n"}
{"text": "# This file is part of Indico.\n# Copyright (C) 2002 - 2016 European Organization for Nuclear Research (CERN).\n#\n# Indico is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as\n# published by the Free Software Foundation; either version 3 of the\n# License, or (at your option) any later version.\n#\n# Indico is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Indico; if not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import unicode_literals\n\nfrom indico.util.i18n import _\nfrom MaKaC.webinterface.pages.admins import WPAdminsBase\nfrom MaKaC.webinterface.pages.base import WPJinjaMixin, WPDecorated\n\n\nclass WPNews(WPJinjaMixin, WPDecorated):\n    template_prefix = 'news/'\n\n    def _getBody(self, params):\n        return self._getPageContent(params)\n\n    def getCSSFiles(self):\n        return WPDecorated.getCSSFiles(self) + self._asset_env['news_sass'].urls()\n\n    def _getTitle(self):\n        return WPDecorated._getTitle(self) + ' - ' + _(\"News\")\n\n\nclass WPManageNews(WPJinjaMixin, WPAdminsBase):\n    sidemenu_option = 'news'\n    template_prefix = 'news/'\n"}
{"text": "\"\"\"Utilities for working with TMDb models.\"\"\"\n\n\nasync def overlapping_movies(people, client=None):\n    \"\"\"Find movies that the same people have been in.\n\n    Arguments:\n      people (:py:class:`collections.abc.Sequence`): The\n        :py:class:`~.Person` objects to find overlapping movies for.\n      client (:py:class:`~.TMDbClient`, optional): The TMDb client\n        to extract additional information about the overlap.\n\n    Returns:\n      :py:class:`list`: The relevant :py:class:`~.Movie` objects.\n\n    \"\"\"\n    return await _overlap(people, 'movie_credits', client, 'get_movie')\n\n\nasync def overlapping_actors(movies, client=None):\n    \"\"\"Find actors that appear in the same movies.\n\n    Arguments:\n      movies (:py:class:`collections.abc.Sequence`): The\n        :py:class:`~.Movie` objects to find overlapping actors for.\n      client (:py:class:`~.TMDbClient`, optional): The TMDb client\n        to extract additional information about the overlap.\n\n    Returns:\n      :py:class:`list`: The relevant :py:class:`~.Person` objects.\n\n    \"\"\"\n    return await _overlap(movies, 'cast', client, 'get_person')\n\nasync def find_overlapping_movies(names, client):\n    \"\"\"Find movies that the same people have been in.\n\n    Warning:\n      This function requires two API calls per name submitted, plus\n      one API call per overlapping movie in the result; it is therefore\n      relatively slow.\n\n    Arguments:\n      names (:py:class:`collections.abc.Sequence`): The names of the\n        people to find overlapping movies for.\n      client (:py:class:`~.TMDbClient`): The TMDb client.\n\n    Returns:\n      :py:class:`list`: The relevant :py:class:`~.Movie` objects.\n\n    \"\"\"\n    return await _find_overlap(names, client, 'find_person', 'get_person',\n                               overlapping_movies)\n\n\nasync def find_overlapping_actors(titles, client):\n    \"\"\"Find actors that have been in the same movies.\n\n    Warning:\n      This function requires two API calls per title submitted, plus\n      one API call per overlapping person in the result; it is therefore\n      relatively slow.\n\n    Arguments:\n      titles (:py:class:`collections.abc.Sequence`): The titles of the\n        movies to find overlapping actors for.\n      client (:py:class:`~.TMDbClient`): The TMDb client.\n\n    Returns:\n      :py:class:`list`: The relevant :py:class:`~.Person` objects.\n\n    \"\"\"\n    return await _find_overlap(titles, client, 'find_movie', 'get_movie',\n                               overlapping_actors)\n\n\nasync def _overlap(items, overlap_attr, client=None, get_method=None):\n    \"\"\"Generic overlap implementation.\n\n    Arguments:\n      item (:py:class:`collections.abc.Sequence`): The objects to\n        find overlaps for.\n      overlap_attr (:py:class:`str`): The attribute of the items to use\n        as input for the overlap.\n      client (:py:class:`~.TMDbClient`, optional): The TMDb client\n        to extract additional information about the overlap.\n      get_method (:py:class:`str`, optional): The method of the\n        client to use for extracting additional information.\n\n    Returns:\n      :py:class:`list`: The relevant result objects.\n\n    \"\"\"\n    overlap = set.intersection(*(getattr(item, overlap_attr) for item in items))\n    if client is None or get_method is None:\n        return overlap\n    results = []\n    for item in overlap:\n        result = await getattr(client, get_method)(id_=item.id_)\n        results.append(result)\n    return results\n\n\nasync def _find_overlap(queries, client, find_method, get_method,\n                        overlap_function):\n    \"\"\"Generic find and overlap implementation.\n\n    Arguments\n      names (:py:class:`collections.abc.Sequence`): The queries of the\n        people to find overlaps for.\n      client (:py:class:`~.TMDbClient`): The TMDb client.\n      find_method (:py:class:`str`): The name of the client method to\n        use for finding candidates.\n      get_method (:py:class:`str`): The name of the client method to\n        use for getting detailed information on a candidate.\n      overlap_function (:py:class:`collections.abc.Callable`): The\n        function to call for the resulting overlap.\n\n    \"\"\"\n    results = []\n    for query in queries:\n        candidates = await getattr(client, find_method)(query)\n        if not candidates:\n            raise ValueError('no result found for {!r}'.format(query))\n        result = await getattr(client, get_method)(id_=candidates[0].id_)\n        results.append(result)\n    return await overlap_function(results, client)\n"}
{"text": "import uuid\nimport os\n\nfrom flask_restful import Resource, reqparse\nfrom flask_login import login_required\nfrom flask import redirect, send_file, g, url_for\n\nfrom massive import api, app\nfrom massive.models import *\nfrom massive.utils import *\n\nfrom io import BytesIO\n\nclass Resource(Resource):\n    method_decorators = [login_required]\n\n@app.route('/')\ndef index():\n    if g.user is not None and g.user.is_authenticated:\n        return redirect('/index.html')\n    else:\n        return redirect(\"/login\")\n\nparser = reqparse.RequestParser()\nparser.add_argument('url', type=str)\nparser.add_argument('tags', type=str, default=None)\n\n\nclass links(Resource):\n    def get(self):\n        user = User.query.get(g.user.id)\n        links = Link.query.join(User).filter(User.email == user.email)\n        return [link.dump() for link in links]\n\n    def post(self, linkId=None):\n        args = parser.parse_args()\n        user = User.query.get(g.user.id)\n\n        if linkId:\n            link = Link.query.get(id=linkId)\n            if not link:\n                return \"no link found\", 404\n\n            #taglist = [t.name for t in link.tags]\n            # for tag in args['tags']:\n            #     if tag not in taglist:\n            #         db_tag = Tags.get(name=tag)\n            #         if not db_tag:\n            #             db_tag = Tags(name=tag)\n            #         link.tags.add(db_tag)\n\n            return link.dump()\n\n        url = args['url']\n        tags = args['tags']\n\n        #prepend if no protocole specified\n        if url.find(\"http://\") == -1 and url.find(\"https://\") == -1:\n            url = \"http://%s\" % url\n\n        if Link.query.filter_by(url=url, user_id=user.id).first():\n            return \"already in db\", 400 \n\n        if tags:\n            tags = tags.split(\",\")\n\n        link = save_link(\n            get_page_title(url),\n            url,\n            tags,\n            user\n        )\n\n        return link.dump()\n\n    def delete(self, linkId=None):\n        if linkId == None:\n            return \"no link provided\", 400\n\n        link = Link.query.get(linkId)\n        if not link:\n            return \"no link found\", 404\n\n        #delete favicon\n        if link.favicon:\n            favicon_path = os.path.join(app.config['FAVICON_REPO'],link.favicon)\n            try:\n                os.remove(favicon_path)\n            except Exception as e:\n                app.logger.warning(\"error while trying to remove a favicon\")\n                app.logger.warning(e)\n\n        db.session.delete(link)\n        db.session.commit()\n        return \"\"\n\n\napi.add_resource(links, '/links', '/links/<string:linkId>')\n\n\n@app.route('/ico/<icoId>')\ndef get_avatar(icoId=None):\n    file_path = os.path.join(app.config['FAVICON_REPO'],icoId)\n\n    if os.path.isfile(file_path):\n        return send_file(file_path, as_attachment=True)\n    else:\n        return \"no favicon found\",404\n\n\ndef save_link(title, url, tags=[], user=None):\n    if not title:\n        title = url.split('/')[-1]\n\n    iconfile_name = \"%s.ico\" % str(uuid.uuid4())\n    favicon = get_page_favicon(url,iconfile_name)\n\n    link = Link(\n        title=title,\n        url=url,\n        favicon=iconfile_name,\n        #tags=db_tags,\n        user=user\n    )\n\n    for tag in tags:\n        db_tag = Tags.query.filter_by(name=tag).first()\n        if not db_tag:\n            db_tag = Tags(name=tag)\n            db.session.add(db_tag)\n        link.tags.append(db_tag)\n    \n    if favicon:\n        link.favicon = favicon\n\n    db.session.add(link)\n    db.session.commit()\n\n    return link\n"}
{"text": "# -*- coding: utf-8 -*-\n# Generated by Django 1.9.5 on 2016-04-11 16:52\nfrom __future__ import unicode_literals\n\nimport datetime\nfrom django.db import migrations, models\nimport django.db.models.deletion\n\n\nclass Migration(migrations.Migration):\n\n    initial = True\n\n    dependencies = [\n        ('profiles', '0001_initial'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='Request',\n            fields=[\n                ('id', models.AutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),\n                ('title', models.TextField(default=b'Type your title here')),\n                ('description', models.TextField(default=b'Type your description here')),\n                ('origin', models.CharField(choices=[(b'Seattle', b'Seattle'), (b'Portland', b'Portland')], default=b'Seattle', max_length=25)),\n                ('destination', models.CharField(choices=[(b'Seattle', b'Seattle'), (b'Portland', b'Portland')], default=b'Seattle', max_length=25)),\n                ('date_created', models.DateTimeField(blank=True, default=datetime.datetime.now)),\n                ('courier', models.ForeignKey(blank=True, null=True, on_delete=django.db.models.deletion.CASCADE, related_name='requests', to='profiles.Profile')),\n                ('sender', models.ForeignKey(on_delete=django.db.models.deletion.CASCADE, related_name='sent_from', to='profiles.Profile')),\n            ],\n        ),\n    ]\n"}
{"text": "#!/usr/bin/python2\n\nimport hashlib\nimport logging\nlogging.getLogger(\"scapy.runtime\").setLevel(logging.ERROR)\nimport os\nfrom scapy.all import *\nimport sys\n\n#returns [(session,carving)]\ndef carve_http(streams):\n\tret=[]\n\trequests=[]\n\n\tfor stream in streams:\n\t\tstart_pos=0\n\t\traw=stream[1]+\"\\r\\n\\r\\n\"\n\t\tcarving=\"\"\n\t\theader=\"\"\n\n\t\twhile raw.find(\"\\r\\n\\r\\n\",start_pos)>=0:\n\t\t\tend_pos=raw.index(\"\\r\\n\\r\\n\",start_pos)\n\t\t\theader=raw[start_pos:end_pos]\n\t\t\tend_pos+=4\n\n\t\t\tif header[:4]==\"HTTP\":\n\t\t\t\ttry:\n\t\t\t\t\theader=dict(re.findall(r'(?P<name>.*?):(?P<value>.*?)\\r\\n',header))\n\t\t\t\t\theader=dict((key.lower(),value) for key,value in header.iteritems())\n\n\t\t\t\t\tif \"transfer-encoding\" in header and header[\"transfer-encoding\"].strip()==\"chunked\":\n\t\t\t\t\t\twhile raw.find(\"\\r\\n\",end_pos)>=0:\n\t\t\t\t\t\t\tchunk_size=raw[end_pos:raw.find(\"\\r\\n\",end_pos)]\n\n\t\t\t\t\t\t\tif len(chunk_size)<=0:\n\t\t\t\t\t\t\t\tbreak\n\n\t\t\t\t\t\t\tchunk_size=int(chunk_size,16)\n\t\t\t\t\t\t\tend_pos=raw.find(\"\\r\\n\",end_pos)+2\n\t\t\t\t\t\t\tcarving+=raw[end_pos:end_pos+chunk_size]\n\t\t\t\t\t\t\tend_pos+=chunk_size+2\n\n\t\t\t\t\telif \"content-length\" in header:\n\t\t\t\t\t\tsize=int(header[\"content-length\"].strip())\n\t\t\t\t\t\tcarving=raw[end_pos:end_pos+size]\n\t\t\t\t\t\tend_pos+=size\n\n\t\t\t\texcept Exception:\n\t\t\t\t\tpass\n\n\t\t\t\tif len(carving)>0 and carving!=\"\\r\\n\\r\\n\":\n\t\t\t\t\tret.append((stream,carving))\n\n\t\t\tstart_pos=end_pos\n\n\treturn ret\n\n#expects carvings in [(session,carving)]\ndef save_carvings(carvings,out,count_start=0):\n\tcount=count_start\n\n\ttry:\n\t\tfor carving in carvings:\n\t\t\tfile_folder=str(carving[0][0])\n\t\t\tfile_folder=file_folder.replace(\" \",\"_\")\n\t\t\tfile_folder=file_folder.replace(\">\",\"TO\")\n\t\t\tfile_path=out+\"/\"+file_folder+\"/\"\n\n\t\t\tif not os.path.isdir(file_path):\n\t\t\t\tos.makedirs(file_path)\n\n\t\t\tfull_path=file_path+hashlib.sha1(carving[1]).hexdigest()\n\t\t\tprint(\"\\tSaving \\\"\"+full_path+\"\\\"\")\n\t\t\tfile=open(full_path,'w')\n\t\t\tfile.write(carving[1])\n\t\t\tfile.close()\n\t\t\tcount+=1\n\n\texcept Exception as error:\n\t\tprint(error)\n\t\traise Exception(\"Error saving files.\")\n\n\tfinally:\n\t\treturn count\n\n#returns [(session,payload)]\ndef get_streams(filename):\n\ttry:\n\t\tcap=rdpcap(filename)\n\t\tsessions=cap.sessions()\n\t\tret=[]\n\n\t\tfor session in sessions:\n\t\t\tpayload=\"\"\n\t\t\tpayload_chunked=\"\"\n\n\t\t\tfor packet in sessions[session]:\n\t\t\t\tif TCP in packet and type(packet[TCP].payload)==Raw:\n\t\t\t\t\tpacket_payload=str(packet[TCP].payload)\n\t\t\t\t\tpayload+=packet_payload\n\n\t\t\tret.append((session,payload));\n\n\t\treturn ret\n\n\texcept Exception:\n\t\traise Exception(\"Error opening pcap \\\"\"+filename+\"\\\".\")\n\nif __name__==\"__main__\":\n\tif len(sys.argv)<=1:\n\t\tprint(\"Usage: ./exorcist.py file.pcap ...\")\n\t\texit(1)\n\n\tfiles_wrote=0\n\n\tfor ii in range(1,len(sys.argv)):\n\t\tfilename=str(sys.argv[ii])\n\t\tprint(\"Processing \\\"\"+filename+\"\\\"\")\n\n\t\ttry:\n\t\t\tstreams=get_streams(filename)\n\t\t\tcarvings=carve_http(streams)\n\t\t\tfiles_wrote=save_carvings(carvings,\"out/\"+filename,files_wrote)\n\n\t\texcept Exception as error:\n\t\t\tprint(error)\n"}
{"text": "import os\n\nimport colorama\n\ntry:\n    import setproctitle\nexcept ImportError:\n    setproctitle = None\n\n\nclass RayError(Exception):\n    \"\"\"Super class of all ray exception types.\"\"\"\n    pass\n\n\nclass RayTaskError(RayError):\n    \"\"\"Indicates that a task threw an exception during execution.\n\n    If a task throws an exception during execution, a RayTaskError is stored in\n    the object store for each of the task's outputs. When an object is\n    retrieved from the object store, the Python method that retrieved it checks\n    to see if the object is a RayTaskError and if it is then an exception is\n    thrown propagating the error message.\n\n    Attributes:\n        function_name (str): The name of the function that failed and produced\n            the RayTaskError.\n        traceback_str (str): The traceback from the exception.\n    \"\"\"\n\n    def __init__(self, function_name, traceback_str):\n        \"\"\"Initialize a RayTaskError.\"\"\"\n        if setproctitle:\n            self.proctitle = setproctitle.getproctitle()\n        else:\n            self.proctitle = \"ray_worker\"\n        self.pid = os.getpid()\n        self.host = os.uname()[1]\n        self.function_name = function_name\n        self.traceback_str = traceback_str\n        assert traceback_str is not None\n\n    def __str__(self):\n        \"\"\"Format a RayTaskError as a string.\"\"\"\n        lines = self.traceback_str.split(\"\\n\")\n        out = []\n        in_worker = False\n        for line in lines:\n            if line.startswith(\"Traceback \"):\n                out.append(\"{}{}{} (pid={}, host={})\".format(\n                    colorama.Fore.CYAN, self.proctitle, colorama.Fore.RESET,\n                    self.pid, self.host))\n            elif in_worker:\n                in_worker = False\n            elif \"ray/worker.py\" in line or \"ray/function_manager.py\" in line:\n                in_worker = True\n            else:\n                out.append(line)\n        return \"\\n\".join(out)\n\n\nclass RayWorkerError(RayError):\n    \"\"\"Indicates that the worker died unexpectedly while executing a task.\"\"\"\n\n    def __str__(self):\n        return \"The worker died unexpectedly while executing this task.\"\n\n\nclass RayActorError(RayError):\n    \"\"\"Indicates that the actor died unexpectedly before finishing a task.\n\n    This exception could happen either because the actor process dies while\n    executing a task, or because a task is submitted to a dead actor.\n    \"\"\"\n\n    def __str__(self):\n        return \"The actor died unexpectedly before finishing this task.\"\n\n\nclass UnreconstructableError(RayError):\n    \"\"\"Indicates that an object is lost and cannot be reconstructed.\n\n    Note, this exception only happens for actor objects. If actor's current\n    state is after object's creating task, the actor cannot re-run the task to\n    reconstruct the object.\n\n    Attributes:\n        object_id: ID of the object.\n    \"\"\"\n\n    def __init__(self, object_id):\n        self.object_id = object_id\n\n    def __str__(self):\n        return (\"Object {} is lost (either evicted or explicitly deleted) and \"\n                + \"cannot be reconstructed.\").format(self.object_id.hex())\n\n\nRAY_EXCEPTION_TYPES = [\n    RayError,\n    RayTaskError,\n    RayWorkerError,\n    RayActorError,\n    UnreconstructableError,\n]\n"}
{"text": "#! -*-coding=gbk-*-\nimport os\nimport subprocess\nimport sys\nimport commands\nimport time\n\n\ndef walk_in_dir(root_path):\n    assd=\"\"\n    try:\n        for root, dirs, files in os.walk(root_path, True):\n            for name in files:\n                if len(name)>0:\n                    if name[0]!=\"$\":\n                        front_name,ext_name=os.path.splitext(name)\n                        if ext_name.lower() in [\".rar\",\".7z\",\".zip\"]:\n                            zipfilepath=root+os.path.sep+name\n                            cmd = '\"'+'./7za.exe'+'\" l \"'+ windows_cmd_sep_copy(zipfilepath)+'\"'\n                            # assd=cmd.decode(\"gbk\")\n                            #print assd\n                            #print type(assd)\n                            #time.sleep(1)\n                            run_result=run_in_subprocesspopen(cmd)\n                            if run_result is not None:\n                                if run_result[\"model\"]!=[]:\n                                    print root+os.path.sep+name+\" find model\"\n                                    print \"\\n\".join(run_result[\"model\"])+\"\\n\"\n                                if run_result[\"motion\"]!=[]:\n                                    print root+os.path.sep+name+\" find motion\"\n                                    print \"\\n\".join(run_result[\"motion\"])+\"\\n\"\n                            else:\n                                print \"unzip error \"+root+os.path.sep+name\n                                \n                        if ext_name.lower() in [\".pmd\",\".mpo\",\".pmx\",\".x\"]:\n                            print root+os.path.sep+name+\" find model\"\n                            print \"\"\n                        if ext_name.lower() in [\".vmd\"]:\n                            print root+os.path.sep+name+\" find motion\"\n                            print \"\"\n    except Exception,e:\n        print str(e)\n        print assd\n                        \n                \n            \n\ndef which_platform():\n   \n    import platform\n    pythonVersion = platform.python_version()\n    uname = platform.uname()\n    if len(uname)>0:\n        return uname[0]\n        \n\ndef run_in_subprocesspopen(cmd):\n    try:\n        flag_mmd_cell={}\n        flag_mmd_cell[\"model\"]=[]\n        flag_mmd_cell[\"motion\"]=[]\n       \n\n        res = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT) \n        k=0\n        result = res.stdout.readlines()\n\n        for i in result:\n            line_c=i.strip()\n            \n\n            if len(line_c)>6:   \n                line_array=line_c.split( )\n                if line_c[0:5]==\"-----\" and len(line_array)==5:\n                    if k==0:\n                        k=1\n                        first_part=\" \".join(line_array[0:4])+\" \"\n                        file_part_len=len(first_part)\n                    else:\n                        k+=1\n                else:\n                    if k==1:\n                        if len(line_c)>(file_part_len+2):\n                            part_line_c=line_c[file_part_len+1:]     \n                            front_name,ext_name=os.path.splitext(part_line_c)\n                            if ext_name.lower() in [\".pmd\",\".mpo\",\".pmx\",\".x\"]:\n                                \n                                flag_mmd_cell[\"model\"].append(part_line_c)\n                            if ext_name.lower() in [\".vmd\"]:\n                                flag_mmd_cell[\"motion\"].append(part_line_c)\n                                \n        return flag_mmd_cell\n    except:\n        s=sys.exc_info()\n        print \"Error '%s' happened on line %d\" % (s[1],s[2].tb_lineno)\n        return None\n\n\n\n                \ndef windows_cmd_sep_copy(org_path):\n    path=org_path.replace('\\\\','\\\\\\\\')\n    return path.encode(\"gbk\")\n       \n        \n\nif len(sys.argv)==2:\n    #walk_in_dir(u\"e:\\\\\")\n    walk_in_dir(sys.argv[1].decode(\"gbk\"))\nelse:\n    print \"Usage:search_run.py {dir path}\"\n"}
{"text": "###########################################################\n#\n# Copyright (c) 2005, Southpaw Technology\n#                     All Rights Reserved\n#\n# PROPRIETARY INFORMATION.  This software is proprietary to\n# Southpaw Technology, and is not to be reproduced, transmitted,\n# or disclosed in any way without written permission.\n#\n#\n#\n\n#\n# DEPRECATED\n#\n\n\n\n__all__ = ['get_app_server', 'get_xmlrpc_server', 'WebWareException', 'WebWare', 'WebWareXmlrpcAdapter']\n\nimport types, os\n\nfrom WebKit.Page import Page\n\nfrom pyasm.common import Config\nfrom pyasm.web import Url\n\nfrom web_environment import *\n\nclass WebWareException(Exception):\n    pass\n\n\n\ndef get_app_server():\n    '''dynamically load in the appserver classes'''\n    from app_server import BaseAppServer\n    from WebKit.Page import Page\n    class AppServer(Page, BaseAppServer):\n\n        def get_adapter(self):\n            adapter = WebWare(self)\n            return adapter\n\n\n        def writeHTML(self):\n            self.writeln( self.get_display() )\n\n    return AppServer\n\n\n\n\ndef get_xmlrpc_server():\n    '''dynamically load in an xmlrpc server'''\n    from WebKit.XMLRPCServlet import XMLRPCServlet\n\n    class XmlrpcServer(XMLRPCServlet):\n        def get_adapter(self):\n            adapter = WebWareXmlrpcAdapter(self.transaction())\n            return adapter\n\n    return XmlrpcServer\n\n\n           \n\nclass WebWare(WebEnvironment):\n    \"\"\"Encapsulates webware environment. Implements the web interface\"\"\"\n\n    def __init__(self,page):\n        super(WebWare,self).__init__()\n        self.request = page.request()\n        self.response = page.response()\n\n\n    def get_context_name(self):\n        '''this includes all of the subdirectories as well as the main\n        context'''\n        dir = self.request.urlPathDir()\n\n        # strip of the / at the front and the back\n        dir = dir.rstrip(\"/\")\n        dir = dir.lstrip(\"/\")\n        return dir\n\n\n\n\n    # form submission methods\n    #def reset_form(self):\n    #    return self.request.fields() = {}\n\n    def get_form_keys(self):\n        return self.request.fields().keys()\n\n    def has_form_key(self, key):\n        return key in self.request.fields():\n\n    def set_form_value(self, name, value):\n        '''Set the form value to appear like it was submitted'''\n        self.request.setField(name, value)\n\n\n    def get_form_values(self, name, raw=False):\n        \"\"\"returns a string list of the values of a form element.\n        If raw is True, then a nonexistant value returns None\"\"\"\n        \n        if self.request.hasValue(name):\n            values = self.request.value(name)\n            if isinstance(values, basestring):\n                values = values.decode('utf-8')\n                values = self._process_unicode(values)\n                return [values]\n            elif isinstance(values, list):\n                new_values = []\n                for value in values:\n                    if isinstance(value, basestring):\n                        value = self._process_unicode(value.decode('utf-8'))\n                    new_values.append(value)\n                return new_values\n            else: # this can be a FieldStorage instance\n                return values\n        else:\n            if raw == True:\n                return None\n            else:\n                return []\n\n\n    def get_form_value(self, name, raw=False):\n        \"\"\"returns the string value of the form element.\n        If raw is True, then a nonexistant value returns None\"\"\"\n        values = self.get_form_values(name,raw)\n        if values == None:\n            return None\n\n        if values.__class__.__name__ == \"FieldStorage\":\n            return values\n        elif len(values) > 0:\n            return values[0]\n        else:\n            return \"\"\n\n    def _process_unicode(self, value):\n        try:\n            value = value.encode(\"ascii\")\n        except:\n            chars = []\n            for char in value:\n                ord_value = ord(char)\n                if ord_value > 128:\n                    chars.append(\"&#%s;\" % ord(char) )\n                else:\n                    chars.append(char)\n            value = \"\".join(chars)\n\n        return value\n\n    # cookie methods\n\n\n    def set_cookie(self, name, value):\n        \"\"\"set a cookie\"\"\"\n        self.response.setCookie(name, value, expires=\"NEVER\")\n\n\n    def get_cookie(self, name):\n        \"\"\"get a cookie\"\"\"\n        if self.request.hasCookie(name):\n            return self.request.cookie(name)\n        else:\n            return \"\"\n\n\n    # environment methods\n\n    def get_env_keys(self):\n        env = self.request.environ()\n        return env.keys()\n\n    def get_env(self, env_var):\n        env = self.request.environ()\n        return env.get(env_var)\n\n\n\n\n\nclass WebWareXmlrpcAdapter(WebWare):\n\n    def __init__(self, transaction):\n        # NOTE: the call to WebWare's super is intentional\n        super(WebWare,self).__init__()\n        self.request = transaction.request()\n        self.response = transaction.response()\n\n\n\n\n\n"}
{"text": "from __future__ import absolute_import\nimport traceback\nimport os, errno\nimport shutil\nfrom .signal import interrupt_protect\n\nclass FileSystemAdapter(object):\n\n    def __init__(self, path, **kwargs):\n        # expand ~ or we'll end up creating a /~ directory\n        # abspath doesn't do this for us\n        self.path = os.path.abspath(os.path.expanduser(path))\n        self.make_sure_path_exists(self.path)\n\n    def make_sure_path_exists(self, key):\n        try:\n            os.makedirs(key)\n        except OSError as exception:\n            if exception.errno != errno.EEXIST:\n                raise\n\n    def key_path(self, key):\n        key = key\n        if key[0] == '/':\n            key = key[1:]\n        return os.path.join(self.path, key)\n\n    def get(self, key):\n        full_path = self.key_path(key)\n        try:\n            with open(full_path,'r') as f:\n                return f.read()\n        except IOError as e:\n            if e.errno == errno.ENOENT:\n                raise KeyError('{}: {}'.format(key,str(e)))\n            raise\n\n    @interrupt_protect\n    def put(self, key, data, **kwargs):\n        full_path = self.key_path(key)\n        directory = os.path.dirname(full_path)\n        self.make_sure_path_exists(directory)\n        with open(full_path,'w') as f:\n            f.write(data)\n\n    def delete(self, key):\n        full_path = self.key_path(key)\n        try:\n            os.remove(full_path)\n        except OSError:\n            # doesn't exist\n            pass\n\n    def exists(self, key):\n        full_path = self.key_path(key)\n        if os.path.isfile(full_path): \n            try:\n                with open(full_path,'r') as f:\n                    return True\n            except IOError:\n                return False\n        else:\n            return False\n\n    def list(self, key='/'):\n        full_path = self.key_path(key)\n        for directory, subdirs, files in os.walk(full_path):\n            for file in files:\n                if file[0] == '.':\n                    continue\n                path = os.path.join(directory, file)\n                # remove our directory\n                path = path.split(self.path)[1]\n                yield path\n\n    def drop_all(self):\n        # delete the directory and then recreate it\n        shutil.rmtree(self.path, ignore_errors=True)\n        self.make_sure_path_exists(self.path)\n"}
{"text": "import h5py as _h5\nimport numpy as _np\nimport logging as _logging\nimport time as _time\n_logger  = _logging.getLogger(__name__)\nimport ipdb as pdb\nimport re as _re\n\n\ndef _timestamp2filename(cls, ftype, filename=None):\n    # ======================================\n    # Get filename from timestamp\n    # ======================================\n    if filename is not None:\n        filename = '{}.{}.h5'.format(filename, ftype)\n    else:\n        try:\n            timestamp = cls.timestamp\n        except RuntimeError as err:\n            _logger.debug('Handled exception: {}'.format(err))\n            timestamp = _time.localtime()\n\n        filename = _time.strftime('%Y.%m.%d.%H%M.%S.{}.h5'.format(ftype), timestamp)\n\n    return filename\n\n\nclass Timestamp(object):\n    def __init__(self):\n        self._timestamp = None\n\n    def _set_timestamp(self, timestamp):\n        self._timestamp = timestamp\n\n    @property\n    def timestamp(self):\n        if self._timestamp is not None:\n            return self._timestamp\n        else:\n            raise RuntimeError('No timestamp: simulation not completed.')\n\n\ndef _write_arrays(group, name, data, parent=None):\n    grefs = group.create_group('_refs_{}'.format(name))\n    ref_dtype = _h5.special_dtype(ref=_h5.Reference)\n    dname = group.create_dataset(name, (_np.size(data),), dtype=ref_dtype)\n    # ======================================\n    # Create datasets\n    # ======================================\n    for i, array in enumerate(data):\n        if array.dtype == _np.dtype(object):\n            # ======================================\n            # If dataset can't be created, nest\n            # ======================================\n            darray = _write_arrays(grefs, '{}'.format(i), array, parent=name)\n        else:\n            darray = grefs.create_dataset(name='{}'.format(i), data=array, shape=_np.shape(array), compression=\"gzip\")\n\n        # ======================================\n        # Store reference in dataset\n        # ======================================\n        dname[i] = darray.ref\n\n    # if parent == 'hist':\n    #     pdb.set_trace()\n\n    # ======================================\n    # Return created dataset\n    # ======================================\n    return dname\n\n\ndef _read_arrays(group, name):\n    refs = group[name]\n    arrays = _np.empty(shape=refs.size, dtype=object)\n    for i, ref in enumerate(refs):\n        arrays[i] = group.file[ref].value\n\n    return arrays\n\n\ndef _write_scalars(group, name, data):\n    return group.create_dataset(name=name, data=data, shape=_np.shape(data), compression=\"gzip\")\n\n\ndef _write_data(group, name, data):\n    if data.dtype == _np.dtype(object):\n        _write_arrays(group, name, data)\n    else:\n        _write_scalars(group, name, data)\n\n\ndef _read_dict(group, name):\n    ret_group = group[name]\n    names = ret_group.keys()\n    valid_names = list()\n    underscore = _re.compile('_')\n\n    dict_layout = {'names': [], 'formats': []}\n\n    for nm in names:\n        if not underscore.match(nm):\n            valid_names.append(nm)\n            dict_layout['names'].append(nm)\n            if type(ret_group[nm].value[0]) == _h5.h5r.Reference:\n                dict_layout['formats'].append(object)\n            else:\n                raise NotImplementedError('Haven''t done this...')\n\n    results_flat = _np.zeros(len(ret_group[valid_names[0]]), dtype=dict_layout)\n\n    for nm in valid_names:\n        # if nm == 'hist':\n        #     pdb.set_trace()\n        values = ret_group[nm]\n        for i, value in enumerate(values):\n            try:\n                array = group.file[value].value\n                if array.size > 0:\n                    if type(array[0]) == _h5.h5r.Reference:\n                        out = _np.empty(len(array), dtype=object)\n                        for j, val in enumerate(array):\n                            out[j] = group.file[val].value\n                    else:\n                        out = group.file[value].value\n                else:\n                    out = _np.array([])\n                results_flat[nm][i] = out\n            except ValueError:\n                _logger.debug('There was a ValueError')\n\n    # pdb.set_trace()\n    return results_flat\n"}
{"text": "# -*- coding: utf-8 -*-\n\nimport time\n\nimport pycurl\nfrom module.network.HTTPRequest import BadHeader\n\nfrom ..hoster.DebridlinkFr import error_description\nfrom ..internal.misc import json\nfrom ..internal.MultiAccount import MultiAccount\n\n\nclass DebridlinkFr(MultiAccount):\n    __name__ = \"DebridlinkFr\"\n    __type__ = \"account\"\n    __version__ = \"0.05\"\n    __status__ = \"testing\"\n\n    __config__ = [(\"mh_mode\", \"all;listed;unlisted\", \"Filter hosters to use\", \"all\"),\n                  (\"mh_list\", \"str\", \"Hoster list (comma separated)\", \"\"),\n                  (\"mh_interval\", \"int\", \"Reload interval in hours\", 12)]\n\n    __description__ = \"\"\"Debridlink.fr account plugin\"\"\"\n    __license__ = \"GPLv3\"\n    __authors__ = [(\"GammaC0de\", \"nitzo2001[AT]yahoo[DOT]com\")]\n\n    TUNE_TIMEOUT = False\n\n    #: See https://debrid-link.fr/api_doc/v2\n    API_URL = \"https://debrid-link.fr/api/\"\n\n    def api_request(self, method, get={}, post={}):\n        api_token = self.info['data'].get('api_token', None)\n        if api_token and method != \"oauth/token\":\n            self.req.http.c.setopt(pycurl.HTTPHEADER, [\"Authorization: Bearer \" + api_token])\n        self.req.http.c.setopt(pycurl.USERAGENT, \"pyLoad/%s\" % self.pyload.version)\n        try:\n            json_data = self.load(self.API_URL + method, get=get, post=post)\n        except BadHeader, e:\n            json_data = e.content\n\n        return json.loads(json_data)\n\n    def _refresh_token(self, client_id, refresh_token):\n        api_data = self.api_request(\"oauth/token\",\n                                    post={'client_id': client_id,\n                                          'refresh_token': refresh_token,\n                                          'grant_type': \"refresh_token\"})\n\n        if 'error' in api_data:\n            if api_data['error'] == 'invalid_request':\n                self.log_error(_(\"You have to use GetDebridlinkToken.py to authorize pyLoad: https://github.com/pyload/pyload/files/4679112/GetDebridlinkToken.zip\"))\n            else:\n                self.log_error(api_data.get('error_description', error_description(api_data[\"error\"])))\n            self.fail_login()\n\n        return api_data['access_token'], api_data['expires_in']\n\n    def grab_hosters(self, user, password, data):\n        api_data = self.api_request(\"v2/downloader/hostnames\")\n\n        if api_data['success']:\n            return api_data['value']\n\n        else:\n            return []\n\n    def grab_info(self, user, password, data):\n        api_data = self.api_request(\"v2/account/infos\")\n\n        if api_data['success']:\n            premium = api_data['value']['premiumLeft'] > 0\n            validuntil = api_data['value']['premiumLeft'] + time.time()\n\n        else:\n            self.log_error(_(\"Unable to retrieve account information\"),\n                           api_data.get('error_description', error_description(api_data[\"error\"])))\n            validuntil = None\n            premium = None\n\n        return {'validuntil': validuntil,\n                'trafficleft': -1,\n                'premium': premium}\n\n    def signin(self, user, password, data):\n        if 'token' not in data:\n            api_token, timeout = self._refresh_token(user, password)\n            data['api_token'] = api_token\n            self.timeout = timeout - 5 * 60  #: Five minutes less to be on the safe side\n\n        api_data = self.api_request(\"v2/account/infos\")\n        if 'error' in api_data:\n            if api_data['error'] == 'badToken':  #: Token expired? try to refresh\n                api_token, timeout = self._refresh_token(user, password)\n                data['api_token'] = api_token\n                self.timeout = timeout - 5 * 60  #: Five minutes less to be on the safe side\n\n            else:\n                self.log_error(api_data.get('error_description', error_description(api_data[\"error\"])))\n                self.fail_login()\n"}
{"text": "# Logger bot configuration.\n{\n    # These messages will dissapear after the bot has been run\n    # Most of these settings can be changed from within the bot\n    # itself.  See the help command.\n    'active_servers': set(),\n    'admin_commands': {'help', 'ignore_server', 'listen_on', 'leave', 'join'},\n    'admin_roles': set(),\n    'admins': set(),\n\n    # Discord user for the bot.\n    'bot_user': 'logger@example.com',\n    'bot_password': 'Password for Discord user'\n\n    # MySQL database connection.\n    'db_host': 'localhost',\n    'db_user': 'logger',\n    'db_password': 'Password for database user',\n    'db_schema': 'discord',\n\n    'ignores': set(),\n\n    # Set of user ids that are masters of bot, and can do any command.\n    'masters': {'your-user-id-number'},\n    'noisy_deny': True,\n    'protected_servers': set(),\n\n    # Character used to triggering commands in the bot.  Setting it\n    # to '!', means commands start with an ! character (e.g, !help).\n    'trigger': '!',\n\n    'user_commands': {'help', 'leave', 'join'},\n}\n"}
{"text": "from collections import defaultdict\nimport csv\nfrom django.core.management.base import BaseCommand\nimport elasticsearch\nimport elasticsearch_dsl\nimport json\n\nimport settings\nfrom seqr.models import Individual\nfrom seqr.views.utils.orm_to_json_utils import _get_json_for_individuals\nfrom xbrowse_server.base.models import Project as BaseProject\n\nEXCLUDE_PROJECTS = ['ext', '1000 genomes', 'DISABLED', 'project', 'interview', 'non-cmg', 'amel']\n\nPER_PAGE = 5000\n\n\nclass Command(BaseCommand):\n\n    def add_arguments(self, parser):\n        parser.add_argument(\"--metadata-only\", action=\"store_true\", help=\"Only get the project/ individual metadata.\")\n        parser.add_argument(\"--use-project-indices-csv\", action=\"store_true\", help=\"Load projects to search from project_indices.csv\")\n        parser.add_argument(\"--index\", nargs='+', help=\"Individual index to use\")\n\n    def handle(self, *args, **options):\n\n        if options[\"index\"]:\n            es_indices = options[\"index\"]\n        elif options[\"use_project_indices_csv\"]:\n            with open('project_indices.csv') as csvfile:\n                reader = csv.DictReader(csvfile)\n                es_indices = {row['index'] for row in reader}\n\n        else:\n            projects_q = BaseProject.objects.filter(genome_version='37')\n            for exclude_project in EXCLUDE_PROJECTS:\n                projects_q = projects_q.exclude(project_name__icontains=exclude_project)\n            indices_for_project = defaultdict(list)\n            for project in projects_q:\n                indices_for_project[project.get_elasticsearch_index()].append(project)\n            indices_for_project.pop(None, None)\n\n            seqr_projects = []\n            with open('project_indices.csv', 'wb') as csvfile:\n                fieldnames = ['projectGuid', 'index']\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n                writer.writeheader()\n                for index, projects in indices_for_project.items():\n                    for project in projects:\n                        seqr_projects.append(project.seqr_project)\n                        writer.writerow({'projectGuid': project.seqr_project.guid, 'index': index})\n\n            individuals = _get_json_for_individuals(Individual.objects.filter(family__project__in=seqr_projects))\n            with open('seqr_individuals.csv', 'wb') as csvfile:\n                fieldnames = ['projectGuid', 'familyGuid', 'individualId', 'paternalId', 'maternalId', 'sex',\n                              'affected']\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n                writer.writeheader()\n                for individual in individuals:\n                    writer.writerow(individual)\n            es_indices = indices_for_project.keys()\n\n        if not options[\"metadata_only\"]:\n            es_client = elasticsearch.Elasticsearch(host=settings.ELASTICSEARCH_SERVICE_HOSTNAME, timeout=10000)\n            search = elasticsearch_dsl.Search(using=es_client, index='*,'.join(es_indices) + \"*\")\n            search = search.query(\"match\", mainTranscript_lof='HC')\n            search = search.source(['contig', 'pos', 'ref', 'alt', '*num_alt', '*gq', '*ab', '*dp', '*ad'])\n\n            print('Searching across {} indices...'.format(len(es_indices)))\n            result_count_search = search.params(size=0)\n            total = result_count_search.execute().hits.total\n            print('Loading {} variants...'.format(total))\n\n            with open('lof_variants.csv', 'a') as csvfile:\n                sample_fields = ['num_alt', 'gq', 'ab', 'dp', 'ad']\n                fieldnames = ['contig', 'pos', 'ref', 'alt', 'index'] + sample_fields\n                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n                if not options[\"index\"]:\n                    writer.writeheader()\n                for i, hit in enumerate(search.scan()):\n                    result = {key: hit[key] for key in hit}\n                    result['index'] = hit.meta.index\n                    for field in sample_fields:\n                        result[field] = json.dumps({\n                            key.rstrip('_{}'.format(field)): val for key, val in result.items() if key.endswith(field)\n                        })\n                    writer.writerow(result)\n                    if i % 10000 == 0:\n                        print('Parsed {} variants'.format(i))\n\n            print('Loaded {} variants'.format(i))\n\n        print('Done')"}
{"text": "#!/usr/bin/env python2.7\n# -*- coding: utf-8 -*-\n'''\n__author__ = 'Chen Xin'\n__mtime__ = '02/06/15'\n'''\nimport json\n\nfrom flask import Flask\nfrom flask.ext.sqlalchemy import SQLAlchemy\nfrom flask.ext.cors import CORS\nfrom flask.ext.restful import Api\nimport requests\n\napp = Flask(__name__)\n\n\n\n# Read Config\napp.config.from_object('config')\n\n# =========================================\n# Custom logging server\n# http://log.dhrproject.ml\n# logger = DHRLogger.getLogger('sink')\n# Now, it is using the default python log lib\n# =========================================\nimport log\nlogger = log.logger\n\nimport error\n\n\n# =========================================\n# get config file\n# =========================================\n\n\ntry:\n    r = requests.get(app.config['CONFIG_URI'], auth=(app.config['CONFIG_USER'], app.config['CONFIG_PASS']))\n    GLOABL_CONFIG = json.loads(r.content)\n\n    DOPERATOR_API = 'http://' + GLOABL_CONFIG['operators']['1']['network']['ip_public'] + ':' + \\\n                    GLOABL_CONFIG['operators']['1']['network']['port_api'] + '/'\n\n    DSOURCES_API = []\n    for item in GLOABL_CONFIG['sources']:\n        source = GLOABL_CONFIG['sources'][item]['network']\n        DSOURCES_API.append('http://' + source['ip_public'] + ':' + source['port_api'] + source['data_api'])\n\n\n    logger.info('get config file from:' + app.config['CONFIG_URI'])\n    logger.info(DSOURCES_API)\nexcept Exception as e:\n    logger.info('failed to get config file from:' + app.config['CONFIG_URI'])\n    logger.info('init failed')\n    logger.info(e)\n    exit()\n\n\n\n# =========================================\n# Init DataBase\n# =========================================\ndb = SQLAlchemy(app)\n\n\n\n# =========================================\n# Flask-restful\n# add prefix here or it won't work when you register blueprint\n# =========================================\napi = Api(app, prefix='/api/v0.1')\n\n\n# =========================================\n# only if you import abort from flask.ext.restful\n# =========================================\n# api = Api(app,prefix='/api/v0.1',errors=error.errorMessages)\n\n\n# =========================================\n# Register blueprint for /api/v0.1\n# =========================================\nfrom .api_0_1 import api_0_1 as api_0_1_blueprint\napp.register_blueprint(api_0_1_blueprint)\n\n\n\n# =========================================\n# Allow cross domain request\n# =========================================\ncors = CORS(app, resources={r\"/*\": {\"origins\": \"*\"}}, allow_headers='*')\n\nimport models,token_verify,utils,db_handler\n\n\n\n\n\n# =========================================\n# Init database\n# =========================================\n# db.drop_all()\ndb.create_all()\n\n\n# =========================================\n# add test users\n# TODO: read test user from config file and create it\n# =========================================\ntestuser = models.Users(app.config['CONFIG_USER'], app.config['CONFIG_PASS'])\ndb.session.add(testuser)\n\n\n\n\n\n"}
{"text": "\"\"\"\nModel fields for the gender app.\n\"\"\"\n\nfrom django.db import models\nfrom django.utils import six\nfrom django.utils.translation import ugettext_lazy as _\n\nfrom .constants import (GENDER_CHOICES,\n                        GENDER_UNKNOWN)\n\n\nclass GenderFieldBase(models.CharField):\n    \"\"\"\n    This database model field can be used to store the gender of a person.\n    \"\"\"\n\n    description = _('A gender type object')\n\n    MAX_LENGTH = 1\n\n    def __init__(self, *args, **kwargs):\n        parent_kwargs = {\n            'max_length': self.MAX_LENGTH,\n            'choices': GENDER_CHOICES,\n            'default': GENDER_UNKNOWN,\n            'blank': True,\n            }\n        parent_kwargs.update(kwargs)\n        super(GenderFieldBase, self).__init__(*args, **parent_kwargs)\n\n    def deconstruct(self):\n        name, path, args, kwargs = super(GenderFieldBase, self).deconstruct()\n        if kwargs['choices'] == GENDER_CHOICES:\n            del kwargs['choices']\n        if kwargs['max_length'] == self.MAX_LENGTH:\n            del kwargs['max_length']\n        if kwargs['default'] == GENDER_UNKNOWN:\n            del kwargs['default']\n        if kwargs['blank']:\n            del kwargs['blank']\n        return name, path, args, kwargs\n\n    def get_internal_type(self):\n        return \"CharField\"\n\n\nclass GenderField(six.with_metaclass(models.SubfieldBase,\n                                     GenderFieldBase)):\n    \"\"\"\n    Database gender field. Can be used to store a gender type.\n    See ``GenderFieldBase`` for details.\n    \"\"\"\n    pass\n"}
{"text": "# -*- coding: utf-8 -*-\n\n\"\"\"\nThis file is part of Radar.\n\nRadar is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nRadar is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\nLesser GNU General Public License for more details.\n\nYou should have received a copy of the Lesser GNU General Public License\nalong with Radar. If not, see <http://www.gnu.org/licenses/>.\n\nCopyright 2015 Lucas Liendo.\n\"\"\"\n\n\nfrom . import NetworkMonitor, NetworkMonitorError\n\n\nclass SelectMonitor(NetworkMonitor):\n    def __new__(cls, *args, **kwargs):\n        try:\n            global select\n            from select import select\n        except ImportError:\n            raise NetworkMonitorError(cls.__name__)\n\n        return super(SelectMonitor, cls).__new__(cls, *args, **kwargs)\n\n    def watch(self):\n        sockets = [self._server.socket] + [c.socket for c in self._server._clients]\n        ready_fds, _, _ = select([s.fileno() for s in sockets], [], [], self._timeout)\n        super(SelectMonitor, self)._watch(ready_fds)\n"}
{"text": "# Copyright 2018-2020 by Christopher C. Little.\n# This file is part of Abydos.\n#\n# Abydos is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Abydos is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Abydos. If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"abydos.tests.fingerprint.test_fingerprint_occurrence.\n\nThis module contains unit tests for abydos.fingerprint.Occurrence\n\"\"\"\n\nimport unittest\n\nfrom abydos.fingerprint import Occurrence\n\n\nclass OccurrenceFingerprintTestCases(unittest.TestCase):\n    \"\"\"Test Cis\u0142ak & Grabowski's occurrence fingerprint functions.\n\n    abydos.fingerprint.Occurrence\n    \"\"\"\n\n    fp = Occurrence()\n\n    def test_occurrence_fingerprint(self):\n        \"\"\"Test abydos.fingerprint.Occurrence.\"\"\"\n        # Base case\n        self.assertEqual(self.fp.fingerprint(''), '0' * 16)\n\n        # https://arxiv.org/pdf/1711.08475.pdf\n        self.assertEqual(self.fp.fingerprint('instance'), '1110111000010000')\n\n        self.assertEqual(self.fp.fingerprint('inst'), '0100111000000000')\n        self.assertEqual(\n            Occurrence(15).fingerprint('instance'), '111011100001000'\n        )\n        self.assertEqual(\n            Occurrence(32).fingerprint('instance'),\n            '11101110000100000000000000000000',\n        )\n        self.assertEqual(\n            Occurrence(64).fingerprint('instance'),\n            '11101110000100000000000000000000' + '0' * 32,\n        )\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "\"\"\"\nGenerate data files and TOC file for the visualization.\n\nDepends on vtn_sieve and pysieve (vtn_sieve is a private repository that cannot be shared)\n\nSee file specification at file_spec.md\n\"\"\"\n\nfrom pysieve import substbased\nfrom seqdistance.matrices import binarySubst, addGapScores, binGapScores\nfrom vtn_sieve import *\nimport pandas as pd\n\ndef generateData(studyClasses, analysisClasses, analysisParams, nperms=10000):\n    for sc in studyClasses:\n        \"\"\"For each study, loop over all analyses and produce files for each.\"\"\"\n        s = sc()\n        for va in s.validAnalyses:\n            s.loadData(**va)\n            s.to_fasta()\n            s.to_treatment_file()\n            for ac in analysisClasses:\n                a = ac(sievedata=s.data)\n                a.initialize(params=analysisParams)\n                a.computeDistance(params=analysisParams)\n                a.computeObserved(distFilter=None)\n                a.permutationTest(nperms, clusterClient=None)\n                a.computePvalues()\n                a.to_distance_csv()\n                a.to_csv()\n\ndef generateTOC(studyClasses, analysisClasses):\n    tocColumns =  ['study','protein','reference','distance_method']\n    toc = {k:[] for k in tocColumns}\n    for sc in studyClasses:\n        \"\"\"For each study, loop over all analyses and produce files for each.\"\"\"\n        s = sc()\n        for va in s.validAnalyses:\n            for ac in analysisClasses:\n                a = ac(None)\n                toc['study'].append(s.studyName)\n                toc['protein'].append(va['proteinName'])\n                toc['reference'].append(va['insertName'])\n                toc['distance_method'].append(a.methodName)\n    tocDf = pd.DataFrame(toc)[tocColumns]\n    return tocDf\n\nif __name__ == '__main__':\n    #studyClasses = [sieveVTN502, sieveVTN503, sieveVTN505, sieveRV144]\n    studyClasses = [sieveVTN502, sieveRV144]\n    analysisClasses = [substbased.vxmatch_siteAnalysis]\n    analysisParams = dict(subst=addGapScores(binarySubst, binGapScores))\n\n    #generateData(studyClasses, analysisClasses, analysisParams)\n    tocDf = generateTOC(studyClasses, analysisClasses)\n    tocDf.to_csv('sieve_toc.csv', index=False)\n>>>>>>> 5174f991c7ddfc922307ecc69d71094e4f2d4787\n"}
{"text": "import optparse\nimport os\n\noptparser = optparse.OptionParser()\noptparser.add_option(\"-l\", \"--language\", dest=\"language\", default=\"French\", help=\"Language to package\")\noptparser.add_option(\"-b\", \"--bucket\", dest=\"bucket\", default=\"brendan.callahan.thesis\", help=\"S3 bucket name\")\noptparser.add_option(\"-p\", \"--prefix\", dest=\"prefix\", help=\"Alternate prefix for the filenames, default is lower case language name\")\noptparser.add_option(\"-S\", action=\"store_true\", dest=\"skip_completed_words\", help=\"Allows multiple passes so we can resume if any failures\")\n(opts, _) = optparser.parse_args()\n\n# TODO: un-hard code the base destination and source paths\nBASE_DESTINATION_PATH = '/mnt/storage2/intermediate/'\nBASE_SOURCE_PATH = '/mnt/storage/'+opts.language+'/'\nBASE_TAR_PATH = BASE_DESTINATION_PATH + opts.language.lower()\nfile_prefix = opts.prefix or opts.language.lower()\nbig_tar_file_name = file_prefix+\"-package.tar\"\nsample_tar_file_name = file_prefix+\"-sample.tar\"\nbig_tar_path = BASE_DESTINATION_PATH + big_tar_file_name\nsample_tar_path = BASE_DESTINATION_PATH + sample_tar_file_name\n\nif not os.path.exists(BASE_DESTINATION_PATH):\n    os.makedirs(BASE_DESTINATION_PATH)\n\nif not opts.skip_completed_words:\n    tar_cmd = \"tar cvf \"+ big_tar_path+\" --files-from /dev/null\"\n    os.system(tar_cmd)\n    sample_cmd = \"tar cvf \"+ sample_tar_path+\" --files-from /dev/null\"\n    os.system(sample_cmd)\n\n    os.system(\"cd \" + BASE_SOURCE_PATH + \" && tar rf \"+big_tar_path+\" all_errors.json\")\n\ntargz_files = []\nfor folder_name in os.listdir(BASE_SOURCE_PATH):\n    print(folder_name)\n    targz_file = folder_name + '.tar.gz'\n    targz_path = BASE_DESTINATION_PATH + targz_file\n    targz_files.append(targz_file)\n\n    # if skip completed words param was passed, and the filepath exists skip this and move onto the next\n    # assume there are no incomplete files (that they are cleaned up manually)\n    if opts.skip_completed_words and os.path.isfile(targz_path):\n        continue\n\n    add_folder_cmd = \"cd \" + BASE_SOURCE_PATH + \" && tar -czf \"+targz_path+\" \"+folder_name\n\n    print(add_folder_cmd)\n    os.system(add_folder_cmd)\n\nadd_folders_cmd = \"cd \" + BASE_DESTINATION_PATH + \" && tar rf \"+big_tar_file_name+\" \"+\" \".join(targz_files)\nprint(add_folders_cmd)\nos.system(add_folders_cmd)\n\nsample_files = sorted(targz_files)[0:100]\nadd_folders_cmd_sample = \"cd \" + BASE_DESTINATION_PATH + \" && tar rf \"+sample_tar_file_name+\" \"+\" \".join(sample_files)\nprint(add_folders_cmd_sample)\nos.system(add_folders_cmd_sample)\n\nos.system(\"cd \" + BASE_DESTINATION_PATH + \" && mv \" + big_tar_file_name + \" ..\")\nos.system(\"cd \" + BASE_DESTINATION_PATH + \" && mv \" + sample_tar_file_name + \" ..\")\n\n\n# TODO make aws upload optional\npackage_upload_cmd = \"aws s3 cp /mnt/storage2/\" + big_tar_file_name + \" s3://\" + opts.bucket + \"/packages/\" + \\\n                     big_tar_file_name\n\nsample_upload_cmd = \"aws s3 cp /mnt/storage2/\" + sample_tar_file_name + \" s3://\" + opts.bucket + \"/samples/\" + \\\n                    sample_tar_file_name\n\nos.system(package_upload_cmd)\nos.system(sample_upload_cmd)\n\n\n"}
{"text": "from hermes2d import Mesh, H1Shapeset, PrecalcShapeset, H1Space, \\\n                WeakForm, Solution, ScalarView, LinSystem, DummySolver, \\\n                MeshView, set_verbose, plot_mesh_mpl_simple\nfrom hermes2d.forms import set_forms\nfrom hermes2d.mesh import read_hermes_format\n\ndef read_mesh(filename):\n    nodes, elements, boundary, nurbs = read_hermes_format(filename)\n    return nodes, elements, boundary, nurbs\n\ndef plot_mesh(mesh, axes=None, plot_nodes=True):\n    nodes, elements, boundary, nurbs = mesh\n    # remove the element markers\n    elements = [x[:-1] for x in elements]\n    return plot_mesh_mpl_simple(nodes, elements, axes=axes,\n            plot_nodes=plot_nodes)\n\ndef poisson_solver(mesh_tuple):\n    \"\"\"\n    Poisson solver.\n\n    mesh_tuple ... a tuple of (nodes, elements, boundary, nurbs)\n    \"\"\"\n    set_verbose(False)\n    mesh = Mesh()\n    mesh.create(*mesh_tuple)\n    mesh.refine_element(0)\n    shapeset = H1Shapeset()\n    pss = PrecalcShapeset(shapeset)\n\n    # create an H1 space\n    space = H1Space(mesh, shapeset)\n    space.set_uniform_order(5)\n    space.assign_dofs()\n\n    # initialize the discrete problem\n    wf = WeakForm(1)\n    set_forms(wf)\n\n    solver = DummySolver()\n    sys = LinSystem(wf, solver)\n    sys.set_spaces(space)\n    sys.set_pss(pss)\n\n    # assemble the stiffness matrix and solve the system\n    sys.assemble()\n    A = sys.get_matrix()\n    b = sys.get_rhs()\n    from scipy.sparse.linalg import cg\n    x, res = cg(A, b)\n    sln = Solution()\n    sln.set_fe_solution(space, pss, x)\n    return sln\n"}
{"text": "import os\nimport Document as Doc\n\nclass FileHandler():\n    def makeLog(self, myFile, msg):\n        logDir = \"logs/\"    \n        os.chdir(logDir)\n        \n        fo = open(myFile, 'a')\n        fo.write(msg + \"\\n\")\n        fo.close()\n    \n    def loadDirs(self, myDir, labelled = False):\n        docs = []\n        basepath = os.getcwd()\n        print \"Loading test data...\" if labelled == False else \"Loading training data...\"\n        for subdir, dirs, files in os.walk(myDir):\n            os.chdir(subdir)\n            \n            for file in files:\n                fo = open(file, 'r')\n                \n                content = fo.read()\n                if not content:\n                    continue\n                doc = Doc.Document(file, content)\n                if (labelled):\n                    doc.category = subdir.split(\"/\")[-1]\n                \n                docs.append(doc)\n                fo.close()\n                \n            os.chdir(basepath)\n        print \"Loaded Documents.\"\n        return docs\n        \n          "}
{"text": "#   Copyright (c) 2008 Mikeal Rogers\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\nfrom django.template import Context\nfrom django.http import HttpResponse\nimport logging\n\nfrom microsite_configuration import microsite\n\nfrom edxmako import lookup_template\nfrom edxmako.middleware import get_template_request_context\nfrom django.conf import settings\nfrom django.core.urlresolvers import reverse\nlog = logging.getLogger(__name__)\n\n\ndef marketing_link(name):\n    \"\"\"Returns the correct URL for a link to the marketing site\n    depending on if the marketing site is enabled\n\n    Since the marketing site is enabled by a setting, we have two\n    possible URLs for certain links. This function is to decides\n    which URL should be provided.\n    \"\"\"\n\n    # link_map maps URLs from the marketing site to the old equivalent on\n    # the Django site\n    link_map = settings.MKTG_URL_LINK_MAP\n    enable_mktg_site = microsite.get_value(\n        'ENABLE_MKTG_SITE',\n        settings.FEATURES.get('ENABLE_MKTG_SITE', False)\n    )\n\n    if enable_mktg_site and name in settings.MKTG_URLS:\n        # special case for when we only want the root marketing URL\n        if name == 'ROOT':\n            return settings.MKTG_URLS.get('ROOT')\n        return settings.MKTG_URLS.get('ROOT') + settings.MKTG_URLS.get(name)\n    # only link to the old pages when the marketing site isn't on\n    elif not enable_mktg_site and name in link_map:\n        # don't try to reverse disabled marketing links\n        if link_map[name] is not None:\n            if link_map[name].startswith('https://'):\n                return link_map[name]\n            else:\n                return reverse(link_map[name])\n    else:\n        log.debug(\"Cannot find corresponding link for name: %s\", name)\n        return '#'\n\n\ndef marketing_link_context_processor(request):\n    \"\"\"\n    A django context processor to give templates access to marketing URLs\n\n    Returns a dict whose keys are the marketing link names usable with the\n    marketing_link method (e.g. 'ROOT', 'CONTACT', etc.) prefixed with\n    'MKTG_URL_' and whose values are the corresponding URLs as computed by the\n    marketing_link method.\n    \"\"\"\n    return dict(\n        [\n            (\"MKTG_URL_\" + k, marketing_link(k))\n            for k in (\n                settings.MKTG_URL_LINK_MAP.viewkeys() |\n                settings.MKTG_URLS.viewkeys()\n            )\n        ]\n    )\n\n\ndef open_source_footer_context_processor(request):\n    \"\"\"\n    Checks the site name to determine whether to use the edX.org footer or the Open Source Footer.\n    \"\"\"\n    return dict(\n        [\n            (\"IS_EDX_DOMAIN\", settings.FEATURES.get('IS_EDX_DOMAIN', False))\n        ]\n    )\n\n\ndef microsite_footer_context_processor(request):\n    \"\"\"\n    Checks the site name to determine whether to use the edX.org footer or the Open Source Footer.\n    \"\"\"\n    return dict(\n        [\n            (\"IS_REQUEST_IN_MICROSITE\", microsite.is_request_in_microsite())\n        ]\n    )\n\n\ndef render_to_string(template_name, dictionary, context=None, namespace='main'):\n\n    # see if there is an override template defined in the microsite\n    template_name = microsite.get_template_path(template_name)\n\n    context_instance = Context(dictionary)\n    # add dictionary to context_instance\n    context_instance.update(dictionary or {})\n    # collapse context_instance to a single dictionary for mako\n    context_dictionary = {}\n    context_instance['settings'] = settings\n    context_instance['EDX_ROOT_URL'] = settings.EDX_ROOT_URL\n    context_instance['marketing_link'] = marketing_link\n\n    # In various testing contexts, there might not be a current request context.\n    request_context = get_template_request_context()\n    if request_context:\n        for item in request_context:\n            context_dictionary.update(item)\n    for item in context_instance:\n        context_dictionary.update(item)\n    if context:\n        context_dictionary.update(context)\n\n    # \"Fix\" CSRF token by evaluating the lazy object\n    KEY_CSRF_TOKENS = ('csrf_token', 'csrf')\n    for key in KEY_CSRF_TOKENS:\n        if key in context_dictionary:\n            context_dictionary[key] = unicode(context_dictionary[key])\n\n    # fetch and render template\n    template = lookup_template(namespace, template_name)\n    return template.render_unicode(**context_dictionary)\n\n\ndef render_to_response(template_name, dictionary=None, context_instance=None, namespace='main', **kwargs):\n    \"\"\"\n    Returns a HttpResponse whose content is filled with the result of calling\n    lookup.get_template(args[0]).render with the passed arguments.\n    \"\"\"\n\n    # see if there is an override template defined in the microsite\n    template_name = microsite.get_template_path(template_name)\n\n    dictionary = dictionary or {}\n    return HttpResponse(render_to_string(template_name, dictionary, context_instance, namespace), **kwargs)\n"}
{"text": "\nfrom __future__ import absolute_import, print_function\n\nimport json\nimport socket\nimport urlparse\nimport uuid\n\nfrom . import state\n\n\nclass Job(object):\n\n    resource = {\n        \"cpus\": 0.01,\n        \"mem\": 10\n    }\n\n    def __init__(self, data):\n        self.data = data\n        self.id = str(uuid.uuid4())\n        self.port = None\n        self.location = None\n        self.running = False\n        self.script, self.public_key = json.loads(self.data)\n\n    def uris(self):\n        # XXX - wrapper location shouldn't live here.\n        base = \"http://{0}:{1}\".format(\n            socket.gethostbyname(socket.gethostname()),\n            state.ARGS.port)\n        return [\n            urlparse.urljoin(base, \"/static/wrapper.bash\"),\n            urlparse.urljoin(base, \"/static/sshd_config\"),\n            urlparse.urljoin(base, \"/job/{0}/script\".format(self.id)),\n            urlparse.urljoin(base, \"/job/{0}/public_key\".format(self.id)),\n            ]\n\n    def connection(self):\n        return \"{0}:{1}\".format(self.location, self.port)\n"}
{"text": "import xadmin\nfrom models import *\n\n\nclass MenstruacaoAdmin(object):\n    list_display = Menstruacao._meta.get_all_field_names()\n\n\nclass AntecedenteAdmin(object):\n    list_display = Antecedente._meta.get_all_field_names()\n\n\nclass OutroAdmin(object):\n    list_display = Outro._meta.get_all_field_names()\n\n\nclass AtividadeAdmin(object):\n    list_display = Atividade._meta.get_all_field_names()\n\n\nclass TabelaAnomaliaAdmin(object):\n    list_display = TabelaAnomalia._meta.get_all_field_names()\n\n\nclass TabelaMamariaAdmin(object):\n    list_display = TabelaMamaria._meta.get_all_field_names()\n\n\nclass TabelaGinecologicaAdmin(object):\n    list_display = TabelaGinecologica._meta.get_all_field_names()\n\nxadmin.site.register(Menstruacao, MenstruacaoAdmin)\nxadmin.site.register(Antecedente, AntecedenteAdmin)\nxadmin.site.register(Outro, OutroAdmin)\nxadmin.site.register(Atividade, AtividadeAdmin)\nxadmin.site.register(TabelaAnomalia, TabelaAnomaliaAdmin)\nxadmin.site.register(TabelaMamaria, TabelaMamariaAdmin)\nxadmin.site.register(TabelaGinecologica, TabelaGinecologicaAdmin)\n"}
{"text": "from cyclone.web import HTTPError\n\nfrom go_api.cyclone.handlers import BaseHandler\nfrom go_api.collections.errors import (\n    CollectionUsageError, CollectionObjectNotFound)\n\nfrom twisted.internet.defer import maybeDeferred\n\n\nclass ContactsForGroupHandler(BaseHandler):\n    \"\"\"\n    Handler for getting all contacts for a group\n\n    Methods supported:\n\n    * ``GET /:group_id/contacts`` - retrieve all contacts of a group.\n    \"\"\"\n    route_suffix = \":group_id/contacts\"\n    model_alias = \"collection\"\n\n    def get(self, group_id):\n        query = self.get_argument('query', default=None)\n        stream = self.get_argument('stream', default='false')\n        if stream == 'true':\n            d = maybeDeferred(self.collection.stream, group_id, query)\n            d.addCallback(self.write_queue)\n        else:\n            cursor = self.get_argument('cursor', default=None)\n            max_results = self.get_argument('max_results', default=None)\n            try:\n                max_results = max_results and int(max_results)\n            except ValueError:\n                raise HTTPError(400, \"max_results must be an integer\")\n            d = maybeDeferred(\n                self.collection.page, group_id, cursor=cursor,\n                max_results=max_results, query=query)\n            d.addCallback(self.write_page)\n        d.addErrback(self.catch_err, 404, CollectionObjectNotFound)\n        d.addErrback(self.catch_err, 400, CollectionUsageError)\n        d.addErrback(\n            self.raise_err, 500,\n            \"Failed to retrieve contacts for group %r.\" % group_id)\n        return d\n"}
{"text": "##############################################################################\n# Copyright (c) 2013-2017, Lawrence Livermore National Security, LLC.\n# Produced at the Lawrence Livermore National Laboratory.\n#\n# This file is part of Spack.\n# Created by Todd Gamblin, tgamblin@llnl.gov, All rights reserved.\n# LLNL-CODE-647188\n#\n# For details, see https://github.com/llnl/spack\n# Please also see the NOTICE and LICENSE files for our notice and the LGPL.\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License (as\n# published by the Free Software Foundation) version 2.1, February 1999.\n#\n# This program is distributed in the hope that it will be useful, but\n# WITHOUT ANY WARRANTY; without even the IMPLIED WARRANTY OF\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the terms and\n# conditions of the GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\n##############################################################################\nfrom spack import *\n\n\nclass RSimpleaffy(RPackage):\n    \"\"\"Provides high level functions for reading Affy .CEL files,\n       phenotypic data, and then computing simple things with it, such as\n       t-tests, fold changes and the like. Makes heavy use of the affy\n       library. Also has some basic scatter plot functions and mechanisms\n       for generating high resolution journal figures...\"\"\"\n\n    homepage = \"http://bioconductor.org/packages/simpleaffy/\"\n    url      = \"https://bioconductor.org/packages/3.5/bioc/src/contrib/simpleaffy_2.52.0.tar.gz\"\n\n    version('2.52.0', 'aa305099a57b3d868be53dc8c539b74e')\n\n    depends_on('r-biocgenerics', type=('build', 'run'))\n    depends_on('r-biobase', type=('build', 'run'))\n    depends_on('r-affy', type=('build', 'run'))\n    depends_on('r-genefilter', type=('build', 'run'))\n    depends_on('r-gcrma', type=('build', 'run'))\n    depends_on('r@3.4.0:3.4.9', when='@2.52.0')\n"}
{"text": "# Copyright 2020 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport json\nimport csv\nimport copy\nimport pandas as pd\n\nCENSUS_DATA_COLUMN_START = 7\n\n\nclass CensusGenericDataLoaderBase(object):\n    GENERIC_TEMPLATE_STAT_VAR = \"\"\"Node: {StatisticalVariable}\ndescription: \"{description}\"\ntypeOf: dcs:StatisticalVariable\npopulationType: schema:Person\nmeasuredProperty: {measuredProperty}\n\n\"\"\"\n\n    GENERIC_TEMPLATE_TMCF = \"\"\"Node: E:IndiaCensus{year}_{dataset_name}->E0\ntypeOf: dcs:StatVarObservation\nvariableMeasured: C:IndiaCensus{year}_{dataset_name}->StatisticalVariable\nobservationDate: C:IndiaCensus{year}_{dataset_name}->Year\nobservationAbout: E:IndiaCensus{year}_{dataset_name}->E1\nvalue: C:IndiaCensus{year}_{dataset_name}->Value\n\nNode: E:IndiaCensus{year}_{dataset_name}->E1\ntypeOf: schema:Place\nindianCensusAreaCode{year}: C:IndiaCensus{year}_{dataset_name}->census_location_id\"\"\"\n    \"\"\"An object that represents Census Data and its variables.\n    \n    Attributes:\n        census_columns (list): It will have all the data column names of a dataset\n        census_year : Census year\n        csv_file_path : Path where cleaned csv file will be saved\n        data_file_path : Input XLS file from Census of India. Can be url or local path.\n        dataset_name : Census dataset name. Eg:Primary_Abstract_Data\n        existing_stat_var (list): List of existing stat vars that we don't need to generate\n        mcf (list): Description\n        mcf_file_path : Description\n        metadata_file_path : Description\n        raw_df : Raw census data as dataframe\n        stat_var_index (dict): local storage for census column name and corresponding statvar\n        tmcf_file_path : Path where generated tmcf file will be saved\n    \"\"\"\n\n    def __init__(self, data_file_path, metadata_file_path, mcf_file_path,\n                 tmcf_file_path, csv_file_path, existing_stat_var, census_year,\n                 dataset_name):\n        \"\"\"\n        Constructor\n        \n        Args:\n            data_file_path :  Input XLS file from Census of India. Can be url or local path\n            metadata_file_path : Meta data csv file which has attribute details\n            mcf_file_path : Path where generated mcf file will be saved\n            tmcf_file_path : Path where generated tmcf file will be saved\n            csv_file_path : Path where cleaned csv file will be saved\n            existing_stat_var : List of existing stat vars that we don't need to generate\n            census_year : Census Year\n            dataset_name : Census dataset name. Eg:Primary_Abstract_Data\n        \"\"\"\n        self.data_file_path = data_file_path\n        self.metadata_file_path = metadata_file_path\n        self.mcf_file_path = mcf_file_path\n        self.csv_file_path = csv_file_path\n        self.tmcf_file_path = tmcf_file_path\n        self.existing_stat_var = existing_stat_var\n        self.census_year = census_year\n        self.dataset_name = dataset_name\n        self.raw_df = None\n        self.stat_var_index = {}\n        self.census_columns = []\n\n    def _download_and_standardize(self):\n        dtype = {\n            'State': str,\n            'District': str,\n            'Subdistt': str,\n            \"Town/Village\": str\n        }\n        self.raw_df = pd.read_excel(self.data_file_path, dtype=dtype)\n        self.census_columns = self.raw_df.columns[CENSUS_DATA_COLUMN_START:]\n\n    def _format_location(self, row):\n        # In this specific format there is no Level defined.\n        # A non zero location code from the lowest administration area\n        # takes the precedence.\n        if row[\"Town/Village\"] != \"000000\":\n            return row[\"Town/Village\"]\n\n        elif row[\"Subdistt\"] != \"00000\":\n            return row[\"Subdistt\"]\n\n        elif row[\"District\"] != \"000\":\n            return row[\"District\"]\n\n        elif row[\"State\"] != \"00\":\n            return row[\"State\"]\n        else:\n            # This is india level location\n            return \"0\"\n\n    def _format_data(self):\n        # This function is overridden in the child class\n        pass\n\n    def _get_base_name(self, row):\n        # This function is overridden in the child class\n        name = \"Count_\"\n        return name\n\n    def _create_variable(self, data_row, **kwargs):\n        # This function is overridden in the child class\n        pass\n\n    def _create_mcf(self):\n        # This function is overridden in the child class\n        pass\n\n    def _create_tmcf(self):\n        with open(self.tmcf_file_path, 'w+', newline='') as f_out:\n            f_out.write(\n                self.GENERIC_TEMPLATE_TMCF.format(\n                    year=self.census_year, dataset_name=self.dataset_name))\n\n    def process(self):\n        self._download_and_standardize()\n        self._create_mcf()\n        self._create_tmcf()\n        self._format_data()\n"}
{"text": "# -*- coding: utf-8 -*-\nimport os\nfrom subprocess import Popen, PIPE\nfrom time import sleep\n\nimport signal\nfrom nose.plugins.skip import SkipTest\n\nfrom tests.functional_tests import isolate\nfrom tuttle.workflow import Workflow\n\n\nclass TestKeyboardInterrupt:\n\n    def setUp(self):\n        if os.name != 'posix':\n            raise SkipTest(\"Testing keyboard interrupt only works on *nix\")\n\n    @isolate(['A'])\n    def test_interrupt_exit_code(self):\n        \"\"\" Should exit with code code when interrupted \"\"\"\n        project = \"\"\"file://B <- file://A\n    echo A produces B\n    sleep 1\n    echo B > B\n\n\"\"\"\n        with open('tuttlefile', \"w\") as f:\n            f.write(project)\n        proc = Popen(['tuttle', 'run'], stdout=PIPE)\n\n        sleep(0.5)\n        proc.send_signal(signal.SIGINT)\n        output = proc.stdout.read()\n        rcode = proc.wait()\n        assert rcode == 2, output\n        # assert output.find(\"Process tuttlefile_1 aborted by user\") > -1, output\n        assert output.find(\"Interrupted\") > -1, output\n        w = Workflow.load()\n        pB = w.find_process_that_creates(\"file://B\")\n        assert pB.end is not None, \"Process that creates B should have ended\"\n        assert pB.success is False, \"Process that creates B should have ended in error\"\n        assert pB.error_message.find(\"aborted\") >= -1, \"Process that creates B should be declared as aborted\"\n\n    @isolate(['A'])\n    def test_relaunch_after_interrupt(self):\n        \"\"\" Tuttle should run again after it has been interrupted\"\"\"\n        project = \"\"\"file://B <- file://A\n    sleep 1\n    echo B > B\n\"\"\"\n        with open('tuttlefile', \"w\") as f:\n            f.write(project)\n        proc = Popen(['tuttle', 'run'], stdout=PIPE)\n\n        sleep(0.5)\n        proc.send_signal(signal.SIGINT)\n        output = proc.stdout.read()\n        rcode = proc.wait()\n        assert rcode == 2, output\n\n        proc = Popen(['tuttle', 'run'], stdout=PIPE, stderr=PIPE)\n        rcode = proc.wait()\n        err = proc.stderr.read()\n        output = proc.stdout.read()\n        assert rcode == 2, output\n        assert output.find(\"already failed\") > -1, output + \"\\n\" + err\n\n    @isolate(['A'])\n    def test_relaunch_after_kill(self):\n        \"\"\" Tuttle should run again after it has been killed (from bug)\"\"\"\n        # raise SkipTest()\n        project = \"\"\"file://B <- file://A\n    echo B > B\n    sleep 1\n\"\"\"\n        with open('tuttlefile', \"w\") as f:\n            f.write(project)\n        proc = Popen(['tuttle', 'run'], stdout=PIPE)\n\n        sleep(0.5)\n        proc.send_signal(signal.SIGKILL)\n        output = proc.stdout.read()\n        rcode = proc.wait()\n        assert rcode == -signal.SIGKILL, output\n\n        proc = Popen(['tuttle', 'run'], stdout=PIPE, stderr=PIPE)\n        rcode = proc.wait()\n        err = proc.stderr.read()\n        assert err.find(\"DISCOVERED\") < 0, err\n"}
{"text": "# Copyright 2019 Tecnativa - David Vidal\n# Copyright 2020 Tecnativa - Pedro M. Baeza\n# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).\nfrom odoo.addons.sale_order_product_recommendation.tests import (\n    test_recommendation_common)\n\n\nclass RecommendationCaseTests(test_recommendation_common.RecommendationCase):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.product_uom_unit = cls.env.ref('uom.product_uom_unit')\n        cls.prod_1.write({\n            'secondary_uom_ids': [\n                (0, 0, {\n                    'name': 'Pack',\n                    'uom_id': cls.product_uom_unit.id,\n                    'factor': 10,\n                })],\n        })\n        cls.secondary_unit_p1 = cls.env['product.secondary.unit'].search([\n            ('product_tmpl_id', '=', cls.prod_1.product_tmpl_id.id),\n        ])\n        cls.prod_1.sale_secondary_uom_id = cls.secondary_unit_p1\n        cls.prod_2.write({\n            'secondary_uom_ids': [\n                (0, 0, {\n                    'name': 'Pack',\n                    'uom_id': cls.product_uom_unit.id,\n                    'factor': 24,\n                })],\n        })\n        cls.secondary_unit_p2 = cls.env['product.secondary.unit'].search([\n            ('product_tmpl_id', '=', cls.prod_2.product_tmpl_id.id),\n        ])\n\n    def test_recommendations_secondary_unit(self):\n        \"\"\"Tests defaults and factor functions\"\"\"\n        wizard = self.wizard()\n        self.assertEqual(len(wizard.line_ids), 3)\n        # Product 1 has a default secondary unit\n        wl_prod1 = wizard.line_ids.filtered(\n            lambda x: x.product_id == self.prod_1)\n        self.assertEqual(wl_prod1.secondary_uom_id, self.secondary_unit_p1)\n        wl_prod1.units_included = 25\n        wl_prod1._onchange_units_included_sale_order_secondary_unit()\n        self.assertAlmostEqual(wl_prod1.secondary_uom_qty, 2.5)\n        wl_prod1.secondary_uom_qty = 3\n        wl_prod1._onchange_secondary_uom()\n        self.assertAlmostEqual(wl_prod1.units_included, 30)\n        # Product 2 has a secondary units, but no default ones\n        wl_prod2 = wizard.line_ids.filtered(\n            lambda x: x.product_id == self.prod_2)\n        self.assertFalse(wl_prod2.secondary_uom_id)\n        # Product 3 has no secondary units\n        wl_prod3 = wizard.line_ids.filtered(\n            lambda x: x.product_id == self.prod_3)\n        self.assertFalse(wl_prod3.secondary_uom_id)\n        wl_prod3._onchange_secondary_uom()\n        wl_prod3._onchange_units_included_sale_order_secondary_unit()\n        self.assertFalse(wl_prod3.secondary_uom_id)\n\n    def test_transfer_of_secondary_unit(self):\n        \"\"\"Products get transferred to SO with their secondary units\"\"\"\n        wizard = self.wizard()\n        wl_prod1 = wizard.line_ids.filtered(\n            lambda x: x.product_id == self.prod_1)\n        wl_prod2 = wizard.line_ids.filtered(\n            lambda x: x.product_id == self.prod_2)\n        wl_prod1.secondary_uom_qty = 2  # 20 units\n        wl_prod1._onchange_secondary_uom()\n        wl_prod2.secondary_uom_id = self.secondary_unit_p2\n        wl_prod2.secondary_uom_qty = 2  # 48 units\n        wl_prod2._onchange_secondary_uom()\n        wizard.action_accept()\n        self.assertEqual(len(self.new_so.order_line), 2)\n        sl_p1 = self.new_so.order_line.filtered(\n            lambda x: x.product_id == self.prod_1)\n        self.assertEqual(sl_p1.secondary_uom_id, self.secondary_unit_p1)\n        self.assertEqual(sl_p1.secondary_uom_qty, 2)\n        self.assertEqual(sl_p1.product_uom_qty, 20)\n        sl_p2 = self.new_so.order_line.filtered(\n            lambda x: x.product_id == self.prod_2)\n        self.assertEqual(sl_p2.secondary_uom_id, self.secondary_unit_p2)\n        self.assertEqual(sl_p2.secondary_uom_qty, 2)\n        self.assertEqual(sl_p2.product_uom_qty, 48)\n        # Update the quantities\n        wizard = self.wizard()\n        wl_prod1 = wizard.line_ids.filtered(\n            lambda x: x.product_id == self.prod_1)\n        wl_prod1.secondary_uom_qty = 1  # 10 units\n        wl_prod1._onchange_secondary_uom()\n        wizard.action_accept()\n        sl_p1 = self.new_so.order_line.filtered(\n            lambda x: x.product_id == self.prod_1)\n        self.assertEqual(sl_p1.secondary_uom_qty, 1)\n        self.assertEqual(sl_p1.product_uom_qty, 10)\n"}
{"text": "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"A very simple MNIST classifier.\n\nSee extensive documentation at\nhttp://tensorflow.org/tutorials/mnist/beginners/index.md\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\n# Import data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf\n\nFLAGS = None\n\n\ndef main(_):\n  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n  # Create the model\n  x = tf.placeholder(tf.float32, [None, 784])\n  W = tf.Variable(tf.zeros([784, 10]))\n  b = tf.Variable(tf.zeros([10]))\n  y = tf.matmul(x, W) + b\n\n  # Define loss and optimizer\n  y_ = tf.placeholder(tf.float32, [None, 10])\n\n  # The raw formulation of cross-entropy,\n  #\n  #   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n  #                                 reduction_indices=[1]))\n  #\n  # can be numerically unstable.\n  #\n  # So here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n  # outputs of 'y', and then average across the batch.\n  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n\n  sess = tf.InteractiveSession()\n  # Train\n  tf.initialize_all_variables().run()\n  for _ in range(50000):\n    batch_xs, batch_ys = mnist.train.next_batch(100)\n    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\n  # Test trained model\n  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n  print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n                                      y_: mnist.test.labels}))\n\nif __name__ == '__main__':\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--data_dir', type=str, default='/tmp/data',\n                      help='Directory for storing data')\n  FLAGS = parser.parse_args()\n  tf.app.run()\n"}
{"text": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jul 11 18:51:28 2018\n\n@author: marian\n\"\"\"\n\nimport os\nimport numpy as np\nimport pickle\n\n# %% set parameters\n\n# set input path\nstrPthPrnt = \"/media/sf_D_DRIVE/MotionLocaliser/UsedPsychoPyScripts/P02/Conditions\"\n\n# provide names of condition files in the order that they were shown\nlstPickleFiles = [\n    'Conditions_run01.pickle',\n    'Conditions_run02.pickle',\n    'Conditions_run03.pickle',\n    'Conditions_run04.pickle',\n    'Conditions_run05.pickle',\n    'Conditions_run06.pickle',\n    ]\n\n# provide the TR in seconds\nvarTr = 3.0\n\n# provide the stimulation time\nvarStmTm = 3.0\n\n# %% load conditions files\n\n# Loop through npz files in target directory:\nlstCond = []\nfor ind, cond in enumerate(lstPickleFiles):\n    inputFile = os.path.join(strPthPrnt, cond)\n\n    with open(inputFile, 'rb') as handle:\n        array1 = pickle.load(handle)\n    aryTmp = array1[\"Conditions\"].astype('int32')\n\n    # append condition to list\n    lstCond.append(aryTmp)\n\n# join conditions across runs\naryCond = np.vstack(lstCond)\n\n# create empty array\naryTmpCond = np.empty((len(aryCond), 4), dtype='float16')\n# get the condition nr\naryTmpCond[:, 0] = aryCond[:, 0]\n# get the onset time\naryTmpCond[:, 1] = np.cumsum(np.ones(len(aryCond))*varTr) - varTr\n# get the duration\naryTmpCond[:, 2] = np.ones(len(aryCond))*varStmTm\n# add the feature identifier\naryTmpCond[:, 3] = aryCond[:, 1]\n# set output name\nstrPthOut = os.path.join(strPthPrnt,'Conditions')\nnp.save(strPthOut, aryTmpCond)\n"}
{"text": "# This program is free software; you can redistribute it and/or modify it under\n# the terms of the GNU General Public License as published by the Free Software\n# Foundation; either version 2 of the License, or (at your option) any later\n# version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details\n#\n# You should have received a copy of the GNU General Public License along with\n# this program; if not, write to the Free Software Foundation, Inc.,\n# 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.\n\nfrom kiwi.environ import Library, environ\nimport os\n\nlib = Library(\"lintgtk\")\nif lib.uninstalled:\n    if os.path.exists(\"/usr/share/lintgtk\"):\n        lib.add_global_resource('glade', '/usr/share/lintgtk/glade')\n        lib.add_global_resource('images', '/usr/share/lintgtk/images')\n    else:\n        environ.add_resource('glade', 'glade')\n        environ.add_resource('images', 'images')\n\n\n"}
{"text": "import sys\nfrom ged4py.parser import GedcomReader\nfrom ged4py.date import DateValueVisitor\n\n\nclass DateFormatter(DateValueVisitor):\n    \"\"\"Visitor class that produces string representation of dates.\n    \"\"\"\n    def visitSimple(self, date):\n        return f\"{date.date}\"\n\n    def visitPeriod(self, date):\n        return f\"from {date.date1} to {date.date2}\"\n\n    def visitFrom(self, date):\n        return f\"from {date.date}\"\n\n    def visitTo(self, date):\n        return f\"to {date.date}\"\n\n    def visitRange(self, date):\n        return f\"between {date.date1} and {date.date2}\"\n\n    def visitBefore(self, date):\n        return f\"before {date.date}\"\n\n    def visitAfter(self, date):\n        return f\"after {date.date}\"\n\n    def visitAbout(self, date):\n        return f\"about {date.date}\"\n\n    def visitCalculated(self, date):\n        return f\"calculated {date.date}\"\n\n    def visitEstimated(self, date):\n        return f\"estimated {date.date}\"\n\n    def visitInterpreted(self, date):\n        return f\"interpreted {date.date} ({date.phrase})\"\n\n    def visitPhrase(self, date):\n        return f\"({date.phrase})\"\n\n\nformat_visitor = DateFormatter()\n\nwith GedcomReader(sys.argv[1]) as parser:\n    # iterate over each INDI record in a file\n    for i, indi in enumerate(parser.records0(\"INDI\")):\n        print(f\"{i}: {indi.name.format()}\")\n\n        # get all possible event types and print their dates,\n        # full list of events is longer, this is only an example\n        events = indi.sub_tags(\"BIRT\", \"CHR\", \"DEAT\", \"BURI\", \"ADOP\", \"EVEN\")\n        for event in events:\n            date = event.sub_tag_value(\"DATE\")\n            # Some event types like generic EVEN can define TYPE tag\n            event_type = event.sub_tag_value(\"TYPE\")\n            # pass a visitor to format the date\n            if date:\n                date_str = date.accept(format_visitor)\n            else:\n                date_str = \"N/A\"\n            print(f\"    event: {event.tag} date: {date_str} type: {event_type}\")\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import models, migrations\nimport datetime\nfrom django.utils.timezone import utc\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('core', '0009_delete_awsconfig'),\n    ]\n\n    operations = [\n        migrations.AddField(\n            model_name='awsinstance',\n            name='key_name',\n            field=models.CharField(max_length=50, null=True, blank=True),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='awsinstance',\n            name='launch_time',\n            field=models.DateTimeField(default=datetime.datetime(2015, 4, 10, 22, 8, 19, 335848, tzinfo=utc), auto_now=True, auto_now_add=True),\n            preserve_default=False,\n        ),\n        migrations.AddField(\n            model_name='awsinstance',\n            name='platform',\n            field=models.CharField(max_length=50, null=True, blank=True),\n            preserve_default=True,\n        ),\n        migrations.AddField(\n            model_name='awsinstance',\n            name='public_dns_name',\n            field=models.CharField(max_length=100, null=True, blank=True),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='awsinstance',\n            name='previous_state',\n            field=models.CharField(default=b'Not known', max_length=15, null=True, blank=True),\n            preserve_default=True,\n        ),\n        migrations.AlterField(\n            model_name='awsinstance',\n            name='state',\n            field=models.CharField(default=b'Un-synced', max_length=15, null=True, blank=True),\n            preserve_default=True,\n        ),\n    ]\n"}
{"text": "# -*- coding: utf-8 -*-\n#\n# PyMysqlPool documentation build configuration file, created by\n# sphinx-quickstart on Wed Aug 23 16:56:20 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath('.'))\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = []\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ['.rst', '.md']\nsource_suffix = '.rst'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = u'PyMysqlPool'\ncopyright = u'2017, Lucifer Jack'\nauthor = u'Lucifer Jack'\n\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = u'0.5'\n# The full version, including alpha/beta/rc tags.\nrelease = u'0.5'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'alabaster'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# This is required for the alabaster theme\n# refs: http://alabaster.readthedocs.io/en/latest/installation.html#sidebars\nhtml_sidebars = {\n    '**': [\n        'about.html',\n        'navigation.html',\n        'relations.html',  # needs 'show_related': True theme option to display\n        'searchbox.html',\n        'donate.html',\n    ]\n}\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'PyMysqlPooldoc'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, 'PyMysqlPool.tex', u'PyMysqlPool Documentation',\n     u'Lucifer Jack', 'manual'),\n]\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, 'pymysqlpool', u'PyMysqlPool Documentation',\n     [author], 1)\n]\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, 'PyMysqlPool', u'PyMysqlPool Documentation',\n     author, 'PyMysqlPool', 'One line description of project.',\n     'Miscellaneous'),\n]\n"}
{"text": "#The following fuction will take the following parameters:\n#A HTML page with the island info\n\n#And it will produce:\n#A timestamp of when the fuction was executed in Local Time in a form of struct_time\n#The type of data produced by the function in a form of a string \"fact\"\n#The yoweb location of the page in a form of a string \"ocean\"\n#A tuple of tuples in a form of ((light_total, light_online, light_island), (dark_total, dark_online, dark_island))\n#A tuple of tuples in a form of \n#\t((time, (atc_fact, atc_ship), (def_fact, def_ship), (isld_1, isld_2), (winner, cond))*)\n\nfrom bs4 import BeautifulSoup\nimport time\n\ndef fact_parse(page, url):\n\n\ttimest = time.localtime()\n\tpage_type = \"fact\"\n\tloc = \"\"\n\tfaction_control = []\n\tbattles = []\n\tsoup = BeautifulSoup(page, \"html.parser\")\n\t\n\tloc = url[url.index('/')+2:url.index('.')]\n\t\n\tif soup.title.get_text() != \"Balance of Power\":\n\t\tprint(\"Invalid Faction Page\")\n\t\treturn None\n\n\tstats = [x.find_all(\"td\") for x in soup.find_all(\"table\")[1].find_all(\"tr\")[3:]]\n\tfaction_control = ((stats[0][1].get_text(), stats[1][1].get_text(), stats[2][1].get_text()),\n\t\t(stats[0][2].get_text(), stats[1][2].get_text(), stats[2][2].get_text()))\n\t\n\tbattle_table = soup.find_all(\"table\")[4].find_all(\"tr\")\n\t\n\tfor i in battle_table:\n\t\tatc, defn = [(x.img.get(\"src\")[19:-7], x.get_text()[1:]) for x in i.find_all(\"b\")]\n\t\tfor j in i.find_all(\"b\"): j.replaceWith(\"\")\n\t\ttext = i.get_text().split('\\n')\n\t\tbattle_time = text[6].strip()[:-1]\n\t\tbattle_place = text[7].strip()[18:-1].split(\" and \")\n\t\twinner = text[9].strip()[4:-1].split(\" vessel \")\n\t\twinner[0] = 0 if winner[0] == \"attacking\" and winner[1] != \"was sunk\" else 1\n\t\twinner[1] = {\"won\":\"battle\", \"disengaged\":\"disengage\", \"was sunk\":\"sink\"}[winner[1]]\n\t\tbattles.append((battle_time, atc, defn, battle_place, winner))\n\n\treturn (timest, page_type, loc, loc + \"_stats\", faction_control, battles)\n"}
{"text": "import threading\nfrom collections import defaultdict\n\nimport sublime_plugin\n\nfrom .sublime import after\n\n\nclass IdleIntervalEventListener(sublime_plugin.EventListener):\n    \"\"\"\n    Base class.\n\n    Monitors view idle time and calls .on_idle() after the specified duration.\n\n    Idle time is defined as time during which no calls to .on_modified[_async]()\n    have been made.\n\n    Subclasses must implement .on_idle(view) and, if necessary, .check(view).\n\n    We don't provide a default implementation of .on_idle(view).\n    \"\"\"\n\n    def __init__(self, *args, duration=500, **kwargs):\n        \"\"\"\n        @duration\n          Interval after which an .on_idle() call will be made, expressed in\n          milliseconds.\n        \"\"\"\n\n        # TODO: Maybe it's more efficient to collect .on_idle() functions and\n        # manage edits globally, then call collected functions when idle.\n        self.edits = defaultdict(int)\n        self.lock = threading.Lock()\n\n        # Expressed in milliseconds.\n        self.duration = duration\n        super().__init__(*args, **kwargs)\n\n    @property\n    def _is_subclass(self):\n        return hasattr(self, 'on_idle')\n\n    def _add_edit(self, view):\n        with self.lock:\n            self.edits[view.id()] += 1\n        # TODO: are we running async or sync?\n        after(self.duration, lambda: self._subtract_edit(view))\n\n    def _subtract_edit(self, view):\n        with self.lock:\n            self.edits[view.id()] -= 1\n            if self.edits[view.id()] == 0:\n                self.on_idle(view)\n\n    def on_modified_async(self, view):\n        # TODO: improve check for widgets and overlays.\n        if not all((view, self._is_subclass, self.check(view))):\n            return\n        self._add_edit(view)\n\n    # Override in derived class if needed.\n    def check(self, view):\n        \"\"\"\n        Returs `True` if @view should be monitored for idleness.\n\n        @view\n          The view that is about to be monitored for idleness.\n        \"\"\"\n        return True\n"}
{"text": "import asyncio\nimport json\nimport signal\nimport sys\nimport time\nfrom asyncio.tasks import FIRST_COMPLETED\n\nfrom config import Config\nfrom pxgrid import PxgridControl\nfrom websockets import ConnectionClosed\nfrom ws_stomp import WebSocketStomp\n\n\nasync def future_read_message(ws, future):\n    try:\n        message = await ws.stomp_read_message()\n        future.set_result(message)\n    except ConnectionClosed:\n        print('Websocket connection closed')\n\nasync def subscribe_loop(config, secret, ws_url, topic):\n    ws = WebSocketStomp(ws_url, config.get_node_name(), secret, config.get_ssl_context())\n    await ws.connect()\n    await ws.stomp_connect(pubsub_node_name)\n    await ws.stomp_subscribe(topic)\n    print(\"Ctrl-C to disconnect...\")\n    while True:\n        future = asyncio.Future()\n        future_read = future_read_message(ws, future)\n        try:\n            await asyncio.wait([future_read], return_when=FIRST_COMPLETED)\n        except asyncio.CancelledError:\n            await ws.stomp_disconnect('123')\n            # wait for receipt\n            await asyncio.sleep(3)\n            await ws.disconnect()\n            break\n        else:\n            message = json.loads(future.result())\n            print(\"message=\" + json.dumps(message))\n\n\nif __name__ == '__main__':\n    config = Config()\n    pxgrid = PxgridControl(config=config)\n\n    while pxgrid.account_activate()['accountState'] != 'ENABLED':\n        time.sleep(60)\n\n    # lookup for session service\n    service_lookup_response = pxgrid.service_lookup('com.cisco.ise.session')\n    service = service_lookup_response['services'][0]\n    pubsub_service_name = service['properties']['wsPubsubService']\n    topic = service['properties']['sessionTopic']\n\n    # lookup for pubsub service\n    service_lookup_response = pxgrid.service_lookup(pubsub_service_name)\n    pubsub_service = service_lookup_response['services'][0]\n    pubsub_node_name = pubsub_service['nodeName']\n    secret = pxgrid.get_access_secret(pubsub_node_name)['secret']\n    ws_url = pubsub_service['properties']['wsUrl']\n\n    loop = asyncio.get_event_loop()\n    subscribe_task = asyncio.ensure_future(subscribe_loop(config, secret, ws_url, topic))\n\n    # Setup signal handlers\n    loop.add_signal_handler(signal.SIGINT, subscribe_task.cancel)\n    loop.add_signal_handler(signal.SIGTERM, subscribe_task.cancel)\n\n    # Event loop\n    loop.run_until_complete(subscribe_task)\n"}
{"text": "# -*- encoding: utf-8 -*-\n################################################################################\n#                                                                              #\n# Copyright (C) 2013-Today  Carlos Eduardo Vercelino - CLVsol                  #\n#                                                                              #\n# This program is free software: you can redistribute it and/or modify         #\n# it under the terms of the GNU Affero General Public License as published by  #\n# the Free Software Foundation, either version 3 of the License, or            #\n# (at your option) any later version.                                          #\n#                                                                              #\n# This program is distributed in the hope that it will be useful,              #\n# but WITHOUT ANY WARRANTY; without even the implied warranty of               #\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the                #\n# GNU Affero General Public License for more details.                          #\n#                                                                              #\n# You should have received a copy of the GNU Affero General Public License     #\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.        #\n################################################################################\n\nfrom openerp.osv import orm, fields\n\nclass oehealth_tag(orm.Model):\n    _inherit = 'oehealth.tag'\n\n    _columns = {\n        'professional_ids': fields.many2many('oehealth.professional', \n                                             'oehealth_professional_tag_rel', \n                                             'tag_id', \n                                             'professional_id', \n                                             'Professionals'),\n    }\n\noehealth_tag()\n"}
{"text": "#! /usr/bin/python\n#\n# SIP Watcher\n# \n# Michael Pilgermann (michael.pilgermann@gmx.de)\n# Version 0.1 (2008-10-31)\n#\n#\n\n#import sys\nimport pjsua as pj\nimport WatcherApplet\nimport threading\nimport gtk\nimport thread\n\n# Globals\n#current_call = None\nacc = None\nacc_cb = None\n#gui = None\n\n\n# Callback to receive events from account\nclass MyAccountCallback(pj.AccountCallback):\n    global acc\n    gui = None\n    sem = None\n\n    def __init__(self, account,  gui):\n        pj.AccountCallback.__init__(self, account)\n        self.gui = gui\n#        print self.__dict__\n        \n    def wait(self):\n        self.sem = threading.Semaphore(0)\n        self.sem.acquire()\n        \n    def on_incoming_call(self, call):\n        print \"Incoming call from \", call.info().remote_uri\n        self.gui.showEvent(\"Test\", 10)\n        if self.sem:\n            self.sem.release()\n\n    def on_reg_state(self):\n        print \"Status of account changed to \",  acc.info().reg_reason\n        \ndef startupSipBackend(gui):\n    global acc\n    global acc_cb\n    try:    \n        lib = pj.Lib()\n        lib.init(log_cfg = None)\n        transport = lib.create_transport(pj.TransportType.UDP, pj.TransportConfig(5060))\n        lib.start()\n        \n        acc_cfg = pj.AccountConfig(\"030.sip.arcor.de\", \"03053140698\", \"67631411\")\n        \n        acc = lib.create_account(acc_cfg)\n        acc_cb = MyAccountCallback(acc,  gui)\n        acc.set_callback(acc_cb)\n        acc.set_basic_status(1)\n        \n        while gui.up:\n            acc_cb.wait()\n        \n        acc.delete()\n        lib.destroy()\n    except pj.Error, err:\n        print 'Error creating account:', err\n\ndef shutdownSipBackend():\n    acc_cb.sem.release()\n\ngtk.gdk.threads_init()\ngui = WatcherApplet.Gui()\nthread.start_new_thread(startupSipBackend, (gui,))\ngtk.gdk.threads_enter()\ngui.initGui()\ngtk.gdk.threads_leave()\nshutdownSipBackend()\n"}
{"text": "#! /usr/bin/env python\n\nimport saga_api, sys, os\n\n##########################################\ndef xyz2shp(fTable):\n    table   = saga_api.SG_Get_Data_Manager().Add_Table()\n    if table.Create(saga_api.CSG_String(fTable)) == 0:\n        table.Add_Field(saga_api.CSG_String('X'), saga_api.SG_DATATYPE_Double)\n        table.Add_Field(saga_api.CSG_String('Y'), saga_api.SG_DATATYPE_Double)\n        table.Add_Field(saga_api.CSG_String('Z'), saga_api.SG_DATATYPE_Double)\n        rec = table.Add_Record()\n        rec.Set_Value(0,0)\n        rec.Set_Value(1,0)\n        rec.Set_Value(2,2)\n        rec = table.Add_Record()\n        rec.Set_Value(0,0)\n        rec.Set_Value(1,1)\n        rec.Set_Value(2,2)\n        rec = table.Add_Record()\n        rec.Set_Value(0,1)\n        rec.Set_Value(1,1)\n        rec.Set_Value(2,1)\n        rec = table.Add_Record()\n        rec.Set_Value(0,1)\n        rec.Set_Value(1,0)\n        rec.Set_Value(2,1)\n\n    points = saga_api.SG_Get_Data_Manager().Add_Shapes(saga_api.SHAPE_TYPE_Point)\n\n    # ------------------------------------\n    if os.name == 'nt':    # Windows\n        saga_api.SG_Get_Module_Library_Manager().Add_Library(os.environ['SAGA_32' ] + '/modules/shapes_points.dll')\n    else:                  # Linux\n        saga_api.SG_Get_Module_Library_Manager().Add_Library(os.environ['SAGA_MLB'] + '/libshapes_points.so')\n\n    m      = saga_api.SG_Get_Module_Library_Manager().Get_Module(saga_api.CSG_String('shapes_points'), 0) # 'Convert Table to Points'\n    p      = m.Get_Parameters()\n    p(saga_api.CSG_String('TABLE' )).Set_Value(table)\n    p(saga_api.CSG_String('POINTS')).Set_Value(points)\n    p(saga_api.CSG_String('X'     )).Set_Value(0)\n    p(saga_api.CSG_String('Y'     )).Set_Value(1)\n    \n    if m.Execute() == 0:\n        print 'ERROR: executing module [' + m.Get_Name().c_str() + ']'\n        return 0\n\n    # ------------------------------------\n    points.Save(saga_api.CSG_String(fTable))\n\n    print 'success'\n    return 1\n\n##########################################\nif __name__ == '__main__':\n    print 'Python - Version ' + sys.version\n    print saga_api.SAGA_API_Get_Version()\n    print\n\n    if len( sys.argv ) != 2:\n        print 'Usage: xyz2shp.py <in: x/y/z-data as text or dbase table>'\n        print '... trying to run with test_data'\n        fTable = './test_pts_xyz.xyz'\n    else:\n        fTable = sys.argv[ 1 ]\n        if os.path.split(fTable)[0] == '':\n            fTable = './' + fTable\n\n    xyz2shp(fTable)\n"}
{"text": "from django.utils.timezone import utc\n\nfrom django_db_utils import pg_bulk_update\n\nfrom example.models import Sample, SampleStatus\n\n\ndef now():\n    from datetime import datetime\n    return datetime.utcnow().replace(tzinfo=utc)\n\n\ndef make_fake_data(samples_to_make=100000, batch_threshold=100000, delete_existing=True, make_statuses=True, years=5):\n    \"\"\"Makes mock data for testing performance. Optionally, resets db.\n    \"\"\"\n    if delete_existing:\n        Sample.objects.all().delete()\n        print \"Deleted existing\"\n\n    # Make up a set of\n    offset = samples_to_make - samples_to_make/52/years\n\n    # Create all the samples.\n    samples = []\n    barcodes = range(samples_to_make)\n    for barcode in barcodes:\n        sample = Sample()\n        sample.barcode = str(barcode)\n        sample.created = now()\n        sample.status_created = sample.created\n        if barcode < offset:\n            sample.status_code = SampleStatus.COMPLETE\n        else:\n            sample.status_code = SampleStatus.LAB\n        sample.production = True\n        samples.append(sample)\n        if len(samples) >= batch_threshold:\n            Sample.objects.bulk_create(samples)\n            del samples[:]\n            print \"Made %s samples.\" % Sample.objects.count()\n    if samples:\n        Sample.objects.bulk_create(samples)\n    print \"Finished making %s samples.\" % Sample.objects.count()\n\n    if not make_statuses:\n        return\n\n    # Pull all ids for samples.\n    sample_ids = Sample.objects.values_list('id', flat=True)\n\n    # Create all the statuses.\n    offset = len(sample_ids)-len(sample_ids)/52/years\n    statuses = []\n    for sample in sample_ids[:offset]:\n        statuses.append(SampleStatus(sample_id=sample, status_code=SampleStatus.RECEIVED, created=now()))\n        statuses.append(SampleStatus(sample_id=sample, status_code=SampleStatus.LAB, created=now()))\n        statuses.append(SampleStatus(sample_id=sample, status_code=SampleStatus.COMPLETE, created=now()))\n        if len(statuses) >= batch_threshold:\n            SampleStatus.objects.bulk_create(statuses)\n            del statuses[:]\n    for sample in sample_ids[offset:]:\n        statuses.append(SampleStatus(sample_id=sample, status_code=SampleStatus.RECEIVED, created=now()))\n        statuses.append(SampleStatus(sample_id=sample, status_code=SampleStatus.LAB, created=now()))\n        if len(statuses) >= batch_threshold:\n            SampleStatus.objects.bulk_create(statuses)\n            del statuses[:]\n            print \"Made %s statuses.\"%SampleStatus.objects.count()\n    if statuses:\n        SampleStatus.objects.bulk_create(statuses)\n    print \"Finished making %s statuses.\"%SampleStatus.objects.count()\n\n    # Make all the denormalized status_code vars match.\n    sync_status(limit=batch_threshold)\n    print \"Statuses synchronized\"\n\n\ndef sync_status(limit=100000):\n    # Stream through all samples.\n    sample_count = Sample.objects.count()\n    for index in range(0, sample_count, limit):\n        vals = Sample.objects.order_by('id', '-statuses__status_code').distinct('id').values_list('id', 'status_code', 'statuses__id', 'statuses__status_code')[index:index+limit]\n        # Pull all mismatching values.\n        ids = []\n        status_codes = []\n#        status_ids = []\n        for sample_id, status_code, status_id, latest_status_code in vals:\n            if status_code != latest_status_code:\n                ids.append(sample_id)\n                status_codes.append(latest_status_code)\n#                status_ids.append(status_id)\n        # Sync using a bulk update.\n        if ids:\n            pg_bulk_update(Sample, 'id', 'status_code', list(ids), list(status_codes))\n#            pg_bulk_update(Sample, 'id', 'status_id', list(ids), list(status_ids))\n        print 'Synced %s out of %s samples at %s'%(len(ids), limit, index)\n"}
{"text": "import media\nimport fresh_tomatoes\n\n# Instances from the base class 'Movie' __init__ function\n# Google URL shortner was used to make the URLs more manageable\n# and follow PEP8 Style Guide\niron_giant = media.Movie(\"The Iron Giant\",\n                         \"Boy befriends a giant robot during the Cold War\",\n                         \"https://goo.gl/uuUvgf\",\n                         \"https://goo.gl/4ipRly\")\n\nbalto = media.Movie(\"Balto\",\n                    \"Wolfdog braves the snow to save the children of Nome\",\n                    \"https://goo.gl/u12LEZ\",\n                    \"https://goo.gl/jYmxor\")\n\ntwlv_angry_men = media.Movie(\"12 Angry Men\",\n                             \"Twelve white jurors decide a black boys' fate\",\n                             \"https://goo.gl/h4eZhW\",\n                             \"https://goo.gl/7btrww\")\n\nbig_lebowski = media.Movie(\"The Big Lebowski\",\n                           \"A californian stoner is subjected to a series of unfortunate events\",\n                           \"https://goo.gl/YCqBbd\",\n                           \"https://goo.gl/PVKR1Q\")\n\n\nv_for_vendetta = media.Movie(\"V for Vendetta\",\n                             \"A vigilante seeks to overthrow the totalitarian British government\",\n                             \"https://goo.gl/ZzDwxa\",\n                             \"https://goo.gl/QvsCKW\")\n\n\ncopying_beethoven = media.Movie(\"Copying Beethoven\",\n                                \"A female copy writer becomes an understudy to Ludwid Van Beethoven\",\n                                \"https://goo.gl/2iVK4z\",\n                                \"https://goo.gl/tAK5Rr\")\n\n\nben_hur = media.Movie(\"Ben Hur\",\n                      \"Jewish man overthrows the Roman Empire to rescue his wife and mother\",\n                      \"https://goo.gl/QTUcWp\",\n                      \"https://goo.gl/uSJKyc\")\n\n\ngladiator = media.Movie(\"Gladiator\",\n                        \"Fallen general becomes a gladiator seeking to overthrow the illegitimate Caesar\",\n                        \"https://goo.gl/JmT1Uy\",\n                        \"https://goo.gl/9IGyCg\")\n\njungle_book = media.Movie(\"The Jungle Book\",\n                          \"Mowgli goes on an adventure to defeat Shere Khan\",\n                          \"https://goo.gl/V0b3P7\",\n                          \"https://goo.gl/JenB1g\")\n\n# fresh_tomatoes file reads this list and outputs a webpage\nmovie_l = [iron_giant, balto, twlv_angry_men, big_lebowski, v_for_vendetta,\n           copying_beethoven, ben_hur, gladiator, jungle_book]\n\n# Opens the movie webpage with the above movies\nfresh_tomatoes.open_movies_page(movie_l)\n"}
{"text": "import os\nimport sys\nfrom setuptools import setup\nfrom setuptools.command.test import test as TestCommand\n\n\nclass PyTest(TestCommand):\n    def finalize_options(self):\n        TestCommand.finalize_options(self)\n        self.test_args = ['--verbose']\n        self.test_suite = True\n\n    def run_tests(self):\n        import pytest\n        errno = pytest.main(self.test_args)\n        sys.exit(errno)\n\n\ndef extract_version():\n    version = None\n    fdir = os.path.dirname(__file__)\n    fnme = os.path.join(fdir, 'yaml2ncml', '__init__.py')\n    with open(fnme) as fd:\n        for line in fd:\n            if (line.startswith('__version__')):\n                _, version = line.split('=')\n                version = version.strip()[1:-1]\n                break\n    return version\n\nrootpath = os.path.abspath(os.path.dirname(__file__))\n\n\ndef read(*parts):\n    return open(os.path.join(rootpath, *parts), 'r').read()\n\nlong_description = '{}\\n{}'.format(read('README.rst'), read('CHANGES.txt'))\nLICENSE = read('LICENSE.txt')\n\nwith open('requirements.txt') as f:\n    require = f.readlines()\ninstall_requires = [r.strip() for r in require]\n\nsetup(name='yaml2ncml',\n      version=extract_version(),\n      packages=['yaml2ncml'],\n      license=LICENSE,\n      description='ncML aggregation from YAML specifications',\n      long_description=long_description,\n      author='Rich Signell',\n      author_email='rsignell@usgs.gov',\n      install_requires=install_requires,\n      entry_points=dict(console_scripts=[\n          'yaml2ncml = yaml2ncml.yaml2ncml:main']\n          ),\n      url='https://github.com/rsignell-usgs/yaml2ncml',\n      keywords=['YAML', 'ncml'],\n      classifiers=['Development Status :: 4 - Beta',\n                   'Programming Language :: Python :: 2.7',\n                   'Programming Language :: Python :: 3.4',\n                   'License :: OSI Approved :: MIT License'],\n      tests_require=['pytest'],\n      cmdclass=dict(test=PyTest),\n      zip_safe=False)\n"}
{"text": "import sys\nsys.path.insert(0, '../spymanager')\nsys.path.insert(0, '../')\n\nfrom tests import create_database_collection\nfrom src.subscriptions import SubscriptionsManager\n\n\n# Database settings\nDATABASE_NAME = 'spies_database'\nCOLLECTION_NAME = 'subscriptions'\nsubscriptions_collection = create_database_collection(DATABASE_NAME, COLLECTION_NAME)\n\nsubscriptions_manager = SubscriptionsManager(subscriptions_collection)\n\n# User to test\nUSERNAME = 'pinheirofellipe'\n\n# Clear before tests\nsubscriptions_manager.remove(USERNAME)\n\nsubscriptions_manager.add(USERNAME)\n\nall_subscritions = subscriptions_manager.all()\n\nassert len(all_subscritions) == 1\n\nuser = subscriptions_manager.get(USERNAME)\n\nassert user.username == USERNAME\n\nassert user.exists() is True\n\nsubscribers = [\n    {\n        \"spy\": \"spy1\",\n        \"group\": \"g1\",\n        \"chat_id\": 123456\n    }, {\n        \"spy\": \"spy2\",\n        \"group\": \"g1\",\n        \"chat_id\": 654321\n    }\n]\n\nuser.add_subscribers(subscribers)\n\nassert len(user.subscribers) == 2\n\nsubscriber_to_remove = {\n    \"spy\": \"spy1\",\n    \"group\": \"g1\",\n    \"chat_id\": 123456\n}\n\nuser.remove_subscriber(subscriber_to_remove)\n\nassert len(user.subscribers) == 1\n\nprint('Well done!')\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import division, print_function\n\nimport argparse\nimport glob\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom geopy.distance import vincenty as point_distance\n\n\ndef ingest(fn, route_id, begin_latlng, end_latlng):\n    df = pd.read_csv(fn, parse_dates=['timestamp'])\n    df = df.drop(['speed', 'trip_headsign'], axis=1)\n    df = df[df.route_id == route_id]\n    df['begin_distances'] = compute_distance(df, begin_latlng)\n    df['end_distances'] = compute_distance(df, end_latlng)\n    return df\n\n\ndef compute_distance(df, latlng):\n    df = df.copy()\n    starts = zip(df.latitude, df.longitude)\n    return [point_distance(latlng, s).meters for s in starts]\n\n\ndef parse_duration(df):\n    '''\n    for each trip id\n        choose a reading nearest the begin stop\n        choose a reading nearest downtown\n        subtract the times for those two readings\n        positive is southbound\n    '''\n\n    mins = df.groupby('trip_id').idxmin()\n    begin_mins = df.loc[mins.begin_distances].set_index('trip_id')\n    end_mins = df.loc[mins.end_distances].set_index('trip_id')\n\n    unneeded_cols = ['begin_distances', 'end_distances', 'latitude', 'longitude']\n    begin_mins.drop(unneeded_cols, axis=1, inplace=True)\n    end_mins.drop(['vehicle_id', 'route_id'] + unneeded_cols, axis=1, inplace=True)\n\n    result = begin_mins.join(end_mins, rsuffix='_begin', lsuffix='_end')\n\n    duration = begin_mins.timestamp - end_mins.timestamp\n    result['duration'] = duration / np.timedelta64(1, 's')\n\n    return result\n\n\ndef parse_duration_by_hour(df):\n    df['duration_abs'] = df['duration'].abs()\n    df['hour'] = df['timestamp_begin'].apply(\n        lambda x: x.tz_localize('UTC').tz_convert('US/Central').hour\n    )\n    df_byhour = df.groupby('hour')\n\n    results = pd.concat([\n        df_byhour['duration_abs'].count(),\n        df_byhour['duration_abs'].mean()\n    ], axis=1, keys=['count', 'mean'])\n\n    return results.reindex(index=range(0, 24))\n\n\ndef parse(capmetrics_path=None, leglob=None, route_id=None, begin_lat=None, begin_lon=None, end_lat=None, end_lon=None, name=None):\n    df_total = pd.DataFrame()\n\n    data_glob = os.path.join(capmetrics_path, 'data', 'vehicle_positions', leglob)\n    files = glob.glob(data_glob)\n    for i, fname in enumerate(files):\n        print('({}/{}) Ingesting {}'.format(i + 1, len(files), fname))\n        try:\n            df_ingested = ingest(fname, route_id, (begin_lat, begin_lon), (end_lat, end_lon))\n            df_duration = parse_duration(df_ingested)\n            df_total = pd.concat([df_total, df_duration])\n        except Exception as e:\n            print(e)\n            print('Skipping ', fname)\n\n    if df_total.empty:\n        print('No vehicle positions found')\n        return\n\n    return parse_duration_by_hour(df_duration)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=main.__doc__)\n    parser.add_argument('--capmetrics_path', help='Path to the capmetrics directory', required=True, type=str)\n    parser.add_argument('--glob', help='Glob of vehicle positions CSV files', required=True, type=str)\n    parser.add_argument('--name', help='Name of the output file', required=True, type=str)\n    parser.add_argument('--route_id', help='Route ID', required=True, type=int)\n    parser.add_argument('--begin_lat', help='Latitude of first stop', required=True, type=float)\n    parser.add_argument('--begin_lon', help='Longitude of first stop', required=True, type=float)\n    parser.add_argument('--end_lat', help='Latitude of second stop', required=True, type=float)\n    parser.add_argument('--end_lon', help='Longitude of second stop', required=True, type=float)\n    args = parser.parse_args()\n\n    results = parse(\n        capmetrics_path=args.capmetrics_path,\n        name=args.name,\n        leglob=args.glob,\n        route_id=args.route_id,\n        begin_lat=args.begin_lat,\n        begin_lon=args.begin_lon,\n        end_lat=args.end_lat,\n        end_lon=args.end_lon\n    )\n\n    output_filename = '{route_id}_{name}_{glob}'.format(route_id=args.route_id, glob=args.glob, name=args.name)\n    output_path_duration_by_hour = 'results/duration_by_hour/{}.csv'.format(output_filename)\n    results.to_csv(output_path_duration_by_hour, header=True, sep='\\t')\n    print('Saved duration by hour to {}'.format(output_path_duration_by_hour))\n\nif __name__ == '__main__':\n    main()\n"}
{"text": "\nfrom rhombus.views import *\n\n@roles( PUBLIC )\ndef index(request):\n    \"\"\" input tags gallery \"\"\"\n\n    static = False\n\n    html = div( div(h3('Input Gallery')))\n\n    eform = form( name='rhombus/gallery', method=POST,\n                action='')\n    eform.add(\n        fieldset(\n            input_hidden(name='rhombus-gallery_id', value='00'),\n            input_text('rhombus-gallery_text', 'Text Field', value='Text field value'),\n            input_text('rhombus-gallery_static', 'Static Text Field', value='Text field static',\n                static=True),\n            input_textarea('rhombus-gallery_textarea', 'Text Area', value='A text in text area'),\n            input_select('rhombus-gallery_select', 'Select', value=2,\n            \toptions = [ (1, 'Opt1'), (2, 'Opt2'), (3, 'Opt3') ]),\n            input_select('rhombus-gallery_select-multi', 'Select (Multiple)', value=[2,3],\n            \toptions = [ (1, 'Opt1'), (2, 'Opt2'), (3, 'Opt3') ], multiple=True),\n            input_select('rhombus-gallery_select2', 'Select2 (Multiple)', value=[2,4],\n            \toptions = [ (1, 'Opt1'), (2, 'Opt2'), (3, 'Opt3'), (4, 'Opt4'), (5, 'Opt5') ], multiple=True),\n            input_select('rhombus-gallery_select2-ajax', 'Select2 (AJAX)'),\n            checkboxes('rhombus-gallery_checkboxes', 'Check Boxes',\n            \tboxes = [ ('1', 'Box 1', True), ('2', 'Box 2', False), ('3', 'Box 3', True)]),\n            submit_bar(),\n        )\n    )\n\n    html.add( div(eform) )\n    code = '''\n$('#rhombus-gallery_select2').select2();\n\n$('#rhombus-gallery_select2-ajax').select2({\n  placeholder: 'Select from AJAX',\n  minimumInputLength: 3,\n  ajax: {\n            url: \"%s\",\n            dataType: 'json',\n            data: function(params) { return { q: params.term, g: \"@ROLES\" }; },\n            processResults: function(data, params) { return { results: data }; }\n        },\n});\n''' % request.route_url('rhombus.ek-lookup')\n\n    return render_to_response('rhombus:templates/generics/page.mako',\n            { 'content': str(html), 'code': code },\n            request = request )"}
{"text": "\"\"\"Basis for different optimization algorithms.\n\nOptimizer provides interface for creating the update rules for gradient based optimization.\nIt includes SGD, NAG, RMSProp, etc.\n\nCopyright 2015 Markus Oberweger, ICG,\nGraz University of Technology <oberweger@icg.tugraz.at>\n\nThis file is part of DeepPrior.\n\nDeepPrior is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nDeepPrior is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with DeepPrior.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"\n\nimport theano\nimport theano.tensor as T\nimport numpy\n\n__author__ = \"Markus Oberweger <oberweger@icg.tugraz.at>\"\n__copyright__ = \"Copyright 2015, ICG, Graz University of Technology, Austria\"\n__credits__ = [\"Paul Wohlhart\", \"Markus Oberweger\"]\n__license__ = \"GPL\"\n__version__ = \"1.0\"\n__maintainer__ = \"Markus Oberweger\"\n__email__ = \"oberweger@icg.tugraz.at\"\n__status__ = \"Development\"\n\n\nclass Optimizer(object):\n    \"\"\"\n    Class with different optimizers of the loss function\n    \"\"\"\n\n    def __init__(self, grads, params):\n        \"\"\"\n        Initialize object\n        :param grads: gradients of the loss function\n        :param params: model parameters that should be updated\n        \"\"\"\n        self.grads = grads\n        self.params = params\n        self.updates = []\n        self.shared = []\n\n        if len(grads) != len(params):\n            print \"Warning: Size of gradients ({}) does not fit size of parameters ({})!\".format(len(grads), len(params))\n\n    def ADAM(self, learning_rate=0.0002, beta1=0.9, beta2=0.999, epsilon=1e-8, gamma=1-1e-8):\n        \"\"\"\n        Adam update rule by Kingma and Ba, ICLR 2015, version 2 (with momentum decay).\n        :param learning_rate: alpha in the paper, the step size\n        :param beta1: exponential decay rate of the 1st moment estimate\n        :param beta2: exponential decay rate of the 2nd moment estimate\n        :param epsilon: small epsilon to prevent divide-by-0 errors\n        :param gamma: exponential increase rate of beta1\n        :return: updates\n        \"\"\"\n\n        t = theano.shared(numpy.cast[theano.config.floatX](1.0))  # timestep, for bias correction\n        beta1_t = beta1*gamma**(t-1.)  # decay the first moment running average coefficient\n\n        for param_i, grad_i in zip(self.params, self.grads):\n            mparam_i = theano.shared(numpy.zeros(param_i.get_value().shape, dtype=theano.config.floatX))  # 1st moment\n            self.shared.append(mparam_i)\n            vparam_i = theano.shared(numpy.zeros(param_i.get_value().shape, dtype=theano.config.floatX))  # 2nd moment\n            self.shared.append(vparam_i)\n\n            m = beta1_t * mparam_i + (1. - beta1_t) * grad_i  # new value for 1st moment estimate\n            v = beta2 * vparam_i + (1. - beta2) * T.sqr(grad_i)  # new value for 2nd moment estimate\n\n            m_unbiased = m / (1. - beta1**t)  # bias corrected 1st moment estimate\n            v_unbiased = v / (1. - beta2**t)  # bias corrected 2nd moment estimate\n            w = param_i - (learning_rate * m_unbiased) / (T.sqrt(v_unbiased) + epsilon)  # new parameter values\n\n            self.updates.append((mparam_i, m))\n            self.updates.append((vparam_i, v))\n            self.updates.append((param_i, w))\n        self.updates.append((t, t + 1.))\n\n        return self.updates\n\n    def RMSProp(self, learning_rate=0.01, decay=0.9, epsilon=1.0 / 100.):\n        \"\"\"\n        RMSProp of Tieleman et al.\n        :param learning_rate: learning rate\n        :param decay: decay rate of gradient history\n        :param epsilon: gradient clip\n        :return: update\n        \"\"\"\n\n        for param_i, grad_i in zip(self.params, self.grads):\n            # Accumulate gradient\n            msg = theano.shared(numpy.zeros(param_i.get_value().shape, dtype=theano.config.floatX))\n            self.shared.append(msg)\n            new_mean_squared_grad = (decay * msg + (1 - decay) * T.sqr(grad_i))\n\n            # Compute update\n            rms_grad_t = T.sqrt(new_mean_squared_grad)\n            rms_grad_t = T.maximum(rms_grad_t, epsilon)\n            delta_x_t = -learning_rate * grad_i / rms_grad_t\n\n            # Apply update\n            self.updates.append((param_i, param_i + delta_x_t))\n            self.updates.append((msg, new_mean_squared_grad))\n\n        return self.updates\n\n"}
{"text": "# -*- coding: utf-8 -*-\n# Generated by Django 1.9 on 2016-11-01 13:34\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('daiquiri_query', '0003_query_language_and_actual_query'),\n    ]\n\n    operations = [\n        migrations.AlterModelOptions(\n            name='queryjob',\n            options={'ordering': ('start_time',), 'permissions': (('view_queryjob', 'Can view QueryJob'),), 'verbose_name': 'QueryJob', 'verbose_name_plural': 'QueryJobs'},\n        ),\n        migrations.RenameField(\n            model_name='queryjob',\n            old_name='tablename',\n            new_name='table_name',\n        ),\n        migrations.AddField(\n            model_name='queryjob',\n            name='database_name',\n            field=models.CharField(default='daiquiri_user', max_length=256),\n            preserve_default=False,\n        ),\n        migrations.AlterField(\n            model_name='queryjob',\n            name='query_language',\n            field=models.CharField(choices=[('mysql', 'MySQL SQL'), ('adql', 'ADQL')], max_length=8),\n        ),\n    ]\n"}
{"text": "import os\nimport unittest\nfrom random import randint\n\nfrom bson import json_util\nfrom bson.objectid import ObjectId\nfrom foodbeazt import fapp\nfrom foodbeazt import initdb as fdb\nfrom foodbeazt.service.ProductService import ProductService\nfrom foodbeazt.service.StoreService import StoreService\n\n\nclass CreateOrderTestCase(unittest.TestCase):\n    \"\"\"\n    Testing order api\n    \"\"\"\n    def setUp(self):\n        fapp.app.config['TESTING'] = True\n        fapp.app.config['MONGO_AUTO_START_REQUEST'] = False\n        self.dbname = 'testFoodbeaztDb'\n        fapp.app.config['MONGO_DBNAME'] = self.dbname\n        self.app = fapp.app.test_client()\n        with fapp.app.app_context():\n            fdb.drop_db(dbname=self.dbname)\n            fdb.setup(dbname=self.dbname, sample_data=True, debug=False)\n            self.price_table_item, self.no_discount_item, self.discount_item = fdb.setup_test_product(\n                dbname=self.dbname)\n\n    def tearDown(self):\n        with fapp.app.app_context():\n            fapp.mongo.cx.drop_database(self.dbname)\n            fapp.mongo.cx.close()\n\n    def test_get_invalid_order(self):\n        \"\"\"Test invalid order fetch\"\"\"\n        result = self.app.get('/api/order/' + str(ObjectId()))\n        self.assertEqual(result.status_code, 404)\n\n    def test_get_order(self):\n        \"\"\"Test valid order fetch\"\"\"\n        order_id = self.test_create_order()\n        result = self.app.get('/api/order/' + str(order_id))\n        self.assertEqual(result.status_code, 200)\n        order = json_util.loads(result.data.decode('utf-8'))\n        self.assertEqual(order.get('status'), 'PENDING')\n        self.assertEqual(order.get('payment_type'), 'cod')\n        self.assertEqual(order.get('total'), 90.0 + 333.0 + (100.0-(100.0*3.3/100.0)) + 40.0)\n\n    def test_create_order(self):\n        \"\"\"Test create order\"\"\"\n        hdrs = {'Content-Type': 'application/json'}\n        request_data = json_util.dumps(self._get_order_data())\n        result = self.app.post(\n            '/api/order/-1', data=request_data, headers=hdrs)\n        if result.status_code != 200:\n            print(result.data)\n        self.assertEqual(result.status_code, 200)\n        res = json_util.loads(result.data.decode('utf-8'))\n        self.assertEqual(res.get('status'), 'success')\n        order = res.get('data')\n        self.assertEqual(order.get('delivery_charges'), 40)\n        self.assertEqual(order.get('total'), 90.0 + 333.0 + (100.0-(100.0*3.3/100.0)) + 40.0)\n        self.assertEqual(order.get('payment_status'), 'success')\n        self.assertEqual(order.get('status'), 'PENDING')\n        self.assertEqual(len(order.get('store_delivery_status')), 1)\n        for s, v in order.get('store_delivery_status').items():\n            self.assertEqual(v.get('status'), 'PENDING')\n            self.assertIsNotNone(v.get('sid'))\n            self.assertIsNotNone(v.get('notified_at'))\n        return order.get('_id')\n\n    def _get_order_data(self):\n        item1 = {\n            'name': 'test item 1',\n            'quantity': 1.0,\n            'product_id': self.price_table_item,\n            'price_detail': {'no': 1}\n        }\n        item2 = {\n            'name': 'test item 2',\n            'quantity': 1.0,\n            'product_id': self.no_discount_item,\n        }\n        item3 = {\n            'name': 'test item 3',\n            'quantity': 1.0,\n            'product_id': self.discount_item,\n        }\n        data = {\n            'items': [item1, item2, item3],\n            'delivery_details': {\n                'name': 'some hungry fellow',\n                'email': 'cackharot@gmail.com',\n                'phone': str(randint(9000000000, 9999999999)),\n                'pincode': '605001',\n                'address': 'some dude address'\n            },\n            'payment_type': 'cod'\n        }\n        return data\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"text": "# =============================================================================\n#  Author:          Teerapat Jenrungrot - https://github.com/mjenrungrot/\n#  FileName:        12394.py\n#  Description:     UVa Online Judge - 12394\n# =============================================================================\n\nwhile True:\n    K, N = list(map(int, input().split()))\n    if K == 0 and N == 0: break\n\n    data = []\n    for i in range(N):\n        tmp = input().split()\n        name = tmp[0]\n        papers = list(map(int, tmp[1:]))\n        data.append((name, papers))\n\n    checked = [True for i in range(N)]\n\n    # Check number of reviews\n    n_reviews = [0 for i in range(N)]\n    for i in range(N):\n        for j in range(K):\n            n_reviews[data[i][1][j] - 1] += 1\n\n    for i in range(N):\n        if n_reviews[i] != K:\n            checked[i] = False\n\n    # Check collaborator\n    for i in range(N):\n        for j in range(K):\n            if data[i][0] == data[data[i][1][j] - 1][0]:\n                checked[data[i][1][j] - 1] = False\n\n    # Check same paper\n    for i in range(N):\n        for j in range(K):\n            for k in range(j+1, K):\n                if data[i][1][j] == data[i][1][k]:\n                    checked[data[i][1][j] - 1] = False\n\n\n    ans = 0\n    for i in range(N):\n        if not checked[i]:\n            ans += 1\n\n    if ans == 0: print(\"NO PROBLEMS FOUND\")\n    elif ans == 1: print(\"1 PROBLEM FOUND\")\n    else: print(\"{} PROBLEMS FOUND\".format(ans))\n"}
{"text": "from ast import literal_eval\n\n\nclass Error(Exception):\n    def __init__(self, message):\n        self.message = message\n\n\nclass CLI(object):\n\n    def __init__(self):\n        self.transformations = []\n        self.configurations = []\n        self.controls = []\n        self.loop = []\n        self.capture_loop = False\n        self.polygon = None\n        self.origin = None\n\n    def add_transformation(self, func):\n        self.transformations.append(func)\n        return func\n\n    def add_configuration(self, func):\n        # The boolean is so we can track if the command has been ran\n        self.configurations.append([func,False])\n        return func\n\n    def add_control(self, func):\n        self.controls.append(func)\n        return func\n\n    @staticmethod\n    def _parse(text):\n        if text[0] == \"#\":\n            return None\n\n        try:\n            command, arguments = text.split(\">\")\n        except ValueError:\n            if text == \"\":\n                raise Error(\"No Command\")\n\n        arguments = arguments.split(\", \")\n        parsed_arguments = {}\n        for i, arg in enumerate(arguments):\n            parsed_arguments.update({i:literal_eval(arg)})\n\n        parsed = [command, parsed_arguments]\n        return parsed\n\n    def run_command(self, func, arguments):\n        try:\n            arguments.update({\"polygon\": self.polygon, \"origin\": self.origin})\n            result = func(arguments)\n\n        except TypeError:\n            if self.polygon is None:\n                raise Error(\"Define a polygon with 'polygon>'\")\n            raise\n        return result\n\n    def step(self, line):\n        try:\n            command, arguments = self._parse(line)\n\n        except TypeError:\n            return \"comment\"\n\n        try:\n            for func in self.controls:\n                if func.__name__ == command:\n                    return \"controls\"\n\n            if self.capture_loop:\n                        self.loop.append(command)\n\n            for func in self.transformations:\n                # Check for transformation commands\n                if func.__name__ == command:\n                    result = self.run_command(func, arguments)\n                    self.polygon = result\n                    # make movements relative\n                    self.origin = self.polygon[0]\n                    return result\n\n            for func in self.configurations:\n                # Check for configuration commands\n                if func[0].__name__ == command:\n                    if func[1]:\n                        # Config commands should only be used once, so raise an error if they try again\n                        raise Error(\"Configuration command already ran\")\n\n                    func[1] = True\n                    self.run_command(func[0], arguments)\n                    return \"config\"  # return \"config\" so we know to delete it\n\n            raise Error(\"Not a Command\")\n\n        except KeyboardInterrupt:\n            print(\"\\n You can exit using 'leave>()'\")\n\n        except:\n            raise\n"}
{"text": "# -*- coding: utf-8 -*-\n# Part of Odoo. See LICENSE file for full copyright and licensing details.\n\nfrom odoo import _, api, fields, models\n\n\nclass FleetVehicleModel(models.Model):\n    _name = 'fleet.vehicle.model'\n    _description = 'Model of a vehicle'\n    _order = 'name asc'\n\n    name = fields.Char('Model name', required=True)\n    brand_id = fields.Many2one('fleet.vehicle.model.brand', 'Manufacturer', required=True, help='Manufacturer of the vehicle')\n    category_id = fields.Many2one('fleet.vehicle.model.category', 'Category')\n    vendors = fields.Many2many('res.partner', 'fleet_vehicle_model_vendors', 'model_id', 'partner_id', string='Vendors')\n    image_128 = fields.Image(related='brand_id.image_128', readonly=True)\n    active = fields.Boolean(default=True)\n    vehicle_type = fields.Selection([('car', 'Car'), ('bike', 'Bike')], default='car', required=True)\n    transmission = fields.Selection([('manual', 'Manual'), ('automatic', 'Automatic')], 'Transmission', help='Transmission Used by the vehicle')\n    vehicle_count = fields.Integer(compute='_compute_vehicle_count')\n\n    @api.depends('name', 'brand_id')\n    def name_get(self):\n        res = []\n        for record in self:\n            name = record.name\n            if record.brand_id.name:\n                name = record.brand_id.name + '/' + name\n            res.append((record.id, name))\n        return res\n\n    def _compute_vehicle_count(self):\n        group = self.env['fleet.vehicle'].read_group(\n            [('model_id', 'in', self.ids)], ['id', 'model_id'], groupby='model_id', lazy=False,\n        )\n        count_by_model = {entry['model_id'][0]: entry['__count'] for entry in group}\n        for model in self:\n            model.vehicle_count = count_by_model.get(model.id, 0)\n\n    def action_model_vehicle(self):\n        self.ensure_one()\n        view = {\n            'type': 'ir.actions.act_window',\n            'view_mode': 'kanban,tree,form',\n            'res_model': 'fleet.vehicle',\n            'name': _('Vehicles'),\n            'context': {'search_default_model_id': self.id, 'default_model_id': self.id}\n        }\n\n        return view\n"}
{"text": "# Copyright 2021 kubeflow.org\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport kfp.dsl as dsl\nfrom kfp_tekton.compiler import TektonCompiler\n\n\nclass Coder:\n    def empty(self):\n        return \"\"\n\n\nTektonCompiler._get_unique_id_code = Coder.empty\n\n\n@dsl.pipeline(name='withparam-global')\ndef pipeline(loopidy_doop: list = [3, 5, 7, 9]):\n    op0 = dsl.ContainerOp(\n        name=\"my-out-cop0\",\n        image='python:alpine3.6',\n        command=[\"sh\", \"-c\"],\n        arguments=[\n            'python -c \"import json; import sys; json.dump([i for i in range(20, 31)], open(\\'/tmp/out.json\\', \\'w\\'))\"'],\n        file_outputs={'out': '/tmp/out.json'},\n    )\n\n    with dsl.ParallelFor(loopidy_doop) as item:\n        op1 = dsl.ContainerOp(\n            name=\"my-in-cop1\",\n            image=\"library/bash:4.4.23\",\n            command=[\"sh\", \"-c\"],\n            arguments=[\"echo no output global op1, item: %s\" % item],\n        ).after(op0)\n\n    op_out = dsl.ContainerOp(\n        name=\"my-out-cop2\",\n        image=\"library/bash:4.4.23\",\n        command=[\"sh\", \"-c\"],\n        arguments=[\"echo no output global op2, outp: %s\" % op0.output],\n    ).after(op1)\n\n\nif __name__ == '__main__':\n    from kfp_tekton.compiler import TektonCompiler\n    TektonCompiler().compile(pipeline, __file__.replace('.py', '.yaml'))\n"}
{"text": "import sys\nimport re\n\n# Temporary solution for string/unicode in py2 vs py3\nif sys.version >= '3':\n    basestring = str\n\n\nclass TypedList(list):\n    \"\"\"Strongly typed list\n\n    All elements of the list must be one of the given types.\n\n    Attributes:\n        init - initial values\n        types - allowed types\n        null - can the list be null\n\n    Types may be either classes or strings. If types are strings then the value\n    of the field may be only a string matching one of the types.\n\n    examples:\n\n        TypedList([Identifier(), Identifier()], (Identifier, Literal))\n        TypedList([], Expression, null=True)\n        ast.field([\"+\",\"-\",\"+\"], (\"+\",\"-\",\"=\"))\n    \"\"\"\n    _type = 'class'  # class | str | pattern\n\n    def __init__(self, types, init=None, null=False):\n        super(TypedList, self).__init__()\n        if isinstance(types, basestring) or not hasattr(types, '__iter__'):\n            self._types = (types,)\n        else:\n            self._types = types\n        tset = set([type(t) for t in self._types])\n        \n        self._null = null\n        if len(tset) == 1:\n            tset = tset.pop()\n            self.__enforceType = self.__selectEnforcementMethod(tset)\n        else:\n            self.__enforceType = self.__enforceTypeMixed\n        if init:\n            self.extend(init)\n        elif null is False:\n            raise TypeError(\"This list must not be empty\")\n\n    def __repr__(self, template=None):\n        #fields = self._fields\n        #if len(field) >= len(list_template):\n        #    list_template += [getfillvalue(self, i)] * (len(field)-len(list_template)+1)\n        #    fields[i] = ''.join(['%s%s' % x for x in zip_longest(\n        #        list_template,\n        #        map(stringify, field),\n        #        fillvalue=''\n        #    )])\n        #else:\n        #    fields[i] = ', '.join(map(stringify, field))\n        #return self._template % fields\n        if template is None:\n            return list.__repr__(self)\n        else:\n            s = template()\n        return s\n\n    def __selectEnforcementMethod(self, t):\n        if issubclass(t, (basestring, int)):\n            return self.__enforceTypeStrInt\n        elif t is re._pattern_type:\n            return self.__enforceTypePattern\n        elif isinstance(t, type):\n            return self.__enforceTypeClass\n\n    def __enforceTypeMixed(self, items):\n        res = []\n        for item in items:\n            et = self.__selectEnforcementMethod(type(item))\n            res.append(et((item,)))\n        if all(res):\n            return\n        raise TypeError('This list accepts only elements: %s' %\n                        ', '.join([str(t) for t in self._types]))\n\n    def __enforceTypeStrInt(self, items):\n        if all(i in self._types for i in items):\n            return True\n        raise TypeError('This list accepts only elements: %s' %\n                        ', '.join([str(t) for t in self._types]))\n\n    def __enforceTypeClass(self, items):\n        if all(isinstance(i, self._types) for i in items):\n            return True\n        raise TypeError('This list accepts only elements: %s' %\n                        ', '.join([str(t) for t in self._types]))\n\n    def __enforceTypePattern(self, items):\n        if all(any(j.match(i) for j in self._types) for i in items):\n            return True\n        raise TypeError('This list accepts only elements: %s' %\n                        ', '.join([str(t) for t in self._types]))\n\n    def append(self, item):\n        self.__enforceType((item,))\n        return super(TypedList, self).append(item)\n\n    def insert(self, pos, item):\n        self.__enforceType((item,))\n        return super(TypedList, self).insert(pos, item)\n\n    def extend(self, items):\n        self.__enforceType(items)\n        return super(TypedList, self).extend(items)\n\n    def pop(self, key=-1):\n        if self._null is False and len(self) == 1:\n            raise TypeError(\"This list must not be empty\")\n        return super(TypedList, self).pop(key)\n\n    def __delitem__(self, k):\n        if self._null is False:\n            if type(k) is slice:\n                absslice = k.indices(len(self))\n                if absslice[1] - absslice[0] >= len(self):\n                    raise TypeError(\"This list must not be empty\")\n            elif len(self) == 1:\n                raise TypeError(\"This list must not be empty\")\n        return list.__delitem__(self, k)\n\n    def __setitem__(self, key, value):\n        self.__enforceType(value if hasattr(value, '__iter__') else (value,))\n        return list.__setitem__(self, key, value)\n\n    def __setslice__(self, i, j, sequence):\n        self.__enforceType(sequence)\n        return list.__setslice__(self, i, j, sequence)\n\n    def __delslice__(self, i, j):\n        absslice = slice(i, j).indices(len(self))\n        if self._null is False and absslice[1] - absslice[0] >= len(self):\n            raise TypeError(\"This list must not be empty\")\n        return list.__delslice__(self, i, j)\n"}
{"text": "# -*- coding: utf-8 -*-\n\n\"\"\"\nClase C{AndOperatorNode} del \u00e1rbol de sint\u00e1xis abstracta.\n\"\"\"\n\nfrom pytiger2c.ast.binarylogicaloperatornode import BinaryLogicalOperatorNode\nfrom pytiger2c.types.integertype import IntegerType\n\n\nclass AndOperatorNode(BinaryLogicalOperatorNode):\n    \"\"\"\n    Clase C{AndOperatorNode} del \u00e1rbol de sint\u00e1xis abstracta.\n    \n    Representa la operaci\u00f3n l\u00f3gica C{AND}, representada con el operador C{&}\n    en Tiger, entre dos n\u00fameros enteros. Este operador retornar\u00e1 1 en caso\n    de que el resultado de evaluar la expresi\u00f3n sea verdadero, 0 en otro caso.\n    \"\"\"\n    \n    def __init__(self, left, right):\n        \"\"\"\n        Inicializa la clase C{AndOperatorNode}.\n        \n        Para obtener informaci\u00f3n acerca de los par\u00e1metros recibidos por\n        este m\u00e9todo consulte la documentaci\u00f3n del m\u00e9todo C{__init__}\n        en la clase C{BinaryOperatorNode}.        \n        \"\"\"\n        super(AndOperatorNode, self).__init__(left, right)\n\n    def generate_code(self, generator):\n        \"\"\"\n        Genera el c\u00f3digo C correspondiente a la estructura del lenguaje Tiger\n        representada por el nodo.\n\n        @type generator: C{CodeGenerator}\n        @param generator: Clase auxiliar utilizada en la generaci\u00f3n del \n            c\u00f3digo C correspondiente a un programa Tiger.        \n        \n        @raise CodeGenerationError: Esta excepci\u00f3n se lanzar\u00e1 cuando se produzca\n            alg\u00fan error durante la generaci\u00f3n del c\u00f3digo correspondiente al nodo.\n            La excepci\u00f3n contendr\u00e1 informaci\u00f3n acerca del error.\n        \"\"\"\n        self.scope.generate_code(generator)\n        result_var = generator.define_local(IntegerType().code_type)\n        self.left.generate_code(generator)\n        generator.add_statement('if (!{left}) {{'.format(left=self.left.code_name))\n        generator.add_statement('{result} = 0; }}'.format(result=result_var))\n        generator.add_statement('else {')\n        self.right.generate_code(generator)\n        generator.add_statement('if ({right}) {{'.format(right=self.right.code_name))\n        generator.add_statement('{result} = 1; }}'.format(result=result_var))\n        generator.add_statement('else {')\n        generator.add_statement('{result} = 0; }}'.format(result=result_var))\n        generator.add_statement('}')\n        self._code_name = result_var\n"}
{"text": "import mahotas\nimport scipy.ndimage\nimport scipy.misc\nimport numpy as np\nimport gzip\nimport cPickle\nimport glob\nimport os\nimport h5py\nimport partition_comparison\n\n#param_path = 'D:/dev/Rhoana/membrane_cnn/results/good3/'\nparam_path = 'D:/dev/Rhoana/membrane_cnn/results/stumpin/'\nparam_files = glob.glob(param_path + \"*.h5\")\n\ntarget_boundaries = mahotas.imread(param_path + 'boundaries.png') > 0\n\noffset_max = 32\n\ntarget_boundaries = target_boundaries[offset_max:-offset_max,offset_max:-offset_max]\ntarget_segs = np.uint32(mahotas.label(target_boundaries)[0])\n\nparam_files = [x for x in param_files if x.find('.ot.h5') == -1]\n\naverage_result = np.zeros(target_boundaries.shape, dtype=np.float32)\nnresults = 0\n\nblur_radius = 3;\ny,x = np.ogrid[-blur_radius:blur_radius+1, -blur_radius:blur_radius+1]\ndisc = x*x + y*y <= blur_radius*blur_radius\n\nfor param_file in param_files:\n\n    if param_file.find('.ot.h5') != -1:\n        continue\n\n    print param_file\n\n    #net_output_file = param_file.replace('.h5','\\\\0005_classify_output_layer6_0.tif')\n    net_output_file = param_file.replace('.h5','\\\\0100_classify_output_layer6_0.tif')\n    net_output = mahotas.imread(net_output_file)\n    net_output = np.float32(net_output) / np.max(net_output)\n\n    offset_file = param_file.replace('.h5', '.sm.ot.h5')\n    h5off = h5py.File(offset_file, 'r')\n    best_offset = h5off['/best_offset'][...]\n    best_sigma = h5off['/best_sigma'][...]\n    h5off.close()\n\n    xoffset, yoffset = best_offset\n\n    offset_output = scipy.ndimage.filters.gaussian_filter(net_output, float(best_sigma))\n\n    offset_output = np.roll(offset_output, xoffset, axis=0)\n    offset_output = np.roll(offset_output, yoffset, axis=1)\n\n    #Crop\n    offset_output = offset_output[offset_max:-offset_max,offset_max:-offset_max]\n\n    average_result += offset_output\n\n    nresults += 1\n\naverage_result = average_result / nresults\n\nsigma_range = arange(0, 3, 0.5)\nthresh_range = arange(0.05,0.7,0.02)\n\nsigma_range = [0]\n#thresh_range = [0.3]\n\nall_voi_results = []\n\nfor smooth_sigma in sigma_range:\n\n    best_score = Inf\n    best_sigma = 0\n    best_thresh = 0\n    best_result = None\n\n    smooth_output = scipy.ndimage.filters.gaussian_filter(average_result, smooth_sigma)\n    max_smooth = 2 ** 16 - 1\n    smooth_output = np.uint16((1 - smooth_output) * max_smooth)\n\n    thresh_voi_results = []\n\n    for thresh in thresh_range:\n\n        below_thresh = smooth_output < np.uint16(max_smooth * thresh)\n\n        #below_thresh = mahotas.morph.close(below_thresh.astype(np.bool), disc)\n        #below_thresh = mahotas.morph.open(below_thresh.astype(np.bool), disc)\n\n        seeds,nseeds = mahotas.label(below_thresh)\n\n        if nseeds == 0:\n            continue\n\n        ws = np.uint32(mahotas.cwatershed(smooth_output, seeds))\n\n        voi_score = partition_comparison.variation_of_information(target_segs.ravel(), ws.ravel())\n\n        thresh_voi_results.append(voi_score)\n\n        print 's={0:0.2f}, t={1:0.2f}, voi_score={2:0.4f}.'.format(smooth_sigma, thresh, voi_score)\n\n        dx, dy = np.gradient(ws)\n        result = np.logical_or(dx!=0, dy!=0)\n\n        figsize(20,20)\n        imshow(result, cmap=cm.gray)\n        plt.show()\n\n        if voi_score < best_score:\n            best_score = voi_score\n            best_sigma = smooth_sigma\n            best_thresh = thresh\n\n            dx, dy = np.gradient(ws)\n            best_result = np.logical_or(dx!=0, dy!=0)\n\n    all_voi_results.append(thresh_voi_results)\n\n    # figsize(20,20)\n    # imshow(best_result, cmap=cm.gray)\n    # plt.show()\n\n    print 'Best VoI score of {0} with {3} segments for sigma {1}, thresh {2}.'.format(best_score, best_sigma, best_thresh, nseeds)\n\nplot_list = []\nfor voi_results in all_voi_results:\n    handle = plot(thresh_range, voi_results)[0]\n    plot_list.append(handle)\nxlabel('Threshold')\nylabel('VoI Score')\nlegend(plot_list, [str(x) for x in sigma_range])\nplt.show\n\nfigsize(20,20);\nimshow(average_result,cmap=cm.gray)\n\n# figsize(20,20);\n# imshow(best_result,cmap=cm.gray)\n\n\n"}
{"text": "# This script joins mutation of all patients to one file\n# Use the following arguments:\n# [1] - regular expression for variants files\n# [2] - name for resultFile\n## v4 - added argparse\n## v5 - join mutations of avinput (annovar input)\n\nimport glob\nimport sys\nimport argparse\nimport re\n\ndef showPercWork(done,allWork):\n    import sys\n    percDoneWork=round((done/allWork)*100,1)\n    sys.stdout.write(\"\\r\"+str(percDoneWork)+\"%\")\n    sys.stdout.flush()\n\n# Readign arguments\npar=argparse.ArgumentParser(description='This script joins mutation of all patients to one file')\npar.add_argument('--varFiles','-v',dest='varFiles',type=str,help='regular expression for choosing files with variations',required=True)\npar.add_argument('--outFile','-o',dest='outFile',type=str,help='directory for output',required=True)\nargs=par.parse_args()\n\nds=glob.glob(args.varFiles)\nif len(ds)==0:\n    print('ERROR: No files were selected! Maybe you write \"~/\" but you should use /home/USERNAME/')\n    print(args.varFiles)\n    exit(0)\nelif len(ds)==1:\n    print('WARNING: Only one file was selected!')\n    print(args.varFiles)\n\ntitlesCheck=False\nallWork=len(ds)\nshowPercWork(0,allWork)\nallData={}\nannp=re.compile('ANN\\=([^\\;]+)')\nfor i,d in enumerate(ds):\n    file=open(d)\n    dPart=d[d.rfind('/')+1:]\n    patName=dPart[:dPart.index('.')]\n    adi=1\n    for string in file:\n        if 'CHROM' in string:\n            if not titlesCheck:\n                titlesCheck=True\n            continue\n        cols=string[:-1].split('\\t')\n        # If there is no alternative alleles\n        ## or there is strand-bias, continue\n        if cols[12]=='.' or 'SB' in cols[14]:\n            continue\n        qual=cols[13]\n        adColNum=cols[16].split(':').index('AD')\n        dpColNum=cols[16].split(':').index('DP')\n        ads=cols[17].split(':')[adColNum].split(',')\n        dp=cols[17].split(':')[dpColNum]\n        adNum=len(ads)\n        # If we meet locus with several alleles first time\n        if adNum>2:\n            # Here we write whole coverage and alt coverage\n            ## But later we will calculate namelt reference coverage\n            ad=dp+','+ads[adi]\n            if adi<adNum-1:\n                adi+=1\n            elif adi==adNum-1:\n                adi=1\n        else:\n            # Here we write whole coverage and alt coverage\n            ## But later we will calculate namelt reference coverage\n            ad=','.join([dp,ads[1]])\n            adi=1\n        annm=annp.findall(cols[15])\n        ann=annm[0]\n        pos=cols[9]\n        ref=cols[11]\n        alt=cols[12]\n        key='\\t'.join(cols[:5])\n        if key not in allData.keys():\n            allData[key]=[patName,qual,ad,ann,pos,ref,alt]\n        else:\n            allData[key][0]+='|'+patName\n            allData[key][1]+='|'+qual\n            allData[key][2]+='|'+ad\n    file.close()\n    showPercWork(i+1,allWork)\nresultFile=open(args.outFile,'w')\nfor key,item in allData.items():\n    resultFile.write(key+'\\t'+'\\t'.join(item)+'\\n')\nresultFile.close()\nprint()\n        \n"}
{"text": "#The MIT License (MIT)\n#Copyright (c) 2014 Microsoft Corporation\n\n#Permission is hereby granted, free of charge, to any person obtaining a copy\n#of this software and associated documentation files (the \"Software\"), to deal\n#in the Software without restriction, including without limitation the rights\n#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n#copies of the Software, and to permit persons to whom the Software is\n#furnished to do so, subject to the following conditions:\n\n#The above copyright notice and this permission notice shall be included in all\n#copies or substantial portions of the Software.\n\n#THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n#SOFTWARE.\n\n\"\"\"Internal class for endpoint discovery retry policy implementation in the Azure Cosmos database service.\n\"\"\"\n\nimport logging\nfrom azure.cosmos.documents import _OperationType\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nlog_formatter = logging.Formatter('%(levelname)s:%(message)s')\nlog_handler = logging.StreamHandler()\nlog_handler.setFormatter(log_formatter)\nlogger.addHandler(log_handler)\n\n\nclass _EndpointDiscoveryRetryPolicy(object):\n    \"\"\"The endpoint discovery retry policy class used for geo-replicated database accounts\n       to handle the write forbidden exceptions due to writable/readable location changes\n       (say, after a failover).\n    \"\"\"\n\n    Max_retry_attempt_count = 120\n    Retry_after_in_milliseconds = 1000\n\n    def __init__(self, connection_policy, global_endpoint_manager, *args):\n        self.global_endpoint_manager = global_endpoint_manager\n        self._max_retry_attempt_count = _EndpointDiscoveryRetryPolicy.Max_retry_attempt_count\n        self.failover_retry_count = 0\n        self.retry_after_in_milliseconds = _EndpointDiscoveryRetryPolicy.Retry_after_in_milliseconds\n        self.connection_policy = connection_policy\n        self.request = args[0] if args else None\n        #clear previous location-based routing directive\n        if (self.request):\n            self.request.clear_route_to_location()\n\n            # Resolve the endpoint for the request and pin the resolution to the resolved endpoint\n            # This enables marking the endpoint unavailability on endpoint failover/unreachability\n            self.location_endpoint = self.global_endpoint_manager.resolve_service_endpoint(self.request)\n            self.request.route_to_location(self.location_endpoint)\n\n    def ShouldRetry(self, exception):\n        \"\"\"Returns true if should retry based on the passed-in exception.\n\n        :param (errors.HTTPFailure instance) exception: \n\n        :rtype:\n            boolean\n\n        \"\"\"\n        if not self.connection_policy.EnableEndpointDiscovery:\n            return False\n\n        if self.failover_retry_count >= self.Max_retry_attempt_count:\n            return False\n\n        self.failover_retry_count += 1\n\n        if self.location_endpoint:\n            if _OperationType.IsReadOnlyOperation(self.request.operation_type):\n                #Mark current read endpoint as unavailable\n                self.global_endpoint_manager.mark_endpoint_unavailable_for_read(self.location_endpoint)\n            else:\n                self.global_endpoint_manager.mark_endpoint_unavailable_for_write(self.location_endpoint)\n\n        # set the refresh_needed flag to ensure that endpoint list is\n        # refreshed with new writable and readable locations\n        self.global_endpoint_manager.refresh_needed = True\n        \n        # clear previous location-based routing directive\n        self.request.clear_route_to_location()\n\n        # set location-based routing directive based on retry count\n        # simulating single master writes by ensuring usePreferredLocations\n        # is set to false\n        self.request.route_to_location_with_preferred_location_flag(self.failover_retry_count, False)\n        \n        # Resolve the endpoint for the request and pin the resolution to the resolved endpoint\n        # This enables marking the endpoint unavailability on endpoint failover/unreachability\n        self.location_endpoint = self.global_endpoint_manager.resolve_service_endpoint(self.request)\n        self.request.route_to_location(self.location_endpoint)\n        return True\n"}
{"text": "# Copyright 2013 IBM Corp.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom webob import exc\n\nfrom nova.api.openstack import common\nfrom nova.api.openstack.compute.schemas.v3 import admin_password\nfrom nova.api.openstack import extensions\nfrom nova.api.openstack import wsgi\nfrom nova.api import validation\nfrom nova import compute\nfrom nova import exception\nfrom nova.openstack.common.gettextutils import _\n\n\nALIAS = \"os-admin-password\"\nauthorize = extensions.extension_authorizer('compute', 'v3:%s' % ALIAS)\n\n\nclass AdminPasswordController(wsgi.Controller):\n\n    def __init__(self, *args, **kwargs):\n        super(AdminPasswordController, self).__init__(*args, **kwargs)\n        self.compute_api = compute.API()\n\n    @wsgi.action('change_password')\n    @wsgi.response(204)\n    @extensions.expected_errors((400, 404, 409, 501))\n    @validation.schema(admin_password.change_password)\n    def change_password(self, req, id, body):\n        context = req.environ['nova.context']\n        authorize(context)\n\n        password = body['change_password']['admin_password']\n        try:\n            instance = self.compute_api.get(context, id, want_objects=True)\n        except exception.InstanceNotFound as e:\n            raise exc.HTTPNotFound(explanation=e.format_message())\n        try:\n            self.compute_api.set_admin_password(context, instance, password)\n        except exception.InstancePasswordSetFailed as e:\n            raise exc.HTTPConflict(explanation=e.format_message())\n        except exception.InstanceInvalidState as e:\n            raise common.raise_http_conflict_for_instance_invalid_state(\n                e, 'change_password')\n        except NotImplementedError:\n            msg = _(\"Unable to set password on instance\")\n            raise exc.HTTPNotImplemented(explanation=msg)\n\n\nclass AdminPassword(extensions.V3APIExtensionBase):\n    \"\"\"Admin password management support.\"\"\"\n\n    name = \"AdminPassword\"\n    alias = ALIAS\n    version = 1\n\n    def get_resources(self):\n        return []\n\n    def get_controller_extensions(self):\n        controller = AdminPasswordController()\n        extension = extensions.ControllerExtension(self, 'servers', controller)\n        return [extension]\n"}
{"text": "import asyncio\nimport asyncio.streams\n\nfrom .client_exceptions import (ClientOSError, ClientPayloadError,\n                                ClientResponseError, ServerDisconnectedError)\nfrom .http import HttpResponseParser, StreamWriter\nfrom .streams import EMPTY_PAYLOAD, DataQueue\n\n\nclass ResponseHandler(DataQueue, asyncio.streams.FlowControlMixin):\n    \"\"\"Helper class to adapt between Protocol and StreamReader.\"\"\"\n\n    def __init__(self, *, loop=None, **kwargs):\n        asyncio.streams.FlowControlMixin.__init__(self, loop=loop)\n        DataQueue.__init__(self, loop=loop)\n\n        self.paused = False\n        self.transport = None\n        self.writer = None\n        self._should_close = False\n\n        self._message = None\n        self._payload = None\n        self._payload_parser = None\n        self._reading_paused = False\n\n        self._timer = None\n        self._skip_status = ()\n\n        self._tail = b''\n        self._upgraded = False\n        self._parser = None\n\n    @property\n    def upgraded(self):\n        return self._upgraded\n\n    @property\n    def should_close(self):\n        if (self._payload is not None and\n                not self._payload.is_eof() or self._upgraded):\n            return True\n\n        return (self._should_close or self._upgraded or\n                self.exception() is not None or\n                self._payload_parser is not None or\n                len(self) or self._tail)\n\n    def close(self):\n        transport = self.transport\n        if transport is not None:\n            transport.close()\n            self.transport = None\n        return transport\n\n    def is_connected(self):\n        return self.transport is not None\n\n    def connection_made(self, transport):\n        self.transport = transport\n        self.writer = StreamWriter(self, transport, self._loop)\n\n    def connection_lost(self, exc):\n        if self._payload_parser is not None:\n            try:\n                self._payload_parser.feed_eof()\n            except Exception:\n                pass\n\n        try:\n            self._parser.feed_eof()\n        except Exception as e:\n            if self._payload is not None:\n                self._payload.set_exception(\n                    ClientPayloadError('Response payload is not completed'))\n\n        if not self.is_eof():\n            if isinstance(exc, OSError):\n                exc = ClientOSError(*exc.args)\n            if exc is None:\n                exc = ServerDisconnectedError()\n            DataQueue.set_exception(self, exc)\n\n        self.transport = self.writer = None\n        self._should_close = True\n        self._parser = None\n        self._message = None\n        self._payload = None\n        self._payload_parser = None\n        self._reading_paused = False\n\n        super().connection_lost(exc)\n\n    def eof_received(self):\n        pass\n\n    def pause_reading(self):\n        if not self._reading_paused:\n            try:\n                self.transport.pause_reading()\n            except (AttributeError, NotImplementedError, RuntimeError):\n                pass\n            self._reading_paused = True\n\n    def resume_reading(self):\n        if self._reading_paused:\n            try:\n                self.transport.resume_reading()\n            except (AttributeError, NotImplementedError, RuntimeError):\n                pass\n            self._reading_paused = False\n\n    def set_exception(self, exc):\n        self._should_close = True\n\n        super().set_exception(exc)\n\n    def set_parser(self, parser, payload):\n        self._payload = payload\n        self._payload_parser = parser\n\n        if self._tail:\n            data, self._tail = self._tail, None\n            self.data_received(data)\n\n    def set_response_params(self, *, timer=None,\n                            skip_payload=False,\n                            skip_status_codes=(),\n                            read_until_eof=False):\n        self._skip_payload = skip_payload\n        self._skip_status_codes = skip_status_codes\n        self._read_until_eof = read_until_eof\n        self._parser = HttpResponseParser(\n            self, self._loop, timer=timer,\n            payload_exception=ClientPayloadError,\n            read_until_eof=read_until_eof)\n\n        if self._tail:\n            data, self._tail = self._tail, b''\n            self.data_received(data)\n\n    def data_received(self, data):\n        if not data:\n            return\n\n        # custom payload parser\n        if self._payload_parser is not None:\n            eof, tail = self._payload_parser.feed_data(data)\n            if eof:\n                self._payload = None\n                self._payload_parser = None\n\n                if tail:\n                    self.data_received(tail)\n            return\n        else:\n            if self._upgraded or self._parser is None:\n                # i.e. websocket connection, websocket parser is not set yet\n                self._tail += data\n            else:\n                # parse http messages\n                try:\n                    messages, upgraded, tail = self._parser.feed_data(data)\n                except BaseException as exc:\n                    import traceback\n                    traceback.print_exc()\n                    self._should_close = True\n                    self.set_exception(\n                        ClientResponseError(code=400, message=str(exc)))\n                    self.transport.close()\n                    return\n\n                self._upgraded = upgraded\n\n                for message, payload in messages:\n                    if message.should_close:\n                        self._should_close = True\n\n                    self._message = message\n                    self._payload = payload\n\n                    if (self._skip_payload or\n                            message.code in self._skip_status_codes):\n                        self.feed_data((message, EMPTY_PAYLOAD), 0)\n                    else:\n                        self.feed_data((message, payload), 0)\n\n                if upgraded:\n                    self.data_received(tail)\n                else:\n                    self._tail = tail\n"}
{"text": "'''\nTo test our new `ti.field` API is functional (#1500)\n'''\n\nimport pytest\n\nimport taichi as ti\n\ndata_types = [ti.i32, ti.f32, ti.i64, ti.f64]\nfield_shapes = [(), 8, (6, 12)]\nvector_dims = [3]\nmatrix_dims = [(1, 2), (2, 3)]\n\n\n@pytest.mark.parametrize('dtype', data_types)\n@pytest.mark.parametrize('shape', field_shapes)\n@ti.host_arch_only\ndef test_scalar_field(dtype, shape):\n    x = ti.field(dtype, shape)\n\n    if isinstance(shape, tuple):\n        assert x.shape == shape\n    else:\n        assert x.shape == (shape, )\n\n    assert x.dtype == dtype\n\n\n@pytest.mark.parametrize('n', vector_dims)\n@pytest.mark.parametrize('dtype', data_types)\n@pytest.mark.parametrize('shape', field_shapes)\n@ti.host_arch_only\ndef test_vector_field(n, dtype, shape):\n    x = ti.Vector.field(n, dtype, shape)\n\n    if isinstance(shape, tuple):\n        assert x.shape == shape\n    else:\n        assert x.shape == (shape, )\n\n    assert x.dtype == dtype\n    assert x.n == n\n    assert x.m == 1\n\n\n@pytest.mark.parametrize('n,m', matrix_dims)\n@pytest.mark.parametrize('dtype', data_types)\n@pytest.mark.parametrize('shape', field_shapes)\n@ti.host_arch_only\ndef test_matrix_field(n, m, dtype, shape):\n    x = ti.Matrix.field(n, m, dtype=dtype, shape=shape)\n\n    if isinstance(shape, tuple):\n        assert x.shape == shape\n    else:\n        assert x.shape == (shape, )\n\n    assert x.dtype == dtype\n    assert x.n == n\n    assert x.m == m\n\n\n@ti.host_arch_only\ndef test_field_needs_grad():\n    # Just make sure the usage doesn't crash, see #1545\n    n = 8\n    m1 = ti.field(dtype=ti.f32, shape=n, needs_grad=True)\n    m2 = ti.field(dtype=ti.f32, shape=n, needs_grad=True)\n    gr = ti.field(dtype=ti.f32, shape=n)\n\n    @ti.kernel\n    def func():\n        for i in range(n):\n            gr[i] = m1.grad[i] + m2.grad[i]\n\n    func()\n\n\n@pytest.mark.parametrize('dtype', [ti.f32, ti.f64])\ndef test_default_fp(dtype):\n    ti.init(default_fp=dtype)\n\n    x = ti.Vector.field(2, float, ())\n\n    assert x.dtype == ti.get_runtime().default_fp\n\n\n@pytest.mark.parametrize('dtype', [ti.i32, ti.i64])\ndef test_default_ip(dtype):\n    ti.init(default_ip=dtype)\n\n    x = ti.Vector.field(2, int, ())\n\n    assert x.dtype == ti.get_runtime().default_ip\n"}
{"text": "# -*- coding: utf-8 -*-\n# This file is part of Shuup.\n#\n# Copyright (c) 2012-2016, Shoop Commerce Ltd. All rights reserved.\n#\n# This source code is licensed under the AGPLv3 license found in the\n# LICENSE file in the root directory of this source tree.\nimport pytest\nfrom django.test import override_settings\n\nfrom shuup.core.models import get_person_contact\nfrom shuup.core.utils.forms import MutableAddressForm\nfrom shuup.front.checkout.addresses import AddressesPhase\nfrom shuup.testing.factories import get_default_shop\nfrom shuup.testing.utils import apply_request_middleware\n\n\ndef test_checkout_addresses_has_no_default_country():\n    form =MutableAddressForm()\n    assert form.fields[\"country\"].initial is None\n\n\n@override_settings(SHUUP_ADDRESS_HOME_COUNTRY=\"FI\")\ndef test_checkout_addresses_has_default_country():\n    form = MutableAddressForm()\n    assert form.fields[\"country\"].initial == \"FI\"\n\n\ndef test_required_address_fields():\n    with override_settings(SHUUP_ADDRESS_FIELD_PROPERTIES={}):\n        form = MutableAddressForm()\n        assert form.fields[\"email\"].required == False\n        assert form.fields[\"email\"].help_text != \"Enter email\"\n        assert form.fields[\"phone\"].help_text != \"Enter phone\"\n\n    with override_settings(\n        SHUUP_ADDRESS_FIELD_PROPERTIES={\n            \"email\": {\"required\": True, \"help_text\": \"Enter email\"},\n            \"phone\": {\"help_text\": \"Enter phone\"}\n        }\n    ):\n        form = MutableAddressForm()\n        assert form.fields[\"email\"].required == True\n        assert form.fields[\"email\"].help_text == \"Enter email\"\n        assert form.fields[\"phone\"].help_text == \"Enter phone\"\n\n\n@pytest.mark.django_db\ndef test_address_phase_authorized_user(rf, admin_user):\n    request = apply_request_middleware(rf.get(\"/\"), shop=get_default_shop(), customer=get_person_contact(admin_user))\n    view_func = AddressesPhase.as_view()\n    resp = view_func(request)\n    assert 'company' not in resp.context_data['form'].form_defs\n\n\n@pytest.mark.django_db\ndef test_address_phase_anonymous_user(rf):\n    request = apply_request_middleware(rf.get(\"/\"), shop=get_default_shop())\n    view_func = AddressesPhase.as_view()\n    resp = view_func(request)\n    assert 'company' in resp.context_data['form'].form_defs\n"}
{"text": "#coding=utf-8\n\"\"\"\nFlask-WeRoBot\n---------------\n\nAdds WeRoBot support to Flask.\n\n:copyright: (c) 2013 by whtsky.\n:license: BSD, see LICENSE for more details.\n\nLinks\n`````\n\n* `documentation <https://flask-werobot.readthedocs.org/>`_\n\"\"\"\n\n__version__ = '0.1.2'\n\nfrom werobot.robot import BaseRoBot\nfrom flask import Flask\n\nclass WeRoBot(BaseRoBot):\n    \"\"\"\n    \u7ed9\u4f60\u7684 Flask \u5e94\u7528\u6dfb\u52a0 WeRoBot \u652f\u6301\u3002\n\n    \u4f60\u53ef\u4ee5\u5728\u5b9e\u4f8b\u5316 WeRoBot \u7684\u65f6\u5019\u4f20\u5165\u4e00\u4e2a Flask App \u6dfb\u52a0\u652f\u6301\uff1a ::\n\n        app = Flask(__name__)\n        robot = WeRoBot(app)\n\n    \u6216\u8005\u4e5f\u53ef\u4ee5\u5148\u5b9e\u4f8b\u5316\u4e00\u4e2a WeRoBot \uff0c\u7136\u540e\u901a\u8fc7 ``init_app`` \u6765\u7ed9\u5e94\u7528\u6dfb\u52a0\u652f\u6301 ::\n\n        robot = WeRoBot()\n\n        def create_app():\n            app = Flask(__name__)\n            robot.init_app(app)\n            return app\n    \n    \"\"\"\n    def __init__(self, app=None, endpoint='werobot', rule=None, *args, **kwargs):\n        super(WeRoBot, self).__init__(*args, **kwargs)\n        if app is not None:\n            self.init_app(app, endpoint=endpoint, rule=rule)\n        else:\n            self.app = None\n\n    def init_app(self, app, endpoint='werobot', rule=None):\n        \"\"\"\n        \u4e3a\u4e00\u4e2a\u5e94\u7528\u6dfb\u52a0 WeRoBot \u652f\u6301\u3002\n        \u5982\u679c\u4f60\u5728\u5b9e\u4f8b\u5316 ``WeRoBot`` \u7c7b\u7684\u65f6\u5019\u4f20\u5165\u4e86\u4e00\u4e2a Flask App \uff0c\u4f1a\u81ea\u52a8\u8c03\u7528\u672c\u65b9\u6cd5\uff1b\n        \u5426\u5219\u4f60\u9700\u8981\u624b\u52a8\u8c03\u7528 ``init_app`` \u6765\u4e3a\u5e94\u7528\u6dfb\u52a0\u652f\u6301\u3002\n        \u53ef\u4ee5\u901a\u8fc7\u591a\u6b21\u8c03\u7528 ``init_app`` \u5e76\u5206\u522b\u4f20\u5165\u4e0d\u540c\u7684 Flask App \u6765\u590d\u7528\u5fae\u4fe1\u673a\u5668\u4eba\u3002\n\n        :param app: \u4e00\u4e2a\u6807\u51c6\u7684 Flask App\u3002\n        :param endpoint: WeRoBot \u7684 Endpoint \u3002\u9ed8\u8ba4\u4e3a ``werobot`` \u3002\n            \u4f60\u53ef\u4ee5\u901a\u8fc7 url_for(endpoint) \u6765\u83b7\u53d6\u5230 WeRoBot \u7684\u5730\u5740\u3002\n            \u5982\u679c\u4f60\u60f3\u8981\u5728\u540c\u4e00\u4e2a\u5e94\u7528\u4e2d\u7ed1\u5b9a\u591a\u4e2a WeRoBot \u673a\u5668\u4eba\uff0c \u8bf7\u4f7f\u7528\u4e0d\u540c\u7684 endpoint .\n        :param rule:\n          WeRoBot \u673a\u5668\u4eba\u7684\u7ed1\u5b9a\u5730\u5740\u3002\u9ed8\u8ba4\u4e3a Flask App Config \u4e2d\u7684 ``WEROBOT_ROLE``\n        \"\"\"\n        assert isinstance(app, Flask)\n        from werobot.utils import check_token\n        from werobot.parser import parse_user_msg\n        from werobot.reply import create_reply\n\n        self.app = app\n        config = app.config\n        token = self.token\n        if token is None:\n            token = config.setdefault('WEROBOT_TOKEN', 'none')\n        if not check_token(token):\n            raise AttributeError('%s is not a vailed WeChat Token.' % token)\n        if rule is None:\n            rule = config.setdefault('WEROBOT_ROLE', '/wechat')\n\n        if not check_token(token):\n            raise AttributeError('%s is an invalid token.' % token)\n        self.token = token\n\n        from flask import request, make_response\n\n        def handler():\n            if not self.check_signature(\n                    request.args.get('timestamp', ''),\n                    request.args.get('nonce', ''),\n                    request.args.get('signature', '')\n            ):\n                return 'Invalid Request.'\n            if request.method == 'GET':\n                return request.args['echostr']\n\n            body = request.data\n            message = parse_user_msg(body)\n            reply = self.get_reply(message)\n            if not reply:\n                return ''\n            response = make_response(create_reply(reply, message=message))\n            response.headers['content_type'] = 'application/xml'\n            return response\n\n        app.add_url_rule(rule, endpoint=endpoint,\n                         view_func=handler, methods=['GET', 'POST'])\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom __future__ import print_function, division\n\nimport unittest\n\nimport pytest\n\nfrom ircelsos.util import print_stations, print_pollutants\nfrom ircelsos import metadata\n\n\ndef strip(s):\n    s = s.splitlines()\n    s = [line.strip() for line in s]\n    s = \"\\n\".join(s)\n    return s\n\n\n@pytest.mark.usefixtures(\"capsys\")\nclass TestTablePrinting():\n\n    def test_print_stations(self, capsys):\n\n        print_stations(['BETR801', 'BETR802'])\n        out, err = capsys.readouterr()\n        expected = \"\"\"name   | EU_code | location   | region | type\n-------+---------+------------+--------+--------\n42R801 | BETR801 | Borgerhout | urban  | Traffic\n42R802 | BETR802 | Borgerhout | urban  | Traffic\n\"\"\"\n        assert strip(out) == strip(expected)\n\n    def test_print_pollutants(self, capsys):\n\n        print_pollutants(['42602 - NO2', '44201 - O3'])\n        out, err = capsys.readouterr()\n        expected = \"\"\"id          | short | name             | stations\n------------+-------+------------------+---------\n42602 - NO2 | no2   | Nitrogen dioxide | 105\n44201 - O3  | o3    | Ozone            | 47\n\"\"\"\n        assert strip(out) == strip(expected)\n"}
{"text": "from __future__ import unicode_literals\n\nfrom django.conf.urls import patterns, url\n\nfrom zhiliao.conf import settings\n\n\nurlpatterns = []\n\nif \"django.contrib.admin\" in settings.INSTALLED_APPS:\n    reset_pattern = \"^reset/(?P<uidb64>[0-9A-Za-z_\\-]+)/(?P<token>.+)/$\"\n    urlpatterns += patterns(\"django.contrib.auth.views\",\n        url(\"^password_reset/$\", \"password_reset\", name=\"password_reset\"),\n        url(\"^password_reset/done/$\", \"password_reset_done\",\n            name=\"password_reset_done\"),\n        url(\"^reset/done/$\", \"password_reset_complete\",\n            name=\"password_reset_complete\"),\n        url(reset_pattern, \"password_reset_confirm\",\n            name=\"password_reset_confirm\"),\n    )\n\nurlpatterns += patterns(\"zhiliao.core.views\",\n    url(\"^edit/$\", \"edit\", name=\"edit\"),\n    url(\"^search/$\", \"search\", name=\"search\"),\n    url(\"^set_site/$\", \"set_site\", name=\"set_site\"),\n    url(\"^set_device/(?P<device>.*)/$\", \"set_device\", name=\"set_device\"),\n    url(\"^asset_proxy/$\", \"static_proxy\", name=\"static_proxy\"),\n    url(\"^displayable_links.js$\", \"displayable_links_js\",\n        name=\"displayable_links_js\"),\n)\n"}
{"text": "\"\"\"Packaging settings.\"\"\"\n\n\nfrom codecs import open\nfrom os.path import abspath, dirname, join\nfrom subprocess import call\n\nfrom setuptools import Command, find_packages, setup\n\nfrom kitchen import __version__\n\n\nthis_dir = abspath(dirname(__file__))\nwith open(join(this_dir, 'README.md'), encoding='utf-8') as file:\n    long_description = file.read()\n\n\nclass RunTests(Command):\n    \"\"\"Run all tests.\"\"\"\n    description = 'run tests'\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        \"\"\"Run all tests!\"\"\"\n        errno = call(['py.test', '--cov=kitchen', '--cov-report=term-missing'])\n        raise SystemExit(errno)\n\n\nsetup(\n    name = 'kitchen',\n    version = __version__,\n    description = 'A basic version control software baked into a \"cooking simulator\"',\n    long_description = long_description,\n    url = 'https://github.com/globz/kitchen-cli',\n    author = 'globz',\n    author_email = 'globz@kittybomber.com',\n    license = 'GPL-2.0',\n    classifiers = [\n        'Intended Audience :: Developers',\n        'Topic :: Utilities',\n        'License :: Public Domain',\n        'Natural Language :: English',\n        'Operating System :: OS Independent',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.2',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n    ],\n    keywords = 'cli',\n    packages = find_packages(exclude=['docs', 'tests*']),\n    install_requires = ['docopt'],\n    extras_require = {\n        'test': ['coverage', 'pytest', 'pytest-cov'],\n    },\n    entry_points = {\n        'console_scripts': [\n            'kitchen=kitchen.cli:main',\n        ],\n    },\n    cmdclass = {'test': RunTests},    \n)\n"}
{"text": "\"\"\"\n.. module:: l_release_group_url\n\nThe **L Release Group Url** Model.\n\nPostgreSQL Definition\n---------------------\n\nThe :code:`l_release_group_url` table is defined in the MusicBrainz Server as:\n\n.. code-block:: sql\n\n    CREATE TABLE l_release_group_url ( -- replicate\n        id                  SERIAL,\n        link                INTEGER NOT NULL, -- references link.id\n        entity0             INTEGER NOT NULL, -- references release_group.id\n        entity1             INTEGER NOT NULL, -- references url.id\n        edits_pending       INTEGER NOT NULL DEFAULT 0 CHECK (edits_pending >= 0),\n        last_updated        TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n        link_order          INTEGER NOT NULL DEFAULT 0 CHECK (link_order >= 0),\n        entity0_credit      TEXT NOT NULL DEFAULT '',\n        entity1_credit      TEXT NOT NULL DEFAULT ''\n    );\n\n\"\"\"\n\nfrom django.db import models\nfrom django.utils.encoding import python_2_unicode_compatible\n\n\n@python_2_unicode_compatible\nclass l_release_group_url(models.Model):\n    \"\"\"\n    Not all parameters are listed here, only those that present some interest\n    in their Django implementation.\n\n    :param int edits_pending: the MusicBrainz Server uses a PostgreSQL `check`\n        to validate that the value is a positive integer. In Django, this is\n        done with `models.PositiveIntegerField()`.\n    :param int link_order: the MusicBrainz Server uses a PostgreSQL `check`\n        to validate that the value is a positive integer. In Django, this is\n        done with `models.PositiveIntegerField()`.\n    \"\"\"\n\n    id = models.AutoField(primary_key=True)\n    link = models.ForeignKey('link')\n    entity0 = models.ForeignKey('release_group', related_name='links_to_url')\n    entity1 = models.ForeignKey('url')\n    edits_pending = models.PositiveIntegerField(default=0)\n    last_updated = models.DateTimeField(auto_now=True)\n    link_order = models.PositiveIntegerField(default=0)\n    entity0 = models.TextField(default='')\n    entity1 = models.TextField(default='')\n\n    def __str__(self):\n        return 'L Release Group Url'\n\n    class Meta:\n        db_table = 'l_release_group_url'\n"}
{"text": "# Copyright 2015, Telles Nobrega\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom sahara_dashboard import api\nfrom sahara_dashboard.test import helpers as test\n\n\nclass SaharaApiTest(test.SaharaAPITestCase):\n    #\n    # Cluster\n    #\n    def test_cluster_create_count(self):\n        saharaclient = self.stub_saharaclient()\n        saharaclient.clusters.create.return_value = \\\n            {\"Clusters\": ['cluster1', 'cluster2']}\n        ret_val = api.sahara.cluster_create(self.request,\n                                            'name',\n                                            'fake_plugin',\n                                            '1.0.0',\n                                            count=2,\n                                            is_public=False,\n                                            is_protected=False)\n\n        self.assertEqual(2, len(ret_val['Clusters']))\n"}
{"text": "# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nDisable legacy file types unless a project has used them previously\n\nRevision ID: f404a67e0370\nRevises: b8fda0d7fbb5\nCreate Date: 2016-12-17 02:58:55.328035\n\"\"\"\n\nfrom alembic import op\n\n\nrevision = \"f404a67e0370\"\ndown_revision = \"b8fda0d7fbb5\"\n\n\ndef upgrade():\n    op.execute(\n        r\"\"\" UPDATE packages\n            SET allow_legacy_files = 'f'\n            WHERE name NOT IN (\n                SELECT DISTINCT ON (packages.name) packages.name\n                FROM packages, release_files\n                WHERE packages.name = release_files.name\n                    AND (\n                        filename !~* '.+?\\.(tar\\.gz|zip|whl|egg)$'\n                        OR packagetype NOT IN (\n                            'sdist',\n                            'bdist_wheel',\n                            'bdist_egg'\n                        )\n                    )\n            )\n        \"\"\"\n    )\n\n\ndef downgrade():\n    raise RuntimeError(\"Order No. 227 - \u041d\u0438 \u0448\u0430\u0433\u0443 \u043d\u0430\u0437\u0430\u0434!\")\n"}
{"text": "# Copyright (c) 2012 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nimport copy\nfrom nova.openstack.common.gettextutils import _\nfrom nova.openstack.common import log as logging\nfrom nova.scheduler.filters import disk_filter\nfrom nova.huawei import utils as h_utils\n\nLOG = logging.getLogger(__name__)\n\n\nclass HuaweiDiskFilter(disk_filter.DiskFilter):\n    \"\"\"Disk Filter with over subscription flag.\"\"\"\n\n    def host_passes(self, host_state, filter_properties):\n        \"\"\"Filter based on disk usage.\"\"\"\n\n        #deep copy a filter properties to avoid changing\n        filter_properties_tmp = copy.deepcopy(filter_properties)\n\n        context = filter_properties_tmp['context']\n        instance = filter_properties_tmp['request_spec']['instance_properties']\n        if h_utils.is_boot_from_volume(context, instance):\n            # just process local disk(ephemeral and swap), so set\n            # root_gb to zero\n            filter_properties_tmp.get('instance_type')['root_gb'] = 0\n\n            # if the request disk size is zero, we should return true.\n            # In negative free disk size condition, the instance booted volume\n            # is not create successfully.\n            instance_type = filter_properties.get('instance_type')\n            requested_disk = (1024 * (instance_type['ephemeral_gb']) +\n                             instance_type['swap'])\n            if requested_disk == 0:\n                return True\n\n        return super(HuaweiDiskFilter, self).host_passes(host_state,\n                                                filter_properties_tmp)\n\n\n"}
{"text": "'''Train a simple deep CNN on the CIFAR10 small images dataset.\n\nGPU run command with Theano backend (with TensorFlow, the GPU is automatically used):\n    THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10_cnn.py\n\nIt gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n(it's still underfitting at that point, though).\n'''\n\nfrom __future__ import print_function\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.utils import np_utils\n\nbatch_size = 32\nnb_classes = 10\nnb_epoch = 200\ndata_augmentation = True\n\n# input image dimensions\nimg_rows, img_cols = 32, 32\n# The CIFAR10 images are RGB.\nimg_channels = 3\n\n# The data, shuffled and split between train and test sets:\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\nprint('X_train shape:', X_train.shape)\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n\n# Convert class vectors to binary class matrices.\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\nmodel = Sequential()\n\nmodel.add(Convolution2D(32, 3, 3, border_mode='same',\n                        input_shape=X_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(32, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Convolution2D(64, 3, 3, border_mode='same'))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(nb_classes))\nmodel.add(Activation('softmax'))\n\n# Let's train the model using RMSprop\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\n\nif not data_augmentation:\n    print('Not using data augmentation.')\n    model.fit(X_train, Y_train,\n              batch_size=batch_size,\n              nb_epoch=nb_epoch,\n              validation_data=(X_test, Y_test),\n              shuffle=True)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n    # Compute quantities required for featurewise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(X_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    model.fit_generator(datagen.flow(X_train, Y_train,\n                                     batch_size=batch_size),\n                        samples_per_epoch=X_train.shape[0],\n                        nb_epoch=nb_epoch,\n                        validation_data=(X_test, Y_test))\n"}
{"text": "import hashlib\nimport os\nimport time\nimport ujson\nimport urllib.request\n\nimport OSMPythonTools\n\nclass CacheObject:\n    def __init__(self, prefix, endpoint, cacheDir='cache', waitBetweenQueries=None, jsonResult=True):\n        self._prefix = prefix\n        self._endpoint = endpoint\n        self.__cacheDir = cacheDir\n        self.__waitBetweenQueries = waitBetweenQueries\n        self.__lastQuery = None\n        self.__jsonResult = jsonResult\n    \n    def query(self, *args, onlyCached=False, shallow=False, **kwargs):\n        queryString, hashString, params = self._queryString(*args, **kwargs)\n        filename = self.__cacheDir + '/' + self._prefix + '-' + self.__hash(hashString + ('????' + urllib.parse.urlencode(sorted(params.items())) if params else ''))\n        if not os.path.exists(self.__cacheDir):\n            os.makedirs(self.__cacheDir)\n        if os.path.exists(filename):\n            with open(filename, 'r') as file:\n                data = ujson.load(file)\n        elif onlyCached:\n            OSMPythonTools.logger.error('[' + self._prefix + '] data not cached: ' + queryString)\n            return None\n        elif shallow:\n            data = shallow\n        else:\n            OSMPythonTools.logger.warning('[' + self._prefix + '] downloading data: ' + queryString)\n            if self._waitForReady() == None:\n                if self.__lastQuery and self.__waitBetweenQueries and time.time() - self.__lastQuery < self.__waitBetweenQueries:\n                    time.sleep(self.__waitBetweenQueries - time.time() + self.__lastQuery)\n            self.__lastQuery = time.time()\n            data = self.__query(queryString, params)\n            with open(filename, 'w') as file:\n                ujson.dump(data, file)\n        result = self._rawToResult(data, queryString, params, shallow=shallow)\n        if not self._isValid(result):\n            msg = '[' + self._prefix + '] error in result (' + filename + '): ' + queryString\n            OSMPythonTools.logger.exception(msg)\n            raise(Exception(msg))\n        return result\n    \n    def deleteQueryFromCache(self, *args, **kwargs):\n        queryString, hashString, params = self._queryString(*args, **kwargs)\n        filename = self.__cacheDir + '/' + self._prefix + '-' + self.__hash(hashString + ('????' + urllib.parse.urlencode(sorted(params.items())) if params else ''))\n        if os.path.exists(filename):\n            OSMPythonTools.logger.info('[' + self._prefix + '] removing cached data: ' + queryString)\n            os.remove(filename)\n    \n    def _queryString(self, *args, **kwargs):\n        raise(NotImplementedError('Subclass should implement _queryString'))\n    \n    def _queryRequest(self, endpoint, queryString, params={}):\n        raise(NotImplementedError('Subclass should implement _queryRequest'))\n    \n    def _rawToResult(self, data, queryString, params, shallow=False):\n        raise(NotImplementedError('Subclass should implement _rawToResult'))\n    \n    def _isValid(self, result):\n        return True\n    \n    def _waitForReady(self):\n        return None\n    \n    def _userAgent(self):\n        return '%s/%s (%s)' % (OSMPythonTools.pkgName, OSMPythonTools.pkgVersion, OSMPythonTools.pkgUrl)\n    \n    def __hash(self, x):\n        h = hashlib.sha1()\n        h.update(x.encode('utf-8'))\n        return h.hexdigest()\n    \n    def __query(self, requestString, params):\n        request = self._queryRequest(self._endpoint, requestString, params=params)\n        if not isinstance(request, urllib.request.Request):\n            request = urllib.request.Request(request)\n        request.headers['User-Agent'] = self._userAgent()\n        try:\n            response = urllib.request.urlopen(request)\n        except urllib.request.HTTPError as err:\n            msg = 'The requested data could not be downloaded. ' + str(err)\n            OSMPythonTools.logger.exception(msg)\n            raise Exception(msg)\n        except:\n            msg = 'The requested data could not be downloaded.  Please check whether your internet connection is working.'\n            OSMPythonTools.logger.exception(msg)\n            raise Exception(msg)\n        encoding = response.info().get_content_charset('utf-8')\n        r = response.read().decode(encoding)\n        return ujson.loads(r) if self.__jsonResult else r\n"}
{"text": "# -*- encoding: utf-8 -*-\n\"\"\"\nopensearch python sdk client.\n\"\"\"\nfrom opensearchsdk.apiclient import api_base\nfrom opensearchsdk.v2 import app\nfrom opensearchsdk.v2 import data\nfrom opensearchsdk.v2 import search\nfrom opensearchsdk.v2 import suggest\nfrom opensearchsdk.v2 import index\nfrom opensearchsdk.v2 import quota\nfrom opensearchsdk.v2 import log\n\n\nAPP_URL = '/index'\nDATA_URL = APP_URL + '/doc'\nSEARCH_URL = '/search'\nINDEX_URL = '/index'\nSUGGEST_URL = '/suggest'\nQUOTA_URL = ''\n# TODO(Yan Haifeng) not yet implement\nLOG_URL = APP_URL + '/error'\n\n\nclass Client(object):\n    \"\"\"opensearch python sdk client.\"\"\"\n\n    def __init__(self, base_url, key, key_id):\n        self.base_url = base_url\n        self.key = key\n        self.key_id = key_id\n        self.http_client = api_base.HTTPClient(base_url)\n        self.app = app.AppManager(self, APP_URL)\n        self.data = data.DataManager(self, DATA_URL)\n        self.search = search.SearchManager(self, SEARCH_URL)\n        self.suggest = suggest.SuggestManager(self, SUGGEST_URL)\n        self.index = index.IndexManager(self, INDEX_URL)\n        self.quota = quota.QuotaManager(self, QUOTA_URL)\n        # TODO(Yan Haifeng) not yet implement\n        self.log = log.LogManager(self, LOG_URL)\n"}
{"text": "# -*- coding: utf-8 -*-\n\nimport os.path\nfrom pybtex.database.input import bibtex\n\n\nclass unite_bibtex(object):\n    \"\"\"\n    Name space for unite_bibtex.vim\n    (not to pollute global name space)\n    \"\"\"\n\n    @staticmethod\n    def _read_file(filename):\n        parser = bibtex.Parser()\n        return parser.parse_file(filename)\n\n    @staticmethod\n    def _check_path(path):\n        path = os.path.abspath(os.path.expanduser(path))\n        if not os.path.exists(path):\n            raise RuntimeError(\"file:%s not found\" % path)\n        return path\n\n    @staticmethod\n    def entry_to_str(entry):\n        try:\n            persons = entry.persons[u'author']\n            authors = [unicode(au) for au in persons]\n        except:\n            authors = [u'unknown']\n        title = entry.fields[u\"title\"] if u\"title\" in entry.fields else \"\"\n        journal = entry.fields[u\"journal\"] if u\"journal\" in entry.fields else \"\"\n        year = entry.fields[u\"year\"] if u\"year\" in entry.fields else \"\"\n        desc = u\"%s %s %s(%s)\" % (\",\".join(authors), title, journal, year)\n        return desc.replace(\"'\", \"\").replace(\"\\\\\", \"\")\n\n    @staticmethod\n    def get_entries(bibpath_list):\n        entries = {}\n        for bibpath in bibpath_list:\n            try:\n                path = unite_bibtex._check_path(bibpath)\n                bibdata = unite_bibtex._read_file(path)\n            except Exception as e:\n                print(\"Fail to read {}\".format(bibpath))\n                print(\"Message: {}\".format(str(e)))\n                continue\n            for key in bibdata.entries:\n                try:\n                    k = key.encode(\"utf-8\")\n                except:\n                    print(\"Cannot encode bibtex key, skip: {}\".format(k))\n                    continue\n                entries[k] = unite_bibtex.entry_to_str(bibdata.entries[key]).encode(\"utf-8\")\n        return entries\n\nif __name__ == '__main__':\n    import sys\n    bibpath_list = sys.argv[1:]\n    entries = unite_bibtex.get_entries(bibpath_list)\n    for k, v in entries.items():\n        print(\"{}:{}\".format(k, v))\n"}
{"text": "from django.conf import settings\nfrom django.contrib.contenttypes.fields import GenericForeignKey\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.db import models\nfrom django.utils.functional import cached_property\n\nfrom .managers import ActivityItemManager\n\n\nclass Activity(models.Model):\n    \"\"\"\n    Stores an action that occurred that is being tracked\n    according to ACTIVITY_MONITOR settings.\n    \"\"\"\n    actor = models.ForeignKey(\n        settings.AUTH_USER_MODEL,\n        related_name=\"subject\",\n        on_delete=\"CASCADE\"\n    )\n    timestamp = models.DateTimeField()\n\n    verb = models.CharField(blank=True, null=True, max_length=255, editable=False)\n    override_string = models.CharField(blank=True, null=True, max_length=255, editable=False)\n\n    target = models.CharField(blank=True, null=True, max_length=255, editable=False)\n    actor_name = models.CharField(blank=True, null=True, max_length=255, editable=False)\n\n    content_object = GenericForeignKey()\n    content_type = models.ForeignKey(ContentType, on_delete=\"CASCADE\")\n    object_id = models.PositiveIntegerField()\n\n    objects = ActivityItemManager()\n\n    class Meta:\n        ordering = ['-timestamp']\n        unique_together = [('content_type', 'object_id')]\n        get_latest_by = 'timestamp'\n        verbose_name_plural = 'actions'\n\n    def __unicode__(self):\n        return \"{0}: {1}\".format(self.content_type.model_class().__name__, self.content_object)\n\n    def save(self, *args, **kwargs):\n        \"\"\"\n        Store a string representation of content_object as target\n        and actor name for fast retrieval and sorting.\n        \"\"\"\n        if not self.target:\n            self.target = str(self.content_object)\n        if not self.actor_name:\n            self.actor_name = str(self.actor)\n        super(Activity, self).save()\n\n    def get_absolute_url(self):\n        \"\"\"\n        Use original content object's\n        get_absolute_url method.\n        \"\"\"\n        return self.content_object.get_absolute_url()\n\n    @cached_property\n    def short_action_string(self):\n        \"\"\"\n        Returns string with actor and verb, allowing target/object\n        to be filled in manually.\n\n        Example:\n        [actor] [verb] or\n        \"Joe cool posted a comment\"\n        \"\"\"\n        output = \"{0} \".format(self.actor)\n        if self.override_string:\n            output += self.override_string\n        else:\n            output += self.verb\n        return output\n\n    @cached_property\n    def full_action_string(self):\n        \"\"\"\n        Returns full string with actor, verb and target content object.\n\n        Example:\n        [actor] [verb] [content object/target] or\n        Joe cool posted a new topic: \"my new topic\"\n        \"\"\"\n        output = \"{} {}\".format(self.short_action_string, self.content_object)\n        return output\n\n    @cached_property\n    def image(self):\n        \"\"\"\n        Attempts to provide a representative image from a content_object based on\n        the content object's get_image() method.\n\n        If there is a another content.object, as in the case of comments and other GFKs,\n        then it will follow to that content_object and then get the image.\n\n        Requires get_image() to be defined on the related model even if it just\n        returns object.image, to avoid bringing back images you may not want.\n\n        Note that this expects the image only. Anything related (caption, etc) should be stripped.\n\n        \"\"\"\n        obj = self.content_object\n        # First, try to get from a get_image() helper method\n        try:\n            image = obj.get_image()\n        except AttributeError:\n            try:\n                image = obj.content_object.get_image()\n            except:\n                image = None\n\n        # if we didn't find one, try to get it from foo.image\n        # This allows get_image to take precedence for greater control.\n        if not image:\n            try:\n                image = obj.image\n            except AttributeError:\n                try:\n                    image = obj.content_object.image\n                except:\n                    return None\n\n        # Finally, ensure we're getting an image, not an image object\n        # with caption and byline and other things.\n        try:\n            return image.image\n        except AttributeError:\n            return image\n"}
{"text": "# Test the FT Calibration state by calling the python class and doing the calculation here\n\n# Moritz Schappler, schappler@irt.uni-hannover.de, 2015-05\n# Institut fuer Regelungstechnik, Universitaet Hannover\n\n\n# remotedebug\n# import pydevd\n# pydevd.settrace('localhost', port=5678, stdoutToServer=True, stderrToServer=True)\n\n\n# import definitions\nfrom calculate_force_torque_calibration_state import CalculateForceTorqueCalibration\nfrom generate_trajectory_from_txtfile_state import GenerateTrajectoryFromTxtfileState\n\n# initialize rospy and rostime\nimport rospy  \nrospy.init_node('calib_test_node', anonymous=True)\n\n# define userdata\nclass Userdata(object):\n    def __init__(self):\n        self.trajectories = []\n        self.ft_calib_data = []\n\n\n\n# Generating the trajectory from text files\n# txtfile_name_left_arm = '~/ft_calib/input/l_arm.txt'\n# txtfile_name_right_arm = '~/ft_calib/input/r_arm.txt'\ntxtfile_name_left_arm = '~/ft_calib/input/SI_E065_FT_Calib_Arms_Payload_Left.txt'\ntxtfile_name_right_arm = '~/ft_calib/input/SI_E065_FT_Calib_Arms_Payload_Right.txt'\ntransitiontime = 0.5\nsettlingtime = 0.5\n\nuserdata = Userdata()\nGTFT = GenerateTrajectoryFromTxtfileState(txtfile_name_left_arm, txtfile_name_right_arm, transitiontime, settlingtime)\nGTFT.execute(userdata)\n\n# Calculation the calibration with data recorded with the behaviour\n# bag_filename = '/home/schappler/ft_calib/ft_logs/FTCalib.bag'\n# bag_filename = '/home/schappler/ft_calib/ft_logs/R05_both_20150426_w_flangue.bag'\n# bag_filename = '/home/schappler/IRT_DRC/Experiments/Output/SI_E047_FT_Calib_Arms/S02_20150504_payload_merge.bag'\nbag_filename = '~/ft_calib/ft_logs/SI_E065_FT_Calib_Arms_Payload.bag'\n\n\n\ncalibration_chain = ['left_arm', 'right_arm']\ntrajectories_command = GTFT._trajectories\n\nCFTC = CalculateForceTorqueCalibration(bag_filename, calibration_chain, settlingtime, trajectories_command)\nCFTC.execute(userdata)\nprint CFTC._ft_calib_data\n"}
{"text": "from __future__ import absolute_import\nfrom myhdl.test.conftest import bug\nfrom myhdl import Signal, uintba, instance, delay, conversion, \\\n    ConcatSignal, TristateSignal, sfixba, always_comb, StopSimulation, \\\n    toVHDL, toVerilog\n\n\ndef bench_SliceSignal():\n    s = Signal(uintba(0, 8))\n    a, b, c = s(7), s(5), s(0)\n    d, e, f, g = s(8, 5), s(6, 3), s(8, 0), s(4, 3)\n    N = len(s)\n\n    @instance\n    def check():\n        for i in range(N):\n            s.next = i\n            yield delay(10)\n            print(int(a))\n            print(int(b))\n            print(int(c))\n            print(int(d))\n            print(int(e))\n            print(int(f))\n            print(int(g))\n\n    return check\n\n\ndef test_SliceSignal():\n    assert conversion.verify(bench_SliceSignal) == 0\n\n\ndef bench_ConcatSignal():\n\n    a = Signal(uintba(0, 5))\n    b = Signal(bool(0))\n    c = Signal(uintba(0, 3))\n    d = Signal(uintba(0, 4))\n\n    s = ConcatSignal(a, b, c, d)\n\n    I_max = 2**len(a)\n    J_max = 2**len(b)\n    K_max = 2**len(c)\n    M_max = 2**len(d)\n\n    @instance\n    def check():\n        for i in range(I_max):\n            for j in range(J_max):\n                for k in range(K_max):\n                    for m in range(M_max):\n                        a.next = i\n                        b.next = j\n                        c.next = k\n                        d.next = m\n                        yield delay(10)\n                        print(int(s))\n\n    return check\n\n\ndef test_ConcatSignal():\n    assert conversion.verify(bench_ConcatSignal) == 0\n\n\ndef bench_ConcatSignalWithConsts():\n\n    a = Signal(uintba(0, 5))\n    b = Signal(bool(0))\n    c = Signal(uintba(0, 3))\n    d = Signal(uintba(0, 4))\n    e = Signal(uintba(0, 1))\n\n    c1 = \"10\"\n    c2 = uintba(3, 3)\n    c3 = '0'\n    c4 = bool(1)\n    c5 = uintba(42, 8)  # with leading zeroes\n\n    s = ConcatSignal(c1, a, c2, b, c3, c, c4, d, c5, e)\n\n    I_max = 2**len(a)\n    J_max = 2**len(b)\n    K_max = 2**len(c)\n    M_max = 2**len(d)\n\n    @instance\n    def check():\n        for i in range(I_max):\n            for j in range(J_max):\n                for k in range(K_max):\n                    for m in range(M_max):\n                        for n in range(2**len(e)):\n                            a.next = i\n                            b.next = j\n                            c.next = k\n                            d.next = m\n                            e.next = n\n                            yield delay(10)\n                            print(int(s))\n\n    return check\n\n\ndef test_ConcatSignalWithConsts():\n    assert conversion.verify(bench_ConcatSignalWithConsts) == 0\n\n\ndef bench_TristateSignal():\n    s = TristateSignal(uintba(0, 8))\n    t = TristateSignal(sfixba(0, 1, 0))\n    a = s.driver()\n    b = s.driver()\n    c = s.driver()\n    d = t.driver()\n    e = t.driver()\n\n    @instance\n    def check():\n        a.next = None\n        b.next = None\n        c.next = None\n        d.next = None\n        e.next = None\n        yield delay(10)\n        a.next = 1\n        d.next = 0\n        yield delay(10)\n        print(int(s))\n        a.next = None\n        b.next = 122\n        yield delay(10)\n        print(int(s))\n        b.next = None\n        c.next = 233\n        yield delay(10)\n        print(int(s))\n        c.next = None\n        yield delay(10)\n\n    return check\n\n\n#@bug(\"Tristate pending\", \"vhdl\")\ndef test_TristateSignal():\n    assert conversion.verify(bench_TristateSignal) == 0\n\n\ndef permute(x, a, mapping):\n\n    p = [a(m) for m in mapping]\n\n    q = ConcatSignal(*p)\n\n    @always_comb\n    def assign():\n        x.next = q\n\n    return assign\n\n\ndef bench_permute(conv=False):\n\n    x = Signal(uintba(0, 3))\n    a = Signal(sfixba(0, 3, -2))\n    mapping = (0, 2, 1)\n\n    if conv:\n        dut = conv(permute, x, a, mapping)\n    else:\n        dut = permute(x, a, mapping)\n\n    @instance\n    def stimulus():\n        for i in range(2**len(a)):\n            a.next = i\n            yield delay(10)\n            print(\"%d %d\" % (x, a))\n            assert x[2] == a[0]\n            assert x[1] == a[2]\n            assert x[0] == a[1]\n        raise StopSimulation()\n\n    return dut, stimulus\n\n\ndef test_permute():\n    assert conversion.verify(bench_permute) == 0\n\nbench_permute(toVHDL)\nbench_permute(toVerilog)\n"}
{"text": "#!/usr/local/bin/python3\n\nimport os, sys, getopt\nimport quandl as ql\nimport pandas as pd\nimport numpy  as np\nfrom pylab import plot, figure, savefig\n\n# read Russell 3000 constituents from a csv\ndef readRuss():\n    try:\n        ticks = [] \n        russ  = open('russ3.csv', 'r').read()\n        split = russ.split('\\n')\n\n        for tick in split:\n            ticks.append('WIKI/' + tick.rstrip())\n        return ticks\n\n    except Exception as e:\n        print('Failed to read Russell 3000:', str(e))\n\n# retrieve stock data from Quandl\ndef getData(query, date):\n    try: \n        return ql.get(query, start_date = date)\n\n    except Exception as e:\n        print('Failed to get stock data:', str(e))\n\n# fit a first-degree polynomial (i.e. a line) to the data\ndef calcTrend(data):\n    return np.polyfit(data.index.values, list(data), 1)\n\ndef main(argv):\n    tick = 'WIKI/'      # ticker will be appended\n    date = '2017/01/01' # default start date\n\n    ql.ApiConfig.api_key = os.environ['QUANDL_KEY']\n    usage = 'usage: screener.py -t <ticker> -d <start_date>'\n\n    if len(argv) == 0:\n        print(usage)\n        sys.exit(2)\n\n    # parse command-line args\n    try:\n        opts, args = getopt.getopt(argv, 'ht:d', ['ticker=', 'date='])\n    except getopt.GetoptError:\n        print(usage)\n        sys.exit(2)\n\n    for opt, arg in opts:\n        if opt == '-h':\n            print(usage)\n            sys.exit(0)        \n        elif opt in ('-t', '--ticker'):\n            tick += arg\n        elif opt in ('-d', '--date'):\n            date = arg\n\n    # retrieve the 4th & 5th cols (Close & Volume)\n    close  = getData(tick + '.4', date)      \n    vol    = getData(tick + '.5', date)\n    data   = pd.concat([close, vol], axis=1).reset_index(drop=True)\n    print(data)\n\n    # calculate trends on price and volume\n    pcoeffs = calcTrend(data['Close'])\n    vcoeffs = calcTrend(data['Volume'])\n\n    print('Price trend:', pcoeffs[0])\n    print('Volume trend:', vcoeffs[0])\n\n    # save plots of trend lines\n    xi = data.index.values\n  \n    figure()\n    pline = pcoeffs[0] * xi + pcoeffs[1]\n    plot(xi, pline, 'r-', xi, list(data['Close']), '-o')\n    savefig('price.png')\n\n    figure()\n    vline = vcoeffs[0] * xi + vcoeffs[1]\n    plot(xi, vline, 'r-', xi, list(data['Volume']), '-o')\n    savefig('volume.png')\n\n#   ticks = readRuss()\n#   q_close = [ tick + '.4' for tick in ticks[:5] ] \n#   q_vol   = [ tick + '.5' for tick in ticks[:5] ]\n#   data = getData(q_close + q_vol, '2017-01-01')\n\nif __name__ == \"__main__\":\n    # execute only if run as a script\n    main(sys.argv[1:])\n"}
{"text": "from math import radians, cos, sin, asin, sqrt\nimport time\n\ncurrent_milli_time = lambda: int(round(time.time() * 1000))\n\ndef haversine(point1, point2, miles = False):\n\n    AVG_EARTH_RADIUS = 6371\n\n    lat1, lng1 = point1\n    lat2, lng2 = point2\n\n    # convert all latitudes/longitudes from decimal degrees to radians\n    lat1, lng1, lat2, lng2 = map(radians, (lat1, lng1, lat2, lng2))\n\n    # calculate haversine\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = sin(lat * 0.5) ** 2 + cos(lat1) * cos(lat2) * sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * asin(sqrt(d))\n    if miles:\n        return h * 0.621371  # in miles\n    else:\n        return h\n\n    pass\n\ndef main():\n\n    # lyon = (45.7597, 4.8422)\n    # paris = (48.8567, 2.3508)\n    lyon = (12.9210784, 77.6936946)  # Saroj\n    paris = (12.9132164, 77.6234387)  # Snapwiz\n\n    totalDelay = 0\n\n    start = current_milli_time()\n    for i in range(0,300000,1):\n        # print i\n        start = current_milli_time()\n        dist = haversine(lyon, paris)\n        end = current_milli_time()\n        delay = start-end\n        if delay > 0:\n            totalDelay += delay\n\n    end = current_milli_time()\n\n    print end\n    print start\n    print \"That's All. Total Delay: \" + str(end-start)\n\n\n    # time.sleep(5)\n    # start = time.time()\n    # print (haversine(lyon, paris, miles=True))\n    # end = time.time()\n    # print \"%.20f\" % start-end\n\n    pass\n\nif __name__ == '__main__':\n    main()\n"}
{"text": "import numpy as np\nfrom cross_section import Plane\nfrom mapping_3d_to_2d import *\n\n\ndef test_initialise_mapping_3d_to_2d_simple():\n    \"\"\"\n    Check that for a plane orthogonal to the x-axis the transformation\n    simply drops the constant x-coordinate.\n    \"\"\"\n    plane1 = Plane([50, 0, 0], n=[1, 0, 0])\n    f1 = Mapping3Dto2D(plane1)\n\n    # Check the 3d -> 2d transformation\n    assert np.allclose(f1.apply([50, -1, 4]), [-1, 4])\n    assert np.allclose(f1.apply([50, 3, 7]), [3, 7])\n\n    # Check the 2d -> 3d transformation\n    assert np.allclose(f1.apply_inv([-1, 4]), [50, -1, 4])\n    assert np.allclose(f1.apply_inv([3, 7]), [50, 3, 7])\n    assert f1.apply_inv([-1, 4]).ndim == 1\n    assert f1.apply_inv([[-1, 4]]).ndim == 2\n    assert np.allclose(f1.apply_inv([[-1, 4], [3, 7]]), [[50, -1, 4], [50, 3, 7]])\n\n    # Regression test: check that applying the transformation does not\n    # change the shape/dimension of the input array.\n    pt1 = np.array([2., 6., 4.])\n    pt2 = np.array([[2., 6., 4.]])\n    _ = f1.apply(pt1)\n    _ = f1.apply(pt2)\n    assert pt1.shape == (3,)\n    assert pt2.shape == (1, 3)\n\n    plane2 = Plane([0, 30, 0], n=[0, 1, 0])\n    f2 = Mapping3Dto2D(plane2)\n\n    # Check the 3d -> 2d transformation\n    assert np.allclose(f2.apply([-1, 30, 4]), [-1, 4])\n    assert np.allclose(f2.apply([3, 30, 7]), [3, 7])\n\n    # Check the 2d -> 3d transformation\n    assert np.allclose(f2.apply_inv([-1, 4]), [-1, 30, 4])\n    assert np.allclose(f2.apply_inv([3, 7]), [3, 30, 7])\n\n    # Regression test: check that applying the inverse transformation\n    # does not change the shape/dimension of the input array.\n    pt1 = np.array([2., 6.])\n    pt2 = np.array([[2., 6.]])\n    _ = f1.apply_inv(pt1)\n    _ = f1.apply_inv(pt2)\n    assert pt1.shape == (2,)\n    assert pt2.shape == (1, 2)\n"}
{"text": "import logging\n\nfrom uamqp import errors as AMQPErrors, constants as AMQPConstants\nfrom azure.servicebus.exceptions import (\n    _create_servicebus_exception,\n    ServiceBusConnectionError,\n    ServiceBusError\n)\n\n\ndef test_link_idle_timeout():\n    logger = logging.getLogger(\"testlogger\")\n    amqp_error = AMQPErrors.LinkDetach(AMQPConstants.ErrorCodes.LinkDetachForced, description=\"Details: AmqpMessageConsumer.IdleTimerExpired: Idle timeout: 00:10:00.\")\n    sb_error = _create_servicebus_exception(logger, amqp_error)\n    assert isinstance(sb_error, ServiceBusConnectionError)\n    assert sb_error._retryable\n    assert sb_error._shutdown_handler\n\n\ndef test_unknown_connection_error():\n    logger = logging.getLogger(\"testlogger\")\n    amqp_error = AMQPErrors.AMQPConnectionError(AMQPConstants.ErrorCodes.UnknownError)\n    sb_error = _create_servicebus_exception(logger, amqp_error)\n    assert isinstance(sb_error,ServiceBusConnectionError)\n    assert sb_error._retryable\n    assert sb_error._shutdown_handler\n\n    amqp_error = AMQPErrors.AMQPError(AMQPConstants.ErrorCodes.UnknownError)\n    sb_error = _create_servicebus_exception(logger, amqp_error)\n    assert not isinstance(sb_error,ServiceBusConnectionError)\n    assert isinstance(sb_error,ServiceBusError)\n    assert not sb_error._retryable\n    assert sb_error._shutdown_handler\n"}
{"text": "import os\nimport sys\nimport getopt\n\ndef result_data(filename):\n    handle = open(filename)\n    data = handle.readlines()\n    handle.close()\n    return data\n\ndef remove_and_insert(data):\n    i = 0\n    for line in data:\n        line = line.replace(\"[\", \"\")\n        line = line.replace(\"]\", \"\")\n        line = line.replace(\"\\n\", \"\")\n        line = line.split(\",\")\n        max_dep = len(line)\n        new_dep = []\n        for dep in line:\n            new_dep.append(max_dep)\n        total_dep = \"%s\" % (new_dep)\n\tprint total_dep\n\ti += 1\n\ndef main(argv):\n\n    # Get the arguments passed to the script by the user\n    arguments = process_arguments(argv)\n    data = result_data(arguments['-f'])\n    remove_and_insert(data)\n   \ndef usage():\n    # This function is used by process_arguments to echo the purpose and usage \n    # of this script to the user. It is called when the user explicitly\n    # requests it or when no arguments are passed\n\n    print(\"rootfixProcess reprocesses the extractPolygons extracts the labelled polygons from a LabelMe XML file\")\n    print(\"Usage: python rootfixProcess.py -f {model output file}\")\n    print(\"-f, the output of the structure induction process\")\n\ndef options_string(options):\n    # This function turns a list of options into the string format required by\n    # getopt.getopt\n\n    stringified = \"\"\n\n    for opt in options:\n        # We remove the first character since it is a dash\n        stringified += opt[1:] + \":\"\n\n    # We always append the help option\n    stringified += \"h\"\n\n    return stringified\n\ndef process_arguments(argv):\n    # This function extracts the script arguments and returns them as a tuple.\n    # It almost always has to be defined from scratch for each new file =/\n\n    if (len(argv) == 0):\n        usage()\n        sys.exit(2)\n\n    arguments = dict()\n    options = [\"-f\"]\n    stroptions = options_string(options)\n\n    try:\n        opts, args = getopt.getopt(argv, stroptions)\n    except getopt.GetoptError:\n        usage()\n        sys.exit(2)\n\n    # Process command line arguments\n    for opt, arg in opts:\n        if opt in (\"-h\"):      \n            usage()                     \n            sys.exit()\n        for o in options:\n            if opt in o:\n                arguments[o] = arg\n                continue\n\n    return arguments\n\nif __name__ == \"__main__\":\n    main(sys.argv[1:])\n"}
{"text": "# -*- coding: utf-8 -*-\n# Copyright 2012-2013 UNED\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom django import template\n\nregister = template.Library()\n\n\n@register.inclusion_tag('courses/usercourses.html', takes_context=True)\ndef usercourses(context):\n    user = context['user']\n    courses = user.courses_as_student.all()\n    return {'courses': courses}\n\n\n@register.inclusion_tag('courses/clone_activity.html', takes_context=False)\ndef clone_activity(course, user):\n    course_student_relation = user.coursestudent_set.get(course=course)\n    return {'course': course,\n            'course_student_relation': course_student_relation}\n"}
{"text": "import os\nimport re\n\nSECRET_KEY = \"DUMMY_SECRET_KEY\"\n\nPROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))\n\nINTERNAL_IPS = []\n\nTEMPLATES = [\n    {\n        \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n        \"DIRS\": [\n            os.path.join(PROJECT_ROOT, \"test_templates\"),\n        ],\n        \"APP_DIRS\": True,\n        \"OPTIONS\": {\n            \"context_processors\": [\n                \"django.contrib.auth.context_processors.auth\",\n                \"django.template.context_processors.debug\",\n                \"django.template.context_processors.i18n\",\n                \"django.template.context_processors.media\",\n                \"django.template.context_processors.request\",\n                \"django.template.context_processors.static\",\n                \"django.template.context_processors.tz\",\n                \"django.contrib.messages.context_processors.messages\",\n            ],\n        },\n    },\n]\n\nDATABASES = {\"default\": {\"ENGINE\": \"django.db.backends.sqlite3\", \"NAME\": \":memory:\"}}\n\nINSTALLED_APPS = (\n    \"django.contrib.auth\",\n    \"django.contrib.admin\",\n    \"django.contrib.sessions\",\n    \"django.contrib.contenttypes\",\n    \"django.contrib.messages\",\n    \"django.contrib.sites\",\n    \"maintenancemode\",\n)\n\nMIDDLEWARE = (\n    \"django.contrib.sessions.middleware.SessionMiddleware\",\n    \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n    \"django.contrib.messages.middleware.MessageMiddleware\",\n    \"maintenancemode.middleware.MaintenanceModeMiddleware\",\n)\n\nROOT_URLCONF = \"test_urls\"\n\nSITE_ID = 1\n\nMAINTENANCE_MODE = False  # or ``True`` and use ``maintenance`` command\nMAINTENANCE_IGNORE_URLS = (re.compile(r\"^/ignored.*\"),)\n"}
{"text": "#!/usr/bin/python\n# -*- coding: utf-8 -*- # coding: utf-8\n#\n# IBM Storwize V7000 autodiscovery script for Zabbix\n#\n# 2013 Matvey Marinin\n#\n# Sends volume/mdisk/pool LLD JSON data to LLD trapper items \"svc.discovery.<volume-mdisk|volume|mdisk|pool>\"\n# Use with \"_Special_Storwize_Perf\" Zabbix template\n#\n# See also http://www.zabbix.com/documentation/2.0/manual/discovery/low_level_discovery\n#\n# Usage:\n# svc_perf_discovery_sender.py [--debug] --clusters <svc1>[,<svc2>...] --user <username> --password <pwd>\n#\n#   --debug    = Enable debug output\n#   --clusters = Comma-separated Storwize node list\n#   --user     = Storwize V7000 user account with Administrator role (it seems that Monitor role is not enough)\n#   --password = User password\n#\nimport pywbem\nimport getopt, sys\nfrom zbxsend import Metric, send_to_zabbix\nimport logging\n\ndef usage():\n  print >> sys.stderr, \"Usage: svc_perf_discovery_sender_zabbix.py [--debug] --clusters <svc1>[,<svc2>...] --user <username> --password <pwd> --discovery-types <type1>,[type2]\"\n  print >> sys.stderr, \"Discovery types: 'volume-mdisk','volume','mdisk','pool'\"\n\n\ntry:\n  opts, args = getopt.gnu_getopt(sys.argv[1:], \"-h\", [\"help\", \"clusters=\", \"user=\", \"password=\", \"debug\", \"discovery-types=\"])\nexcept getopt.GetoptError, err:\n  print >> sys.stderr, str(err)\n  usage()\n  sys.exit(2)\n\ndebug = False\nclusters = []\nDISCOVERY_TYPES = []\nuser = None\npassword = None\nfor o, a in opts:\n  if o == \"--clusters\" and not a.startswith('--'):\n    clusters.extend( a.split(','))\n  elif o == \"--user\" and not a.startswith('--'):\n    user = a\n  elif o == \"--password\" and not a.startswith('--'):\n    password = a\n  elif o == \"--debug\":\n    debug = True\n  elif o == \"--discovery-types\":\n    DISCOVERY_TYPES.extend( a.split(','))\n  elif o in (\"-h\", \"--help\"):\n    usage()\n    sys.exit()\n\nif not clusters:\n  print >> sys.stderr, '--clusters option must be set'\n  usage()\n  sys.exit(2)\n\nif not DISCOVERY_TYPES:\n  print >> sys.stderr, '--discovery-types option must be set'\n  usage()\n  sys.exit(2)\n\nif not user or not password:\n  print >> sys.stderr, '--user and --password options must be set'\n  usage()\n  sys.exit(2)\n\ndef debug_print(message):\n  if debug:\n    print message\n\nfor cluster in clusters:\n  debug_print('Connecting to: %s' % cluster)\n  conn = pywbem.WBEMConnection('https://'+cluster, (user, password), 'root/ibm')\n  conn.debug = True\n\n  for discovery in DISCOVERY_TYPES:\n    output = []\n\n    if discovery == 'volume-mdisk' or discovery == 'volume':\n      for vol in conn.ExecQuery('WQL', 'select DeviceID, ElementName from IBMTSSVC_StorageVolume'):\n        output.append( '{\"{#TYPE}\":\"%s\", \"{#NAME}\":\"%s\", \"{#ID}\":\"%s\"}' % ('volume', vol.properties['ElementName'].value, vol.properties['DeviceID'].value) )\n\n    if discovery == 'volume-mdisk' or discovery == 'mdisk':\n      for mdisk in conn.ExecQuery('WQL', 'select DeviceID, ElementName from IBMTSSVC_BackendVolume'):\n        output.append( '{\"{#TYPE}\":\"%s\", \"{#NAME}\":\"%s\", \"{#ID}\":\"%s\"}' % ('mdisk', mdisk.properties['ElementName'].value, mdisk.properties['DeviceID'].value) )\n\n    if discovery == 'pool':\n      for pool in conn.ExecQuery('WQL', 'select PoolID, ElementName from IBMTSSVC_ConcreteStoragePool'):\n        output.append( '{\"{#TYPE}\":\"%s\",\"{#NAME}\":\"%s\",\"{#ID}\":\"%s\"}' % ('pool', pool.properties['ElementName'].value, pool.properties['PoolID'].value) )\n\n    json = []\n    json.append('{\"data\":[')\n\n    for i, v in enumerate( output ):\n      if i < len(output)-1:\n        json.append(v+',')\n      else:\n        json.append(v)\n    json.append(']}')\n\n    json_string = ''.join(json)\n    print(json_string)\n\n    trapper_key = 'svc.discovery.%s' % discovery\n    debug_print('Sending to host=%s, key=%s' % (cluster, trapper_key))\n\n    #send json to LLD trapper item with zbxsend module\n    if debug:\n      logging.basicConfig(level=logging.INFO)\n    else:\n      logging.basicConfig(level=logging.WARNING)\n    send_to_zabbix([Metric(cluster, trapper_key, json_string)], 'localhost', 10051)\n    debug_print('')\n\n\n\n\n"}
{"text": "from decimal import Decimal\nfrom unittest import TestCase\n\nfrom graphql.lexer import GraphQLLexer\nfrom graphql.exceptions import LexerError\n\n\nclass GraphQLLexerTest(TestCase):\n    lexer = GraphQLLexer()\n\n    def assert_output(self, lexer, expected):\n        actual = list(lexer)\n        len_actual = len(actual)\n        len_expected = len(expected)\n        self.assertEqual(\n            len_actual,\n            len_expected,\n            'Actual output length %s does not match expected length %s\\n'\n            'Actual: %s\\n'\n            'Expected: %s' % (len_actual, len_expected, actual, expected)\n        )\n        for i, token in enumerate(actual):\n            self.assertEqual(token.type, expected[i][0])\n            self.assertEqual(token.value, expected[i][1])\n\n    def test_punctuator(self):\n        self.assert_output(self.lexer.input('!'), [('BANG', '!')])\n        self.assert_output(self.lexer.input('$'), [('DOLLAR', '$')])\n        self.assert_output(self.lexer.input('('), [('PAREN_L', '(')])\n        self.assert_output(self.lexer.input(')'), [('PAREN_R', ')')])\n        self.assert_output(self.lexer.input(':'), [('COLON', ':')])\n        self.assert_output(self.lexer.input('='), [('EQUALS', '=')])\n        self.assert_output(self.lexer.input('@'), [('AT', '@')])\n        self.assert_output(self.lexer.input('['), [('BRACKET_L', '[')])\n        self.assert_output(self.lexer.input(']'), [('BRACKET_R', ']')])\n        self.assert_output(self.lexer.input('{'), [('BRACE_L', '{')])\n        self.assert_output(self.lexer.input('}'), [('BRACE_R', '}')])\n        self.assert_output(self.lexer.input('...'), [('SPREAD', '...')])\n\n    def test_name(self):\n        for name in ('a', 'myVar_42', '__LOL__', '_', '_0'):\n            self.assert_output(self.lexer.input(name), [('NAME', name)])\n\n    def test_reserved_words(self):\n        reserved = ('fragment', 'query', 'mutation', 'on')\n        for word in reserved:\n            self.assert_output(self.lexer.input(word), [(word.upper(), word)])\n        # A word made of reserved words should be treated as a name\n        for word in ('queryType', 'mutation42', 'on_fragment'):\n            self.assert_output(self.lexer.input(word), [('NAME', word)])\n\n    def test_true(self):\n        self.assert_output(self.lexer.input('true'), [('TRUE', True)])\n        self.assert_output(self.lexer.input('True'), [('NAME', 'True')])\n\n    def test_false(self):\n        self.assert_output(self.lexer.input('false'), [('FALSE', False)])\n        self.assert_output(self.lexer.input('False'), [('NAME', 'False')])\n\n    def test_null(self):\n        self.assert_output(self.lexer.input('null'), [('NULL', None)])\n        self.assert_output(self.lexer.input('Null'), [('NAME', 'Null')])\n\n    def test_int(self):\n        for val in ('0', '-0', '42', '-42'):\n            self.assert_output(\n                self.lexer.input(val),\n                [('INT_VALUE', int(val))],\n            )\n\n    def test_float(self):\n        for val in ('-0.5e+42', '42.0', '2E64', '2.71e-0002'):\n            self.assert_output(\n                self.lexer.input(val),\n                [('FLOAT_VALUE', Decimal(val))],\n            )\n\n    def test_string(self):\n        for s in ('\"\"', u'\"\"', '\"42\"', r'\"\\t\\n\\u0042 ^\"'):\n            self.assert_output(\n                self.lexer.input(s),\n                [('STRING_VALUE', s.strip('\"'))]\n            )\n\n    def test_comment(self):\n        lexer = self.lexer.input(\"\"\"\n            42 # lol this is a number. But this -> 9000 is not.\n            \"\" # lol this is a string. But this -> \"gav\" is not.\n            # lol the whole line commented\n            #\n        \"\"\")\n        self.assert_output(lexer, [('INT_VALUE', 42), ('STRING_VALUE', '')])\n\n    def test_illegal_chars(self):\n        for s in ('\"', '^'):\n            try:\n                list(self.lexer.input(s))\n                self.fail('Illegal char exception not raised for %s' % repr(s))\n            except LexerError as e:\n                self.assertEqual(1, e.line)\n                self.assertEqual(1, e.column)\n                self.assertTrue(\n                    str(e).startswith('Line 1, col 1: Illegal character')\n                )\n                self.assertEqual(s, e.value)\n\n    def test_positional_info(self):\n        for i, t in enumerate(self.lexer.input('1\\n  3\\n    5\\n')):\n            self.assertEqual(i + 1, t.lineno)\n            self.assertEqual(i * 2 + 1, self.lexer.find_column(t))\n\n"}
{"text": "#!/usr/bin/env python\n'''\nmobile devices parser\n\nVersion 0.1\n\nby Roy Firestein (roy@firestein.net)\n\n\nParse mobile devices audit plugin and export to CSV\n\n'''\n\n\nimport os\nimport xml.dom.minidom\nfrom optparse import OptionParser\n\n\nparser = OptionParser()\nparser.add_option(\"-f\", \"--file\",  action=\"store\", type=\"string\", dest=\"file\", help=\"Nessus file to parse\")\nparser.add_option(\"-o\", \"--output\",  action=\"store\", type=\"string\", dest=\"output\", help=\"output file name\")\n(menu, args) = parser.parse_args()\n\ndevices = {\"Android\": [], \"iPhone\": [], \"iPad\": []}\n\ndef main():\n    nes_file = menu.file\n    report = xml.dom.minidom.parse(nes_file)\n    \n    for el in report.getElementsByTagName('ReportItem'):\n        if el.getAttribute(\"pluginID\") == \"60035\":\n            # find plugin_output element\n            output = get_plugin_output(el)\n            \n            model = get_model(output)\n            version = get_version(output)\n            user = get_user(output)\n            serial = get_serial(output)\n            \n            item = {\"serial\": serial, \"version\": version, \"user\": user}\n            \n            if not item in devices[model]:\n                devices[model].append(item)\n                print \"%s\\t%s\\t%s\\t%s\" %(model, version, user, serial)\n    \n    if len(devices['iPhone']) > 0 or len(devices['iPad']) > 0 or len(devices['Android']) > 0:\n        save_csv(devices)\n\n\ndef save_csv(devices):\n    fh = open(menu.output, \"w\")\n    fh.write(\"Platform,User,Version,Serial\\n\")\n    \n    for d in devices['iPhone']:\n        fh.write('\"%s\",\"%s\",\"%s\",\"%s\"\\n' %(\"iPhone\", d['user'], d['version'], d['serial']))\n        \n    for d in devices['iPad']:\n        fh.write('\"%s\",\"%s\",\"%s\",\"%s\"\\n' %(\"iPad\", d['user'], d['version'], d['serial']))\n        \n    for d in devices['Android']:\n        fh.write('\"%s\",\"%s\",\"%s\",\"%s\"\\n' %(\"Android\", d['user'], d['version'], d['serial']))\n    \n    fh.close()\n            \n\ndef getText(nodelist):\n    rc = []\n    for node in nodelist:\n        if node.nodeType == node.TEXT_NODE:\n            rc.append(node.data)\n    return ''.join(rc)\n    \ndef get_plugin_output(el):\n    a = el.getElementsByTagName(\"plugin_output\")[0]\n    return getText(a.childNodes)\n    \ndef get_model(data):\n    for line in data.split(\"\\n\"):\n        if line.startswith(\"Model\"):\n            return line.split(\" \")[2]\n    return None\n\ndef get_version(data):\n    for line in data.split(\"\\n\"):\n        if line.startswith(\"Version\"):\n            return line.split(\" \")[2]\n    return None\n\ndef get_user(data):\n    for line in data.split(\"\\n\"):\n        if line.startswith(\"User\"):\n            return line.split(\" \")[2]\n    return None\n\ndef get_serial(data):\n    for line in data.split(\"\\n\"):\n        if line.startswith(\"Serial\"):\n            return line.split(\" \")[3]\n    return None\n    \n\nif __name__ == \"__main__\":\n    main()\n"}
{"text": "import json\nfrom os.path import join\n\nimport pytest\nfrom conda_build.utils import on_win\nimport conda_build._link as _link\nfrom conda_build.conda_interface import PathType, EntityEncoder, CrossPlatformStLink\n\n\ndef test_pyc_f_2():\n    assert _link.pyc_f('sp/utils.py', (2, 7, 9)) == 'sp/utils.pyc'\n\ndef test_pyc_f_3():\n    for f, r in [\n            ('sp/utils.py',\n             'sp/__pycache__/utils.cpython-34.pyc'),\n            ('sp/foo/utils.py',\n             'sp/foo/__pycache__/utils.cpython-34.pyc'),\n    ]:\n        assert _link.pyc_f(f, (3, 4, 2)) == r\n\n\ndef test_pathtype():\n    hardlink = PathType(\"hardlink\")\n    assert str(hardlink) == \"hardlink\"\n    assert hardlink.__json__() == 'hardlink'\n\n    softlink = PathType(\"softlink\")\n    assert str(softlink) == \"softlink\"\n    assert softlink.__json__() == \"softlink\"\n\n\ndef test_entity_encoder(tmpdir):\n    test_file = join(str(tmpdir), \"test-file\")\n    test_json = {\"a\": PathType(\"hardlink\"), \"b\": 1}\n    with open(test_file, \"w\") as f:\n        json.dump(test_json, f, cls=EntityEncoder)\n\n    with open(test_file, \"r\") as f:\n        json_file = json.load(f)\n    assert json_file == {\"a\": \"hardlink\", \"b\": 1}\n\n\n@pytest.mark.skipif(on_win, reason=\"link not available on win/py2.7\")\ndef test_crossplatform_st_link(tmpdir):\n    from os import link\n    test_file = join(str(tmpdir), \"test-file\")\n    test_file_linked = join(str(tmpdir), \"test-file-linked\")\n    test_file_link = join(str(tmpdir), \"test-file-link\")\n\n    open(test_file, \"a\").close()\n    open(test_file_link, \"a\").close()\n    link(test_file_link, test_file_linked)\n    assert 1 == CrossPlatformStLink.st_nlink(test_file)\n    assert 2 == CrossPlatformStLink.st_nlink(test_file_link)\n    assert 2 == CrossPlatformStLink.st_nlink(test_file_linked)\n\n\n@pytest.mark.skipif(not on_win, reason=\"already tested\")\ndef test_crossplatform_st_link_on_win(tmpdir):\n    test_file = join(str(tmpdir), \"test-file\")\n    open(test_file, \"a\").close()\n    assert 1 == CrossPlatformStLink.st_nlink(test_file)\n\n"}
{"text": "# encoding: utf-8\n# module PyQt4.QtNetwork\n# from /usr/lib/python3/dist-packages/PyQt4/QtNetwork.cpython-34m-x86_64-linux-gnu.so\n# by generator 1.135\n# no doc\n\n# imports\nimport PyQt4.QtCore as __PyQt4_QtCore\n\n\nclass QNetworkCookie(): # skipped bases: <class 'sip.simplewrapper'>\n    \"\"\"\n    QNetworkCookie(QByteArray name=QByteArray(), QByteArray value=QByteArray())\n    QNetworkCookie(QNetworkCookie)\n    \"\"\"\n    def domain(self): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.domain() -> str \"\"\"\n        return \"\"\n\n    def expirationDate(self): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.expirationDate() -> QDateTime \"\"\"\n        pass\n\n    def isHttpOnly(self): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.isHttpOnly() -> bool \"\"\"\n        return False\n\n    def isSecure(self): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.isSecure() -> bool \"\"\"\n        return False\n\n    def isSessionCookie(self): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.isSessionCookie() -> bool \"\"\"\n        return False\n\n    def name(self): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.name() -> QByteArray \"\"\"\n        pass\n\n    def parseCookies(self, QByteArray): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.parseCookies(QByteArray) -> list-of-QNetworkCookie \"\"\"\n        pass\n\n    def path(self): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.path() -> str \"\"\"\n        return \"\"\n\n    def setDomain(self, p_str): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.setDomain(str) \"\"\"\n        pass\n\n    def setExpirationDate(self, QDateTime): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.setExpirationDate(QDateTime) \"\"\"\n        pass\n\n    def setHttpOnly(self, bool): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.setHttpOnly(bool) \"\"\"\n        pass\n\n    def setName(self, QByteArray): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.setName(QByteArray) \"\"\"\n        pass\n\n    def setPath(self, p_str): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.setPath(str) \"\"\"\n        pass\n\n    def setSecure(self, bool): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.setSecure(bool) \"\"\"\n        pass\n\n    def setValue(self, QByteArray): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.setValue(QByteArray) \"\"\"\n        pass\n\n    def toRawForm(self, QNetworkCookie_RawForm_form=None): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.toRawForm(QNetworkCookie.RawForm form=QNetworkCookie.Full) -> QByteArray \"\"\"\n        pass\n\n    def value(self): # real signature unknown; restored from __doc__\n        \"\"\" QNetworkCookie.value() -> QByteArray \"\"\"\n        pass\n\n    def __eq__(self, *args, **kwargs): # real signature unknown\n        \"\"\" Return self==value. \"\"\"\n        pass\n\n    def __ge__(self, *args, **kwargs): # real signature unknown\n        \"\"\" Return self>=value. \"\"\"\n        pass\n\n    def __gt__(self, *args, **kwargs): # real signature unknown\n        \"\"\" Return self>value. \"\"\"\n        pass\n\n    def __init__(self, *__args): # real signature unknown; restored from __doc__ with multiple overloads\n        pass\n\n    def __le__(self, *args, **kwargs): # real signature unknown\n        \"\"\" Return self<=value. \"\"\"\n        pass\n\n    def __lt__(self, *args, **kwargs): # real signature unknown\n        \"\"\" Return self<value. \"\"\"\n        pass\n\n    def __ne__(self, *args, **kwargs): # real signature unknown\n        \"\"\" Return self!=value. \"\"\"\n        pass\n\n    __weakref__ = property(lambda self: object(), lambda self, v: None, lambda self: None)  # default\n    \"\"\"list of weak references to the object (if defined)\"\"\"\n\n\n    Full = 1\n    NameAndValueOnly = 0\n    RawForm = None # (!) real value is ''\n    __hash__ = None\n\n\n"}
{"text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"from python cookbook 2nd edition.\"\"\"\n\n\nclass RingBuffer(object):\n    \"\"\" a ringbuffer not filled \"\"\"\n    def __init__(self, size_max):\n        self.max = size_max\n        self.data = []\n\n    class __Full(object):\n        \"\"\" a ringbuffer filled \"\"\"\n        def append(self, x):\n            self.data[self.cur] = x\n            self.cur = (self.cur + 1) % self.max\n\n        def tolist(self):\n            \"\"\" return the list with real order \"\"\"\n            return self.data[self.cur:] + self.data[:self.cur]\n\n    def append(self, x):\n        \"\"\" add an element at the end of the buffer \"\"\"\n        self.data.append(x)\n        if len(self.data) == self.max:\n            self.cur = 0\n            # chang the state of the instance to \"FULL\" forever\n            self.__class__ = self.__Full\n\n    def tolist(self):\n        \"\"\" return the list with real order \"\"\"\n        return self.data\n\n\ndef main():\n    x = RingBuffer(5)\n    x.append(1)\n    x.append(2)\n    x.append(3)\n    x.append(4)\n    print x.__class__, x.tolist()\n    x.append(5)\n    x.append(6)\n    x.append(7)\n    print x.__class__, x.tolist()\n    x.append(8)\n    x.append(9)\n    x.append(10)\n    print x.__class__, x.tolist()\n\nif __name__ == \"__main__\":\n    main()\n"}
{"text": "#-*- coding: utf-8 -*-\nfrom PyQt4 import QtGui\nfrom ui.AppIcons import *\n\nclass BleListItem(QtGui.QListWidgetItem):\n    \n    def __init__(self, name, address, rssi):\n        QtGui.QListWidgetItem.__init__(self)\n        self.name, self.address, self.rssi = name, address, rssi\n        self.setBleInfo(rssi)\n        self.conflag = False\n    \n    def setConnected(self, flag):\n        self.conflag = flag\n        self.setBackgroundColor(QtGui.QColor(flag and 0x00ff00 or 0xffffff))\n    \n    def isConnected(self):\n        return self.conflag\n        \n    def setBleInfo(self, rssi):\n        iconPath = \":app/icons/app/sig_1.png\"\n        if rssi > -45:\n            iconPath = \":app/icons/app/sig_4.png\"\n        elif rssi > -60:\n            iconPath = \":app/icons/app/sig_3.png\"\n        elif rssi > -80:\n            iconPath = \":app/icons/app/sig_2.png\"\n            \n        self.setIcon(QtGui.QIcon(iconPath))\n        self.setText(\"%s\\n%s    %ddb\\n\" % (self.name, self.address, rssi))\n        \n    def updateRssi(self, rssi):\n        self.setBleInfo(rssi)\n    \n    def getAddress(self):\n        return self.address"}
{"text": "from .events import ContactCreated, ContactUpdated, ContactDeleted\nfrom .policy import policy, PhoneNumberLongEnough\n\n@policy(PhoneNumberLongEnough())\nclass CreateContactHandler:\n\n    def __init__(self, repo, bus):\n        self.repo = repo\n        self.bus = bus\n\n    def __call__(self, cmd):\n        self.repo.create(cmd.name, cmd.phone)\n        self.bus.publish(ContactCreated(name=cmd.name, phone=cmd.phone))\n\n\nclass UpdateContactHandler:\n\n    def __init__(self, repo, bus):\n        self.repo = repo\n        self.bus = bus\n\n    def __call__(self, cmd):\n        self.repo.update(cmd.name, cmd.phone)\n        self.bus.publish(ContactUpdated(name=cmd.name, phone=cmd.phone))\n\n\nclass DeleteContactHandler:\n\n    def __init__(self, repo, bus):\n        self.repo = repo\n        self.bus = bus\n\n    def __call__(self, cmd):\n        self.repo.delete(cmd.name)\n        self.bus.publish(ContactDeleted(name=cmd.name))\n\n\nclass ReadContactHandler:\n\n    def __init__(self, repo, bus):\n        self.repo = repo\n        self.bus = bus\n\n    def __call__(self, cmd):\n        return self.repo.read(cmd.name)\n\n\nclass ContactEventsHandler:\n\n    def __call__(self, event):\n        print(event)\n"}
{"text": "from argparse import ArgumentParser\nimport falcon\nimport gunicorn.app.base\nimport json\nimport multiprocessing\nimport sys\n\n\n# A Falcon resource that returns the same quote every time\nclass QuoteResource(object):\n    def on_get(self, req, resp):\n        \"\"\"Handles GET requests\"\"\"\n        quote = {'quote': 'I\\'ve always been more interested in the future than in the past.', 'author': 'Grace Hopper'}\n\n        resp.body = json.dumps(quote)\n\n\n# A Falcon resource that explains what this server is\nclass IndexResource(object):\n    def __init__(self, prefix):\n        self.prefix = prefix\n\n    def on_get(self, req, resp):\n        \"\"\"Handles GET requests\"\"\"\n        resp.body = \"\"\"\n<html>\n  <head>\n    <title>Quote API Server</title>\n  </head>\n  <body>\n    <p>This is a toy JSON API server example.</p>\n    <p>Make a GET request to <a href=\"%s/quote\">%s/quote</a></p>\n  </body>\n</html>\n\"\"\" % (self.prefix, self.prefix)\n        resp.content_type = \"text/html\"\n        resp.status = falcon.HTTP_200\n\n\n# A Falcon middleware to implement validation of the Host header in requests\nclass HostFilter(object):\n    def __init__(self, hosts):\n        # falcon strips the port out of req.host, even if it isn't 80.\n        # This is probably a bug in Falcon, so we work around it here.\n        self.hosts = [falcon.util.uri.parse_host(host)[0] for host in hosts]\n\n    def process_request(self, req, resp):\n        # req.host has the port stripped from what the browser\n        # sent us, even when it isn't 80, which is probably a bug\n        # in Falcon. We deal with that in __init__ by removing\n        # ports from self.hosts.\n        if req.host not in self.hosts:\n            print(\"Attempted request with Host header '%s' denied\" % req.host)\n            raise falcon.HTTPForbidden(\"Bad Host header\", \"Cannot connect via the provided hostname\")\n\n\n# the gunicorn application\nclass QuoteApplication(gunicorn.app.base.BaseApplication):\n    def __init__(self, port, prefix, hosts):\n        assert prefix is not None\n        assert port is not None\n        self.application = falcon.API(middleware=HostFilter(hosts))\n        # add_route is pedantic about this\n        if prefix != '' and not prefix.startswith(\"/\"):\n            prefix = \"/\" + prefix\n        self.application.add_route(prefix + '/quote', QuoteResource())\n        self.application.add_route(prefix + \"/\", IndexResource(prefix))\n        self.port = port\n        super(QuoteApplication, self).__init__()\n\n        print(\"Only connections via these hosts are allowed: \" + repr(hosts))\n        print(\"Starting API server. Try http://localhost:%s%s\" % (self.port, prefix + '/quote'))\n\n    def load_config(self):\n        # Note that --kapsel-host is NOT this address; it is NOT\n        # the address to listen on. --kapsel-host specifies the\n        # allowed values of the Host header in an http request,\n        # which is totally different. Another way to put it is\n        # that --kapsel-host is the public hostname:port browsers will\n        # be connecting to.\n        self.cfg.set('bind', '%s:%s' % ('0.0.0.0', self.port))\n        self.cfg.set('workers', (multiprocessing.cpu_count() * 2) + 1)\n\n    def load(self):\n        return self.application\n\n# arg parser for the standard kapsel options\nparser = ArgumentParser(prog=\"quote-api\", description=\"API server that returns a quote.\")\nparser.add_argument('--kapsel-host', action='append', help='Hostname to allow in requests')\nparser.add_argument('--kapsel-no-browser', action='store_true', default=False, help='Disable opening in a browser')\nparser.add_argument('--kapsel-use-xheaders',\n                    action='store_true',\n                    default=False,\n                    help='Trust X-headers from reverse proxy')\nparser.add_argument('--kapsel-url-prefix', action='store', default='', help='Prefix in front of urls')\nparser.add_argument('--kapsel-port', action='store', default='8080', help='Port to listen on')\nparser.add_argument('--kapsel-iframe-hosts',\n                    action='append',\n                    help='Space-separated hosts which can embed us in an iframe per our Content-Security-Policy')\n\nif __name__ == '__main__':\n    # This app accepts but ignores --kapsel-no-browser because we never bother to open a browser,\n    # and accepts but ignores --kapsel-iframe-hosts since iframing an API makes no sense.\n    args = parser.parse_args(sys.argv[1:])\n    if not args.kapsel_host:\n        args.kapsel_host = ['localhost:' + args.kapsel_port]\n    QuoteApplication(port=args.kapsel_port, prefix=args.kapsel_url_prefix, hosts=args.kapsel_host).run()\n"}
{"text": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\n\nfrom django.db import migrations, models\n\n\nclass Migration(migrations.Migration):\n\n    dependencies = [\n        ('Inventory', '0014_auto_20151227_1250'),\n    ]\n\n    operations = [\n        migrations.CreateModel(\n            name='OrderHistoryModel',\n            fields=[\n                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),\n                ('created', models.DateTimeField(auto_now_add=True)),\n                ('modified', models.DateTimeField(auto_now=True)),\n                ('DocumentId', models.CharField(max_length=30)),\n                ('CustVendName', models.CharField(default=None, max_length=100)),\n                ('DocumentDate', models.DateField()),\n                ('DocumentTotal', models.DecimalField(max_digits=20, decimal_places=2)),\n                ('Qty', models.IntegerField(default=0)),\n                ('Price', models.DecimalField(max_digits=10, decimal_places=2)),\n                ('SubTotal', models.DecimalField(null=True, max_digits=20, decimal_places=2, blank=True)),\n                ('Item', models.ForeignKey(related_name='OrderHistory', default=None, blank=True, to='Inventory.ItemModel', null=True)),\n            ],\n            options={\n                'abstract': False,\n            },\n        ),\n    ]\n"}
{"text": "import bottle\nfrom bottle import route, get, post, request, run, template, static_file, error\n\nfrom pprint import pprint\n\nbottle.debug(True)\n\n# Static Routes\n@get(\"/css/<filepath:re:.*\\.css>\")\ndef css(filepath):\n    return static_file(filepath, root=\"../client/static/css\")\n\n@get(\"/font/<filepath:re:.*\\.(eot|otf|svg|ttf|woff|woff2?)>\")\ndef font(filepath):\n    return static_file(filepath, root=\"../client/static/font\")\n\n@get(\"/img/<filepath:re:.*\\.(jpg|png|gif|ico|svg)>\")\ndef img(filepath):\n    return static_file(filepath, root=\"../client/static/img\")\n\n@get(\"/js/<filepath:re:.*\\.js>\")\ndef js(filepath):\n    return static_file(filepath, root=\"../client/static/js\")\n\n@get('/')\ndef root():\n    return static_file('index.html', root='../client/static')\n\n@post('/')\ndef root():\n    print \"Wisdom: \", request.forms.get(\"wisdom\")\n    print \"URL: \", request.forms.get(\"url\")\n\n    return static_file('confirmed.html', root='../client/static')\n\n@route('/dontpanic/oblique')\ndef get_oblique_strategy():\n\ttestquote = \"You have reached the end of the universe\"\n\treturn template('{{quote}}', quote=testquote)\n\n@error(404)\ndef error404(error):\n        return \"Lost in translation\"\n\nrun(server='paste', host='localhost', port=8080)\n\n"}
{"text": "from functools import reduce\r\nimport operator\r\n\r\nfrom django.db import models\r\nfrom django.db.models import Q\r\nfrom django.conf import settings\r\nfrom django.utils import timezone\r\nfrom django.urls import reverse\r\nfrom django.utils.translation import gettext_lazy as _\r\n\r\nfrom ckeditor.fields import RichTextField\r\nfrom cuser.middleware import CuserMiddleware\r\n\r\nfrom diventi.core.models import (\r\n    TimeStampedModel, \r\n    PromotableModel, \r\n    PublishableModel,\r\n    PublishableModelQuerySet,\r\n    Category, \r\n    DiventiImageModel, \r\n    DiventiCoverModel, \r\n    Element,\r\n    DiventiColModel,\r\n)\r\n\r\n\r\nclass BlogCover(DiventiCoverModel, Element):\r\n    \"\"\"\r\n        Stores cover images for the blog page.\r\n    \"\"\"\r\n\r\n    class Meta:\r\n        verbose_name = _('Blog Cover')\r\n        verbose_name_plural = _('Blog Covers')\r\n\r\n\r\nclass ArticleCategory(Category):\r\n    \"\"\"\r\n        Defines the main argument of any article.\r\n    \"\"\"\r\n    pass\r\n\r\n\r\nclass ArticleQuerySet(PublishableModelQuerySet):\r\n    \r\n    # Selet articles' related objects\r\n    def prefetch(self):\r\n        articles = self.select_related('category')\r\n        articles = articles.select_related('author')\r\n        articles = articles.prefetch_related('related_articles')\r\n        articles = articles.prefetch_related('promotions')\r\n        return articles\r\n\r\n    # Get the list of published articles from the most recent to the least \r\n    def history(self):\r\n        articles = self.published()\r\n        articles = articles.order_by('-publication_date')\r\n        return articles\r\n\r\n    # Get the list of published articles of a certain category\r\n    def category(self, category_title):\r\n        articles = self.history().filter(category__title=category_title)\r\n        return articles\r\n\r\n    # Get the featured articles\r\n    def hot(self):\r\n        articles = self.history().filter(hot=True)\r\n        return articles\r\n\r\n    # Get the hottest article\r\n    def hottest(self):\r\n        article = self.hot().latest('publication_date')\r\n        return article\r\n\r\n    #\u00a0Fetch all the promotions related to the article\r\n    def promotions(self):\r\n        article = self.prefetch_related('promotions')\r\n        return article\r\n\r\n    # Get the most recent article\r\n    def current(self):\r\n        try:\r\n            article = self.hottest()\r\n        except Article.DoesNotExist:\r\n            article = self.published().latest('publication_date')\r\n        return article\r\n\r\n\r\nclass Article(TimeStampedModel, PromotableModel, PublishableModel, DiventiImageModel, DiventiColModel, Element):\r\n    \"\"\"\r\n        Blog posts are built upon a specific category and are always \r\n        introduced by a nice heading picture.\r\n    \"\"\"\r\n    category = models.ForeignKey(ArticleCategory, null=True, verbose_name=_('category'), on_delete=models.SET_NULL)    \r\n    content = RichTextField(verbose_name=_('content'))\r\n    hot = models.BooleanField(default=False, verbose_name=_('hot'))\r\n    slug = models.SlugField(unique=True, verbose_name=_('slug'))\r\n    author = models.ForeignKey(settings.AUTH_USER_MODEL, null=True, related_name='articles', verbose_name=_('author'), on_delete=models.SET_NULL)\r\n    related_articles = models.ManyToManyField(\r\n        'self',\r\n        related_name='related_articles', \r\n        blank=True, \r\n        verbose_name=_('related articles'),\r\n    ) # Connect this article to others\r\n\r\n    objects = ArticleQuerySet.as_manager()\r\n\r\n    class Meta:\r\n        verbose_name = _('article')\r\n        verbose_name_plural = _('articles')\r\n    \r\n    def __str__(self):\r\n        return self.title\r\n\r\n    def get_absolute_url(self):\r\n        return reverse('blog:detail', args=[str(self.slug)])\r\n\r\n    def search(self, query, *args, **kwargs):\r\n        results = Article.objects.history()\r\n        query_list = query.split()\r\n        results = results.filter(\r\n            reduce(operator.and_,\r\n                   (Q(title__icontains=q) for q in query_list)) |\r\n            reduce(operator.and_,\r\n                   (Q(description__icontains=q) for q in query_list))\r\n        )\r\n        return results\r\n\r\n    def reporting(self, *args, **kwargs):\r\n        results = Article.objects.none()\r\n        return results\r\n\r\n    def class_name(self):\r\n        return _('article')\r\n"}
{"text": "#=========================================================================\n# Dependencies / Libraries\n#=========================================================================\nimport time\nimport serial\nimport MySQLdb\nimport subprocess\nfrom time import sleep\nimport datetime\n#=========================================================================\n# Fonction Tableau/Dictionnaire\n#=========================================================================\ndef checksum (etiquette, valeur):\n    sum = 32\n    for c in etiquette: sum = sum + ord(c)\n    for c in valeur: sum = sum + ord(c)\n    sum = (sum & 63) + 32\n    return chr(sum)\n#=========================================================================\n# Fonction LireTeleinfo\n#=========================================================================\ndef ReadTeleinfo ():\n    # Attendre le debut du message\n    while ser.read(1) != chr(2): pass\n\n    message = \"\"\n    fin = False\n    \n    while not fin:\n        char = ser.read(1)\n        if char != chr(2):\n            message = message + char\n        else:\n            fin = True\n    \n    trames = [\n        trame.split(\" \")\n        for trame in message.strip(\"\\r\\n\\x03\").split(\"\\r\\n\")\n        ]\n            \n    tramesValides = dict([\n        [trame[0],trame[1]]\n        for trame in trames\n        if (len(trame) == 3) and (checksum(trame[0],trame[1]) == trame[2])\n        ])\n            \n    return tramesValides\n# print('Lecture des trames Teleinformation avec la carte RPIDOM') \n#=========================================================================\n# Connexion au port\n#=========================================================================\nser = serial.Serial(\n    port='/dev/ttyAMA0',\n    baudrate=1200,\n    parity=serial.PARITY_EVEN,\n    stopbits=serial.STOPBITS_ONE,\n    bytesize=serial.SEVENBITS )\n#=========================================================================\n# Definition variables de trame et chargement d'une valeur initiale\n#=========================================================================\nvIINST = 0 \nvMOTDETAT = 0\nvOPTARIF = 0\nvISOUSC = 0\nvADCO = 0\nvPAPP = 0\nvIMAX = 0\nvBASE = 0\nvADPS = 0\n#=========================================================================\n# Read serial data\n#=========================================================================\n#print '\\nPremiere voie'\nser.write('A')\nsleep(1)\nser.flushInput()\ntramesOk = ReadTeleinfo()\ntrouve = False\nfor etiquette in tramesOk:\n    if etiquette == 'IINST':\n        #print etiquette , \":\", tramesOk[etiquette]\n        vIINST = tramesOk[etiquette]\n    if etiquette ==  'MOTDETAT':\n        #print etiquette , \":\", tramesOk[etiquette]\n        vMOTDETAT = tramesOk[etiquette]\n    if etiquette ==  'OPTARIF':\n        #print etiquette , \":\", tramesOk[etiquette]\n        vOPTARIF = tramesOk[etiquette]\n    if etiquette ==  'ISOUSC':\n        #print etiquette , \":\", tramesOk[etiquette]\n        vISOUSC = tramesOk[etiquette]\n    if etiquette ==  'ADCO':\n        #print etiquette , \":\", tramesOk[etiquette]\n        vADCO = tramesOk[etiquette]\n    if etiquette ==  'PAPP':\n        #print etiquette , \":\", tramesOk[etiquette]\n        vPAPP = tramesOk[etiquette]\n    if etiquette ==  'IMAX':\n        #print etiquette , \":\", tramesOk[etiquette]\n        vIMAX = tramesOk[etiquette]\n    if etiquette ==  'BASE':\n        #print etiquette , \":\", tramesOk[etiquette]\n        vBASE = tramesOk[etiquette]\n    if etiquette ==  'ADPS':\n        #print etiquette , \":\", tramesOk[etiquette]\n        vADPS = tramesOk[etiquette]    \n#=========================================================================\n# Date and Hour\n#=========================================================================\nvHEURE = datetime.datetime.now().strftime('%H:%M')\nvDATE = datetime.datetime.today().strftime('%Y-%m-%d')\n#=========================================================================\n# Connect and insert into DB\n#=========================================================================\ndb = MySQLdb.connect(host=\"192.168.1.250\",port=3307,user=\"root\",passwd=\"MariaQiou\",db=\"edf\" )\ncursor = db.cursor()\nif vBASE > 0:\n    cursor.execute(\"\"\"INSERT INTO teleinfo(DATE, HEURE, IINST, MOTDETAT, OPTARIF, ISOUSC, ADCO, PAPP, IMAX, BASE, ADPS) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\" ,(vDATE, vHEURE, vIINST, vMOTDETAT, vOPTARIF, vISOUSC, vADCO, vPAPP, vIMAX, vBASE, vADPS))\n# Write into DB\ndb.commit()\ndb.rollback()\ndb.close()\n#=========================================================================    \nser.close()\n"}
{"text": "# This file is part of DroidTrail.\n#\n# bl4ckh0l3 <bl4ckh0l3z at gmail.com>\n#\n# DroidTrail is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Lesser General Public License as published by\n# the Free Software Foundation, either version 2 of the License, or\n# (at your option) any later version.\n#\n# DroidTrail is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with DroidTrail. If not, see <http://www.gnu.org/licenses/>.\n#\n# **********************************************************************\n# NOTE: This file is part of Androguard;\n#         Copyright (C) 2012, Anthony Desnos <desnos at t0t0.fr>\n#         All rights reserved.\n#\n#         It is a modified and sanitized version for DroidTrail,\n#         created by bl4ckh0l3 <bl4ckh0l3z at gmail.com>.\n# **********************************************************************\n#\n\n__author__ = 'desnos'\n__license__ = 'GPL v2'\n__maintainer__ = 'bl4ckh0l3'\n__email__ = 'bl4ckh0l3z@gmail.com'\n\nimport logging\nfrom struct import unpack\n\nclass ARSCResTableConfig:\n    def __init__(self, buff):\n        self.start = buff.get_idx()\n        self.size = unpack('<i', buff.read(4))[0]\n        self.imsi = unpack('<i', buff.read(4))[0]\n        self.locale = unpack('<i', buff.read(4))[0]\n        self.screenType = unpack('<i', buff.read(4))[0]\n        self.input = unpack('<i', buff.read(4))[0]\n        self.screenSize = unpack('<i', buff.read(4))[0]\n        self.version = unpack('<i', buff.read(4))[0]\n\n        self.screenConfig = 0\n        self.screenSizeDp = 0\n\n        if self.size >= 32:\n            self.screenConfig = unpack('<i', buff.read(4))[0]\n\n            if self.size >= 36:\n                self.screenSizeDp = unpack('<i', buff.read(4))[0]\n\n        self.exceedingSize = self.size - 36\n        if self.exceedingSize > 0:\n            logging.warning(\"too much bytes !\")\n            self.padding = buff.read(self.exceedingSize)\n\n        #print \"ARSCResTableConfig\", hex(self.start), hex(self.size), hex(self.imsi), hex(self.locale), repr(self.get_language()), repr(self.get_country()), hex(self.screenType), hex(self.input), hex(self.screenSize), hex(self.version), hex(self.screenConfig), hex(self.screenSizeDp)\n\n    def get_language(self):\n        x = self.locale & 0x0000ffff\n        return chr(x & 0x00ff) + chr((x & 0xff00) >> 8)\n\n    def get_country(self):\n        x = (self.locale & 0xffff0000) >> 16\n        return chr(x & 0x00ff) + chr((x & 0xff00) >> 8)"}
{"text": "# vim:fileencoding=utf-8\n# Copyright (c) 2008-2011 gocept gmbh & co. kg\n# See also LICENSE.txt\n# $Id$\n\"\"\"pycountry\"\"\"\n\nimport os.path\n\nimport pycountry.db\n\n\nLOCALES_DIR = os.path.join(os.path.dirname(__file__), 'locales')\nDATABASE_DIR = os.path.join(os.path.dirname(__file__), 'databases')\n\n\nclass Countries(pycountry.db.Database):\n    \"\"\"Provides access to an ISO 3166 database (Countries).\"\"\"\n\n    field_map = dict(alpha_2_code='alpha2',\n                     alpha_3_code='alpha3',\n                     numeric_code='numeric',\n                     name='name',\n                     official_name='official_name',\n                     common_name='common_name')\n    data_class_name = 'Country'\n    xml_tag = 'iso_3166_entry'\n\n\nclass Scripts(pycountry.db.Database):\n    \"\"\"Providess access to an ISO 15924 database (Scripts).\"\"\"\n\n    field_map = dict(alpha_4_code='alpha4',\n                     numeric_code='numeric',\n                     name='name')\n    data_class_name = 'Script'\n    xml_tag = 'iso_15924_entry'\n\n\nclass Currencies(pycountry.db.Database):\n    \"\"\"Providess access to an ISO 4217 database (Currencies).\"\"\"\n\n    field_map = dict(letter_code='letter',\n                     numeric_code='numeric',\n                     currency_name='name')\n    data_class_name = 'Currency'\n    xml_tag = 'iso_4217_entry'\n\n\nclass Languages(pycountry.db.Database):\n    \"\"\"Providess access to an ISO 639-1/2 database (Languages).\"\"\"\n\n    field_map = dict(iso_639_2B_code='bibliographic',\n                     iso_639_2T_code='terminology',\n                     iso_639_1_code='alpha2',\n                     common_name='common_name',\n                     name='name')\n    data_class_name = 'Language'\n    xml_tag = 'iso_639_entry'\n\n\nclass Subdivision(pycountry.db.Data):\n\n    parent_code = None\n\n    def __init__(self, element, **kw):\n        super(Subdivision, self).__init__(element, **kw)\n        self.type = element.parentNode.attributes.get('type').value\n        self.country_code = self.code.split('-')[0]\n        if self.parent_code is not None:\n            self.parent_code = '%s-%s' % (self.country_code, self.parent_code)\n\n    @property\n    def country(self):\n        return countries.get(alpha2=self.country_code)\n\n    @property\n    def parent(self):\n        return subdivisions.get(code=self.parent_code)\n\n\nclass Subdivisions(pycountry.db.Database):\n\n    # Note: subdivisions can be hierarchical to other subdivisions. The\n    # parent_code attribute is related to other subdivisons, *not*\n    # the country!\n\n    xml_tag = 'iso_3166_2_entry'\n    data_class_base = Subdivision\n    data_class_name = 'Subdivision'\n    field_map = dict(code='code',\n                     name='name',\n                     parent='parent_code')\n    no_index = ['name', 'parent_code']\n\n    def __init__(self, *args, **kw):\n        super(Subdivisions, self).__init__(*args, **kw)\n\n        # Add index for the country code.\n        self.indices['country_code'] = {}\n        for subdivision in self:\n            divs = self.indices['country_code'].setdefault(\n                subdivision.country_code, set())\n            divs.add(subdivision)\n\n\ncountries = Countries(os.path.join(DATABASE_DIR, 'iso3166.xml'))\nscripts = Scripts(os.path.join(DATABASE_DIR, 'iso15924.xml'))\ncurrencies = Currencies(os.path.join(DATABASE_DIR, 'iso4217.xml'))\nlanguages = Languages(os.path.join(DATABASE_DIR, 'iso639.xml'))\nsubdivisions = Subdivisions(os.path.join(DATABASE_DIR, 'iso3166_2.xml'))\n"}
{"text": "# Copyright 2021 The ML Collections Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Lint as: python 3\n\"\"\"Config file that raises ValueError on import.\n\nWhen trying loading the configuration file as a flag, the flags library catches\nValueError exceptions then recasts them as a IllegalFlagValueError and rethrows\n(b/63877430). The rethrow does not include the stacktrace from the original\nexception, so we manually add the stracktrace in configflags.parse(). This is\ntested in `ConfigFlagTest.testValueError` in `config_overriding_test.py`.\n\"\"\"\n\n\ndef value_error_function():\n  raise ValueError('This is a ValueError.')\n\n\ndef get_config():\n  return {'item': value_error_function()}\n"}
{"text": "# This code is so you can run the samples without installing the package\nimport sys\nimport os\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))\n#\n\n\nimport cocos\nfrom cocos.director import director\nfrom cocos.actions import *\nfrom cocos.layer import *\nfrom cocos.scenes import *\nfrom cocos.sprite import *\nimport pyglet\nfrom pyglet.gl import *\n\nclass BackgroundLayer( cocos.layer.Layer ):\n    def __init__(self):\n        super( BackgroundLayer, self ).__init__()\n        self.img = pyglet.resource.image('background_image.png')\n\n    def draw( self ):\n        glPushMatrix()\n        self.transform()\n        self.img.blit(0,0)\n        glPopMatrix()\n\n\nif __name__ == \"__main__\":\n    director.init( resizable=True )\n    scene1 = cocos.scene.Scene()\n    scene2 = cocos.scene.Scene()\n\n    colorl = ColorLayer(32,32,255,255)\n    sprite = Sprite( 'grossini.png', (320,240) )\n    colorl.add( sprite )\n\n    scene1.add( BackgroundLayer(), z=0 )\n    scene2.add( colorl, z=0 )\n\n    director.run( FadeTRTransition( scene1, 2, scene2) )\n"}
{"text": "import json\n\n\ndef required(param_list, args):\n    for param in param_list:\n        if type(param) != str:\n            raise Exception(\"param must be a string value\")\n        if param not in args:\n            raise Exception(\"%s is required.\" % (param,))\n\n\ndef semi_required(param_variations, args):\n    atleast = False\n    all = True\n    for param in param_variations:\n        arg = param in args\n        atleast = atleast or arg\n        all = all and arg\n\n    if all:\n        raise Exception(\"All variations cannot be in one request simultaneously\")\n    if not atleast:\n        raise Exception(\"None of variations is in the arguments list\")\n\n\ndef optional(param, args, default=None, possible_values=None):\n    if param not in args:\n        args[param] = default\n\n    try:\n        args[param] = json.loads(args[param], encoding='utf-8')\n    except:\n        args[param] = args[param]\n\n    def check_arg(arg, values):\n        if arg not in values:\n            raise Exception(\"%s not in %s\" % (arg, values))\n\n    if type(args[param]) == list and type(possible_values) == list:\n        for arg in args[param]:\n            check_arg(arg, possible_values)\n\n    if type(args[param]) != list and type(possible_values) == list:\n        check_arg(args[param], possible_values)\n\n\ndef make_boolean(params, arr):\n    for param in params:\n        arr[param] = bool(arr[param])\n\n\ndef check_empty(res, message):\n    if not res or len(res) == 0:\n        raise Exception(message)\n\n\ndef date_to_str(date):\n    return date.strftime(\"%Y-%m-%d %H:%M:%S\")"}
{"text": "#!/usr/bin/python\n#\n# Peteris Krumins (peter@catonmat.net)\n# http://www.catonmat.net  --  good coders code, great reuse\n#\n# http://www.catonmat.net/blog/python-library-for-google-sets/\n#\n# Code is licensed under MIT license.\n#\n\nimport re\nimport urllib.request, urllib.parse, urllib.error\nimport random\nfrom html.entities import name2codepoint\nfrom .BeautifulSoup import BeautifulSoup\n\nfrom .browser import Browser, BrowserError\n\nclass GSError(Exception):\n    \"\"\" Google Sets Error \"\"\"\n    pass\n\nclass GSParseError(Exception):\n    \"\"\"\n    Parse error in Google Sets results.\n    self.msg attribute contains explanation why parsing failed\n    self.tag attribute contains BeautifulSoup object with the most relevant tag that failed to parse\n    Thrown only in debug mode\n    \"\"\"\n     \n    def __init__(self, msg, tag):\n        self.msg = msg\n        self.tag = tag\n\n    def __str__(self):\n        return self.msg\n\n    def html(self):\n        return self.tag.prettify()\n\nLARGE_SET = 1\nSMALL_SET = 2\n\nclass GoogleSets(object):\n    URL_LARGE = \"http://labs.google.com/sets?hl=en&q1=%s&q2=%s&q3=%s&q4=%s&q5=%s&btn=Large+Set\"\n    URL_SMALL = \"http://labs.google.com/sets?hl=en&q1=%s&q2=%s&q3=%s&q4=%s&q5=%s&btn=Small+Set+(15+items+or+fewer)\"\n\n    def __init__(self, items, random_agent=False, debug=False):\n        self.items = items\n        self.debug = debug\n        self.browser = Browser(debug=debug)\n\n        if random_agent:\n            self.browser.set_random_user_agent()\n\n    def get_results(self, set_type=SMALL_SET):\n        page = self._get_results_page(set_type)\n        results = self._extract_results(page)\n        return results\n\n    def _maybe_raise(self, cls, *arg):\n        if self.debug:\n            raise cls(*arg)\n\n    def _get_results_page(self, set_type):\n        if set_type == LARGE_SET:\n            url = GoogleSets.URL_LARGE\n        else:\n            url = GoogleSets.URL_SMALL\n\n        safe_items = [urllib.parse.quote_plus(i) for i in self.items]\n        blank_items = 5 - len(safe_items)\n        if blank_items > 0:\n            safe_items += ['']*blank_items\n\n        safe_url = url % tuple(safe_items)\n\n        try:\n            page = self.browser.get_page(safe_url)\n        except BrowserError as e:\n            raise GSError(\"Failed getting %s: %s\" % (e.url, e.error))\n\n        return BeautifulSoup(page)\n\n    def _extract_results(self, soup):\n        a_links = soup.findAll('a', href=re.compile('/search'))\n        ret_res = [a.string for a in a_links]\n        return ret_res\n\n"}
{"text": "# -*- coding: utf-8 -*-\n\n__author__= \"Luis C. P\u00e9rez Tato (LCPT)\"\n__copyright__= \"Copyright 2016, LCPT\"\n__license__= \"GPL\"\n__version__= \"3.0\"\n__email__= \"l.pereztato@gmail.com\"\n\nfrom rough_calculations import ng_rebar_def\nfrom materials.sia262 import SIA262_materials\nfrom postprocess.reports import common_formats as fmt\nfrom miscUtils import LogMessages as lmsg\n\n\nclass RCSection(object):\n  tensionRebars= None\n  concrete= SIA262_materials.c25_30\n  b= 0.25\n  h= 0.25\n  def __init__(self,tensionRebars,concrete,b,h):\n    self.tensionRebars= tensionRebars\n    self.concrete= concrete\n    self.b= b\n    self.h= h\n    self.stressLimitUnderPermanentLoads= 230e6\n  def setArmature(self,tensionRebars):\n    self.tensionRebars= tensionRebars\n  def getMinReinfAreaUnderFlexion(self):\n    return self.tensionRebars.getMinReinfAreaUnderFlexion(self.concrete,self.h)\n  def getMinReinfAreaUnderTension(self):\n    return self.tensionRebars.getMinReinfAreaUnderTension(self.concrete,self.h)\n  def getMR(self):\n    return self.tensionRebars.getMR(self.concrete,self.b,self.h)\n  def getVR(self,Nd,Md):\n    return self.tensionRebars.getVR(self.concrete,Nd,Md,self.b,self.h)\n  def writeResultFlexion(self,outputFile,Nd,Md,Vd):\n    famArm= self.tensionRebars\n    concrete= self.concrete\n    AsMin= self.getMinReinfAreaUnderFlexion()\n    ancrage= famArm.getBasicAnchorageLength(concrete)\n    outputFile.write(\"  Dimensions coupe; b= \"+ fmt.Longs.format(self.b)+ \"m, h= \"+ fmt.Longs.format(self.h)+ \"m\\\\\\\\\\n\")\n    ng_rebar_def.writeRebars(outputFile,concrete,famArm,AsMin)\n    if(abs(Md)>0):\n      MR= self.getMR()\n      outputFile.write(\"  Verif. en flexion: Md= \"+ fmt.Esf.format(Md/1e3)+ \" kN m, MR= \"+ fmt.Esf.format(MR/1e3)+ \"kN m\")\n      ng_rebar_def.writeF(outputFile,\"  F(M)\", MR/Md)\n    if(abs(Vd)>0):\n      VR= self.getVR(Nd,Md)\n      outputFile.write(\"  V\u00e9rif. eff. tranchant: Vd= \"+ fmt.Esf.format(Vd/1e3)+ \"kN,  VR= \"+ fmt.Esf.format(VR/1e3)+ \"kN\")\n      ng_rebar_def.writeF(outputFile,\"  F(V)\",VR/Vd)\n  def writeResultTraction(self,outputFile,Nd):\n    famArm= self.tensionRebars\n    concrete= self.concrete\n    AsMin= self.getMinReinfAreaUnderTension()/2\n    ancrage= famArm.getBasicAnchorageLength(concrete)\n    ng_rebar_def.writeRebars(outputFile,concrete,famArm,AsMin)\n    if(abs(Nd)>0):\n      lmsg.error(\"ERROR; tension not implemented.\")\n  def writeResultCompression(self,outputFile,Nd,AsTrsv):\n    ''' Results for compressed rebars.\n\n    :param AsTrsv: Rebar area in transverse direction.\n     '''\n    famArm= self.tensionRebars #Even if they're not in tension...\n    concrete= self.concrete\n    AsMin= 0.2*AsTrsv # 20% of the transversal area.\n    ng_rebar_def.writeRebars(outputFile,concrete,famArm,AsMin)\n    if(abs(Nd)!=0.0):\n      lmsg.error(\"ERROR; not implemented.\")\n      \n  def writeResultStress(self,outputFile,M):\n    '''Cheking of stresses under permanent loads (SIA 262 fig. 31)'''\n    concrete= self.concrete\n    if(abs(M)>0):\n      stress= M/(0.9*self.h*self.tensionRebars.getAs())\n      outputFile.write(\"  Verif. contraintes: M= \"+ fmt.Esf.format(M/1e3)+ \" kN m, $\\sigma_s$= \"+ fmt.Esf.format(stress/1e6)+ \" MPa\\\\\\\\\\n\")\n      outputFile.write(\"    $\\sigma_{lim}$= \"+ fmt.Esf.format(self.stressLimitUnderPermanentLoads/1e6)+ \" MPa\")\n      ng_rebar_def.writeF(outputFile,\"  F($\\sigma_s$)\", self.stressLimitUnderPermanentLoads/stress)\n"}
{"text": "\"\"\"\nduniter public and private keys\n\n@author: inso\n\"\"\"\n\nimport libnacl.public\nfrom pylibscrypt import scrypt\nfrom .base58 import Base58Encoder\nfrom .signing_key import _ensure_bytes\n\n\nSEED_LENGTH = 32  # Length of the key\ncrypto_sign_BYTES = 64\nSCRYPT_PARAMS = {'N': 4096,\n                 'r': 16,\n                 'p': 1\n                 }\n\n\nclass SecretKey(libnacl.public.SecretKey):\n    def __init__(self, salt, password):\n        salt = _ensure_bytes(salt)\n        password = _ensure_bytes(password)\n        seed = scrypt(password, salt,\n                    SCRYPT_PARAMS['N'], SCRYPT_PARAMS['r'], SCRYPT_PARAMS['p'],\n                    SEED_LENGTH)\n\n        super().__init__(seed)\n        self.public_key = PublicKey(Base58Encoder.encode(self.pk))\n\n    def encrypt(self, pubkey, noonce, text):\n        text_bytes = _ensure_bytes(text)\n        noonce_bytes = _ensure_bytes(noonce)\n        recipient_pubkey = PublicKey(pubkey)\n        crypt_bytes = libnacl.public.Box(self, recipient_pubkey).encrypt(text_bytes, noonce_bytes)\n        return Base58Encoder.encode(crypt_bytes[24:])\n\n    def decrypt(self, pubkey, noonce, text):\n        sender_pubkey = PublicKey(pubkey)\n        noonce_bytes = _ensure_bytes(noonce)\n        encrypt_bytes = Base58Encoder.decode(text)\n        decrypt_bytes = libnacl.public.Box(self, sender_pubkey).decrypt(encrypt_bytes, noonce_bytes)\n        return decrypt_bytes.decode('utf-8')\n\n\nclass PublicKey(libnacl.public.PublicKey):\n    def __init__(self, pubkey):\n        key = Base58Encoder.decode(pubkey)\n        super().__init__(key)\n\n    def base58(self):\n        return Base58Encoder.encode(self.pk)\n\n"}
{"text": "# coding: utf-8\n\n\"\"\"\nDjango settings for smmaranim project.\n\nGenerated by 'django-admin startproject' using Django 1.10.6.\n\nFor more information on this file, see\nhttps://docs.djangoproject.com/en/1.10/topics/settings/\n\nFor the full list of settings and their values, see\nhttps://docs.djangoproject.com/en/1.10/ref/settings/\n\"\"\"\n\n# Imports\nfrom decouple import config\nfrom decouple import Csv\nimport os\n\n# Build paths inside the project like this: os.path.join(BASE_DIR, ...)\nBASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n# Quick-start development settings - unsuitable for production\n# See https://docs.djangoproject.com/en/1.10/howto/deployment/checklist/\n\n# SECURITY WARNING: keep the secret key used in production secret!\nSECRET_KEY = config('SECRET_KEY')\n\n# SECURITY WARNING: don't run with debug turned on in production!\nDEBUG = config('DEBUG', cast = bool)\n\nALLOWED_HOSTS = config('ALLOWED_HOSTS', cast = Csv())\n\n\n# Application definition\n\nINSTALLED_APPS = [\n    'app',\n    'django.contrib.admin',\n    'django.contrib.auth',\n    'django.contrib.contenttypes',\n    'django.contrib.sessions',\n    'django.contrib.messages',\n    'django.contrib.staticfiles',\n    'jquery'\n]\n\nMIDDLEWARE = [\n    'django.middleware.security.SecurityMiddleware',\n    'django.contrib.sessions.middleware.SessionMiddleware',\n    'django.middleware.common.CommonMiddleware',\n    'django.middleware.csrf.CsrfViewMiddleware',\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\n    'django.contrib.messages.middleware.MessageMiddleware',\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\n]\n\nROOT_URLCONF = 'smmaranim.urls'\n\nTEMPLATES = [\n    {\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\n        'DIRS': [],\n        'APP_DIRS': True,\n        'OPTIONS': {\n            'context_processors': [\n                'app.context_processors.set_alerts',\n                'app.context_processors.set_consts',\n                'app.context_processors.set_incls',\n                'app.context_processors.set_menus',\n                'django.template.context_processors.debug',\n                'django.template.context_processors.request',\n                'django.contrib.auth.context_processors.auth',\n                'django.contrib.messages.context_processors.messages',\n            ],\n        },\n    },\n]\n\nWSGI_APPLICATION = 'smmaranim.wsgi.application'\n\n\n# Database\n# https://docs.djangoproject.com/en/1.10/ref/settings/#databases\n\nDATABASES = {\n    'default': {\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'NAME': config('DB_NAME'),\n        'USER' : config('DB_USER'),\n        'PASSWORD' : config('DB_PASSWORD'),\n        'HOST' : config('DB_HOST'),\n        'PORT' : ''\n    }\n}\n\n\n# Password validation\n# https://docs.djangoproject.com/en/1.10/ref/settings/#auth-password-validators\n\nAUTH_PASSWORD_VALIDATORS = [\n    {\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\n    },\n    {\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\n    },\n]\n\n\n# Internationalization\n# https://docs.djangoproject.com/en/1.10/topics/i18n/\n\nLANGUAGE_CODE = 'fr'\n\nTIME_ZONE = 'Europe/Paris'\n\nUSE_I18N = True\n\nUSE_L10N = True\n\nUSE_TZ = False\n\n\n# Static files (CSS, JavaScript, Images)\n# https://docs.djangoproject.com/en/1.10/howto/static-files/\n\nMEDIA_ROOT = os.path.join(BASE_DIR, 'media')\n\nMEDIA_URL = '/media/'\n\nSTATIC_ROOT = os.path.join(BASE_DIR, 'static')\n\nSTATIC_URL = '/static/'\n\n\n# Param\u00e8tres personnalis\u00e9s\nADMINS = [(\n    '{0} {1}'.format(config('MAIN_ACCOUNT_FIRSTNAME'), config('MAIN_ACCOUNT_LASTNAME')), config('MAIN_ACCOUNT_EMAIL')\n)]\nEMAIL_BACKEND = config('EMAIL_BACKEND')\nEMAIL_FILE_PATH = os.path.join(BASE_DIR, 'mail_dumps')"}
{"text": "from setuptools import find_packages\nfrom setuptools import setup\n\n\nsetup(\n    name='pre_commit',\n    description=(\n        'A framework for managing and maintaining multi-language pre-commit '\n        'hooks.'\n    ),\n    url='https://github.com/pre-commit/pre-commit',\n    version='0.5.5',\n\n    author='Anthony Sottile',\n    author_email='asottile@umich.edu',\n\n    platforms='linux',\n    classifiers=[\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python :: 2',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.3',\n        'Programming Language :: Python :: 3.4',\n        'Programming Language :: Python :: Implementation :: CPython',\n        'Programming Language :: Python :: Implementation :: PyPy',\n    ],\n\n    packages=find_packages('.', exclude=('tests*', 'testing*')),\n    package_data={\n        'pre_commit': [\n            'resources/hook-tmpl',\n            'resources/pre-push-tmpl',\n            'resources/rbenv.tar.gz',\n            'resources/ruby-build.tar.gz',\n            'resources/ruby-download.tar.gz',\n        ]\n    },\n    install_requires=[\n        'argparse',\n        'aspy.yaml',\n        'cached-property',\n        'jsonschema',\n        'nodeenv>=0.11.1',\n        'ordereddict',\n        'pyyaml',\n        'simplejson',\n        'virtualenv-hax',\n    ],\n    entry_points={\n        'console_scripts': [\n            'pre-commit = pre_commit.main:main',\n            'pre-commit-validate-config = pre_commit.clientlib.validate_config:run',  # noqa\n            'pre-commit-validate-manifest = pre_commit.clientlib.validate_manifest:run',  # noqa\n        ],\n    },\n)\n"}
{"text": "#!/usr/bin/env python3\n\n# Minimal sed replacement\n# Copyright (C) 2014 Ingo Ruhnke <grumbel@gmail.com>\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n\nimport argparse\nimport fnmatch\nimport os\nimport re\nimport sys\n\n\ndef minised_on_lines(lines, pattern, replacement, ignore_case, only_replaced_lines=False):\n    result = []\n\n    flags = 0\n    if ignore_case:\n        flags |= re.I\n\n    rx = re.compile(pattern, flags)\n    for line in lines:\n        m = rx.search(line)\n        if m:\n            left = line[:m.span()[0]]\n            right = line[m.span()[1]:]\n            middle = line[m.span()[0]:m.span()[1]]\n\n            if replacement is not None:\n                expanded = m.expand(replacement)\n\n                print(\"- %s%s%s%s%s\" % (left, \"{{{\", middle, \"}}}\", right))\n                print(\"+ %s%s%s%s%s\" % (left, \"{{{\", expanded, \"}}}\", right))\n                print()\n\n                result.append(left + expanded + right)\n            else:\n                print(\"%s%s%s%s%s\" % (left, \"{{{\", middle, \"}}}\", right))\n                result.append(line)\n        else:\n            if not only_replaced_lines:\n                result.append(line)\n\n    return result\n\n\ndef minised_on_file(filename, outfile, pattern, replace, ignore_case, dry_run):\n    if filename:\n        with open(filename, 'rt', encoding='latin-1') as fin:\n            lines = fin.read().splitlines()\n    else:\n        lines = sys.stdin.read().splitlines()\n\n    output = minised_on_lines(lines, pattern, replace, ignore_case)\n\n    if not dry_run:\n        if outfile:\n            with open(outfile, 'wt', newline='\\r\\n', encoding='latin-1', errors=\"replace\") as fout:\n                for line in output:\n                    fout.write(line)\n                    fout.write(\"\\n\")\n        else:\n            for line in output:\n                print(line)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"A minimal sed-like tool\")\n    parser.add_argument('FILE', action='store', nargs='?',\n                        help=\"files to process\")\n    parser.add_argument(\"-i\", \"--in-place\", action='store_true', default=False,\n                        help=\"modify files in place\")\n    parser.add_argument(\"-o\", \"--output\", metavar=\"FILE\", type=str, default=None,\n                        help=\"write output to FILE\")\n    parser.add_argument(\"-n\", \"--dry-run\", action='store_true', default=False,\n                        help=\"only show modifications, without actually modifying anything\")\n    parser.add_argument(\"-p\", \"--pattern\", metavar=\"PAT\", type=str, required=True,\n                        help=\"the search pattern expression\")\n    parser.add_argument(\"-r\", \"--replace\", metavar=\"REPL\", type=str,\n                        help=\"the replacement  expression, if not given just print the match\")\n    parser.add_argument(\"-R\", \"--recursive\", metavar=\"GLOB\", type=str,\n                        help=\"interprets the FILE argument as perform replacement in all files matching GLOB\")\n    parser.add_argument(\"-I\", \"--ignore-case\", action='store_true', default=False,\n                        help=\"ignore case\")\n    parser.add_argument(\"-v\", \"--verbose\", action='store_true', default=False,\n                        help=\"display the replacements are performed\")\n    args = parser.parse_args()\n\n    if args.replace is None:\n        dry_run = True\n    else:\n        dry_run = args.dry_run\n\n    if args.recursive is not None:\n        if args.output:\n            raise Exception(\"can't use --output and recursive together\")\n\n        for path, dirs, files in os.walk(args.FILE):\n            for fname in files:\n                filename = os.path.join(path, fname)\n\n                if args.in_place:\n                    outfile = filename\n                else:\n                    outfile = None\n\n                if fnmatch.fnmatch(fname.lower(), args.recursive.lower()):\n                    print(\"%s:\" % filename)\n                    minised_on_file(filename, outfile, args.pattern, args.replace, args.ignore_case, dry_run)\n\n    else:\n        if args.output:\n            if args.recursive:\n                raise Exception(\"can't use --output and recursive together\")\n            else:\n                outfile = args.output\n        elif args.in_place:\n            outfile = args.FILE\n        else:\n            outfile = None\n\n        minised_on_file(args.FILE, outfile, args.pattern, args.replace, args.ignore_case, dry_run)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n# EOF #\n"}
