# CSCI 662 Project

### Objective
We seek to address a central claim of the paper [Context Compression for Auto-regressive Transformers with Sentinel Tokens](https://aclanthology.org/2023.emnlp-main.794.pdf): "Compared to sparse attention baselines, [KV Compression] is able to effectively compress historical key-value cache with significantly reduced degradation in perplexity". In addressing the this claim we evaluate two specific claims:
* Claim #1: LoRA fine-tuning RedPajama-3B with KV compression will result in  WikiText-2 test set perplexity that is approximately as low as (or lower than) the perplexity obtained after LoRA fine-tuning with local attention or scattered attention, for compression rates in $\{0.1, 0.2, \ldots, 0.9\}$.
* Claim #2: Texts generated by RedPajama-3B, with nucleus sampling and conditioned on prefixes from 200 sampled C4 documents, are more similar to the actual text (as measured by Rouge-L, BERTScore) and more natural (as measured by Pythia-1B perplexity) after LoRA fine-tuning with KV compression compared to generations obtained after LoRA fine-tuning the model with local attention, for compression rates in $\{0.1, 0.2, \ldots, 0.9\}$.

### Setup
To run the code in this repository you will first need to follow these steps to set up your environment:
1. Create a new `conda` environment with `python==3.8`.
2. Activate the new environment.
3. Install the following packages into the environment: `transformers==4.28.0`, `datasets==1.8.0`, `torch==1.13.1`, `accelerate==0.17.0`, `peft==0.2.0`, `tensorboardx`. Then install `huggingface_hub==0.26.2`.
4. Run `chmod +x *.sh`.

### Setup 2: Spacy compatability
1. pip install -U spacy
2. pip install thinc==8.3.0
3. pip install nltk
4. python -m spacy download en_core_web_sm

### Evaluating Claim #1
To fill in the table cell for the "KV Compression" method with compression ratio of 0.1, run `sh run_clm_kv_compression.sh 0.1`. To fill in the table cell for the "Local Attention" method with compression ratio of 0.1, run `sh run_clm_local_attention.sh 0.1`. To fill in the table cell for the "Scattered Attention" method with compression ratio of 0.1, run `sh run_clm_scattered_attention.sh 0.1`. Filling in table cells for different compression ratios is as easy as just replacing 0.1 with a different value.
