# CSCI 662 Project

### Objective
We seek to address a central claim of the paper [Context Compression for Auto-regressive Transformers with Sentinel Tokens](https://aclanthology.org/2023.emnlp-main.794.pdf): "Compared to sparse attention baselines, [KV Compression] is able to effectively compress [the] key-value cache with significantly reduced degradation in perplexity". In addressing the this claim we evaluate two specific claims:
* Claim #1: LoRA fine-tuning RedPajama-3B on the WikiText-2 train set with KV compression results in  WikiText-2 test set perplexity that is approximately as low as (or lower than) the perplexity obtained after LoRA fine-tuning with local attention or scattered attention, for KV cache compression rates in {0.1, 0.2, ..., 0.9}.
* Claim #2: Texts generated by RedPajama-3B, with nucleus sampling and conditioned on prefixes from 200 sampled C4 documents, are more similar to the actual document text (as measured by Rouge-L, BERTScore) and more natural (as measured by Pythia-1B perplexity) after LoRA fine-tuning on the WikiText-2 train set with KV compression compared to generations obtained after LoRA fine-tuning with local attention, for compression rates in {0.1, 0.2, ..., 0.9}.

### Setup
To run the code in this repository you will first need to follow these steps to set up your environment:
1. Create a new `conda` environment with `python==3.8`.
2. Activate the new environment.
3. Install the following packages into the new environment: `transformers==4.27.0`, `datasets==1.8.0`, `torch==1.13.1`, `accelerate==0.17.0`
4. Uninstall `huggingface-hub` and install version `0.26.2`
5. Install the following packages into the new environment: `peft==0.2.0`, `tensorboardx==2.6.2.2`
6. Uninstall `transformers` and install version `4.28.0`

Note there is no need to download data as it is all included in this repository. Data preprocessing is also including automatically in the training and evaluation scripts, and doesn't need to be done separately.

### Evaluating Claim #1
Text here.

### Claim #1 Results
These tables show the perplexities from the original paper and our reproduction.

ðŸ‘‡For local attention
| Compression Ratio | Reproduced PPL | Original PPL |
|-------------------|----------------|--------------|
| 0.0               | 10.33          | 11.2         |
| 0.1               | 10.99          | 11.3         |
| 0.2               | 11.31          | 11.5         |
| 0.3               | 11.85          | 11.8         |
| 0.4               | 12.12          | 12.2         |
| 0.5               | 12.83          | 12.9         |
| 0.6               | 13.62          | 13.6         |
| 0.7               | 15.00          | 14.8         |
| 0.8               | 17.45          | 17.2         |
| 0.9               | 24.07          | 22.9         |

ðŸ‘‡For scattered attention
| Compression Ratio | Reproduced PPL | Original PPL |
|-------------------|----------------|--------------|
| 0.0               | 10.33          | 11.2         |
| 0.1               | 12.80          | 13.1         |
| 0.2               | 16.03          | 16.5         |
| 0.3               | 20.23          | 20.7         |
| 0.4               | 26.52          | 26.6         |
| 0.5               | 35.11          | 34.9         |
| 0.6               | 46.87          | 47.2         |
| 0.7               | 64.38          | 65.7         |
| 0.8               | 90.35          | 91.6         |
| 0.9               | 138.28         | 135.9        |

ðŸ‘‡For KV compression
| Compression Ratio | Reproduced PPL | Original PPL |
|-------------------|----------------|--------------|
| 0.0               | 10.33          | 11.2         |
| 0.1               | 11.55          | 11.4         |
| 0.2               | 12.15          | 11.5         |
| 0.3               | 12.66          | 11.8         |
| 0.4               | 13.08          | 11.9         |
| 0.5               | 13.46          | 12.3         |
| 0.6               | 13.53          | 13.5         |
| 0.7               | 13.68          | 13.7         |
| 0.8               | 13.79          | 13.8         |
| 0.9               | 13.87          | 14.0         |

### Evaluating Claim #2
Text here.

### Claim #2 Results
Table here.
