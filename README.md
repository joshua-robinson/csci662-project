# CSCI 662 Project

### Objective
We seek to address a central claim of the paper [Context Compression for Auto-regressive Transformers with Sentinel Tokens](https://aclanthology.org/2023.emnlp-main.794.pdf): "Compared to sparse attention baselines, [KV Compression] is able to effectively compress [the] key-value cache with significantly reduced degradation in perplexity". In addressing the this claim we evaluate two specific claims:
* Claim #1: LoRA fine-tuning RedPajama-3B on the WikiText-2 train set with KV compression results in  WikiText-2 test set perplexity that is approximately as low as (or lower than) the perplexity obtained after LoRA fine-tuning with local attention or scattered attention, for KV cache compression rates in {0.1, 0.2, ..., 0.9}.
* Claim #2: Texts generated by RedPajama-3B, with nucleus sampling and conditioned on prefixes from 200 sampled C4 documents, are more similar to the actual document text (as measured by Rouge-L, BERTScore) and more natural (as measured by Pythia-1B perplexity) after LoRA fine-tuning on the WikiText-2 train set with KV compression compared to generations obtained after LoRA fine-tuning with local attention, for compression rates in {0.1, 0.2, ..., 0.9}.

### Setup
To run the code in this repository you will first need to follow these steps to set up your environment:
1. Create a new `conda` environment with `python==3.8`.
2. Activate the new environment.
3. Install the following packages into the environment: `transformers==4.28.0`, `datasets==1.8.0`, `torch==1.13.1`, `accelerate==0.17.0`, `peft==0.2.0`, `tensorboardx`. Then install `huggingface_hub==0.26.2`.

### Evaluating Claim #1
Text here.

### Claim #1 Results
Table here.

### Evaluating Claim #2
Text here.

### Claim #2 Results
Table here.
